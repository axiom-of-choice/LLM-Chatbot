{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/FundamentlPartners/abinvenv-sol/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceEndpoint\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# from transformers import BloomTokenizerFast, BloomForQuestionAnswering, BloomForCausalLM, TrainingArguments, Trainer\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#utils\n",
    "# from src.utils import connect_index\n",
    "\n",
    "# Memory\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "import sys \n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../')))\n",
    "\n",
    "from config import *\n",
    "from src.utils import connect_index\n",
    "from langchain.vectorstores import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon = HuggingFaceEndpoint(endpoint_url=\"https://xq3zregp35kcrpzg.us-east-1.aws.endpoints.huggingface.cloud\",task=\"text-generation\", huggingfacehub_api_token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = CohereEmbeddings(model=COHERE_MODEL_NAME, cohere_api_key=COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = connect_index(PINECONE_INDEX_NAME)\n",
    "vectorstore = Pinecone(index, embeddings.embed_query, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='In order to help correct for the inaccuracies and errors that are present in the two types of data and take advantage of their individual strengths, research institutions have produced rainfall datasets that combine station and satellite/radar observations. Datasets that combine these observations are the recommended source of data for humanitarian purposes.\\n\\nObservational precipitation data can be presented in various formats, each providing unique insights into precipitation patterns. The most common representation is through absolute values, which reflect the actual amount of precipitation observed at a specific location and time, typically measured in millimeters. Precipitation averages, which are calculated from historical records over a period of typically at least 10 years, can be produced for a specific location, season, or year. A precipitation anomaly represents the deviation from the expected or average precipitation over a particular period, which can be expressed as a difference in millimeters or as a percentage. The purpose of the analysis determines the most useful representation. Absolute values provide insight into the total precipitation received at a location, while anomalies and averages help to understand long-term trends, including dry or wet periods, which is valuable for assessing drought or excessive rainfall conditions severity.\\n\\nCENTRE FOR HUMANITARIAN DATA\\n\\n3\\n\\nCommon sources of observational rainfall data\\n\\nAmong the many sources of data available, we introduce here two common sources of observational rainfall data, both of which combine station and satellite data: CHIRPS and ARC2. CHIRPS refers to the Climate Hazards Group InfraRed Precipitation with Station data and is produced by scientists from the US Geological Survey and the Climate Hazards Center at the University of Santa Barbara. ARC2 refers to Africa Rainfall Climatology Version 2 and it is produced by The National Oceanic and Atmospheric Administration’s Climate Prediction Center for the Famine Early Warning System. You can access the data directly from these providers, or use the tabular CHIRPS datasets that provides pre-computed rainfall metrics at admin2 and is available on HDX.1', metadata={'chunk': 2.0, 'data_source': 'Local', 'id': 'cff59eebaaf740dca3bd4f7e825cc89c', 'page': 'None', 'source': 'guidance_observationalrainfalldata.pdf'}),\n",
       " Document(page_content='In order to help correct for the inaccuracies and errors that are present in the two types of data and take advantage of their individual strengths, research institutions have produced rainfall datasets that combine station and satellite/radar observations. Datasets that combine these observations are the recommended source of data for humanitarian purposes.\\n\\nObservational precipitation data can be presented in various formats, each providing unique insights into precipitation patterns. The most common representation is through absolute values, which reflect the actual amount of precipitation observed at a specific location and time, typically measured in millimeters. Precipitation averages, which are calculated from historical records over a period of typically at least 10 years, can be produced for a specific location, season, or year. A precipitation anomaly represents the deviation from the expected or average precipitation over a particular period, which can be expressed as a difference in millimeters or as a percentage. The purpose of the analysis determines the most useful representation. Absolute values provide insight into the total precipitation received at a location, while anomalies and averages help to understand long-term trends, including dry or wet periods, which is valuable for assessing drought or excessive rainfall conditions severity.\\n\\nCENTRE FOR HUMANITARIAN DATA\\n\\n3\\n\\nCommon sources of observational rainfall data\\n\\nAmong the many sources of data available, we introduce here two common sources of observational rainfall data, both of which combine station and satellite data: CHIRPS and ARC2. CHIRPS refers to the Climate Hazards Group InfraRed Precipitation with Station data and is produced by scientists from the US Geological Survey and the Climate Hazards Center at the University of Santa Barbara. ARC2 refers to Africa Rainfall Climatology Version 2 and it is produced by The National Oceanic and Atmospheric Administration’s Climate Prediction Center for the Famine Early Warning System. You can access the data directly from these providers, or use the tabular CHIRPS datasets that provides pre-computed rainfall metrics at admin2 and is available on HDX.1', metadata={'chunk': 2.0, 'data_source': 'Local', 'id': 'cb6ace0becdb4476b782b02dbe02712f', 'page': 'None', 'source': 'guidance_observationalrainfalldata.pdf'}),\n",
       " Document(page_content='Observational rainfall data is produced by measuring the amount of rainfall at precise locations, which are called measuring stations or gauges (e.g., the stars in Figure 1 below). Station data is typically obtained from national or regional meteorological services and should only be used in accordance with their guidance. Some of the advantages of station data are that it is accurate at the specific locations, it is collected frequently and it has been collected over the long-term, allowing for trend analysis. On the other hand, station data is only available for specific locations and is subject to bias due to wind, evaporation, and changes in measurement devices. Contact the local or regional meteorological services to learn whether station data is available, where the stations are located, how often the data is updated, and whether historical records are available.\\n\\nCENTRE FOR HUMANITARIAN DATA\\n\\n2\\n\\nRainfall can also be estimated using satellite or radar images that cover areas several square kilometers wide (e.g., the squares in Figure 1 below). Satellite imagery is typically obtained by global providers and academic or research centers (e.g., the Climate Hazards Center) who share their derived estimates publicly. Satellite or radar images are useful for understanding patterns over larger geographic areas. Some of the advantages of satellite data are that it has extensive geographic coverage, including in remote and inaccessible areas, and that it is shared in a continuous and consistent way. On the other hand, satellite measurements are subject to errors due to cloud contamination, and the data has a shorter historical record.\\n\\nFigure 1. Weather Stations in Zambia (red stars). The grid superimposed onto the map consists of individual boxes, each of which corresponds to a particular location where satellite weather measurements are conducted by certain providers. In this case, the grid is the one used by the ARC2 dataset (see below). The size of each box is 0.1 degree (about 11 km at the equator).', metadata={'chunk': 1.0, 'data_source': 'Local', 'id': 'cb6ace0becdb4476b782b02dbe02712f', 'page': 'None', 'source': 'guidance_observationalrainfalldata.pdf'}),\n",
       " Document(page_content='Observational rainfall data is produced by measuring the amount of rainfall at precise locations, which are called measuring stations or gauges (e.g., the stars in Figure 1 below). Station data is typically obtained from national or regional meteorological services and should only be used in accordance with their guidance. Some of the advantages of station data are that it is accurate at the specific locations, it is collected frequently and it has been collected over the long-term, allowing for trend analysis. On the other hand, station data is only available for specific locations and is subject to bias due to wind, evaporation, and changes in measurement devices. Contact the local or regional meteorological services to learn whether station data is available, where the stations are located, how often the data is updated, and whether historical records are available.\\n\\nCENTRE FOR HUMANITARIAN DATA\\n\\n2\\n\\nRainfall can also be estimated using satellite or radar images that cover areas several square kilometers wide (e.g., the squares in Figure 1 below). Satellite imagery is typically obtained by global providers and academic or research centers (e.g., the Climate Hazards Center) who share their derived estimates publicly. Satellite or radar images are useful for understanding patterns over larger geographic areas. Some of the advantages of satellite data are that it has extensive geographic coverage, including in remote and inaccessible areas, and that it is shared in a continuous and consistent way. On the other hand, satellite measurements are subject to errors due to cloud contamination, and the data has a shorter historical record.\\n\\nFigure 1. Weather Stations in Zambia (red stars). The grid superimposed onto the map consists of individual boxes, each of which corresponds to a particular location where satellite weather measurements are conducted by certain providers. In this case, the grid is the one used by the ARC2 dataset (see below). The size of each box is 0.1 degree (about 11 km at the equator).', metadata={'chunk': 1.0, 'data_source': 'Local', 'id': 'cff59eebaaf740dca3bd4f7e825cc89c', 'page': 'None', 'source': 'guidance_observationalrainfalldata.pdf'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"What is rainfall Data?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQAWithSourcesChain.from_chain_type(llm=falcon, retriever=vectorstore.as_retriever(), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is blockchain?\"\n",
    "# Send question as a query to qa chain\n",
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is blockchain?',\n",
       " 'answer': '\\nThe following is a summary of the results of the experiments. The first experiment was performed in the',\n",
       " 'sources': ''}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following sections from various documents and a question, \n",
    "generate a final answer with references (\"SOURCES\"). If the answer is unknown, \n",
    "indicate as such without attempting to fabricate a response. Ensure to always \n",
    "include a \"SOURCES\" section in your answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "FINAL ANSWER:\"\"\"\n",
    "\n",
    "my_prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"summaries\", \"question\"],\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "                                llm=falcon, \n",
    "                                chain_type=\"map_reduce\", \n",
    "                                retriever=vectorstore.as_retriever(),\n",
    "                                reduce_k_below_max_tokens=False,\n",
    "                                return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1642 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "query = \"What is rainfall data?\"\n",
    "# Send question as a query to qa chain\n",
    "result = qa_chain({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is rainfall data?',\n",
       " 'answer': ' The president did not mention Michael Jackson.\\n',\n",
       " 'sources': '\\nQUESTION: What is the name',\n",
       " 'source_documents': [Document(page_content=\"EXPLORING THE ROLE OF PUNCTUATION IN PARSING NATURAL TEXT\\n\\nFew, if any, current NLP systems make any significant use of punctuation. Intuitively, a treatment of punctuation seems necessary to the analysis and production of text. Whilst this has been suggested in the fields of discourse structure, it is still unclear whether punctuation can help in the syntactic field. This investigation attempts to answer this question by parsing some corpus-based material with two similar grammars -- one including rules for punctuation, the other ignoring it. The punctuated grammar significantly out-performs the unpunctuated one, and so the conclusion is that punctuation can play a useful role in syntactic processing.\\n\\nINTRODUCTION\\n\\nThere are no current text based natural language analysis or generation systems that make full use of punctuation, and while there are some that make limited use, like the Editor's Assistant [Dale 1990], they tend to be the exception rather than the rule. Instead, punctuation is usually stripped out of the text before processing, and is not included in generated text.\\n\\nIntuitively, this seems very wrong. Punctuation is such an integral part of written language that it is difficult to imagine naturally producing any significant body of unpunctuated text, or being able to easily understand any such body of text.\\n\\nHowever, this is what has been done in the computational linguistics field. The reason that it has always been too difficult to incorporate any coherent account of punctuation into any system is because no such coherent account exists.\\n\\nPunctuation has long been considered to be intimately related to intonation: that is that different punctuation marks simply give the reader cues as to the possible prosodic and pausal characteristics of the text [Markwardt, 1942]. This claim is questioned by Nunberg [1990], since such a transcriptional view of punctuation is theoretically uninteresting, and also correlates rather badly with intonation in any case.\\n\\nHowever, even if we recognise that punctuation fulfils a linguistic role of its own, it is by no means clear how this role is defined. Since there is still no concise linguistic account of the function of punctuation, we have to rely mainly on personal intuitions. This in turn introduces new problems, since there is a great deal of idiosyncrasy associated with the use of punctuation marks. Whilst most people may agree on core situations in which use of a given punctuation mark is desirable, or even necessary, there are still many situations where their use is less clear.\\n\\nIn his recent review, Humphreys [1993] suggests that accounts of punctuation fall into three categories: ``The first ...is selflessly dedicated to the task of bringing Punctuation to the Peasantry ...The second sort is the Style Guide, written by editors and printers for the private pleasure of fellow professionals ...The third, on the linguistics of the punctuation system, is much the rarest of all.''\", metadata={'chunk': 0.0, 'data_source': 'Local', 'id': 'f924f7ceeaca4f7c9e0ebd64b03c39d5', 'page': 'None', 'source': '9505024.xml'}),\n",
       "  Document(page_content='Collecting Word Collocations\\n\\nIn Japanese, kanzi character sequences longer than three are usually compound nouns, This tendency is confirmed by comparing the occurrence frequencies of kanzi character words in texts and those headwords in dictionaries. We investigated the tendency by using sample texts from newspaper articles and encyclopedias, and Bunrui Goi Hyou (BGH for short), which is a standard Japanese thesaurus. The sample texts include about 220,000 sentences. We found that three character words and longer represent 4% in the thesaurus, but 71% in the sample texts. Therefore a collection of four kanzi character words would be used as a corpus of compound nouns.\\n\\nFour kanzi character sequences are useful to extract binary relations of nouns, because dividing a four kanzi character sequence in the middle often gives correct segmentation. Our preliminary investigation shows that the accuracy of the above heuristics is 96 % (961/1000).\\n\\nAssigning Thesaurus Categories\\n\\nFortunately, there are few words that are assigned multiple categories in BGH. Therefore, we use method (1). Word collocations containing words with multiple categories represent about 1/3 of the corpus. If we used other thesauruses, which assign multiple categories to more words, we would need to use method (2), (3), or (4).\\n\\nCounting Occurrence of Category Collocations\\n\\nAfter assigning the thesaurus categories to words, we count occurrence frequencies of category collocations as follows: 1. collect word collocations, at this time we collect only patterns of word collocations, but we do not care about occurrence frequencies of the patterns 2. replace thesaurus categories with words to produce category collocation patterns 3. count the number of category collocation patterns Note: we do not care about frequencies of word collocations prior to replacing words with thesaurus categories.\\n\\nAlgorithm\\n\\nThe analysis consists of three steps: 1. enumerate possible segmentations of an input compound noun by consulting headwords of the thesaurus (BGH) 2. assign thesaurus categories to all words 3. calculate the preferences of every structure of the compound noun according to the frequencies of category collocations\\n\\nWe assume that a structure of a compound noun can be expressed by a binary tree. We also assume that the category of the right branch of a (sub)tree represents the category of the (sub)tree itself. This assumption exsists because Japanese is a head-final language, a modifier is on the left of its modifiee. With these assumptions, a preference value of a structure is calculated by recursive function p as follows:', metadata={'chunk': 1.0, 'data_source': 'Local', 'id': '9484e97a022b494b80c6f805b04e344d', 'page': 'None', 'source': '9412008.xml'}),\n",
       "  Document(page_content=\"Artificial Intelligence, v.13, pp. 231-278. Pericliev, V. (1986). Non-projective con-structions in Bulgarian. 2nd World Congress of Bulgaristics, Sofia, pp. 271-280 (in Bulgarian). Pericliev, V. and I. Ilarionov (1986). Testing the projectivity hypothesis. COLING'86, Bonn, pp. 56- 58. Pericliev, V. (1992a). A referent grammar treatment of some problems in the Bulgarian nominal phrase. Studia Linguistica, Stockholm, pp. 49-62. Pericliev, V. (1992b). The ID/LP format:  counter-evidence from Bulgarian, (ms). Pericliev, V. and A. Grigorov (1992). Extending Definite Clause Grammar to handle flexible word order. B. du Boulay et al. (eds.) Artificial Intelligence V, North Holland, pp. 161-170. Pollard C., I. Sag (1987). Information-Based Syntax and Semantics. Vol. 1:  Fundamentals. CSLI Lecture Notes No. 13, Stanford, CA. Sag, I. (1987). Grammatical hierarchy and linear precedence. Syntax and Semantics, v.20, pp. 303- 339. Saint-Dizier, P. (1988). Contextual Discon-tinuous Grammars. Natural Language Understanding and Logic Programming, II, North Holland, pp. 29-43. Steele, S. (1981). Word order variation:  a typological study. G. Greenberg (ed.) Universals of Language, v.4, Stanford. Uszkoreit, H. (1985). Linear precedence in discontinuous constituents: complex fronting in German. SRI International, Technical Note 371. Zwicky, A. (1986). Immediate precedence in GPSG. OSU WPL 32, pp. 133-138.\\n\\nFootnotes\\n\\nThis often results in discontinuities (or non-projectivities). For an automated way of discovering and a description of such constructs in Bulgarian, cf. Pericliev and Ilarionov 1986, and Pericliev 1986. For the difficulties in handling the adjectival clitics in pure DCG, cf. Pericliev 1992a.\", metadata={'chunk': 8.0, 'data_source': 'Local', 'id': 'f3a0167c20454148a444186de9341960', 'page': 'None', 'source': '9505007.xml'}),\n",
       "  Document(page_content=\"Since the probability of the most probable parse decreases exponentially in sentence length, the number of random samples needed to find this most probable parse increases exponentially in sentence length. Thus, when using the Monte Carlo algorithm, one is left with the uncomfortable choice of exponentially decreasing the probability of finding the most probable parse, or exponentially increasing the runtime.\\n\\nWe admit that this is a somewhat informal argument. Still, the Monte Carlo algorithm has never been tested on sentences longer than those in the ATIS corpus; there is good reason to believe the algorithm will not work as well on longer sentences. Note that our algorithm has true runtime O(Tn[3]), as shown previously.\\n\\nAnalysis of Bod's Data\\n\\nIn the DOP model, a sentence cannot be given an exactly correct parse unless all productions in the correct parse occur in the training set. Thus, we can get an upper bound on performance by examining the test corpus and finding which parse trees could not be generated using only productions in the training corpus. Unfortunately, while Bod provided us with his data, he did not specify which sentences were test and which were training. We can however find an upper bound on average case performance, as well as an upper bound on the probability that any particular level of performance could be achieved.\\n\\nThe table is arranged from least generous to most generous: in the upper left hand corner is a technique Bod might reasonably have used; in that case, the probability of getting the test set he described is less than one in a million. In the lower right corner we give Bod the absolute maximum benefit of the doubt: we assume he used a parser capable of parsing unary branching productions, that he used a very overgenerating grammar, and that he used a loose definition of ``Exact Match.'' Even in this case, there is only about a 1.5% chance of getting the test set Bod describes.\\n\\nConclusion\\n\\nWe have given efficient techniques for parsing the DOP model. These results are significant since the DOP model has perhaps the best reported parsing accuracy; previously the full DOP model had not been replicated due to the difficulty and computational complexity of the existing algorithms. We have also shown that previous results were partially due to an unlikely choice of test data, and partially due to the heavy cleaning of the data, which reduced the difficulty of the task.\\n\\nOf course, this research raises as many questions as it answers. Were previous results due only to the choice of test data, or are the differences in implementation partly responsible? In that case, there is significant future work required to understand which differences account for Bod's exceptional performance. This will be complicated by the fact that sufficient details of Bod's implementation are not available.\\n\\nThis research also shows the importance of testing on more than one small test set, as well as the importance of not making cross-corpus comparisons; if a new corpus is required, then previous algorithms should be duplicated for comparison.\\n\\nBibliography\", metadata={'chunk': 3.0, 'data_source': 'Local', 'id': '7943eadad78545d3bd4d3f6c3d10183b', 'page': 'None', 'source': '9604008.xml'})]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/isaac/FundamentlPartners/abinvenv-sol\n",
      "INFO 2023-08-13 14:51:45,088 [botocore.credentials:credentials.py:load:1254]\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "\n",
      "INFO 2023-08-13 14:51:46,448 [sentence_transformers.SentenceTransformer:SentenceTransformer.py:__init__:66]\n",
      "Load pretrained SentenceTransformer: sentence-transformers/stsb-xlm-r-multilingual\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/FundamentlPartners/abinvenv-sol/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2023-08-13 14:51:49,057 [sentence_transformers.SentenceTransformer:SentenceTransformer.py:__init__:105]\n",
      "Use pytorch device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Memory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "import sys \n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../')))\n",
    "\n",
    "from config import *\n",
    "from src.utils import connect_index\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "from src.models.HuggingFaceEmbeddings import InferenceEndpointHuggingFaceEmbeddings\n",
    "embedding_self_hosted = InferenceEndpointHuggingFaceEmbeddings(HUGGING_FACE_EMBEDDINGS_ENDPOINT, HUGGING_FACE_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 2023-08-13 14:51:53,002 [root:__init__.py:_maybe_print_use_warning:468]\n",
      "\n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/isaac/FundamentlPartners/abinvenv-sol/venv/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 14:51:53.002 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/isaac/FundamentlPartners/abinvenv-sol/venv/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2023-08-13 14:51:53,236 [src.utils:utils.py:connect_index:13]\n",
      "Connected to Pinecone index stab-test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 14:51:53.236 Connected to Pinecone index stab-test\n"
     ]
    }
   ],
   "source": [
    "index = connect_index(PINECONE_INDEX_NAME)\n",
    "vectorstore = Pinecone(index, embedding_self_hosted.embed_query, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2023-08-13 14:51:55,282 [src.models.HuggingFaceEmbeddings:HuggingFaceEmbeddings.py:embed_query:36]\n",
      "Querying Hugging Face API for What is rainfall Data? inputs:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 14:51:55.282 Querying Hugging Face API for What is rainfall Data? inputs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='4\\n\\nWas the rainfall last month above or below normal? And by how much? Observational rainfall data goes back decades and can therefore be used to determine whether rainfall for a month or a season is above, at or below the historical average.\\n\\nWere there more rainy days this month than last month? Observational rainfall data is\\n\\navailable for each day and can be used to determine if it rained or not on a given day and to count the number of dry or wet days during a specific period of time.\\n\\nHow many dry spells were there during the past rainy season? How long was the\\n\\nlongest dry spell and the heaviest rain streak this past rainy season? Observational rainfall data can be used to detect the occurrence, location and duration of dry spells and prolonged periods of heavy rain.\\n\\nHow often in the past decade were more than x millimeters of rainfall measured?\\n\\nObservational rainfall data provides daily rainfall totals in millimeters and can be used to assess how often an area received rain above or below a given threshold.\\n\\nWhich region received the most rain last month? Observational rainfall data can be\\n\\naggregated to provide rainfall totals for larger geographic areas and then used to compare rain for that area.\\n\\nObservational rainfall data cannot answer the following questions:\\n\\nHow much will it rain during a period of time in the future? Observational rainfall data is referred to as observational because it describes what happened in the past. It does not tell you anything about future rainfall.\\n\\nIs a severe flooding event about to occur? Are we going through a severe drought?\\n\\nObservational rainfall data does not provide sufficient information to inform you about the risk of flooding in the near future or how severe a drought is likely to be. This is because floods and droughts are complex phenomena caused by several interacting factors and they cannot be explained exclusively by the amount of precipitation.\\n\\nHow much rainfall from a given period of time in the past was due to specific tropical', metadata={'chunk': 4.0, 'data_source': 'Local', 'id': '2993c5d0da884942afcafa93fbe39afe', 'page': 'None', 'source': 'guidance_observationalrainfalldata.pdf'}),\n",
       " Document(page_content='records over a period of typically at least 10 years, can be produced for a specific location, \\nseason, or year. A precipitation anomaly represents the deviation from the expected or average \\nprecipitation over a particular period, which can be expressed as a difference in millimeters \\nor as a percentage. The purpose of the analysis determines the most useful representation. \\nAbsolute values provide insight into the total precipitation received at a location, while \\nanomalies and averages help to understand long-term trends, including dry or wet periods, \\nwhich is valuable for assessing drought or excessive rainfall conditions severity.', metadata={'chunk': 1.0, 'data_source': 'Local', 'id': '11c30aedcdf34f17aefd2fcd30723737', 'page': '2', 'source': 'guidance_observationalrainfalldata.pdf'}),\n",
       " Document(page_content='3. Observational rainfall data is usually provided as a raster dataset, where the geographic area is represented by a contiguous grid of cells, or pixels. Each cell represents a discrete area of land, and has a value associated with it but the cells do not neatly fit within administrative boundaries. Calculating rainfall for an administrative area requires an aggregation. The choice of the aggregation method (average, maximum, total etc.) as well as the choice of which cells to include within a given administrative division (all cells fully included, or all cells touched etc.) will impact the results. This is especially true for lower administrative divisions. As a result, adding together the total rainfall for all the districts in a region may not equal the total rainfall calculated for the region.\\n\\n4. If using station data, it is important to bear in mind that stations are easily subject to\\n\\ndeterioration and require frequent maintenance, which may affect their accuracy. For this reason, calculating temporal averages, anomalies or trends over long periods of time should be performed with care. When possible, verify with the provider the adequacy of the measurements over long periods of time. This limitation does not apply to CHIRPS and ARC2 datasets, which have been calibrated and combined with satellite observations by climate institutions.\\n\\n5. For station data, differences exist between measuring instruments and quality of their\\n\\nmeasurements. This may heavily affect comparisons between different locations. Therefore, when performing geographical averages, it is preferable to use radar data, satellite data or observational datasets already calibrated by institutions or universities such as CHIRPS or ARC2.\\n\\n4 Learn more: https://www.chc.ucsb.edu/data/chirps\\n\\n5 Learn more here: https://wiki.chc.ucsb.edu/CHIRPS_FAQ#What_is_the_spatial_density_of_station_data_used_in_CHIRPS.3F\\n\\nCENTRE FOR HUMANITARIAN DATA\\n\\n6', metadata={'chunk': 6.0, 'data_source': 'Local', 'id': '2993c5d0da884942afcafa93fbe39afe', 'page': 'None', 'source': 'guidance_observationalrainfalldata.pdf'}),\n",
       " Document(page_content='risk of flooding in the near future or how severe a drought is likely to be. This is because \\nfloods and droughts are complex phenomena caused by several interacting factors and they \\ncannot be explained exclusively by the amount of precipitation.\\n• How much rainfall from a given period of time in the past was due to specific tropical \\nstorm or typhoon?  It is hard to differentiate between the amount of rain caused by \\naverage atmospheric conditions and the amount of rain induced by a specific atmospheric \\nphenomenon. As a result, using observational rainfall data alone, it is not possible to say how \\nmuch rainfall is due strictly to a tropical storm or typhoon. More data is needed to infer the \\norigin of a heavy rainfall event.', metadata={'chunk': 1.0, 'data_source': 'Local', 'id': 'ae9e6421bc1343a599c7d0d757a9bd6e', 'page': '4', 'source': 'guidance_observationalrainfalldata.pdf'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"What is rainfall Data?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
