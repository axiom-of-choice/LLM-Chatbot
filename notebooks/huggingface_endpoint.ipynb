{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/FundamentlPartners/abinvenv-sol/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceEndpoint\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# from transformers import BloomTokenizerFast, BloomForQuestionAnswering, BloomForCausalLM, TrainingArguments, Trainer\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#utils\n",
    "# from src.utils import connect_index\n",
    "\n",
    "# Memory\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "import sys \n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../')))\n",
    "\n",
    "from config import *\n",
    "from src.utils import connect_index\n",
    "from langchain.vectorstores import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon = HuggingFaceEndpoint(endpoint_url=\"https://xq3zregp35kcrpzg.us-east-1.aws.endpoints.huggingface.cloud\",task=\"text-generation\", huggingfacehub_api_token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = CohereEmbeddings(model=COHERE_MODEL_NAME, cohere_api_key=COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 15:21:48.791 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/isaac/FundamentlPartners/abinvenv-sol/venv/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2023-08-04 15:21:49.024 Connected to Pinecone index stab-test\n"
     ]
    }
   ],
   "source": [
    "index = connect_index(PINECONE_INDEX_NAME)\n",
    "vectorstore = Pinecone(index, embeddings.embed_query, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQAWithSourcesChain.from_chain_type(llm=falcon, retriever=vectorstore.as_retriever(), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is blockchain?\"\n",
    "# Send question as a query to qa chain\n",
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is blockchain?',\n",
       " 'answer': '\\nThe following is a summary of the results of the experiments. The first experiment was performed in the',\n",
       " 'sources': ''}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following sections from various documents and a question, \n",
    "generate a final answer with references (\"SOURCES\"). If the answer is unknown, \n",
    "indicate as such without attempting to fabricate a response. Ensure to always \n",
    "include a \"SOURCES\" section in your answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "FINAL ANSWER:\"\"\"\n",
    "\n",
    "my_prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"summaries\", \"question\"],\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "                                llm=falcon, \n",
    "                                chain_type=\"map_reduce\", \n",
    "                                retriever=vectorstore.as_retriever(),\n",
    "                                reduce_k_below_max_tokens=False,\n",
    "                                return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1642 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "query = \"What is rainfall data?\"\n",
    "# Send question as a query to qa chain\n",
    "result = qa_chain({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is rainfall data?',\n",
       " 'answer': ' The president did not mention Michael Jackson.\\n',\n",
       " 'sources': '\\nQUESTION: What is the name',\n",
       " 'source_documents': [Document(page_content=\"EXPLORING THE ROLE OF PUNCTUATION IN PARSING NATURAL TEXT\\n\\nFew, if any, current NLP systems make any significant use of punctuation. Intuitively, a treatment of punctuation seems necessary to the analysis and production of text. Whilst this has been suggested in the fields of discourse structure, it is still unclear whether punctuation can help in the syntactic field. This investigation attempts to answer this question by parsing some corpus-based material with two similar grammars -- one including rules for punctuation, the other ignoring it. The punctuated grammar significantly out-performs the unpunctuated one, and so the conclusion is that punctuation can play a useful role in syntactic processing.\\n\\nINTRODUCTION\\n\\nThere are no current text based natural language analysis or generation systems that make full use of punctuation, and while there are some that make limited use, like the Editor's Assistant [Dale 1990], they tend to be the exception rather than the rule. Instead, punctuation is usually stripped out of the text before processing, and is not included in generated text.\\n\\nIntuitively, this seems very wrong. Punctuation is such an integral part of written language that it is difficult to imagine naturally producing any significant body of unpunctuated text, or being able to easily understand any such body of text.\\n\\nHowever, this is what has been done in the computational linguistics field. The reason that it has always been too difficult to incorporate any coherent account of punctuation into any system is because no such coherent account exists.\\n\\nPunctuation has long been considered to be intimately related to intonation: that is that different punctuation marks simply give the reader cues as to the possible prosodic and pausal characteristics of the text [Markwardt, 1942]. This claim is questioned by Nunberg [1990], since such a transcriptional view of punctuation is theoretically uninteresting, and also correlates rather badly with intonation in any case.\\n\\nHowever, even if we recognise that punctuation fulfils a linguistic role of its own, it is by no means clear how this role is defined. Since there is still no concise linguistic account of the function of punctuation, we have to rely mainly on personal intuitions. This in turn introduces new problems, since there is a great deal of idiosyncrasy associated with the use of punctuation marks. Whilst most people may agree on core situations in which use of a given punctuation mark is desirable, or even necessary, there are still many situations where their use is less clear.\\n\\nIn his recent review, Humphreys [1993] suggests that accounts of punctuation fall into three categories: ``The first ...is selflessly dedicated to the task of bringing Punctuation to the Peasantry ...The second sort is the Style Guide, written by editors and printers for the private pleasure of fellow professionals ...The third, on the linguistics of the punctuation system, is much the rarest of all.''\", metadata={'chunk': 0.0, 'data_source': 'Local', 'id': 'f924f7ceeaca4f7c9e0ebd64b03c39d5', 'page': 'None', 'source': '9505024.xml'}),\n",
       "  Document(page_content='Collecting Word Collocations\\n\\nIn Japanese, kanzi character sequences longer than three are usually compound nouns, This tendency is confirmed by comparing the occurrence frequencies of kanzi character words in texts and those headwords in dictionaries. We investigated the tendency by using sample texts from newspaper articles and encyclopedias, and Bunrui Goi Hyou (BGH for short), which is a standard Japanese thesaurus. The sample texts include about 220,000 sentences. We found that three character words and longer represent 4% in the thesaurus, but 71% in the sample texts. Therefore a collection of four kanzi character words would be used as a corpus of compound nouns.\\n\\nFour kanzi character sequences are useful to extract binary relations of nouns, because dividing a four kanzi character sequence in the middle often gives correct segmentation. Our preliminary investigation shows that the accuracy of the above heuristics is 96 % (961/1000).\\n\\nAssigning Thesaurus Categories\\n\\nFortunately, there are few words that are assigned multiple categories in BGH. Therefore, we use method (1). Word collocations containing words with multiple categories represent about 1/3 of the corpus. If we used other thesauruses, which assign multiple categories to more words, we would need to use method (2), (3), or (4).\\n\\nCounting Occurrence of Category Collocations\\n\\nAfter assigning the thesaurus categories to words, we count occurrence frequencies of category collocations as follows: 1. collect word collocations, at this time we collect only patterns of word collocations, but we do not care about occurrence frequencies of the patterns 2. replace thesaurus categories with words to produce category collocation patterns 3. count the number of category collocation patterns Note: we do not care about frequencies of word collocations prior to replacing words with thesaurus categories.\\n\\nAlgorithm\\n\\nThe analysis consists of three steps: 1. enumerate possible segmentations of an input compound noun by consulting headwords of the thesaurus (BGH) 2. assign thesaurus categories to all words 3. calculate the preferences of every structure of the compound noun according to the frequencies of category collocations\\n\\nWe assume that a structure of a compound noun can be expressed by a binary tree. We also assume that the category of the right branch of a (sub)tree represents the category of the (sub)tree itself. This assumption exsists because Japanese is a head-final language, a modifier is on the left of its modifiee. With these assumptions, a preference value of a structure is calculated by recursive function p as follows:', metadata={'chunk': 1.0, 'data_source': 'Local', 'id': '9484e97a022b494b80c6f805b04e344d', 'page': 'None', 'source': '9412008.xml'}),\n",
       "  Document(page_content=\"Artificial Intelligence, v.13, pp. 231-278. Pericliev, V. (1986). Non-projective con-structions in Bulgarian. 2nd World Congress of Bulgaristics, Sofia, pp. 271-280 (in Bulgarian). Pericliev, V. and I. Ilarionov (1986). Testing the projectivity hypothesis. COLING'86, Bonn, pp. 56- 58. Pericliev, V. (1992a). A referent grammar treatment of some problems in the Bulgarian nominal phrase. Studia Linguistica, Stockholm, pp. 49-62. Pericliev, V. (1992b). The ID/LP format:  counter-evidence from Bulgarian, (ms). Pericliev, V. and A. Grigorov (1992). Extending Definite Clause Grammar to handle flexible word order. B. du Boulay et al. (eds.) Artificial Intelligence V, North Holland, pp. 161-170. Pollard C., I. Sag (1987). Information-Based Syntax and Semantics. Vol. 1:  Fundamentals. CSLI Lecture Notes No. 13, Stanford, CA. Sag, I. (1987). Grammatical hierarchy and linear precedence. Syntax and Semantics, v.20, pp. 303- 339. Saint-Dizier, P. (1988). Contextual Discon-tinuous Grammars. Natural Language Understanding and Logic Programming, II, North Holland, pp. 29-43. Steele, S. (1981). Word order variation:  a typological study. G. Greenberg (ed.) Universals of Language, v.4, Stanford. Uszkoreit, H. (1985). Linear precedence in discontinuous constituents: complex fronting in German. SRI International, Technical Note 371. Zwicky, A. (1986). Immediate precedence in GPSG. OSU WPL 32, pp. 133-138.\\n\\nFootnotes\\n\\nThis often results in discontinuities (or non-projectivities). For an automated way of discovering and a description of such constructs in Bulgarian, cf. Pericliev and Ilarionov 1986, and Pericliev 1986. For the difficulties in handling the adjectival clitics in pure DCG, cf. Pericliev 1992a.\", metadata={'chunk': 8.0, 'data_source': 'Local', 'id': 'f3a0167c20454148a444186de9341960', 'page': 'None', 'source': '9505007.xml'}),\n",
       "  Document(page_content=\"Since the probability of the most probable parse decreases exponentially in sentence length, the number of random samples needed to find this most probable parse increases exponentially in sentence length. Thus, when using the Monte Carlo algorithm, one is left with the uncomfortable choice of exponentially decreasing the probability of finding the most probable parse, or exponentially increasing the runtime.\\n\\nWe admit that this is a somewhat informal argument. Still, the Monte Carlo algorithm has never been tested on sentences longer than those in the ATIS corpus; there is good reason to believe the algorithm will not work as well on longer sentences. Note that our algorithm has true runtime O(Tn[3]), as shown previously.\\n\\nAnalysis of Bod's Data\\n\\nIn the DOP model, a sentence cannot be given an exactly correct parse unless all productions in the correct parse occur in the training set. Thus, we can get an upper bound on performance by examining the test corpus and finding which parse trees could not be generated using only productions in the training corpus. Unfortunately, while Bod provided us with his data, he did not specify which sentences were test and which were training. We can however find an upper bound on average case performance, as well as an upper bound on the probability that any particular level of performance could be achieved.\\n\\nThe table is arranged from least generous to most generous: in the upper left hand corner is a technique Bod might reasonably have used; in that case, the probability of getting the test set he described is less than one in a million. In the lower right corner we give Bod the absolute maximum benefit of the doubt: we assume he used a parser capable of parsing unary branching productions, that he used a very overgenerating grammar, and that he used a loose definition of ``Exact Match.'' Even in this case, there is only about a 1.5% chance of getting the test set Bod describes.\\n\\nConclusion\\n\\nWe have given efficient techniques for parsing the DOP model. These results are significant since the DOP model has perhaps the best reported parsing accuracy; previously the full DOP model had not been replicated due to the difficulty and computational complexity of the existing algorithms. We have also shown that previous results were partially due to an unlikely choice of test data, and partially due to the heavy cleaning of the data, which reduced the difficulty of the task.\\n\\nOf course, this research raises as many questions as it answers. Were previous results due only to the choice of test data, or are the differences in implementation partly responsible? In that case, there is significant future work required to understand which differences account for Bod's exceptional performance. This will be complicated by the fact that sufficient details of Bod's implementation are not available.\\n\\nThis research also shows the importance of testing on more than one small test set, as well as the importance of not making cross-corpus comparisons; if a new corpus is required, then previous algorithms should be duplicated for comparison.\\n\\nBibliography\", metadata={'chunk': 3.0, 'data_source': 'Local', 'id': '7943eadad78545d3bd4d3f6c3d10183b', 'page': 'None', 'source': '9604008.xml'})]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
