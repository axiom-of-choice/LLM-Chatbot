{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/FundamentlPartners/abinvenv-sol/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from uuid import uuid4\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dotenv import load_dotenv\n",
    "from config.config import BASE_DIR, DATA_DIR, EMBEDDING_MODEL_NAME, PINECONE_INDEX_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = os.path.dirname('../data/')\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = Document(base + '/external/common.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Is senior living the same as a nursing home? '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common.paragraphs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText_docx(file:Document) -> str:\n",
    "    content = []\n",
    "    for paragraph in file.paragraphs:\n",
    "        print(paragraph.text)\n",
    "        content.append(paragraph.text)\n",
    "    return '\\n'.join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Is senior living the same as a nursing home? \n",
      "Think of it this way, senior living is a social model providing care when you need it while preserving your independence. Nursing homes primarily provide nursing services to the chronically ill. It’s important to note, nursing homes often provide a broader range of skilled nursing. Senior living, on the other hand, offers various lifestyle options for older adults who want to maintain their independence while living in their own apartments.\n",
      "2. Will I lose my independence when I move in?\n",
      "No, in fact, quite the opposite. At Carlton, we take pride in our philosophy of independence with assistance. In our communities, you have the freedom to live life while knowing that a helping hand is always available when you need it. Whether it’s assistance with daily activities, medication management, or simply having someone there for peace of mind, we are here to provide the support that complements and enhances your independence.\n",
      "3. Is senior living affordable?\n",
      "The affordability of senior living can vary depending on factors such as location, level of care required, amenities offered, and the specific community you choose. However, many are surprised to learn that the cost of senior living is often lower than the cost of staying in their current homes. To help you get a more accurate comparison, we have created a Cost Comparison Calculator so you can compare costs side by side.  for your personalized comparison.\n",
      "4. What are some benefits of senior living?\n",
      "Senior living offers numerous benefits, including: \n",
      "Socializing on your terms – You can socialize in the community whenever you want to, and retreat to the comfort and privacy of your own apartment to relax and unwind.\n",
      "A Supportive Environment – Trained staff are always happy and available to lend a helping hand while providing person-centered care. This involves taking a holistic approach to care that addresses physical, emotional, and social well-being.\n",
      "Safety, Security & Technology – Carlton is designed with safety in mind, with features like emergency response systems, secure premises, and 24/7 staff availability. Including innovations like voice-activated technology through Alexa, Artificial Intelligence enabled fall-prevention systems.\n",
      "Chore-free living – Say goodbye to the burdens of cleaning, laundry, home maintenance, preparing meals, and other unpleasant tasks; Carlton takes care of all these responsibilities.\n",
      "Entertainment at your doorstep – From exercise classes to art workshops, to educational seminars, you will have access to the best entertainment whenever you want it.\n",
      "5. How do I know when to consider senior living?\n",
      "Asking yourself these questions can be an excellent place to start:\n",
      "Are you looking for good company, opportunities to socialize, and a sense of community?\n",
      "Do you have safety concerns like falls, navigating stairs, or driving?\n",
      "Is keeping up with household chores, home maintenance, and yardwork overwhelming or unmanageable?\n",
      "Is traveling to the store, shopping for groceries, and preparing healthy and nutritious meals a challenge or a worry?\n",
      "Could you use some help with daily tasks and personal care?\n",
      "If you answered “yes” to any of these questions, it may be time to consider Carlton Senior Living. Start with open conversations with friends, family, healthcare professionals, and senior living experts like us. We will guide and support you while you navigate through this decision-making process.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = getText_docx(common)\n",
    "type(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/raw/common.docx'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(base, 'raw', 'common.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeText(content:str, filename: str, base_path:Optional[str]=base):\n",
    "    write_dir = os.path.join(base_path,'raw', filename) \n",
    "    with open(write_dir, 'w') as f:\n",
    "        f.write(content)\n",
    "    return f'File {filename} written in {write_dir}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File common.txt written in ../data/raw/common.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeText(content, 'common.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "658"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text:str) -> int:\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "tiktoken_len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=20,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use local documents to do the splitting without parse it as strings in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.Document"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"../data/raw/common.txt\") #use local documents to split documents\n",
    "type(loader.load()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1. Is senior living the same as a nursing home? \\nThink of it this way, senior living is a social model providing care when you need it while preserving your independence. Nursing homes primarily provide nursing services to the chronically ill. It’s important to note, nursing homes often provide a broader range of skilled nursing. Senior living, on the other hand, offers various lifestyle options for older adults who want to maintain their independence while living in their own apartments.\\n2. Will I lose my independence when I move in?\\nNo, in fact, quite the opposite. At Carlton, we take pride in our philosophy of independence with assistance. In our communities, you have the freedom to live life while knowing that a helping hand is always available when you need it. Whether it’s assistance with daily activities, medication management, or simply having someone there for peace of mind, we are here to provide the support that complements and enhances your independence.\\n3. Is senior living affordable?\\nThe affordability of senior living can vary depending on factors such as location, level of care required, amenities offered, and the specific community you choose. However, many are surprised to learn that the cost of senior living is often lower than the cost of staying in their current homes. To help you get a more accurate comparison, we have created a Cost Comparison Calculator so you can compare costs side by side.  for your personalized comparison.\\n4. What are some benefits of senior living?\\nSenior living offers numerous benefits, including: \\nSocializing on your terms – You can socialize in the community whenever you want to, and retreat to the comfort and privacy of your own apartment to relax and unwind.\\nA Supportive Environment – Trained staff are always happy and available to lend a helping hand while providing person-centered care. This involves taking a holistic approach to care that addresses physical, emotional, and social well-being.\\nSafety, Security & Technology – Carlton is designed with safety in mind, with features like emergency response systems, secure premises, and 24/7 staff availability. Including innovations like voice-activated technology through Alexa, Artificial Intelligence enabled fall-prevention systems.\\nChore-free living – Say goodbye to the burdens of cleaning, laundry, home maintenance, preparing meals, and other unpleasant tasks; Carlton takes care of all these responsibilities.\\nEntertainment at your doorstep – From exercise classes to art workshops, to educational seminars, you will have access to the best entertainment whenever you want it.\\n5. How do I know when to consider senior living?\\nAsking yourself these questions can be an excellent place to start:\\nAre you looking for good company, opportunities to socialize, and a sense of community?\\nDo you have safety concerns like falls, navigating stairs, or driving?\\nIs keeping up with household chores, home maintenance, and yardwork overwhelming or unmanageable?\\nIs traveling to the store, shopping for groceries, and preparing healthy and nutritious meals a challenge or a worry?\\nCould you use some help with daily tasks and personal care?\\nIf you answered “yes” to any of these questions, it may be time to consider Carlton Senior Living. Start with open conversations with friends, family, healthcare professionals, and senior living experts like us. We will guide and support you while you navigate through this decision-making process.\\n', metadata={'source': '../data/raw/common.txt'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks  = text_splitter.split_documents(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1. Is senior living the same as a nursing home? \\nThink of it this way, senior living is a social model providing care when you need it while preserving your independence. Nursing homes primarily provide nursing services to the chronically ill. It’s important to note, nursing homes often provide a broader range of skilled nursing. Senior living, on the other hand, offers various lifestyle options for older adults who want to maintain their independence while living in their own apartments.\\n2. Will I lose my independence when I move in?\\nNo, in fact, quite the opposite. At Carlton, we take pride in our philosophy of independence with assistance. In our communities, you have the freedom to live life while knowing that a helping hand is always available when you need it. Whether it’s assistance with daily activities, medication management, or simply having someone there for peace of mind, we are here to provide the support that complements and enhances your independence.\\n3. Is senior living affordable?\\nThe affordability of senior living can vary depending on factors such as location, level of care required, amenities offered, and the specific community you choose. However, many are surprised to learn that the cost of senior living is often lower than the cost of staying in their current homes. To help you get a more accurate comparison, we have created a Cost Comparison Calculator so you can compare costs side by side.  for your personalized comparison.\\n4. What are some benefits of senior living?\\nSenior living offers numerous benefits, including: \\nSocializing on your terms – You can socialize in the community whenever you want to, and retreat to the comfort and privacy of your own apartment to relax and unwind.\\nA Supportive Environment – Trained staff are always happy and available to lend a helping hand while providing person-centered care. This involves taking a holistic approach to care that addresses physical, emotional, and social well-being.\\nSafety, Security & Technology – Carlton is designed with safety in mind, with features like emergency response systems, secure premises, and 24/7 staff availability. Including innovations like voice-activated technology through Alexa, Artificial Intelligence enabled fall-prevention systems.\\nChore-free living – Say goodbye to the burdens of cleaning, laundry, home maintenance, preparing meals, and other unpleasant tasks; Carlton takes care of all these responsibilities.\\nEntertainment at your doorstep – From exercise classes to art workshops, to educational seminars, you will have access to the best entertainment whenever you want it.\\n5. How do I know when to consider senior living?\\nAsking yourself these questions can be an excellent place to start:\\nAre you looking for good company, opportunities to socialize, and a sense of community?\\nDo you have safety concerns like falls, navigating stairs, or driving?\\nIs keeping up with household chores, home maintenance, and yardwork overwhelming or unmanageable?', metadata={'source': '../data/raw/common.txt'}),\n",
       " Document(page_content='Is keeping up with household chores, home maintenance, and yardwork overwhelming or unmanageable?\\nIs traveling to the store, shopping for groceries, and preparing healthy and nutritious meals a challenge or a worry?\\nCould you use some help with daily tasks and personal care?\\nIf you answered “yes” to any of these questions, it may be time to consider Carlton Senior Living. Start with open conversations with friends, family, healthcare professionals, and senior living experts like us. We will guide and support you while you navigate through this decision-making process.', metadata={'source': '../data/raw/common.txt'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Is senior living the same as a nursing home? \\nThink of it this way, senior living is a social model providing care when you need it while preserving your independence. Nursing homes primarily provide nursing services to the chronically ill. It’s important to note, nursing homes often provide a broader range of skilled nursing. Senior living, on the other hand, offers various lifestyle options for older adults who want to maintain their independence while living in their own apartments.\\n2. Will I lose my independence when I move in?\\nNo, in fact, quite the opposite. At Carlton, we take pride in our philosophy of independence with assistance. In our communities, you have the freedom to live life while knowing that a helping hand is always available when you need it. Whether it’s assistance with daily activities, medication management, or simply having someone there for peace of mind, we are here to provide the support that complements and enhances your independence.\\n3. Is senior living affordable?\\nThe affordability of senior living can vary depending on factors such as location, level of care required, amenities offered, and the specific community you choose. However, many are surprised to learn that the cost of senior living is often lower than the cost of staying in their current homes. To help you get a more accurate comparison, we have created a Cost Comparison Calculator so you can compare costs side by side.  for your personalized comparison.\\n4. What are some benefits of senior living?\\nSenior living offers numerous benefits, including: \\nSocializing on your terms – You can socialize in the community whenever you want to, and retreat to the comfort and privacy of your own apartment to relax and unwind.\\nA Supportive Environment – Trained staff are always happy and available to lend a helping hand while providing person-centered care. This involves taking a holistic approach to care that addresses physical, emotional, and social well-being.\\nSafety, Security & Technology – Carlton is designed with safety in mind, with features like emergency response systems, secure premises, and 24/7 staff availability. Including innovations like voice-activated technology through Alexa, Artificial Intelligence enabled fall-prevention systems.\\nChore-free living – Say goodbye to the burdens of cleaning, laundry, home maintenance, preparing meals, and other unpleasant tasks; Carlton takes care of all these responsibilities.\\nEntertainment at your doorstep – From exercise classes to art workshops, to educational seminars, you will have access to the best entertainment whenever you want it.\\n5. How do I know when to consider senior living?\\nAsking yourself these questions can be an excellent place to start:\\nAre you looking for good company, opportunities to socialize, and a sense of community?\\nDo you have safety concerns like falls, navigating stairs, or driving?\\nIs keeping up with household chores, home maintenance, and yardwork overwhelming or unmanageable?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = embeddings.embed_documents(chunks[1].page_content)\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "def loadFilesinDirectory(path: str, glob: Optional[str] = None) -> List[Document]:\n",
    "    if glob is None:\n",
    "        loader = DirectoryLoader(path = path)\n",
    "    else:\n",
    "        loader = DirectoryLoader(path = path, glob = glob)\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loadFilesinDirectory(path='../data/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1. Is senior living the same as a nursing home? Think of it this way, senior living is a social model providing care when you need it while preserving your independence. Nursing homes primarily provide nursing services to the chronically ill. It’s important to note, nursing homes often provide a broader range of skilled nursing. Senior living, on the other hand, offers various lifestyle options for older adults who want to maintain their independence while living in their own apartments. 2. Will I lose my independence when I move in? No, in fact, quite the opposite. At Carlton, we take pride in our philosophy of independence with assistance. In our communities, you have the freedom to live life while knowing that a helping hand is always available when you need it. Whether it’s assistance with daily activities, medication management, or simply having someone there for peace of mind, we are here to provide the support that complements and enhances your independence. 3. Is senior living affordable? The affordability of senior living can vary depending on factors such as location, level of care required, amenities offered, and the specific community you choose. However, many are surprised to learn that the cost of senior living is often lower than the cost of staying in their current homes. To help you get a more accurate comparison, we have created a Cost Comparison Calculator so you can compare costs side by side. for your personalized comparison. 4.\\n\\nWhat are some benefits of senior living? Senior living offers numerous benefits, including: Socializing on your terms – You can socialize in the community whenever you want to, and retreat to the comfort and privacy of your own apartment to relax and unwind. A Supportive Environment – Trained staff are always happy and available to lend a helping hand while providing person-centered care. This involves taking a holistic approach to care that addresses physical, emotional, and social well-being. Safety, Security & Technology – Carlton is designed with safety in mind, with features like emergency response systems, secure premises, and 24/7 staff availability. Including innovations like voice-activated technology through Alexa, Artificial Intelligence enabled fall-prevention systems. Chore-free living – Say goodbye to the burdens of cleaning, laundry, home maintenance, preparing meals, and other unpleasant tasks; Carlton takes care of all these responsibilities. Entertainment at your doorstep – From exercise classes to art workshops, to educational seminars, you will have access to the best entertainment whenever you want it. 5. How do I know when to consider senior living? Asking yourself these questions can be an excellent place to start: Are you looking for good company, opportunities to socialize, and a sense of community? Do you have safety concerns like falls, navigating stairs, or driving?\\n\\nIs keeping up with household chores, home maintenance, and yardwork overwhelming or unmanageable? Is traveling to the store, shopping for groceries, and preparing healthy and nutritious meals a challenge or a worry? Could you use some help with daily tasks and personal care? If you answered “yes” to any of these questions, it may be time to consider Carlton Senior Living. Start with open conversations with friends, family, healthcare professionals, and senior living experts like us. We will guide and support you while you navigate through this decision-making process.', metadata={'source': '../data/raw/common.txt'}),\n",
       " Document(page_content='Distributional Part\\n\\n\\n\\nof\\n\\n\\n\\nSpeech Tagging\\n\\nThis paper presents an algorithm for tagging words whose part-of-speech properties are unknown. Unlike previous work, the algorithm categorizes word tokens in context instead of word types. The algorithm is evaluated on the Brown Corpus.\\n\\nIntroduction\\n\\nSince online text becomes available in ever increasing volumes and an ever increasing number of languages, there is a growing need for robust processing techniques that can analyze text without expensive and time-consuming adaptation to new domains and genres. This need motivates research on fully automatic text processing that may rely on general principles of linguistics and computation, but does not depend on knowledge about individual words.\\n\\nRelated Work\\n\\nWhat these approaches have in common is that they classify words instead of individual occurrences. Given the widespread part-of-speech ambiguity of words this is problematic. How should a word like ``plant\\'\\' be categorized if it has uses both as a verb and as a noun? How can a categorization be considered meaningful if the infinitive marker ``to\\'\\' is not distinguished from the homophonous preposition?\\n\\nTag induction\\n\\nWe start by constructing representations of the syntactic behavior of a word with respect to its left and right context. Our working hypothesis is that syntactic behavior is reflected in co-occurrence patterns. Therefore, we will measure the similarity between two words with respect to their syntactic behavior to, say, their left side by the degree to which they share the same neighbors on the left. If the counts of neighbors are assembled into a vector (with one dimension for each neighbor), the cosine can be employed to measure similarity. It will assign a value close to 1.0 if two words share many neighbors, and 0.0 if they share none. We refer to the vector of left neighbors of a word as its left context vector, and to the vector of right neighbors as its right context vector. The unreduced context vectors in the experiment described here have 250 entries, corresponding to the 250 most frequent words in the Brown corpus.\\n\\nThis basic idea of measuring distributional similarity in terms of shared neighbors must be modified because of the sparseness of the data. Consider two infrequent adjectives that happen to modify different nouns in the corpus. Their right similarity according to the cosine measure would be zero. This is clearly undesirable. But even with high-frequency words, the simple vector model can yield misleading similarity measurements. A case in point is ``a\\'\\' vs. ``an\\'\\'. These two articles do not share any right neighbors since the former is only used before consonants and the latter only before vowels. Yet intuitively, they are similar with respect to their right syntactic context despite the lack of common right neighbors.\\n\\nOur solution to these problems is the application of a singular value decomposition. We can represent the left vectors of all words in the corpus as a matrix C with n rows, one for each word whose left neighbors are to be represented, and kcolumns, one for each of the possible neighbors. SVD can be used to approximate the row and column vectors of C in a low-dimensional space. In more detail, SVD decomposes  a matrix C, the matrix of left vectors in our case, into three matrices T0, S0, and D0 such that:\\n\\nC= T0 S0 D0\\'\\n\\ninduction based on word type only\\n\\ninduction based on word type and context\\n\\ninduction based on word type and context, restricted to ``natural\\'\\' contexts\\n\\ninduction based on word type and context, using generalized left and right context vectors\\n\\nInduction based on word type only Induction based on word type and context The right context vector of the preceding word.\\n\\nThe left context vector of w.\\n\\nThe right context vector of w.\\n\\nThe left context vector of the following word.\\n\\nAgain, an SVD is applied to address the problems of sparseness and generalization. We randomly selected 20,000 word triplets from the corpus and formed concatenations of four context vectors as described above. The singular value decomposition of the resulting 20,000-by-1,000 matrix defines a mapping from the 1,000-dimensional space of concatenated context vectors to a 50-dimensional reduced space. Our tag set was then induced by clustering the reduced vectors of the 20,000 selected occurrences into 200 classes. Each of the 200 tags is defined by the centroid of the corresponding class (the sum of its members). Distributional tagging of an occurrence of a word w proceeds then by retrieving the four relevant context vectors (right context vector of previous word, left context vector of following word, both context vectors of w) concatenating them to one 1000-component vector, mapping this vector to 50 dimensions, computing the correlations with the 200 cluster centroids and, finally, assigning the occurrence to the closest cluster. This procedure was applied to all tokens of the Brown corpus.\\n\\nWe will see below that this method of distributional tagging, although partially successful, fails for many tokens whose neighbors are punctuation marks. The context vectors of punctuation marks contribute little information about syntactic categorization since there are no grammatical dependencies between words and punctuation marks, in contrast to strong dependencies between neighboring words.\\n\\nFor this reason, a second induction on the basis of word type and context was performed, but only for those tokens with informative contexts. Tokens next to punctuation marks and tokens with rare words as neighbors were not included. Contexts with rare words (less than ten occurrences) were also excluded for similar reasons: If a word only occurs nine or fewer times its left and right context vectors capture little information for syntactic categorization. In the experiment, 20,000 natural contexts were randomly selected, processed by the SVD and clustered into 200 classes. The classification was then applied to all natural contexts of the Brown corpus.\\n\\nGeneralized context vectors\\n\\nThe context vectors used so far only capture information about distributional interactions with the 250 most frequent words. Intuitively, it should be possible to gain accuracy in tag induction by using information from more words. One way to do this is to let the right context vector record which classes of left context vectors occur to the right of a word. The rationale is that words with similar left context characterize words to their right in a similar way. For example, ``seemed\\'\\' and ``would\\'\\' have similar left contexts, and they characterize the right contexts of ``he\\'\\' and ``the firefighter\\'\\' as potentially containing an inflected verb form. Rather than having separate entries in its right context vector for ``seemed\\'\\', ``would\\'\\', and ``likes\\'\\', a word like ``he\\'\\' can now be characterized by a generalized entry for ``inflected verb form occurs frequently to my right\\'\\'.\\n\\nAnother argument for the two-step derivation is that many words don\\'t have any of the 250 most frequent words as their left or right neighbor. Hence, their vector would be zero in the word-based scheme. The class-based scheme makes it more likely that meaningful representations are formed for all words in the vocabulary.\\n\\nThe generalized context vectors were input to the tag induction procedure described above for word-based context vectors: 20,000 word triplets were selected from the corpus, encoded as 1,000-dimensional vectors (consisting of four generalized context vectors), decomposed by a singular value decomposition and clustered into 200 classes. The resulting classification was applied to all tokens in the Brown corpus.\\n\\nResults\\n\\nIt is clear from the tables that incorporating context improves performance considerably. The F score increases for all tags except CD, with an average improvement of more than 0.20. The tag CD is probably better thought of as describing a word class. There is a wide range of heterogeneous syntactic functions of cardinals in particular contexts: quantificational and adnominal uses, bare NP\\'s (``is one of\\'\\'), dates and ages (``Jan 1\\'\\', ``gave his age as 25\\'\\'), and enumerations. In this light, it is not surprising that the word-type method does better on cardinals.\\n\\nEven for ``natural\\'\\' contexts, performance varies considerably. It is fairly good for prepositions, determiners, pronouns, conjunctions, the infinitive marker, modals, and the possessive marker. Tag induction fails for cardinals (for the reasons mentioned above) and for ``-ing\\'\\' forms. Present participles and gerunds are difficult because they exhibit both verbal and nominal properties and occur in a wide variety of different contexts whereas other parts of speech have a few typical and frequent contexts.\\n\\nIt may seem worrying that some of the tags are assigned a high number of clusters (e.g., 49 for N, 36 for ADN). A closer look reveals that many clusters embody finer distinctions. Some examples: Nouns in cluster 0 are heads of larger noun phrases, whereas the nouns in cluster 1 are full-fledged NPs. The members of classes 29 and 111 function as subjects. Class 49 consists of proper nouns. However, there are many pairs or triples of clusters that should be collapsed into one on linguistic grounds. They were separated on distributional criteria that don\\'t have linguistic correlates.\\n\\nAn analysis of the divergence between our classification and the manually assigned tags revealed three main sources of errors: rare words and rare syntactic phenomena, indistinguishable distribution, and non-local dependencies.\\n\\nRare words are difficult because of lack of distributional evidence. For example, ``ties\\'\\' is used as a verb only 2 times (out of 15 occurrences in the corpus). Both occurrences are miscategorized, since its context vectors do not provide enough evidence for the verbal use. Rare syntactic constructions pose a related problem: There are not enough instances to justify the creation of a separate cluster. For example, verbs taking bare infinitives were classified as adverbs since this is too rare a phenomenon to provide strong distributional evidence (``we do not DARE speak of\\'\\', ``legislation could HELP remove\\'\\').\\n\\nThe case of the tags ``VBN\\'\\' and ``PRD\\'\\' (past participles and predicative adjectives) demonstrates the difficulties of word classes with indistinguishable distributions. There are hardly any distributional clues for distinguishing ``VBN\\'\\' and ``PRD\\'\\' since both are mainly used as complements of ``to be\\'\\'. A common tag class was created for ``VBN\\'\\' and ``PRD\\'\\' to show that they are reasonably well distinguished from other parts of speech, even if not from each other. Semantic understanding is necessary to distinguish between the states described by phrases of the form ``to be adjective\\'\\'  and the processes described by phrases of the form ``to be past participle\\'\\'.\\n\\nFinally, the method fails if there are no local dependencies that could be used for categorization and only non-local dependencies are informative. For example, the adverb in ``Mc*N. Hester, CURRENTLY Dean of ...\\'\\' and the conjunction in ``to add that, IF United States policies ...\\'\\' have similar immediate neighbors (comma, NP). The decision to consider only immediate neighbors is responsible for this type of error since taking a wider context into account would disambiguate the parts of speech in question.\\n\\nFuture Work\\n\\nThere are three avenues of future research we are interested in pursuing. First, we are planning to apply the algorithm to an as yet untagged language. Languages with a rich morphology may be more difficult than English since with fewer tokens per type, there is less data on which to base a categorization decision.\\n\\nConclusion\\n\\nIn this paper, we have attempted to construct an algorithm for fully automatic distributional tagging, using unannotated corpora as the sole source of information. The main innovation is that the algorithm is able to deal with part-of-speech ambiguity, a pervasive phenomenon in natural language that was unaccounted for in previous work on learning categories from corpora. The method was systematically evaluated on the Brown corpus. Even if no automatic procedure can rival the accuracy of human tagging, we hope that the algorithm will facilitate the initial tagging of texts in new languages and sublanguages.\\n\\nAcknowledgments\\n\\nI am grateful for helpful comments to Steve Finch, Jan Pedersen and two anonymous reviewers (from ACL and EACL). I\\'m also indebted to Michael Berry for SVDPACK and to the Penn Treebank Project for the parsed Brown corpus.\\n\\nBibliography\\n\\nSteven Abney. 1991. Parsing by chunks. In Berwick, Abney, and Tenny, editors, Principle-Based Parsing. Kluwer Academic Publishers.\\n\\nMichael W. Berry. 1992. Large-scale sparse singular value computations. The International Journal of Supercomputer Applications, 6(1):13-49.\\n\\nDouglas Biber. 1993. Co-occurrence patterns among collocations: A tool for corpus-based lexical knowledge acquisition. Computational Linguistics, 19(3):531-538.\\n\\nEric Brill and Mitch Marcus. 1992a. Tagging an unfamiliar text with minimal human supervision. In Robert Goldman, editor, Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language. AAAI Press.\\n\\nEric Brill and Mitchell Marcus. 1992b. Automatically acquiring phrase structure using distributional analysis. In Proceedings of the DARPA workshop \"Speech and Natural Language\", pages 155-159.\\n\\nEric Brill, David Magerman, Mitch Marcus, and Beatrice Santorini. 1990. Deducing linguistic structure from the statistics of large corpora. In Proceedings of the DARPA Speech and Natural Language Workshop, pages 275-282.\\n\\nEric Brill. 1993. Automatic grammar induction and parsing free text: A transformation-based approach. In Proceedings of ACL 31, Columbus OH.\\n\\nEugene Charniak, Curtis Hendrickson, Neil Jacobson, and Mike Perkowitz. 1993. Equations for part-of-speech tagging. In Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 784-789.\\n\\nEugene Charniak, Glenn Carroll, John Adcock, Anthony Cassandra, Yoshihiko Gotoh, Jeremy Katz, Michael Littman, and John McCann. 1994. Taggers for parsers. Technical Report CS-94-06, Brown University.\\n\\nKenneth W. Church. 1989. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of ICASSP-89, Glasgow, Scotland.\\n\\nDoug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1991. A practical part-of-speech tagger. In The 3rd Conference on Applied Natural Language Processing, Trento, Italy.\\n\\nDouglas R. Cutting, Jan O. Pedersen, David Karger, and John W. Tukey. 1992. Scatter/gather: A cluster-based approach to browsing large document collections. In Proceedings of SIGIR \\'92, pages 318-329.\\n\\nC. G. deMarcken. 1990. Parsing the LOB corpus. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pages 243-259.\\n\\nJeffrey L. Elman.\\n\\n1990.\\n\\nFinding structure in time.\\n\\nCognitive Science, 14:179\\n\\n\\n\\n211.\\n\\nSteven Finch and Nick Chater. 1992. Bootstrapping syntactic categories using statistical methods. In Walter Daelemans and David Powers, editors, Background and Experiments in Machine Learning of Natural Language, pages 229-235, Tilburg University. Institute for Language Technology and AI.\\n\\nSteven Paul Finch. 1993. Finding Structure in Language. Ph.D. thesis, University of Edinburgh.\\n\\nW.N. Francis and F. Kucera. 1982. Frequency Analysis of English Usage. Houghton Mifflin, Boston.\\n\\nF. Jelinek. 1985. Robust part-of-speech tagging using a hidden markov model. Technical report, IBM, T.J. Watson Research Center.\\n\\nReinhard Kneser and Hermann Ney. 1993. Forming word classes by statistical clustering for statistical language modelling. In Reinhard Khler and Burghard B. Rieger, editors,   Contributions to Quantitative Linguistics, pages 221-226. Kluwer Academic Publishers, Dordrecht, The Netherlands.\\n\\nJulian Kupiec. 1992. Robust part-of-speech tagging using a hidden markov model. Computer Speech and Language, 6:225-242.\\n\\nJulian Kupiec. 1993. Murax: A robust linguistic approach for question answering using an on-line encyclopedia. In Proceedings of SIGIR \\'93, pages 181-190.\\n\\nJohn R. Ross. 1972. The category squish: Endstation Hauptwort. In Papers from the Eighth Regional Meeting. Chicago Linguistic Society.\\n\\nHinrich Schtze. 1993. Part-of-speech induction from scratch. In Proceedings of ACL 31, pages 251-258, Columbus OH.\\n\\nWhitney Tabor. 1994. Syntactic Innovation: A Connectionist Model. Ph.D. thesis, Stanford University.\\n\\nC. J. van Rijsbergen.\\n\\n1979.\\n\\nInformation Retrieval.\\n\\nButterworths, London.\\n\\nSecond Edition.\\n\\nFootnotes\\n\\nAlthough bib93 classifies collocations, these can also be ambiguous. For example, ``for certain\\'\\' has both senses of ``certain\\'\\': ``particular\\'\\' and ``sure\\'\\'. The small difference in overall frequency in the tables is due to the fact that some word-based context vectors consist entirely of zeros. There were about a hundred word triplets whose four context vectors did not have non-zero entries and could not be assigned a cluster. Because of phrases like ``I had sweet potatoes\\'\\', forms of ``have\\'\\' cannot serve as a reliable discriminator either.', metadata={'source': '../data/raw/cmplg-xml/9503009.xml'}),\n",
       " Document(page_content=\"A Note on the Complexity of Restricted Attribute-Value Grammars\\n\\nThe recognition problem for attribute-value grammars(AVGs) was shown to be undecidable by Johnson in 1988. Therefore, the general form of AVGs is of no practical use. In this paper we study a very restricted form of AVG, for which the recognition problem is decidable (though still\\n\\ncomplete), the R-AVG. We show that\\n\\nthe R-AVG formalism captures all of the context free languages and\\n\\nmore, and introduce a variation on the so-called off-line\\n\\nparsability constraint, the honest\\n\\nparsability constraint, which lets different types of\\n\\nR-AVG coincide precisely with well-known time complexity classes.\\n\\nIntroduction\\n\\nAlthough a universal feature theory does not exist, there is a general understanding of its objects. The objects of feature theories are abstract linguistic objects, e.g., an object ``sentence,'' an object ``masculine third person singular,'' an object ``verb,'' an object ``noun phrase.'' These abstract objects have properties like ``tense,'' ``number,'' ``predicate,'' ``subject.'' The values of these properties are either atomic, like ``present'' and ``singular,'' or abstract objects, like ``verb'' and ``noun-phrase.'' The abstract objects are fully described by their properties and their values. Multiple descriptions for the properties and values of the abstract linguistic objects are presented in the literature. Examples are:\\n\\n1. Feature graphs, which are labeled rooted directed acyclic graphs G=(V,A), where F is a collection of labels, a sink in the graph represents an atomic value and the labeling function is an injective function\\n\\n. 2. Attribute-value matrices, which are matrices in which the entries consist of an attribute and a value or a reentrance symbol. The values are either atomic or attribute-value matrices.\\n\\nhardness proof of the recognition problem.\\n\\n. Likewise, R-AVGL is a proper subset of the class of context sensitive languages, unless\\n\\nor\\n\\n. That is, for any language L that has an\\n\\nDefinitions and Notation\\n\\nAttribute\\n\\n\\n\\nValue Grammars\\n\\nDefinition thedefctr: An f-edge from x to s is a triple (x,f,s) such that x is a variable, f is an attribute, and s is a constant or a variable. A path, p, is a, possibly empty, sequence of f-edges\\n\\nin which the xi are variables and s is either a variable or a constant. Often a path is denoted by the sequence of its edges' attributes, in reversed order, e.g.,\\n\\n. Let p be a path, ps denotes the path that starts from s, where s is a constant only if p is the empty path. If the path is nonempty,\\n\\n, then s is a variable. For paths ps and qt we write\\n\\niff p and q start in sand t respectively and end in the same variable or constant. The expression\\n\\nis called a path equation. A feature graph is either a pair\\n\\n, or a pair (x,E) where x is the root and E a finite set of f-edges such that: 1. if (y,f,s) and (y,f,t) are in E, then s=t; 2. if (y,f,s) is in E, then there is a path from x to y in E.\\n\\nDefinition thedefctr:\\n\\nAn attribute\\n\\n\\n\\nvalue language\\n\\nAssume a finite set\\n\\n(of lexical forms) and a finite set\\n\\n(of categories).\\n\\nwill play the role of the set of terminals and\\n\\nwill play the role of the set of nonterminals in the productions.\\n\\nDefinition thedefctr: A constituent structure tree (CST) is a labeled tree in which the internal nodes are labeled with elements of Cat and the leaves are labeled with elements of Lex.\\n\\nDefinition thedefctr: Let T be a constituent structure tree and F be a set of formulas in an attribute-value language\\n\\n. An annotated constituent structure tree is a triple\\n\\n, where h is a function that maps internal nodes in T onto variables in F.\\n\\nDefinition thedefctr: A lexicon is a finite subset of\\n\\nA set of syntactic rules is a finite subset of\\n\\n. An attribute-value grammar is a triple\\n\\n>, where lexicon is a lexicon, rules is a set of syntactic rules and start is an element of\\n\\n.\\n\\nof sets is recursively presentable iff there is an effective enumeration\\n\\nof deterministic Turing machines which halt on all their inputs, and such that\\n\\n2. We say that a class of grammars\\n\\nis recursively presentable iff the class of sets\\n\\nis recursively presentable.\\n\\nRestricted Attribute\\n\\n\\n\\nValue Grammars\\n\\nThe only formulas that are allowed in the attribute-value language of restricted attribute-value grammars (R-AVGs) are path-equations and conjunctions of path-equations (i.e. disjunctions and negations are out). We will denote the attribute-value language of an R-AVG by\\n\\nto make the distinction clear. The CST of an R-AVG is produced by a chain- and\\n\\nrule free\\n\\nregular grammar. The CST of an R-AVG can be either a left-branching\\n\\nor a right-branching tree, since the grammar contains\\n\\nat most one nonterminal in each rule.\\n\\nDefinition thedefctr:\\n\\nThe set of syntactic rules of a restricted attribute-value\\n\\ngrammar is a subset of\\n\\n>. A restricted attribute-value grammar is a pair\\n\\n>, where rules is a set of syntactic rules and start is an element of\\n\\n.\\n\\nDefinition thedefctr:\\n\\nAn R\\n\\n\\n\\nAVG\\n\\n> generates an annotated constituent structure tree\\n\\niff 1. the root node of T is start, and 2. every internal node of T is licensed by a syntactic rule, and 3. the set F is consistent, i.e., describes a feature graph. Let\\n\\nstand for the formula\\n\\nin which all variable y is substituted for variable x. An internal node v of an annotated constituent structure tree is licensed by a syntactic rule\\n\\niff 1. the node v is labeled with category c0,\\n\\nh(v) = n0, and 2. all daughters of v are leaves, which are labeled with\\n\\n,\\n\\nand\\n\\n3.\\n\\nis in the set F. An internal node v of an annotated constituent structure tree is licensed by a syntactic rule\\n\\niff 1. the node v is labeled with category c0,\\n\\nh(v) = n0, and 2. one of v's daughters is an internal node, v1, which is labeled with category c1, and\\n\\nh(v1) = n1, and 3. the daughters of v that are leaves are labeled with\\n\\n,\\n\\nand\\n\\n4.\\n\\nis in the set F.\\n\\nWeak Generative Capacity\\n\\nLet L be a context free language. There exists an R-AVG G such that L=L(G).\\n\\nProof.If L is a context free language, then there exists a context free grammar G' in Greibach normal form such that L=L(G'). From this grammar G', we can construct a pushdown store M that accepts exactly the words in L(G')=L. Such a pushdown store M is actually a finite state automaton M' with a stack S. The finite state automaton M' may be simulated by a chain- and\\n\\nrule free regular grammar.\\n\\nFurthermore, we can construct an attribute-value language\\n\\nthat simulates the stack S. Thus it should be clear that there exists an R-AVG G that produces word w iff\\n\\nThe Honest Parsability Constraint and Consequences\\n\\nAs a more liberal constraint on R-AVGs we propose an analogous variation on the OLP Definition thedefctr: A grammar G satisfies the Honest Parsability Constraint(HPC) iff there exists a polynomial p s.t. for each win L(G) there exists a derivation with at most\\n\\nsteps.\\n\\nFrom Smolka's algorithm and Trautwein's observation it trivially follows that any attribute-value grammar that satisfies the HPC (HP-AVG) has an\\n\\nrecognition algorithm. The problem with the HPC is of course that it is not a syntactic property of grammars. The question whether a given AVG satisfies the HPC (or the OLP for that matter) may well be undecidable. Nonetheless, we can produce a set of rules that, when added to an attribute-value grammar enforces the HPC. The newly produced language is then a subset of the old produced language with an\\n\\nrecognition algorithm. Because of the fact that our addition may simulate any polynomial restriction, we regain the full class of AVG's that satisfy the HPC. In fact\\n\\nTheorem  4.1 The class, P-AVGL, of languages produced by the HP-AVGs is recursively presentable.\\n\\nFor any language L that has an\\n\\nrecognition algorithm, there exists a restricted attribute-value grammar G that respects the HPC and such that L=L(G).\\n\\nProof. (Sketch) Let M be the Turing machine that decides\\n\\n. Use a variation of Johnson's construction of a Turing machine to create an R-AVG that can produce any string w that is recognized by M. Add the set of rules that guarantee that only strings that can be produced with a polynomial number of rules can be produced by the grammar.\\n\\nVeer out the HPC\\n\\nTheorem  5.1 The class of languages generated by R-AVGs obeying the LDP condition is exactly\\n\\n.\\n\\nAcknowledgements\\n\\nWe are indebted to E. Aarts and W.C. Rounds for their valuable suggestions on an early presentation of this work.\\n\\nBibliography\\n\\nJ. Balczar, J. Daz, and J. Gabarr. Structural Complexity I. Springer-Verlag, New York, 1988.\\n\\nP. Blackburn and E. Spaan. A modal perspective on the computational complexity of attribute value grammar. Journal of Logic, Language and Information, 2(2):129-169, 1993.\\n\\nN. Chomsky. Three models for the description of language. IRE Transactions on Information Theory, 2(3):113-124, 1956.\\n\\nJ. Earley. An efficient context-free parsing algorithm. Communications of the Association for Computing Machinery, 13(2):94-102, February 1970.\\n\\nJ. Hopcroft and J. Ullman. Introduction to Automata Theory, Languages, and Computation. Addison Wesley, Reading, MA, 1979.\\n\\nM. Johnson. Attribute-Value Logic and the Theory of Grammar, volume 16 of CSLI Lecture Notes. CSLI, Stanford, 1988.\\n\\nC. Perrault. On the mathematical properties of linguistic theories. Computational Linguistics, 10(3-4):165-176, 1984.\\n\\nG. Smolka. Feature-constraint logics for unification grammars. Journal of Logic Programming, 12(1):51-87, 1992.\\n\\nT. Sudkamp. Languages and Machines: An introduction to the Theory of Computer Science. Addison Wesley, Reading, MA, 1988.\\n\\nM. Trautwein. Assessing complexity results in feature theories. ILLC Research Report and Technical Notes Series LP-95-01, University of Amsterdam, Amsterdam, 1995. Submitted to the CMP-LG archive.\\n\\nSimulating a Context Free Grammar in GNF\\n\\nA context free grammar (CFG) is a quadruple\\n\\n, where N is a set of nonterminals,\\n\\nis a set of terminals, P is a set of productions, and\\n\\nis the start nonterminal. A CFG is in Greibach normalform (GNF) if, and only if, the productions are of one of the following forms, where\\n\\nand\\n\\nGiven a GNF\\n\\nThe three syntactic abbreviations below are used to clarify the simulation. We use represent a stack by a Greek letter, or a string of symbols; the top of the stack is the leftmost symbol of the string. Let x0 encode a stack\\n\\n,\\n\\nthen the formulas in\\n\\nthe abbreviation\\n\\nexpress that x1 encodes\\n\\na stack\\n\\n. Likewise, the formulas in the abbreviation\\n\\nexpress that x0 encodes a stack\\n\\n, and X1 encodes the stack\\n\\n. The abbreviation EMPTY-STACK expresses that x0 encodes an empty stack.\\n\\nWe have to prove that GNF Gand its simulation by R-AVG G'generate (almost) the same language. Obviously, R-AVG G'cannot generate the empty string. However, for all non-empty strings the following theorem holds.\\n\\nTheorem  6.1 Start nonterminal S of GNF Gderives string\\n\\n(\\n\\n) if, and only if, start nonterminal S of R-AVG G'derives string\\n\\nwith the empty stack.\\n\\nProof.There are two cases to consider. First, S derives string\\n\\nin one step. Second, S derives string\\n\\nin more than one step. The lemma below is needed in the proof of the second case.\\n\\nCase I Let start nonterminal S derive string\\n\\nin one step. GNF G contains a production\\n\\niff R-AVG G'   contains a production\\n\\nwith the equation\\n\\nEMPTY\\n\\n\\n\\nSTACK.\\n\\nSo, S derives\\n\\nin a derivation of GNF G iff S derives\\n\\nwith an empty stack in the derivation of R-AVG G'. Case II Initial nonterminal S of GNF G derives string\\n\\nin more than one step iff there is a left-most derivation\\n\\n. GNF G contains production\\n\\niff R\\n\\n\\n\\nAVG G' contains production\\n\\nwith the equation EMPTY-STACK. By the next lemma:\\n\\niff\\n\\nwith\\n\\nthe empty stack.\\n\\nHence S derives\\n\\nfor GNF G iff\\n\\nS derives\\n\\nwith empty stack for R-AVG G'.\\n\\nConstructing an Honestly Parsable Attribute-Value Grammar\\n\\nArithmetic by AVGs\\n\\nWe start with a little bit of arithmetic.\\n\\nNatural numbers.\\n\\nThe AVMs below encode natural numbers in binary notation. The sequences of attributes  0 and  1 in these AVMs encode natural numbers, from least- to most-significant bit. The attribute  V has value 1 (or 0) if, and only if, it has a sister attribute  1 (or  0). 1. The AVMs\\n\\nand\\n\\nencode the natural numbers zero and one. 2. The AVMs\\n\\nand\\n\\nencode natural numbers iff the AVM [F] encodes a natural number.\\n\\nSyntactic rules that tests two numbers for equality.\\n\\nAssume a nonterminal A with some AVM\\n\\n, where [F] and [H] encode natural number x and y, respectively. We present one syntactic rule that derives from this nonterminal A a nonterminal B with AVM\\n\\nif x = y.\\n\\nClearly, this simple test takes one step. A more sophisticated test, which also tests for inequality, would compare [F]and [G] bit-by-bit. Such a test would take\\n\\nderivation steps.\\n\\nSyntactic rules that multiply by two.\\n\\nAssume a nonterminal A with some AVM\\n\\n, where [F] encodes natural number x. We present one syntactic rule that derives from this nonterminal A a nonterminal B with the AVM\\n\\n, where [H] encodes natural number 2x.\\n\\nThe number  N in [H] equals two times  N in [F]if, and only if, the least-significant bit of  N in [H]is 0, and the remaining bits form the same sequence as the number  N in [F]. Multiplication by two takes one derivation step.\\n\\nSyntactic rules that increments by one.\\n\\nAssume a nonterminal A with some AVM\\n\\n, where [F] encodes natural number x. We present five syntactic rules that derive from this nonterminal A a nonterminal C with AVM\\n\\n, where [H] encodes natural number x+1.\\n\\nThe increment of  N requires two additional pointers in the AVM of A: attribute  P points to the next bit that has to be incremented; attribute  Q points to the most-significant bit of the (intermediate) result. These additional pointers are hidden from the AVMs of the nonterminals A and C.\\n\\ntakes\\n\\nderivation steps.\\n\\nRules, similar to the ones above, can be given that decrement the attribute  N by one. We only have to take a little extra care that the number 0 cannot be decremented.\\n\\nSyntactic rules that sum two numbers.\\n\\nIn this section we use the previous test and increment rules (indicated by =). Assume a nonterminal A with some AVM\\n\\n, where [F'] encodes the natural number x + y.\\n\\nThe increment of  N by  M is similar to the increment by one. Here, three additional pointers are required: the attributes  P and  Q point to the bits in  N and M respectively that have to be summed next; attribute  R points to the most-significant bit of the (intermediate) result. In the addition two states are distinguished. In the one state, the carry bit is zero, indicated by nonterminal A'. In the other state, the carry bit is one, indicated by nonterminal B. We claim that\\n\\ntakes\\n\\nderivation steps.\\n\\nSyntactic rules that sum a sequence of numbers.\\n\\nIn this section we use the previous summation rules (indicated by =). Assume a nonterminal A with some AVM\\n\\n, where [F'] encodes a list of numbers. To wit\\n\\n, where [F] encodes the natural number\\n\\n.\\n\\nThe summation requires an additional pointer in the AVM [F']: attribute  P points to the next element in the list that has to be summed. We claim that\\n\\ntakes\\n\\nderivation steps.\\n\\nCreating a counter of logarithmic size\\n\\nCreate an AVM of the following form:\\n\\nAttribute  COUNTER is used to distinguish the AVMs that encodes the counter from those in the original attribute-value grammar. We will neglect the attribute  COUNTER in the remainder of this section, because it is not essential here. The attributes  SIZE,  N,  M and  POLY encode natural numbers. The attribute  SIZE records the size of the string that will be generated. The attribute  POLY records the maximum number of derivation steps that is allowed for a string of size  SIZE. The attributes  N and  M are auxiliary numbers.\\n\\nThe construction of the counter starts with an initiation-step. The further construction of the counter consists of cycles of two phases. Each cycle starts in nonterminal A.\\n\\nInitiation step and first phase.\\n\\nThe initiation-step sets the numbers  SIZE and  N to 0, and the numbers  M and  POLY to 1. In the first phase of each cycle, the numbers  SIZE and  N are incremented by 1.\\n\\nThe second phase of the cycle.\\n\\nIn this phase the numbers  N and  M are compared. If  N is twice  M, then (i) number POLY is extended by k bits, (ii) number  M is doubled, and (iii) number  N is set to 0. If  N is less than twice  M, nothing happens.\\n\\nThe left rule of the second phase doubles the number  M in the second and the third equation. The test ``Is  N equal to 2 M?'' therefore reduces to one (the first) equation. The fourth equation extend the number  POLY with k bits. The fifth and sixth equations set the number  N to 0.\\n\\nThe right rule is always applicable. If the right rule is used where the left rule was applicable, then the number  N will never be equal to\\n\\nin the rest of the derivation. Thus  POLY will not be extended any more.\\n\\nWe claim that the left rule appears\\n\\ntimes and the right rule O(n) times in a derivation for input of size n. Obviously, the number  POLY is\\n\\nwhen the number  SIZE is i.\\n\\nFrom AVG to HP\\n\\n\\n\\nAVG\\n\\nIn this section we show how to transform an AVG into an AVG that satisfies the HPC (HP-AVG). Since all computation steps of the HP-AVG only require a linear amount of derivation steps, total derivations of HP-AVGs have polynomial length.\\n\\nWe can divide the attributes of the HP-AVG into two groups. The attributes that encode the counters, and the attributes of the original AVG. The former will be embedded under the attribute COUNTER, the latter under the attribute  GRAMMAR. In the sequel, we mean by\\n\\nthe formula\\n\\nembedded under the attribute  GRAMMAR, i.e., the formula obtained from\\n\\nby substituting the variables xi by\\n\\n.\\n\\nSecond, the HP-AVG contains an extension of the lexicon of the AVG. The entries of the lexicon are extended in the following way. The size of the lexical form is set to one, and the amount of derivation steps is zero. Thus, if\\n\\nis the lexicon of the AVG, then\\n\\nis the lexicon of the HP-AVG, where\\n\\nThird, the HP-AVG contains extensions of the syntactic rules of the AVG. The syntactic rules are extended in the following way. The numbers POLY and  SIZE of the daughter nonterminals are collected in the lists  PLIST and  SLIST. Both lists are summed. The number  SIZE of the mother nonterminal is equal to the sum of  SIZE's, and the number  POLY of the mother nonterminal is one more than the sum of  POLY's. Thus, if\\n\\nis a syntactic rule of the AVG, then\\n\\nis a syntactic rule of the HP-AVG, where\\n\\nNow, a derivation for the HP-AVG starts with a nondeterministic construction of a counter  SIZE with value n and a counter POLY with value O(n[k]). Then, the derivation of the original AVG is simulated, such that (i) the mother nonterminal produces a string of size n if, and only if the daughter nonterminals together produce a string of size n, and (ii) the mother nonterminal makes n[k]+1 derivation steps if, and only if the daughter nonterminals together make n[k] derivation steps.\\n\\nFootnotes\\n\\nThe author was supported in part by HCM grant ERB4050PL93-0516. The author was supported by the Foundation for language, speech and logic (TSL), which is funded by the Netherlands organization for scientific research (NWO)\", metadata={'source': '../data/raw/cmplg-xml/9503021.xml'}),\n",
       " Document(page_content=\"Text Chunking using Transformation\\n\\n\\n\\nBased Learning\\n\\nEric Brill introduced transformation-based learning and showed that it can do part-of-speech tagging with fairly high accuracy. The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ``baseNP'' chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that partition the sentence. Some interesting adaptations to the transformation-based learning approach are also suggested by this application.\\n\\nIntroduction\\n\\nText Chunking\\n\\nExisting Chunk Identification Techniques\\n\\nExisting efforts at identifying chunks in text have been focused primarily on low-level noun group identification, frequently as a step in deriving index terms, motivated in part by the limited coverage of present broad-scale parsers when dealing with unrestricted text. Some researchers have applied grammar-based methods, combining lexical data with finite state or other grammar constraints, while others have worked on inducing statistical models either directly from the words or from automatically assigned part-of-speech classes.\\n\\nRunning Church's program on test material, however, reveals that the definition of NP embodied in Church's program is quite simplified in that it does not include, for example, structures or words conjoined within NP by either explicit conjunctions like ``and'' and ``or'', or implicitly by commas. Church's chunker thus assigns the following NP chunk structures:\\n\\n[a Skokie] , [Ill.] , [subsidiary] [newer] , [big-selling prescriptions drugs] [the inefficiency] , [waste] and [lack] of [coordination] [Kidder] , [Peabody]   [Co]\\n\\nIt is difficult to compare performance figures between studies; the definitions of the target chunks and the evaluation methodologies differ widely and are frequently incompletely specified. All of the cited performance figures above also appear to derive from manual checks by the investigators of the system's predicted output, and it is hard to estimate the impact of the system's suggested chunking on the judge's determination. We believe that the work reported here is the first study which has attempted to find NP chunks subject only to the limitation that the structures recognized do not include recursively embedded NPs, and which has measured performance by automatic comparison with a preparsed corpus.\\n\\nDeriving Chunks from Treebank Parses\\n\\nThe goal of the ``baseNP'' chunks was to identify essentially the initial portions of non-recursive noun phrases up to the head, including determiners but not including postmodifying prepositional phrases or clauses. These chunks were extracted from the Treebank parses, basically by selecting NPs that contained no nested NPs. The handling of conjunction followed that of the Treebank annotators as to whether to show separate baseNPs or a single baseNP spanning the conjunction. Possessives were treated as a special case, viewing the possessive marker as the first word of a new baseNP, thus flattening the recursive structure in a useful way. The following sentences give examples of this baseNP chunk structure: During [N the third quarter N] , [N Compaq N] purchased [N a former Wang Laboratories manufacturing facility N] in [N Sterling N] , [N Scotland N] , which will be used for [N international service and repair operations N] .\\n\\n[N The government N] has [N other agencies and instruments N] for pursuing [N these other objectives N] .\\n\\nEven [N Mao Tse-tung N] [N 's China N] began in [N 1949 N] with [N a partnership N] between [N the communists N] and [N a number N] of [N smaller , non-communist parties N] .\\n\\nThe chunks in the partitioning chunk experiments were somewhat closer to Abney's model, where the prepositions in prepositional phrases are included with the object NP up to the head in a single N-type chunk. This created substantial additional ambiguity for the system, which had to distinguish prepositions from particles. The handling of conjunction again follows the Treebank parse with nominal conjuncts parsed in the Treebank as a single NP forming a single N chunk, while those parsed as conjoined NPs become separate chunks, with any coordinating conjunctions attached like prepositions to the following N chunk.\\n\\nThe portions of the text not involved in N-type chunks were grouped as chunks termed V-type, though these ``V'' chunks included many elements that were not verbal, including adjective phrases. The internal structure of these V-type chunks loosely followed the Treebank parse, though V chunks often group together elements that were sisters in the underlying parse tree. Again, the possessive marker was viewed as initiating a new N-type chunk. The following sentences are annotated with these partitioning N and V chunks: [N Some bankers N]  [V are reporting V] [N more inquiries than usual N] [N about CDs N] [N since Friday N] .\\n\\n[N Eastern Airlines N] [N ' creditors N] [V have begun exploring V] [N alternative approaches N] [N to a Chapter 11 reorganization N] [V because V] [N they N] [V are unhappy V] [N with the carrier N] [N 's latest proposal N] .\\n\\n[N Indexing N] [N for the most part N] [V has involved simply buying V] [V and then holding V] [N stocks N] [N in the correct mix N] [V to mirror V] [N a stock market barometer N] .\\n\\nThese two kinds of chunk structure derived from the Treebank data were encoded as chunk tags attached to each word and provided the targets for the transformation-based learning.\\n\\nThe Transformation\\n\\n\\n\\nbased Learning Paradigm\\n\\nTo learn a model, one first applies the baseline heuristic to produce initial hypotheses for each site in the training corpus. At each site where this baseline prediction is not correct, the templates are then used to form instantiated candidate rules with patterns that test selected features in the neighborhood of the word and actions that correct the currently incorrect tag assignment. This process eventually identifies all the rule candidates generated by that template set that would have a positive effect on the current tag assignments anywhere in the corpus.\\n\\nThose candidate rules are then tested against the rest of corpus, to identify at how many locations they would cause negative changes. One of those rules whose net score (positive changes minus negative changes) is maximal is then selected, applied to the corpus, and also written out as the first rule in the learned sequence. This entire learning process is then repeated on the transformed corpus: deriving candidate rules, scoring them, and selecting one with the maximal positive effect. This process is iterated, leading to an ordered sequence of rules, with rules discovered first ordered before those discovered later. The predictions of the model on new text are determined by beginning with the baseline heuristic prediction and then applying each rule in the learned rule sequence in turn.\\n\\nTransformational Text Chunking\\n\\nThis section discusses how text chunking can be encoded as a tagging problem that can be conveniently addressed using transformational learning. We also note some related adaptations in the procedure for learning rules that improve its performance, taking advantage of ways in which this task differs from the learning of part-of-speech tags.\\n\\nEncoding Choices\\n\\nIn the baseNP experiments aimed at non-recursive NP structures, we use the chunk tag set {I, O, B}, where words marked I are inside some baseNP, those marked O are outside, and the B tag is  used to mark the left most item of a baseNP which immediately follows another baseNP. In these tests, punctuation marks were tagged in the same way as words.\\n\\nIn the experiments that partitioned text into N and V chunks, we use the chunk tag set {BN, N, BV, V, P}, where BN marks the first word and N the succeeding words in an N-type group while BV and V play the same role for V-type groups. Punctuation marks, which are ignored in Abney's chunk grammar, but which the Treebank data treats as normal lexical items with their own part-of-speech tags, are unambiguously assigned the chunk tag P. Items tagged P are allowed to appear within N or V chunks; they are irrelevant as far as chunk boundaries are concerned, but they are still available to be matched against as elements of the left hand sides of rules.\\n\\nEncoding chunk structure with tags attached to words rather than non-recursive bracket markers inserted between words has the advantage that it limits the dependence between different elements of the encoded representation. While brackets must be correctly paired in order to derive a chunk structure, it is easy to define a mapping that can produce a valid chunk structure from any sequence of chunk tags; the few hard cases that arise can be handled completely locally. For example, in the baseNP tag set, whenever a B tag immediately follows an O, it must be treated as an I, and, in the partitioning chunk tag set, wherever a V tag immediately follows an N tag without any intervening BV, it must be treated as a BV.\\n\\nBaseline System\\n\\nRule Templates\\n\\nIn transformational learning, the space of candidate rules to be searched is defined by a set of rule templates that each specify a small number of particular feature sets as the relevant factors that a rule's left-hand-side pattern should examine, for example, the part-of-speech tag of the word two to the left combined with the actual word one to the left. In the preliminary scan of the corpus for each learning pass, it is these templates that are applied to each location whose current tag is not correct, generating a candidate rule that would apply at least at that one location, matching those factors and correcting the chunk tag assignment.\\n\\nThe same 10 patterns can also be used to match against part-of-speech tags, encoded as P0, P-1, etc. (In other tests, we have explored mixed templates, that match against both word and part-of-speech values, but no mixed templates were used in these experiments.) These 20 word and part-of-speech patterns were then combined with each of the 5 different chunk tag patterns shown on the right side of the table. The cross product of the 20 word and part-of-speech patterns with the 5 chunk tag patterns determined the full set of 100 templates used.\\n\\nAlgorithm Design Issues\\n\\nThe large increase in the number of rule templates in the text chunking application when compared to part-of-speech tagging pushed the training process against the available limits in terms of both space and time, particularly when combined with the desire to work with the largest possible training sets. Various optimizations proved to be crucial to make the tests described feasible.\\n\\nOrganization of the Computation\\n\\nThe power of that approach is dependent on the fact that the confusion matrix for part-of-speech tagging partitions the space of candidate rules into a relatively large number of classes, so that one is likely to be able to exclude a reasonably large portion of the search space. In a chunk tagging application, with only 3 or 4 tags in the effective tagset, this approach based on the confusion matrix offers much less benefit.\\n\\nHowever, even though the confusion matrix does not usefully subdivide the space of possible rules when the tag set is this small, it is still possible to apply a similar optimization by sorting the entire list of candidate rules on the basis of their positive scores, and then processing the candidate rules (which means determining their negative scores and thus their net scores) in order of decreasing positive scores. By keeping track of the rule with maximum benefit seen so far, one can be certain of having found one of the globally best rules when one reaches candidate rules in the sorted list whose positive score is not greater than the net score of the best rule so far.\\n\\nIndexing Static Rule Elements\\n\\nHowever, it is possible to construct a limited index that lists for each candidate rule those locations in the corpus at which the static portions of its left-hand-side pattern match. Because this index involves only the stable word identity and part-of-speech tag values, it does not require updating; thus it can be stored more compactly, and it is also not necessary to maintain back pointers from corpus locations to the applicable rules. This kind of partial static index proved to be a significant advantage in the portion of the program where candidate rules with relatively high positive scores are being tested to determine their negative scores, since it avoids the necessity of testing such rules against every location in the corpus.\\n\\nHeuristic Disabling of Unlikely Rules\\n\\nWe also investigated a new heuristic to speed up the computation: After each pass, we disable all rules whose positive score is significantly lower than the net score of the best rule for the current pass. A disabled rule is then reenabled whenever enough other changes have been made to the corpus that it seems possible that the score of that rule might have changed enough to bring it back into contention for the top place. This is done by adding some fraction of the changes made in each pass to the positive scores of the disabled rules, and reenabling rules whose adjusted positive scores came within a threshold of the net score of the successful rule on some pass.\\n\\nNote that this heuristic technique introduces some risk of missing the actual best rule in a pass, due to its being incorrectly disabled at the time. However, empirical comparisons between runs with and without rule disabling suggest that conservative use of this technique can produce an order of magnitude speedup while imposing only a very slight cost in terms of suboptimality of the resulting learned rule sequence.\\n\\nResults\\n\\nThe first line in each table gives the performance of the baseline system, which assigned a baseNP or chunk tag to each word on the basis of the POS tag assigned in the prepass. Performance is stated in terms of recall (percentage of correct chunks found) and precision (percentage of chunks found that are correct), where both ends of a chunk had to match exactly for it to be counted. The raw percentage of correct chunk tags is also given for each run, and for each performance measure, the relative error reduction compared to the baseline is listed. The partitioning chunks do appear to be somewhat harder to predict than baseNP chunks. The higher error reduction for the former is partly due to the fact that the part-of-speech baseline for that task is much lower.\\n\\nAnalysis of Initial Rules\\n\\nContribution of Lexical Templates\\n\\nFrequent Error Classes\\n\\nA rough hand categorization of a sample of the errors from a baseNP run indicates that many fall into classes that are understandably difficult for any process using only local word and part-of-speech patterns to resolve. The most frequent single confusion involved words tagged VBG and VBN, whose baseline prediction given their part-of-speech tag was O, but which also occur frequently inside baseNPs. The system did discover some rules that allowed it to fix certain classes of VBG and VBN mistaggings, for example, rules that retagged VBNs as I when they preceded an NN or NNS tagged I. However, many also remained unresolved, and many of those appear to be cases that would require more than local word and part-of-speech patterns to resolve.\\n\\nThe second most common class of errors involved conjunctions, which, combined with the former class, make up half of all the errors in the sample. The Treebank tags the words ``and'' and frequently ``,'' with the part-of-speech tag CC, which the baseline system again predicted would fall most often outside of a baseNP. However, the Treebank parses do also frequently classify conjunctions of Ns or NPs as a single baseNP, and again there appear to be insufficient clues in the word and tag contexts for the current system to make the distinction. Frequently, in fact, the actual choice of structure assigned by the Treebank annotators seemed largely dependent on semantic indications unavailable to the transformational learner.\\n\\nFuture Directions\\n\\nWe are planning to explore several different paths that might increase the system's power to distinguish the linguistic contexts in which particular changes would be useful. One such direction is to expand the template set by adding templates that are sensitive to the chunk structure. For example, instead of referring to the word two to the left, a rule pattern could refer to the first word in the current chunk, or the last word of the previous chunk. Another direction would be to enrich the vocabulary of chunk tags, so that they could be used during the learning process to encode contextual features for use by later rules in the sequence.\\n\\nWe would also like to explore applying these same kinds of techniques to building larger scale structures, in which larger units are assembled or predicate/argument structures derived by combining chunks. One interesting direction here would be to explore the use of chunk structure tags that encode a form of dependency grammar, where the tag ``N+2'' might mean that the current word is to be taken as part of the unit headed by the N two words to the right.\\n\\nConclusions\\n\\nBy representing text chunking as a kind of tagging problem, it becomes possible to easily apply transformation-based learning. We have shown that this approach is able to automatically induce a chunking model from supervised training that achieves recall and precision of 92% for baseNP chunks and 88% for partitioning N and V chunks. Such chunking models provide a useful and feasible next step in textual interpretation that goes beyond part-of-speech tagging, and that serve as a foundation both for larger-scale grouping and for direct extraction of subunits like index terms. In addition, some variations in the transformation-based learning algorithm are suggested by this application that may also be useful in other settings.\\n\\nAcknowledgments\\n\\nWe would like to thank Eric Brill for making his system widely available, and Ted Briscoe and David Yarowsky for helpful comments, including the suggestion to test the system's performance without lexical rule templates.\\n\\nBibliography\\n\\nAbney, Steven. 1991. Parsing by chunks. In Berwick, Abney, and Tenny, editors, Principle-Based Parsing. Kluwer Academic Publishers.\\n\\nBourigault, D. 1992. Surface grammatical analysis for the extraction of terminological noun phrases. In Proceedings of the Fifteenth International Conference on Computational Linguistics, pages 977-981.\\n\\nBrill, Eric. 1993a. Automatic grammar induction and parsing free text: A transformation-based approach. In Proceedings of the DARPA Speech and Natural Language Workshop, 1993, pages 237-242.\\n\\nBrill, Eric. 1993b. A Corpus-Based Approach to Language Learning. Ph.D. thesis, University of Pennsylvania.\\n\\nBrill, Eric. 1993c. Rule based tagger, version 1.14. Available from ftp.cs.jhu.edu in the directory /pub/brill/programs/.\\n\\nBrill, Eric. 1994. Some advances in transformation-based part of speech tagging. In Proceedings of the Twelfth National Conference on Artificial Intelligence, pages 722-727. (cmp-lg/9406010).\\n\\nBrill, Eric and Philip Resnik. 1994. A rule-based approach to prepositional attachment disambiguation. In Proceedings of the Sixteenth International Conference on Computational Linguistics. (cmp-lg/9410026).\\n\\nChurch, Kenneth. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Second Conference on Applied Natural Language Processing. ACL.\\n\\nEjerhed, Eva I. 1988. Finding clauses in unrestricted text by finitary and stochastic methods. In Second Conference on Applied Natural Language Processing, pages 219-227. ACL.\\n\\nGee, James Paul and Franois Grosjean. 1983. Performance structures: A psycholinguistic and linguistic appraisal. Cognitive Psychology, 15:411-458.\\n\\nKupiec, Julian. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proceedings of the 31st Annual Meeting of the ACL, pages 17-22.\\n\\nMarcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: A revised corpus design for extracting predicate argument structure. In Human Language Technology, ARPA March 1994 Workshop. Morgan Kaumann.\\n\\nRamshaw, Lance A. and Mitchell P. Marcus. 1994. Exploring the statistical derivation of transformational rule sequences for part-of-speech tagging. In Proceedings of the ACL Balancing Act Workshop on Combining Symbolic and Statistical Approaches to Language, pages 86-95. (cmp-lg/9406011).\\n\\nVoutilainen, Atro. 1993. NPTool, a detector of English noun phrases. In Proceedings of the Workshop on Very Large Corpora, pages 48-57. ACL, June. (cmp-lg/9502010).\\n\\nFootnotes\\n\\nThis heuristic fails in some cases. For example, Treebank uses the label NAC for some NPs functioning as premodifiers, like ``Bank of England'' in ``Robin Leigh-Pemberton, Bank of England governor, conceded..''; in such cases, ``governor'' is not included in any baseNP chunk. Non-constituent NP conjunction, which Treebank labels NX, is another example that still causes problems. Note that this is one of the cases where Church's chunker allows separate NP fragments to count as chunks.\", metadata={'source': '../data/raw/cmplg-xml/9505040.xml'}),\n",
       " Document(page_content=\"Free-ordered CUG on Chemical Abstract Machine\\n\\nWe propose a paradigm for concurrent natural language generation. In order to represent grammar rules distributively, we adopt categorial unification grammar (CUG) where each category owns its functional type. We augment typed lambda calculus with several new combinators, to make the order of lambda-conversions free for partial / local processing. The concurrent calculus is modeled with Chemical Abstract Machine. We show an example of a Japanese causative auxiliary verb that requires a drastic rearrangement of case domination.\\n\\nIntroduction\\n\\nParallel and distributed computation is expected to be the main stream of information processing. In the conventional generation, the rules for composition are given from the outside and those rules control all the behavior of the symbols or the objects, for assembling a hierarchical tree structure. For example, all the linguistic objects, such as words and phrases must be applied to so-called grammar rules to form grammatical structures or rational semantic representations, under a strict controller process. However, this kind of formalization obviously contradicts the partial / distributed processing that would be required in parallel architecture in future.\\n\\nIn order to represent grammar rules distributively, we adopt categorial grammar, where we can an attach local grammar rule to each word and phrase. What we aim in this paper is to propose a paradigm that enables partial / local generation through decompositions and reorganizations of tentative local structures.\\n\\nIn the following section, we introduce the extended lambda-calculus. Thereafter we introduce the ChAM model and we reinterpret the model in terms of natural language processings. Then we show the model of membrane interaction model with the example of Japanese causative sentence that requires drastic change of domination of cases. Finally we will discuss the future of the model.\\n\\nExtended typed lambda\\n\\n\\n\\ncalculus\\n\\nLambda\\n\\n\\n\\ncalculus of polymorphic type\\n\\nWe use greek letters, for type schemas. For type constants we use\\n\\nwhile for type variables we use\\n\\nrepresents that the object a is of type\\n\\n.\\n\\nIf\\n\\nand\\n\\nare types, then\\n\\nis a type.\\n\\nThe purpose of type inference is to infer the type of an object from a set of objects whose types are known. We presuppose that two type variables\\n\\nand\\n\\nare unified with a unifier\\n\\n.\\n\\nWe\\n\\nuse\\n\\nfor this set of type-known objects. The most important two rules are as follows:\\n\\nExtended combinators\\n\\nC-combinator changes the order of lambda-variables as follows:\\n\\nAnother requirement for exchanges of the order of lambda-conversion is the following case. Suppose that we are required to compose all the following typed objects:\\n\\nIn such a case, we need to concatenate g and a first, and then g(a)becomes applicable to f. However, with the help of the following B-combinator:\\n\\nCost of unification\\n\\nwhere\\n\\nrepresents a unifier of two DAG's: one's syntactic case is x and the other's semantic role is y. k is some constant larger than 1 (k ] 1).\\n\\nChemical Abstract Machine\\n\\nWe assume the process of natural language recognition as follows. Whenever a linguistic object is recognized, it is thrown into the solution of ChAM, and acts as a  molecule. Verbs and some other auxiliary verbs introduces  membranes. These membranes becomes their scopes for case (or role) domination; namely, each verb searches for molecules (noun phrases) that are necessary to satisfy each verb's case (role) frame, within its membrane. In some occasions, if multiple verbs exist in one sentence, they may conflict as to which verb dominates which noun phrase. In such a case, two membranes can interact and can exchange some molecules.\\n\\nWe use\\n\\nfor membranes. When a membrane si contains a molecule\\n\\n,\\n\\nwe\\n\\ndenote as\\n\\nThe supporting relation (\\n\\n) can be interpreted as an inclusion relation (\\n\\n) in this case. Two membranes can interact when they contact with the notation `\\n\\n', as\\n\\n. If there is a floating molecule (that which is not yet concatenated with other molecules) on one side, it can move through the porous membranes. Valences for concatenation of each molecule are represented by typed lambda-variables. If one membrane contains only one composite structure, and it still has surplus valences, we can regard that whole the membrane has those surplus valences as follows.\\n\\nNow, we will apply our notions above to the actual problem of sentence generation.\\n\\nExample: Japanese causative sentence\\n\\n' means that each word is recognized in the general world however a verb `yomu' introduced a special membrane s1 as a subworld of W. Each DAG means a polymorphic type of the lexical item.\\n\\n's are the records of unification, that contain the costs and the original types; they become necessary when they are backtracked, and in that meaning, those bindings are transitive.\\n\\nNow, let us recapitulate what has occurred in the membrane s1. There were four lexical items in the set, and they are duly organized to a sentence and s1 becomes a singleton.\\n\\nthat it bit agent, so that the comparison should be made between\\n\\nand\\n\\n.\\n\\nHowever,\\n\\nbecause both of\\n\\nand\\n\\nConclusion\\n\\nIntroducing free-ordered typed lambda-calculus, together with the notion of unification costs in types, we have shown the structuring of natural language syntax, by distributively represented types in random orders. We adopted a model of Chemical Abstract Machine for the partial/ concurrent computation model.\\n\\nAlthough we introduced the concept of costs and termination was assured, the efficiency of constructing a parsing tree would be far slower than sequential processing. However our objective is not to propose a faster algorithm, but is to show the possibility of distributed processing of natural languages. We could show that natural language syntax is self-organizable, in that each linguistic objects do not need to be poured into `molds', viz., externally given grammar.\\n\\nBibliography\\n\\nG. Berry and G. Boudol. The chemical abstract machine. In 17th Annual ACM Symposium on Principles of Programming Languages, pages 81-93, 1990.\\n\\nH. B. Curry and R. Feys. Combinatory Logic, volume 1. North Holland, Amsterdam, Netherlands, 1968.\\n\\nD. Dowty. Type Raising, Functional Composition, and Non-constituent Conjunction - in Categorial Grammars and Natural Language Structures, pages 153-197. D. Reidel, 1988.\\n\\nJ. R. Hindley and Seldin J. P. Introduction to Combination and\\n\\nCalculus.\\n\\nCambridge University Press, 1986.\\n\\nS. M. Shieber.\\n\\nAn Introduction to Unification\\n\\n\\n\\nBased Approaches to Grammar.\\n\\nCSLI, Stanford University, 1986.\\n\\nM. Steedman. Combinators and grammars - in Categorial Grammars and Natural Language Structures, pages 417-442. D. Reidel, 1988.\\n\\nS. Tojo. Categorial analysis of sentence generation. In The Logic Programming Conference (LPC '91), pages 229-238. Institute of New Generation Computer (ICOT), 1991.\\n\\nH. Uszkoreit. Categorial unification grammars. In Proc. of COLING '86, pages 187-194, 1986.\\n\\nT. Gunji.\\n\\nJapanese Phrase Structure Grammar.\\n\\nD. Reidel, 1987.\\n\\nFootnotes\\n\\nare for\\n\\nand for\\n\\n,\\n\\nrespectively.\\n\\nunifies\\n\\nwhich appears in both type declarations.\\n\\nare for\\n\\nand for\\n\\n,\\n\\nrespectively.\\n\\nunifies\\n\\nwhich appears in both type declarations.\", metadata={'source': '../data/raw/cmplg-xml/9411021.xml'}),\n",
       " Document(page_content='Resolving Anaphors in Embedded Sentences\\n\\nWe propose an algorithm to resolve anaphors, tackling mainly the problem of intrasentential antecedents. We base our methodology on the fact that such antecedents are likely to occur in embedded sentences. Sidner\\'s focusing mechanism is used as the basic algorithm in a more complete approach. The proposed algorithm has been tested and implemented as a part of a conceptual analyser, mainly to process pronouns. Details of an evaluation are given.\\n\\nIntroduction\\n\\nWe first present how intrasentential antecedents occur in embedded sentences. We recall the main ideas of the focusing approach, then expand on the main hypotheses which led to the design of the anaphora resolution algorithm.\\n\\nIntrasentential Antecedents Embedded sentences and elementary events\\n\\nAn embedded sentence contains either more than one verb or a verb and derivations of other verbs (see sentence 1 with verbs said and forming). 1) Three of the world\\'s leading advertising groups, Agence Havas S.A. of France, Young  Rubicam of the U.S. and Dentsu Inc. of Japan, said they are forming a global advertising joint venture. Broadly speaking embedded sentences concern more than one fact. In sentence 1 there is the fact of saying something and that of forming a joint venture. We call such a fact an elementary event (EE hereafter). Thus an embedded sentence will contain several EEs.\\n\\nFactors that influence embedded sentences are mainly semantic features of verbs. For example the verb to say, that takes a sentence complement favours an introduction of a new fact, i.e., ``to say something\\'\\', and the related fact. There are other classes of verbs such as want to, hope that, and so on. In the following, subordinate phrases, like relative or causal sentences, will be also considered as embedded ones.\\n\\nEmbedded sentences with intrasentential antecedents\\n\\nFirst of all, we will distinguish the Possessive, Reciprocal and Reflexive pronouns (PRR hereafter) from the other pronouns (non-PRR hereafter).\\n\\nOn the basis of 120 articles, of 4 sentences on average, containing 332 pronouns altogether, we made the following assumption (1): Assumption : non-PRR pronouns can have intrasentential antecedents, only if these pronouns occur in an embedded sentence. The statistics below show that of 262 non-PRR pronouns, there are 244 having intrasentential antecedents, all of which occur in embedded sentences and none in a ``simple\\'\\' sentence. The remaining 18 non-PRR pronouns have intersentential antecedents. Pronouns \\t\\t                      332non-PRR \\t\\t                      262With intrasentential antecedents \\t\\t   244in an embedded sentence \\t\\t With intrasentential in a simple \\t\\t      0sentence \\t\\t With intersentential antecedents \\t\\t       18\\n\\nOur assumption means that, while the PRR pronouns may find their antecedents in an non embedded sentence (e.g., sentences 2 and 3) the non-PRR pronouns can not. 2) Vulcan made  its initial Investment in Telescan in May, 1992. 3) The agencies HCM and DYR are themselves joint ventures. Without jumping to conclusions, we cannot avoid making a parallel with the topological relations defined in the binding theory (Chomsky, 1980), between two coreferring phrases in the syntactic tree level. Assumption 1 redefines these relations in an informal and less rigorous way, at the semantic level, i.e., considering semantic parameters such as the type of verbs that introduce embedded sentences.\\n\\nUsing Sidner\\'s Focusing Approach\\n\\nTo resolve anaphors one of the most suitable existing approaches when dealing with anaphor issues in a conceptual analysis process is the focusing approach proposed by Sidner. However, this mechanism is not suitable for intrasentential cases. We propose to exploit its main advantages in order to build our anaphora resolution mechanism extending it to deal also with intrasentential antecedents.\\n\\nThe expected focus algorithm that selects an initial focus called the \"expected focus\". This selection may be ``confirmed\\'\\' or ``rejected\\'\\' in subsequent sentences. The expected focus is generally chosen on the basis of the verb semantic categories. There is a preference in terms of thematic position: the ``theme\\'\\' (as used by Gruber and Anderson, 1976 for the notion of the object case of a verb) is the first, followed by the goal, the instrument and the location ordered according to their occurrence in the sentence; the final item is the agent that is selected when no other role suits.\\n\\nThe anaphora interpreter uses the state of the focus and a set of algorithms associated with each anaphor type to determine which element of the data structures is the antecedent. Each algorithm is a filter containing several interpretation rules (IR). Each IR in the algorithm appropriate to an anaphor suggests one or several antecedents depending on the focus and on the anaphor type.\\n\\nAn evaluation of the proposed antecedents is performed using different kinds of criteria (syntactic, semantic, inferential, etc.)\\n\\nThe focusing algorithm makes use of data structures, i.e., the focus registers that represent the state of the focus: the current focus (CF) representation, alternate focus list (AFL) that contains the other phrases of the sentence and the focus stack (FS). A parallel structure to the CF is also set to deal with the agentive pronouns. The focusing algorithm updates the state of the focus after each sentence anaphor (except the first sentence). After the first sentence, it confirms or rejects the predicted focus taking into account the results of anaphor interpretation. In the case of rejection, it determines which phrase is to move into focus.\\n\\nThis is a brief example (Sidner 1983) : a Alfred and Zohar liked to play baseball. b They played it every day after school before dinner. c After their game, Alfred and Zohar had ice cream cones. d They tasted really good.\\n\\nIn a) the expected focus is ``baseball\\'\\' (the theme)\\n\\nIn b) ``it\\'\\' refers to ``baseball\\'\\' (CF). ``they\\'\\' refers to Alfred and Zohar (AF)\\n\\nThe focusing algorithm confirms the CF.\\n\\nIn d) ``they\\'\\' refers to ``ice cream cones\\'\\' in AFL.\\n\\nThe focusing algorithm decides that since no anaphor refers to the CF, the CF is stacked and ``ice cream cones\\'\\' is the new CF (focus movement).\\n\\nthe focusing algorithm\\n\\nfollowed by the interpretation of anaphors,\\n\\nthen by the evaluation of the proposed antecedents.\\n\\nWhat needs to be improved in the focusing approach? Intrasentential antecedents Initial Anaphors\\n\\nThe focusing mechanism fails in the expected focus algorithm when encountering anaphors occurring in the first sentence of a text, which we call initial anaphors, such as They in sentence (1). The problem with initial anaphors is that the focus registers cannot be initialised or may be wrongly filled if there are anaphors inside the first sentence of the text. It is clear that taking the sentence in its classical meaning as the unit of processing in the focusing approach, is not suitable when sentences are embedded.\\n\\nWe will focus on the mechanisms and algorithmic aspects of the resolution (how to fill the registers, how to structure algorithms, etc.) and not on the rule aspects, like how IRs decide to choose Bill and not John (sentence 4).\\n\\nOur Solution\\n\\nAs stated above, embedded sentences include several elementary events (EEs). EEs are represented as conceptual entities in our work. We consider that such successive EEs involve the same context that is introduced by several successive short sentences. Moreover, our assumption states that when non-PRR anaphors have intrasentential antecedents, they occur in embedded sentences. Starting with these considerations, the algorithm is governed by the hypotheses expanded below.\\n\\nMain hypotheses\\n\\nFirst hypothesis : EE is the unit of processing in the basic focusing cycle. An EE is the unit of processing in our resolution algorithm instead of the sentence. The basic focusing cycle is applied on each EE in turn and not sentence by sentence. Notice that a simple sentence coincides with its EE.\\n\\nSecond hypothesis : The ``initial\\'\\'  EE of a well formed first sentence does not contain non-PRR pronouns just as an initial simple sentence cannot.\\n\\nFor example, in the splitting of sentence 1 into two EEs (see below), EE1 does not contain non-PRR pronouns because it is the initial EE of the whole discourse.\\n\\nEE1) ``Three of the world\\'s leading advertising groups, Agence Havas S.A. of France, Young  Rubicam of the U.S. and Dentsu Inc. of Japan, said\\'\\' EE2) ``they are forming a global advertising joint venture.\\'\\'\\n\\nThird hypothesis : PRR pronouns require special treatment.\\n\\nPRR could refer to intrasentential antecedents in simple sentences (such as in those of sentences 3 and 4). An initial EE could then contain an anaphor of the PRR type. Our approach is to add a special phase that resolves first the PRRs occurring in the initial EE before applying the expected focusing algorithm on the same initial EE. In all other cases, PRRs are treated equally to other pronouns.\\n\\nThis early resolution relies on the fact that the PRR pronouns may refer to the agent, as in sentence 3, as well as to the complement phrases. However the ambiguity will not be huge at this first level of the treatment. Syntactic and semantic features can easily be used to resolve these anaphors. This relies also on the fact that the subject of the initial EE cannot be a pronoun (second hypothesis).\\n\\nHaving mentioned this particular case of PRR in initial EE, we now expand on the whole algorithm of resolution.\\n\\nThe Algorithm\\n\\napplying the resolution rules,\\n\\napplying the focusing algorithm, i.e., updating the focus registers\\n\\nthe evaluation of the proposed antecedents for each anaphor.\\n\\nThe algorithm is based on the decomposition of the sentence into EEs and the application of the basic focusing cycle on each EE in turn and not sentence by sentence.\\n\\nsplit the sentence into EEs\\n\\napply Step 3 then Step 4.\\n\\nMain Results : 1. Intrasentential antecedents are taken into account when applying the focusing algorithm. For example, in sentence 1, the intrasentential antecedent Bill will be taken into account, because EE1 would be processed beforehand by the expected focusing algorithm. 2. The problem of initial anaphors is then resolved. The expected focusing algorithm is applied only on the initial EE which must not contain anaphors.\\n\\nExamples and results\\n\\nTo illustrate the algorithm, let\\'s consider the following sentence : Lafarge Coppee said it would buy 10 percent in National Gypsum, the number two plasterboard company in the US, a purchase which allows it to be present on the world\\'s biggest plasterboard market. At the conceptual level, there are 3 EEs. They are involved respectively by the said, buy, and allows verbs. They correspond respectively to the following surface sentences: EE1 ``Lafarge Coppee said\\'\\' EE2 ``it would buy 10 percent in National Gypsum, the number two plasterboard company in the US\\'\\' EE3 ``a purchase which allows it to be present on the world\\'s biggest plasterboard market.\\'\\'\\n\\nthe expected focusing algorithm is applied to the first EE, EE1, which contains non-PRR anaphors.\\n\\nEE2 contains only one pronoun it, which is resolved by the basic focusing cycle\\n\\nit in EE3 will be resolved in the same way.\\n\\n5) Mary sacked out in his apartment before Sam could kick her out. 6) Girls who he has dated say that Sam is charming.\\n\\nOur algorithm fails in resolving his in 5, because the algorithm searches only for the entities that preceed the anaphor in the text. The same applies for he in 6. However improving our algorithm to process classical cases of cataphors, such as those in sentence 6, should not require major modifications, only change in the order in which the EEs are searched through.\\n\\nFor example, to process pronouns of the sentence 6 split into two EES (see below), the algorithm must consider EE2 before EE1. This means applying the step 2 of the algorithm to EE2, then step 3 to EE1. The sentence 5 should require specific treatment, though.\\n\\nEE1) ``that Sam is charming\\'\\' EE2) ``Girls who he has dated say\\'\\'\\n\\nHobbs also pointed out the cases of ``picture noun\\'\\' examples, as in sentences 7 and 8: 7) John saw a picture of him. 8) John\\'s father\\'s portrait of him. In 7 our algorithm is successful, i.e., it will not identify him with John because of our previous assumption (section 2.2). However our algorithm would fail in 8 because the non-PRR pronoun him could refer to John which occurs in the same EE.\\n\\nFor example in the embedded sentence 9 where either the reflexive (himself) or non reflexive pronouns (him) may be used, it is more natural to make use of him. 9) John claimed that the picture of him hanging in the post office was a fraud.\\n\\nThe Conceptual Level\\n\\nWe comment here on the main aspects of the conceptual analysis that are related to the anaphora resolution process. They concern mainly the way of splitting embedded sentences and the problems of determining the theme and of managing the other ambiguities and the several readings.\\n\\nThe conceptual analyser\\'s strategy consists of a continuous step-by-step translation of the original natural language sentences into conceptual structures (CS hereafter). This translation uses the results of the syntactic analysis (syntactic tree). It is a progressive substitution of the NL terms located in the syntactic tree with concepts and templates of the conceptual representation language. Triggering rules are evoked by words of the sentence and allow the activation of well-formed CS templates when the syntactico-semantic filter is unified with the syntactic tree. The values caught by the filter variables are the arguments of the CS roles, i.e., they fill the CS roles. If they are anaphors, they are considered to be unbound variables and result in unfilled roles in the CS. The anaphora resolution aims therefore at filling the unfilled roles with the corresponding antecedents.\\n\\nSplitting into EE move information (from ``to say\\'\\'),\\n\\nproduce an agreement (from ``to agree\\'\\'),\\n\\nproduce a joint venture (from ``to form\\'\\').\\n\\nDetermining the theme Managing other ambiguities Multiple readings The set of conceptual structures for the current reading Ri on which the resolution is performed, given that several readings could arise from previous ambiguity processing.\\n\\nThe set of conceptual structures of the current sentence Si where the anaphor occurs;\\n\\nThe set of conceptual structures of the current elementary event EEi where the anaphor occurs after the Si splitting.\\n\\nThe state of the focus (content of the registers), SFi\\n\\nConclusion\\n\\nWe have proposed a methodology to resolve anaphors occurring in embedded sentences. The main idea of the methodology is the use of other kinds of restrictions between the anaphor and its antecedents than the syntactic ones. We demonstrated that anaphors with intrasentential antecedents are closely related to embedded sentences and we showed how to exploit this data to design the anaphora resolution methodology. Mainly, we exploited Sidner\\'s focusing mechanism, refining the classical unit of processing, that is the sentence, to that of the elementary event. The algorithm has been implemented (in Common Lisp, Sun Sparc) to deal with pronouns as a part of a deep analyser. The main advantages of the proposed algorithm is that it is independent from the knowledge representation language used and the deep understanding approach in which it is integrated. Thus, it could be set up in any conceptual analyser, as long as a semantic representation of the text is available. Moreover Sidner\\'s approach does not impose its own formalisms (syntactic or semantic) for its application. The improvment of the proposed algorithm requires dealing with special cases of anaphors such as cataphors and also with specific cases which are not easily handled in the literature. For example, we saw that a solution to processing cataphors could be to reconsider the order in which the conceptual structures (elementary events beforehand) are searched.\\n\\nAcknowledgements\\n\\nThis work has been supported by the European Community Grant LE1-2238 (AVENTINUS project).\\n\\nBibliography\\n\\nAnderson, S.R. 1977. Formal syntax. In Wasow and Akmajian, editors, Comment on the paper by Wasow in Culicover. Academic Press, pages 361-376.\\n\\nAzzam, Saliha. 1995a. Computation of Ambiguities (Anaphors and PPs) in NL texts. CLAM : The prototype. Ph.D. thesis, Paris Sorbonne University.\\n\\nAzzam, Saliha. 1995b. Anaphors, pps and disambiguation process for conceptual analysis. In 14th International Joint Conference on Artificial Intelligence (IJCAI\\'95). San Mateo (CA): Morgan Kaufmann.\\n\\nCarter, David. 1987. Interpreting Anaphors in natural language texts. Chichester : Ellis Horwood.\\n\\nGruber, J.S. 1976. Lexical structures in syntax and semantics. North-Holland.\\n\\nHobbs, Jerry. 1985. Resolving pronoun references. In B. Grosz K. Sparck-Jones B. Webber, editor, Readings in Natural Language, volume 44. Morgan Kaufmann Publishers Los Altos California, pages 311-338.\\n\\nLappin, S. and H.J. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20:535-561.\\n\\nMerlo, P. 1993. For an incremental computation of intrasentential coreference. In the 13th International Joint Conference on Artificial Intelligence (IJCAI\\'93), pages 1216-1221. San Mateo (CA): Morgan Kaufmann.\\n\\nSidner, C. 1979. Toward a computation of intrasentential coreference. Technical Report TR-537, MIT. Artificial Intelligence Labratory.\\n\\nSidner, C. 1981. Focusing for interpretation of pronouns. American Journal of Computational Linguistics, 7:217-231.\\n\\nSidner, C. 1983. Focusing in the comprehension of definite anaphora. In Brady. M and Berwick R.C, editors, Computational Models of Discourse. Cambridge (MA) : The MIT Press.', metadata={'source': '../data/raw/cmplg-xml/9605007.xml'}),\n",
       " Document(page_content=\"Learning Dependencies between Case Frame Slots\\n\\nWe address the problem of automatically acquiring case frame patterns (selectional patterns) from large corpus data. In particular, we propose a method of learning dependencies between case frame slots. We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution, where random variables represent case slots. We then formalize the dependencies between case slots as the     probabilistic dependencies between these random variables. Since the number of parameters in a multi-dimensional joint distribution is exponential in general, it is infeasible to accurately estimate them in practice. To overcome this difficulty, we settle with approximating the target joint distribution by the product of     low order component distributions, based on corpus data. In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task. Our experimental results indicate that for certain classes of verbs, the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies.\\n\\nIntroduction\\n\\nWe address the problem of automatically acquiring case frame patterns (selectional patterns) from large corpus data. The acquisition of case frame patterns normally involves the following three subproblems: 1) Extracting case frames from corpus data, 2) Generalizing case frame slots within these case frames, 3) Learning dependencies that exist between these generalized case frame slots.\\n\\nIn this paper, we propose a method of learning dependencies between case frame slots. By `dependency' is meant the relation that exists between case slots which constrains the possible values assumed by each of those case slots. As illustrative examples, consider the following sentences.\\n\\nWe see that an `airline company' can be the subject of verb `fly' (the value of slot `arg1'), when the direct object (the value of slot `arg2') is an `airplane' but not when it is an `airline company'. These examples indicate that the possible values of case slots depend in general on those of the other case slots: that is, there exist `dependencies' between different case slots.\\n\\nThe knowledge of such dependencies is useful in various tasks in natural language processing, especially in analysis of sentences involving multiple prepositional phrases, such as\\n\\nNote in the above example that the slot of `from' and that of `to' should be considered dependent and the attachment site of one of the prepositional phrases (case slots) can be determined by that of the other with high accuracy and confidence.\\n\\nIn this paper, we view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution, where random variables represent case slots. We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables. Since the number of parameters that exist in a multi-dimensional joint distribution is exponential if we allow n-ary dependencies in general, it is infeasible to estimate them with high accuracy with a data size available in practice. It is also clear that relatively few of these random variables (case slots) are actually dependent on each other with any significance. Thus it is likely that the target joint distribution can be approximated reasonably well by the product of component distributions of low order, drastically reducing the number of parameters that need to be considered. This is indeed the approach we take in this paper.\\n\\nProbabilistic Models for Case Frame Patterns\\n\\nis given a specific probability value by a word-based model. In contrast,\\n\\nis assigned a specific probability by a slot-based model.\\n\\nWe then formulate the dependencies between case slots as the   probabilistic dependencies between the random variables in each of these three models. In the absence of any constraints, however, the number of parameters in each of the above three models is exponential (even the slot-based model has O(2[n]) parameters ), and thus it is infeasible to estimate them in practice. A simplifying assumption that is often made to deal with this difficulty is that random variables (case slots) are mutually independent.\\n\\nSuppose for example that in the analysis of the sentence\\n\\nthe following alternative interpretations are given.\\n\\nWe wish to select the more appropriate of the two interpretations. A heuristic word-based method for disambiguation, in which the random variables (case slots) are assumed to be dependent, is to calculate the following values of word-based likelihood and to select the interpretation corresponding to the higher likelihood value.\\n\\nand\\n\\nThe independence assumption can also be made in the case of a class-based model or a slot-based model. For slot-based models, with the independence assumption, the following probabilities\\n\\nAssuming that random variables (case slots) are mutually independent would drastically reduce the number of parameters. (Note that under the independence assumption the number of parameters in a slot-based model becomes O(n).) As illustrated in Section 1, this assumption is not necessarily valid in practice.\\n\\nWhat seems to be true in practice is that some case slots are in fact dependent but overwhelming majority of them are independent, due partly to the fact that usually only a few case slots are obligatory and most others are optional. Thus the target joint distribution is likely to be approximable by the product of several component distributions of low order, and thus have in fact a reasonably small number of parameters. We are thus lead to the approach of approximating the target joint distribution by such a simplified model, based on corpus data.\\n\\nApproximation by Dendroid Distribution\\n\\nWithout loss of generality, any n-dimensional joint distribution can be written as\\n\\nfor some permutation (\\n\\nm1,m2,...mn) of 1,2,..,n, where we let\\n\\nP(Xm1|Xm0) denote\\n\\nP(Xm1).\\n\\nA plausible assumption on the dependencies between random variables is intuitively that each variable directly depends on at most one other variable. (Note that this assumption is the simplest among those that relax the independence assumption.) For example, if a joint distribution\\n\\nP(X1,X2,X3) over 3 random variables\\n\\nX1,X2,X3can be written (approximated) as follows, it (approximately) satisfies such an assumption.\\n\\nExperimental Results\\n\\nWe conducted some experiments to test the performance of the proposed method as a method of acquiring case frame patterns. In particular, we tested to see how effective the patterns acquired by our method are in a structural disambiguation experiment. We will describe the results of this experimentation in this section.\\n\\nExperiment 1: Slot\\n\\n\\n\\nbased Model\\n\\nExperiment 2: Class\\n\\n\\n\\nbased Model\\n\\nOur experimental result verifies the validity in practice of the assumption widely made in statistical natural language processing that class-based case slots (and also word-based case slots) are mutually independent, at least when the data size available is that provided by the current version of the Penn Tree Bank. This is an empirical finding that is worth noting, since up to now the independence assumption was based solely on human intuition, to the best of our knowledge.\\n\\nConclusions\\n\\nWe conclude this paper with the following remarks. 1. The primary contribution of research reported in this paper is that we have proposed a method of learning dependencies between case frame slots, which is theoretically sound and efficient, thus providing an effective tool for acquiring case dependency information. 2. For the slot-based model, sometimes case slots are found to be dependent. Experimental results demonstrate that using the dependency information, when dependency does exist, structural disambiguation results can be improved. 3. For the word-based or class-based models, case slots are judged independent, with the data size currently available in the Penn Tree Bank. This empirical finding verifies the independence assumption widely made in practice in statistical natural language processing.\\n\\nWe proposed to use dependency forests to represent case frame patterns. It is possible that more complicated probabilistic dependency graphs like Bayesian networks would be more appropriate for representing case frame patterns. This would require even more data and thus the problem of how to collect sufficient data would be a crucial issue, in addition to the methodology of learning case frame patterns as probabilistic dependency graphs. Finally the problem of how to determine obligatory/optional cases based on dependencies acquired from data should also be addressed.\\n\\nAcknowledgement\\n\\nWe thank Mr.K.Nakamura, Mr.T.Fujita, and Dr.K.Kobayashi of NEC CC Res. Labs. for their constant encouragement. We thank Mr.R.Isotani of NEC Information Technology Res. Labs. for his comments. We thank Ms. Y.Yamaguchi of NIS for her programming effort.\\n\\nBibliography\\n\\nHiyan Alshawi and David Carter. 1995. Training and scaling preference functions for disambiguation. Computational Linguistics, 20(4):635-648.\\n\\nLalit R. Bahl, Frederick Jelinek, and Robert Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transaction on Pattern Analysis and Machine Intelligence, 5(2):179-190.\\n\\nEric Brill and Philip Resnik. 1994. A rule-based approach to prepositional phrase attachment disambiguation. Proceedings of the 15th COLING, pages 1198-1204.\\n\\nJing-Shin Chang, Yih-Fen Luo, and Keh-Yih Su. 1992. GPSM: A generalized probabilistic semantic model for ambiguity resolution. Proceedings of the 30th ACL, pages 177-184.\\n\\nC.K. Chow and C.N. Liu. 1968. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 14(3):462-467.\\n\\nMichael Collins and James Brooks. 1995. Prepositional phrase attachment through a backed-off model. Proceedings of the 3rd Workshop on Very Large Corpora.\\n\\nThomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley  Sons Inc.\\n\\nWilliams A. Gale and Kenth W. Church. 1990. Poor estimates of context are worse than none. Proceedings of the DARPA Speech and Natural Language Workshop, pages 283-287.\\n\\nRalph Grishman and John Sterling. 1994. Generalizing automatically generated selectional patterns. Proceedings of the 15th COLING, pages 742-747.\\n\\nDonald Hindle and Mats Rooth. 1991. Structural ambiguity and lexical relations. Proceedings of the 29th ACL, pages 229-236.\\n\\nHang Li and Naoki Abe. 1995. Generalizing case frames using a thesaurus and the MDL principle. Proceedings of Recent Advances in Natural Language Processing, pages 239-248.\\n\\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The penn treebank. Computational Linguistics, 19(1):313-330.\\n\\nJudea Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers Inc.\\n\\nJ. Ross Quinlan and Ronald L. Rivest. 1989. Inferring decision trees using the minimum description length principle. Information and Computation, 80:227-248.\\n\\nAdwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994. A maximum entropy model for prepositional phrase attachment. Proceedings of ARPA Workshop on Human Language Technology, pages 250-255.\\n\\nPhilip Resnik. 1992. Semantic classes and syntactic ambiguity. Proceedings of ARPA Workshop on Human Language Technology.\\n\\nJorma Rissanen. 1978. Modeling by shortest data description. Automatic, 14:37-38.\\n\\nJorma Rissanen. 1983. A universal prior for integers and estimation by minimum description length. The Annals of Statistics, 11(2):416-431.\\n\\nJorma Rissanen. 1984. Universal coding, information, predication and estimation. IEEE Transaction on Information Theory, 30(4):629-636.\\n\\nJorma Rissanen. 1986. Stochastic complexity and modeling. The Annals of Statistics, 14(3):1080-1100.\\n\\nJorma Rissanen. 1989. Stochastic Complexity in Statistical Inquiry. World Scientific Publishing Co.\\n\\nSatoshi Sekine, Jeremy J. Carroll, Sofia Ananiadou, and Jun'ichi Tsujii. 1992. Automatic learning for semantic collocation. Proceedings of the 3rd Conference on Applied Natural Language Processing, pages 104-110.\\n\\nFrank Smadja. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143-177.\\n\\nJoe Suzuki. 1993. A construction of bayesian networks from databases based on an MDL principle. Proceedings of Uncertainty in AI '93.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9605013.xml'}),\n",
       " Document(page_content='A Support Tool for Tagset Mapping Motivation\\n\\nTagsets used in existing corpora have usually been designed to satisfy the needs of specific projects. A tagset used for robust parsing will tend to stress distributional properties, whereas a corpus within a lexical resource specially designed for human interaction (which might include a human oriented dictionary) will most likely distinguish word classes along traditional linguistic lines.\\n\\nObjectivisation and standardisation of similar information: Millions of words have been analysed in the past, using different annotation schemes. Especially the manually analysed linguistic data is expensive to produce and extremely valuable. With a standardised tagset, linguistic information from different corpora of the same language can be reused and thus merged into a large data base. Such data bases improve the performance of statistical methods and are a useful resource for the production of balanced corpora.\\n\\nShared use of language resources: Corpus manipulation tools such as retrieval tools can be applied to merged resources in a uniform format without much customisation. As well, users of these tools will find it easier to work with a corpus tagged in a standardised tagset. Now,  they have to memorize only one scheme of tag classes (class names, class semantics, exceptions), as opposed to several schemes for several corpora before.\\n\\nComparison of annotation schemes: A comparison of the granularity and degree of similarity of tagsets can be carried out more objectively, once the mapping results are available. The validation of the suggestions of the LRE-project EAGLES is an application in this field.\\n\\nWe believe that standards are important for the linguistic community, especially from the point of view of reusablility.\\n\\nOf course, there are limits: proposals for standard tagsets should be regarded as approaches towards a neutral platform between projects and different theories, rather than as ready-made tagsets that will never be changed. It is important indeed that standards and their support tools be flexible about possible extensions and improvements.\\n\\nA standardised tagset As the tagset is constraint-based, a flexible generalisation is possible over all atomic constraints and combinations of constraints. As a formal grammar is   used to define syntactically well-formed specifications of word forms, we can regard our standard tagset as a specification language. Example: The specification [pos = v  vtype = aux  pers = 3] denotes 3rd person auxiliary verbs.\\n\\nThe tagset is also typed, which adds to the naturalness of the specifications of wordforms and helps discover semantic errors in specifications (inconsistent combinations of features, wrong values for features). In our implementation, we follow the closed-world-assumption, which leads to a coherent interpretation for underspecified and/or negated descriptions. Example: [pos = v  (vform = fin |  case != gen)] is a syntactically correct, but ill-typed specification, as the Types v (Verb) and gen (Genitive) are not type compatible.\\n\\nThe tagset can be easily modified because its manually written definition is compiled into a system internal format. As the design of a tagset involves a cycle with feedback phases, including manual tagging and the writing of guidelines, there will be frequent   modifications to the tagset, especially in the initial phase.\\n\\nTag mapping: the problems\\n\\nMapping tags of an existing, flat-labeled tagset or source annotation scheme to tags of a specification language (target annotation scheme) is an instance of the retagging problem. It is straightforward only in the trivial cases 1:1 (renaming) and n:1. In the latter case, the physical tagset makes finer distinctions than the target annotation scheme. This case introduces no problem for the mapping itself even if not all information contained in the corpus can be accessed. Unfortunately, what we usually find in the mapping business is a mixture of two more problematic cases:\\n\\nMapping Rules Class coverage rules describe a correspondence of source and target annotation classes. The rule format is as follows:   for each physical tag, the equivalent expression in the specification language is named. Example:  [pos = \\'NN\\']    =]   [n  ( common sg  |mass ) ]. The word forms that are annotated with the physical tag NN are ``common singular nouns or mass nouns\\'\\' in the terms of the specification language.\\n\\nMtree: Internal representation\\n\\ndefinition holes: Either target or source annotation schemes are not covered by a mapping rule (classes have been forgotten by the person writing the mapping rules).\\n\\nnondisjunctiveness of classes: A target annotation class has several source annotation correspondences. Although this might be an instance of case n:1, a warning is issued, because most such cases occur due to a conceptual error.\\n\\nSystem Support Compilation of the tagset definition: useful for tagsets with many non-hierarchical, i.e. combinatory features (which would have to be multiplied out manually otherwise.)\\n\\nResults and Outlook\\n\\nFor test purposes, we wrote mapping rules for the UPenn and SUSANNE tagsets. The number of coverage rules is equivalent to the number of physical tags. Rules are easy to formulate, once users have got used to the class semantics of the standard tag set. Information input are tagging guidelines, if the source annotation scheme comes with a comprehensive description of the intended class semantics, or corpus queries otherwise, which is more time consuming.\\n\\nWe wrote exemplary exception lexicon entries for auxiliary verbs and some for noun exceptions, but more work can be put into the exception lexicon to improve the accuracy in the lexically determinable cases of discrepancies.\\n\\nApart from being used for the validation of the EAGLES standard for English and German, the tool has been integrated into a corpus query system (Christ 94, Schulze 94) to allow for ``more abstract\\'\\' and corpus independent queries. A typical query (content verbs in infinitive or primary auxiliaries in past tense) to a specific corpus (here: UPenn) looks like this:\\n\\nQuery] [(vtype=con  vform=inf) | (vtype=prim  tense=past)]. %% warning:  Noise from [con  fin  imp] %%             and from [con  fin  sub] %%              (Due to tag \"VB\")! [((pos = \"VB\"  word != \"be|do|have\")| (pos = \"VBD\"  word = \"was|were|had|did\")| (pos = \"VBN\"  word = \"been|had|done\"))] We get the information that the system will query for tags VB, VBD, VBN (with lexical constraints) in the UPenn corpus; however, we must expect to find finite content verbs (namely imperative and subjunctive forms) in our output (1:n case).\\n\\nIt would be particularly interesting to explore ways of how to use an MRD to build an exception lexicon automatically, which is especially useful for closed word classes.\\n\\nAnother interesting case are multi-word tags and discrepancies with respect to the assignment of word boundaries (tokenising). Compare the following cases (UPenn tokenising and tagging):\\n\\nPeter/NP   \\'s/POS  house\\n\\nhe/PP   \\'s/VBZ  not at home\\n\\nIn our opinion, Peter\\'s should be regarded as one nominal item (with genitive as value for the case attribute), whereas he and \\'s should be kept as two words. We are thinking about designing a rule construct to express this kind of word bundelling with conditional features.\\n\\nBibliography\\n\\nERIC ATWELL, JOHN HUGHES, CLIVE SOUTER: AMALGAM: Automatic Mapping Among Lexico-Grammatical Annotation Models, Internal Paper,  CCALAS, Leeds University, Aug. 1994.\\n\\nOLIVER CHRIST: A modular and flexible architecture for an integrated corpus query system. In:   Proceedings of COMPLEX\\'94 (3rd Conference on Computational Lexicography and Text Research). Budapest, Hungary, Jul. 1994.\\n\\nEAGLES LEXICON WORKING GROUP, Interim Report, draft version, ILC Pisa, Oct. 94, to appear Feb. 95.\\n\\nROGER GARSIDE, GEOFFREY LEECH, GEOFFREY SAMPSON (EDS. ): The Computational Analysis of English - A Corpus-based Approach, Longman, London, 1987.\\n\\nSIDNEY GREENBAUM, RANDOLPH QUIRK: A Student\\'s Grammar of the English Language, Longman, London,  1990.\\n\\nGEOFFREY LEECH, ANDREW WILSON: Morphosyntactic Corpus Annotation, EAGLES, Text Corpora Working Group, Subtask 3.1: Invitation draft. University of Lancester, Dec. 1993.\\n\\nS.A. MAMRAK, C. S. O\\'CONNELL: Technical Documentation for The Integrated Chameleon Architecture, Ohio State University, Columbus, Mar. 1992.\\n\\nMITCH MARCUS, BEATRICE SANTORINI, MARY ANN MARCINKIEWICZ: Building a large natural language corpus of English: The Penn Treebank. - Computational Linguistics 19, 313-330, 1993.\\n\\nMONICA MONACHINI, NICOLETTA CALZOLARI: Synopsis and Comparison of Morphosyntactic Phenomena Encoded in Lexicon and Corpora, Internal Document, EAGLES Lexicon Group, ILC, Universit Pisa, Oct. 1994.\\n\\nBEATRICE SANTORINI:   Part-of-Speech Tagging Guidelines for the Penn Treebank Project. Technical Report. Department of Computer and Information Science, University of Pennsylvania, Mar. 1991.\\n\\nBRUNO MAXIMILIAN SCHULZE:   Entwurf und Implementierung eines Anfragesystems fr Textcorpora, Diplomarbeit Nr. 1059, IMS, Universitt Stuttgart, Feb. 1994.\\n\\nSIMONE TEUFEL: Linguistisch motivierte Corpuserschlieung: Spezifikationssprache und Anfrageinterpreter, Diplomarbeit Nr. 1058, Institut fr Maschinelle Sprachverarbeitung, Universitt Stuttgart, Jun. 1994.\\n\\nTEXT ENCODING INITIATIVE: List of Common Morphological Features. - TEI-AI1W2, Working Paper, Draft Version, Chicago, Jun. 1991.\\n\\nFootnotes', metadata={'source': '../data/raw/cmplg-xml/9506005.xml'}),\n",
       " Document(page_content=\"Qualitative and Quantitative Models of Speech Translation\\n\\nThis paper compares a qualitative reasoning model of translation with a quantitative statistical model. We consider these models within the context of two hypothetical speech translation systems, starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second, quantitative design. The quantitative language and translation models are based on relations between lexical heads of phrases. Statistical parameters for structural dependency, lexical transfer, and linear order are used to select a set of implicit relations between words in a source utterance, a corresponding set of relations between target language words, and the most likely translation of the original utterance.\\n\\nIntroduction\\n\\nIn recent years there has been a resurgence of interest in statistical approaches to natural language processing. Such approaches are not new, witness the statistical approach to machine translation suggested by Weaver (1955), but the current level of interest is largely due to the success of applying hidden Markov models and N-gram language models in speech recognition. This success was directly measurable in terms of word recognition error rates, prompting language processing researchers to seek corresponding improvements in performance and robustness. A speech translation system, which by necessity combines speech and language technology, is a natural place to consider combining the statistical and conventional approaches and much of this paper describes probabilistic models of structural language analysis and translation. Our aim will be to provide an overall model for translation with the best of both worlds. Various factors will lead us to conclude that a lexicalist statistical model with dependency relations is well suited to this goal.\\n\\nAs well as this quantitative approach, we will consider a constraint/logic based approach and try to distinguish characteristics that we wish to preserve from those that are best replaced by statistical models. Although perhaps implicit in many conventional approaches to translation, a characterization in logical terms of what is being done is rarely given, so we will attempt to make that explicit here, more or less from first principles.\\n\\nQualitative and Quantitative Models\\n\\nOne contrast often taken for granted is the identification of a `statistical-symbolic' distinction in language processing as an instance of the empirical vs. rational debate. I believe this contrast has been exaggerated though historically it has had some validity in terms of accepted practice. Rule based approaches have become more empirical in a number of ways: First, a more empirical approach is being adopted to grammar development whereby the rule set is modified according to its performance against corpora of natural text (e.g. Taylor, Grover, and Briscoe 1989). Second, there is a class of techniques for learning rules from text, a recent example being Brill 1993. Conversely, it is possible to imagine building a language model in which all probabilities are estimated according to intuition without reference to any real data, giving a probabilistic model that is not empirical.\\n\\nMost language processing labeled as statistical involves associating real-number valued parameters to configurations of symbols. This is not surprising given that natural language, at least in written form, is explicitly symbolic. Presumably, classifying a system as symbolic must refer to a different set of (internal) symbols, but even this does not rule out many statistical systems modeling events involving nonterminal categories and word senses. Given that the notion of a symbol, let alone an `internal symbol', is itself a slippery one, it may be unwise to build our theories of language, or even the way we classify different theories, on this notion.\\n\\nInstead, it would seem that the real contrast driving the shift towards statistics in language processing is a contrast between qualitative systems dealing exclusively with combinatoric constraints, and quantitative systems that involve computing numerical functions. This bears directly on the problems of brittleness and complexity that discrete approaches to language processing share with, for example, reasoning systems based on traditional logical inference. It relates to the inadequacy of the dominant theories in linguistics to capture `shades' of meaning or degrees of acceptability which are often recognized by people outside the field as important inherent properties of natural language. The qualitative-quantitative distinction can also be seen as underlying the difference between classification systems based on feature specifications, as used in unification formalisms (Shieber 1986), and clustering based on a variable degree of granularity (e.g. Pereira, Tishby and Lee 1993).\\n\\nIt seems unlikely that these continuously variable aspects of fluent natural language can be captured by a purely combinatoric model. This naturally leads to the question of how best to introduce quantitative modeling into language processing. It is not, of course, necessary for the quantities of a quantitative model to be probabilities. For example, we may wish to define real-valued functions on parse trees that reflect the extent to which the trees conform to, say, minimal attachment and parallelism between conjuncts. Such functions have been used in tandem with statistical functions in experiments on disambiguation (for instance Alshawi and Carter 1994). Another example is connection strengths in neural network approaches to language processing, though it has been shown that certain networks are effectively computing probabilities (Richard and Lippmann 1991).\\n\\nNevertheless, probability theory does offer a coherent and relatively well understood framework for selecting between uncertain alternatives, making it a natural choice for quantitative language processing. The case for probability theory is strengthened by a well developed empirical methodology in the form of statistical parameter estimation. There is also the strong connection between probability theory and the formal theory of information and communication, a connection that has been exploited in speech recognition, for example using the concept of entropy to provide a motivated way of measuring the complexity of a recognition problem (Jelinek et al. 1992).\\n\\nEven if probability theory remains, as it currently is, the method of choice in making language processing quantitative, this still leaves the field wide open in terms of carving up language processing into an appropriate set of events for probability theory to work with. For translation, a very direct approach using parameters based on surface positions of words in source and target sentences was adopted in the Candide system (Brown et al. 1990). However, this does not capture important structural properties of natural language. Nor does it take into account generalizations about translation that are independent of the exact word order in source and target sentences. Such generalizations are, of course, central to qualitative structural approaches to translation (e.g. Isabelle and Macklovitch 1986, Alshawi et al. 1992).\\n\\nDissecting a Logic\\n\\n\\n\\nBased System\\n\\nWe now consider a hypothetical speech translation system in which the language processing components follow a conventional qualitative transfer design. Although hypothetical, this design and its components are similar to those used in existing database query (Rayner and Alshawi 1992) and translation systems (Alshawi et al 1992). More recent versions of these systems have been gradually taking on a more quantitative flavor, particularly with respect to choosing between alternative analyses, but our hypothetical system will be more purist in its qualitative approach.\\n\\nThe overall design is as follows. We assume that a speech recognition subsystem delivers a list of text strings corresponding to transcriptions of an input utterance. These recognition hypotheses are passed to a parser which applies a logic-based grammar and lexicon to produce a set of logical forms, specifically formulas in first order logic corresponding to possible interpretations of the utterance. The logical forms are filtered by contextual and word-sense constraints, and one of them is passed to the translation component. The translation relation is expressed by a set of first order axioms which are used by a theorem prover to derive a target language logical form that is equivalent (in some context) to the source logical form. A grammar for the target language is then applied to the target form, generating a syntax tree whose fringe is passed to a speech synthesizer.\\n\\nTaking the various components in turn, we make a note of undesirable properties that might be improved by quantitative modeling.\\n\\nAnalysis and Generation\\n\\nA grammar, expressed as a set of syntactic rules (axioms) Gsyn and a set of semantic rules (axioms) Gsem is used to support a relation form holding between strings s and logical forms\\n\\nexpressed in first order logic:\\n\\n. The relation form is many-to-many, associating a string with linguistically possible logical form interpretations. In the analysis direction, we are given s and search for logical forms\\n\\n, while in generation we search for strings s given\\n\\n.\\n\\nFor analysis and generation, we are treating strings sand logical forms\\n\\nas object level entities. In interpretation and translation, we will move down from this meta-level reasoning to reasoning with the logical forms as propositions.\\n\\nThe list of text strings handed by the recognizer to the parser can be assumed to be ordered in accordance with some acoustic scoring scheme internal to the recognizer. The magnitude of the scores is ignored by our qualitative language processor; it simply processes the hypotheses one at a time until it finds one for which it can produce a complete logical form interpretation that passes grammatical and interpretation constraints, at which point it discards the remaining hypotheses. Clearly, discarding the acoustic score and taking the first hypothesis that satisfies the constraints may lead to an interpretation that is less plausible than one derivable from a hypothesis further down in the recognition list. But there is no point in processing these later hypotheses since we will be forced to select one interpretation essentially at random.\\n\\nSyntax\\n\\nThe syntactic rules in Gsyn relate `category' predicates\\n\\nc0, c1, c2holding of a string and two spanning substrings (we limit the rules here to two daughters for simplicity):\\n\\n(Here, and subsequently, variables like s0 and s1 are implicitly universally quantified.) Gsyn also includes lexical axioms for particular strings wconsisting of single words: c1(w),   ...  cm(w). For a feature-based grammar, these rules can include conjuncts constraining the values,\\n\\n, of discrete-valued functions fon the strings:\\n\\n.\\n\\nThe main problem here is that such grammars have no notion of a degree of grammatical acceptability - a sentence is either grammatical or ungrammatical. For small grammars this means that perfectly acceptable strings are often rejected; for large grammars we get a vast number of alternative trees so the chance of selecting the correct tree for simple sentences can get worse as the grammar coverage increases. There is also the problem of requiring increasingly complex feature sets to describe idiosyncrasies in the lexicon.\\n\\nSemantics\\n\\nSemantic grammar axioms belonging to Gsem specify a `composition' function g for deriving a logical form for a phrase from those for its subphrases:\\n\\nThe interpretation rules for strings bottom out in a set of lexical semantic rules associating words with predicates (\\n\\n) corresponding to `word senses'. For a particular word and syntactic category, there will be a (small, possibly empty) finite set of such word sense predicates:\\n\\n...\\n\\n.\\n\\nFirst order logic was assumed as the semantic representation language because it comes with well understood, if not very practical, inferential machinery for constraint solving. However, applying this machinery requires making logical forms fine grained to a degree often not warranted by the information the speaker of an utterance intended to convey. An example of this is explicit scoping which leads (again) to large numbers of alternatives which the qualitative model has difficulty choosing between. Also, many natural language sentences cannot be expressed in first order logic without resort to elaborate formulas requiring complex semantic composition rules. These rules can be simplified by using a higher order logic but at the expense of even less practical inferential machinery.\\n\\nIn applying the grammar in generation we are faced with the problem of balancing over and under-generation by tweaking grammatical constraints, there being no way to prefer fully grammatical target sentences over more marginal ones. Qualitative approaches to grammar tend to emphasize the ability to capture generalizations as the main measure of success in linguistic modeling. This might explain why producing appropriate lexical collocations is rarely addressed seriously in these models, even though lexical collocations are important for fluent generation. The study of collocations for generation fits in more naturally with statistical techniques, as illustrated by Smajda and McKeown (1990).\\n\\nInterpretation\\n\\nIn the logic-based model, interpretation is the process of identifying from the possible interpretations\\n\\nof\\n\\ns for which\\n\\nhold, ones that are consistent with the context of interpretation. We can state this as follows:\\n\\n. Here, we have separated the context into a contingent set of contextual propositions S and a set R of (monolingual) `meaning postulates', or selectional restrictions, that constrain the word sense predicates in all contexts. A is a set of assumptions sufficient to support the interpretation\\n\\ngiven S and R. In other words, this is `interpretation as abduction' (Hobbs et al. 1988), since abduction, not deduction, is needed to arrive at the assumptions A.\\n\\nThe most common types of meaning postulates in R are those for restriction, hyponymy, and disjointness, expressed as follows:\\n\\nrestriction;\\n\\nhyponymy;\\n\\ndisjointness. Although there are compilation techniques (e.g. Mellish 1988) which allow selectional constraints stated in this fashion to be implemented efficiently, the scheme is problematic in other respects. To start with, the assumption of a small set of senses for a word is at best awkward because it is difficult to arrive at an optimal granularity for sense distinctions. Disambiguation with selectional restrictions expressed as meaning postulates is also problematic because it is virtually impossible to devise a set of postulates that will always filter all but one alternative. We are thus forced to under-filter and make an arbitrary choice between remaining alternatives.\\n\\nLogic based translation\\n\\nIn both the quantitative and qualitative models we take a transfer approach to translation. We do not depend on interlingual symbols, but instead map a representation with constants associated with the source language into a corresponding expression with constants from the target language. For the qualitative model, the operable notion of correspondence is based on logical equivalence and the constants are source word sense predicates\\n\\nand\\n\\ntarget sense predicates\\n\\n.\\n\\nMore specifically, we will say the translation relation between a source logical form\\n\\nand a target logical form\\n\\nholds\\n\\nif we have\\n\\nwhere B is a set of monolingual and bilingual meaning postulates, and S is a set of formulas characterizing the current context. A' is a set of assumptions that includes the assumptions A which supported\\n\\n. Here bilingual meaning postulates are first order axioms relating source and target sense predicates. A typical bilingual postulate for translating between p1 and q1 might be of the form:\\n\\n.\\n\\nThe need for the assumptions A' arises when a source language word is vaguer that its possible translations in the target language, so different choices of target words will correspond to translations under different assumptions. For example, the condition p5(x1) above might be proved from the input logical form, or it might need to be assumed.\\n\\nIn the general case, finding solutions (i.e.\\n\\npairs) for the abductive schema is an undecidable theorem proving problem. This can be alleviated by placing restrictions on the form of meaning postulates and input formulas and using heuristic search methods. Although such an approach was applied with some success in a limited-domain system translating logical forms into database queries (Rayner and Alshawi 1992), it is likely to be impractical for language translation with tens of thousands of sense predicates and related axioms.\\n\\nSetting aside the intractability issue, this approach does not offer a principled way of choosing between alternative solutions proposed by the prover. One would like to prefer solutions with `minimal' sets of assumptions, but it is difficult to find motivated definitions for this minimization in a purely qualitative framework.\\n\\nQuantitative Model Components\\n\\nMoving to a Quantitative Model A transfer organization with analysis, transfer, and generation components.\\n\\nMonolingual models that can be used for both analysis and generation.\\n\\nTranslation models that exclusively code contrastive (cross-linguistic) information.\\n\\nHierarchical phrases capturing recursive linguistic structure.\\n\\nInstead of feature based syntax trees and first-order logical forms we will adopt a simpler, monostratal representation that is more closely related to those found in dependency grammars (e.g. Hudson 1984). Dependency representations have been used in large scale qualitative machine translation systems, notably by McCord (1988). The notion of a lexical `head' of a phrase is central to these representations because they concentrate on relations between such lexical heads. In our case, the dependency representation is monostratal in that the relations may include ones normally classified as belonging to syntax, semantics or pragmatics.\\n\\nOne salient property of our language model is that it is strongly lexical: it consists of statistical parameters associated with relations between lexical items and the number and ordering of dependents of lexical heads. This lexical anchoring facilitates statistical training and sensitivity to lexical variation and collocations. In order to gain the benefits of probabilistic modeling, we replace the task of developing large rule sets with the task of estimating large numbers of statistical parameters for the monolingual and translation models. This gives rise to a new cost trade-off in human annotation/judgement versus barely tractable fully automatic training. It also necessitates further research on lexical similarity and clustering (e.g. Pereira, Tishby and Lee 1993, Dagan, Marcus and Markovitch 1993) to improve parameter estimation from sparse data.\\n\\nTranslation via Lexical Relation Graphs\\n\\nThe model associates phrases with relation graphs. A relation graph is a directed labeled graph consisting of a set of relation edges. Each edge has the form of an atomic proposition\\n\\nr(wi,wj)where r is a relation symbol, wi is the lexical head of a phrase and wj is the lexical head of another phrase (typically a subphrase of the phrase headed by wi). The nodes wi and wj are word occurrences representable by a word and an index, the indices uniquely identifying particular occurrences of the words in a discourse or corpus. The set of relation symbols is open ended, but the first argument of the relation is always interpreted as the head and the second as the dependent with respect to this relation. The relations in the models for the source and target languages need not be the same, or even overlap. To keep the language models simple, we will mainly restrict ourselves here to dependency graphs that are trees with unordered siblings. In particular, phrases will always be contiguous strings of words and dependents will always be heads of subphrases.\\n\\nIgnoring algorithmic issues relating to compactly representing and efficiently searching the space of alternative hypotheses, the overall design of the quantitative system is as follows. The speech recognizer produces a set of word-position hypotheses (perhaps in the form of a word lattice) corresponding to a set of string hypotheses for the input. The source language model is used to compute a set of possible relation graphs, with associated probabilities, for each string hypothesis. A probabilistic graph translation model then provides, for each source relation graph, the probabilities of deriving corresponding graphs with word occurrences from the target language. These target graphs include all the words of possible translations of the utterance hypotheses but do not specify the surface order of these words. Probabilities for different possible word orderings are computed according to ordering parameters which form part of the target language model.\\n\\nIn the following section we explain how the probabilities for these various processing stages are combined to select the most likely target word sequence. This word sequence can then be handed to the speech synthesizer. For tighter integration between generation and synthesis, information about the derivation of the target utterance can also be passed to the synthesizer.\\n\\nIntegrated Statistical Model\\n\\nThe probabilities associated with phrases in the above description are computed according to the statistical models for analysis, translation, and generation. In this section we show the relationship between these models to arrive at an overall statistical model of speech translation. We are not considering training issues in this paper, though a number of now familiar techniques ranging from methods for maximum likelihood estimation to direct estimation using fully annotated data are applicable.\\n\\nAs: (acoustic evidence for) source language speech\\n\\nWs: source language word string\\n\\nWt: target language word string\\n\\nCs: source language relation graph\\n\\nCt: target language relation graph\\n\\nGiven a spoken input in the source language, we wish to find a target language string that is the most likely translation of the input. We are thus interested in the conditional probability of Wt given As. This conditional probability can be expressed as follows (cf. Chang and Su 1993):\\n\\nP(W_t | A_s) = _W_s,C_s,C_t  P(W_s | A_s)  P(C_s | W_s,A_s) P(C_t | C_s,W_s,A_s)  P(W_t | C_t,C_s,W_s,A_s).\\n\\nWe now apply some simplifying independence assumptions concerning relation graphs. Specifically, that their derivation from word strings is independent of acoustic information; that their translation is independent of the original words and acoustics involved; and that target word string generation from target relation edges is independent of the source language representations. The extent to which these (Markovian) assumptions hold depend on the extent to which relation edges represent all the relevant information for translation. In particular it means they should express aspects of surface relevant to meaning, such as topicalization, as well as predicate argument structure. In any case, the simplifying assumptions give the following:\\n\\nP(W_t | A_s) _W_s,C_s,C_t P(W_s | A_s)  P(C_s | W_s) P(C_t | C_s)  P(W_t | C_t). This can be rewritten with two applications of Bayes rule:\\n\\n_W_s,C_s,C_t  P(A_s | W_s)  (1/P(A_s))  P(W_s | C_s) P(C_s)  P(C_t | C_s)  P(W_t | C_t).\\n\\nSince As is given, 1/P(As) is a constant which can be ignored in finding the maximum of\\n\\nP(Wt | As). Determining\\n\\nWt that maximizes\\n\\nP(Wt | As) therefore involves the following factors:\\n\\nP(As | Ws): source language acoustics\\n\\nP(Ws | Cs): source language generation\\n\\nP(Cs):       source content relations\\n\\nP(Ct | Cs): source to target transfer\\n\\nP(Wt | Ct): target language generation\\n\\nFinally note that by another application of Bayes rule we can replace the two factors\\n\\nP(Cs) P(Ct | Cs) by\\n\\nP(Ct) P(Cs | Ct) without changing other parts of the model. This latter formulation allows us to apply constraints imposed by the target language model to filter inappropriate possibilities suggested by analysis and transfer. In some respects this is similar to Dagan and Itai's (1994) approach to word sense disambiguation using statistical associations in a second language.\\n\\nLanguage Models\\n\\nLanguage Production Model\\n\\nOur language model can be viewed in terms of a probabilistic generative process based on the choice of lexical `heads' of phrases and the recursive generation of subphrases and their ordering. For this purpose, we can define the head word of a phrase to be the word that most strongly influences the way the phrase may be combined with other phrases. This notion has been central to a number of approaches to grammar for some time, including theories like dependency grammar (Hudson 1976, 1990) and HPSG (Pollard and Sag 1987). More recently, the statistical properties of associations between words, and more particularly heads of phrases, has become an active area of research (e.g. Chang, Luo, and Su 1992; Hindle and Rooth 1993).\\n\\nThe language model factors the statistical derivation of a sentence with word string W as follows:\\n\\nwhere C ranges over relation graphs. The content model, P(C), and generation model, P(W | C), are components of the overall statistical model for spoken language translation given earlier. This decomposition of P(W) can be viewed as first deciding on the content of a sentence, formulated as a set of relation edges according to a statistical model for P(C), and then deciding on word order according to P(W | C).\\n\\nOf course, this decomposition simplifies the realities of language production in that real language is always generated in the context of some situation S (real or imaginary), so a more comprehensive model would be concerned with P(C | S), i.e. language production in context. This is less important, however, in the translation setting since we produce Ct in the context of a source relation graph Cs and we assume the availability of a model for\\n\\nP(Ct | Cs).\\n\\nContent Derivation Model\\n\\nThe model for deriving the relation graph of a phrase is taken to consist of choosing a lexical head h0 for the phrase (what the phrase is `about') followed by a series of `node expansion' steps. An expansion step takes a node and chooses a possibly empty set of edges (relation labels and ending nodes) starting from that node. Here we consider only the case of relation graphs that are trees with unordered siblings.\\n\\nTo start with, let us take the simplified case where a head word h has no optional or duplicated dependents (i.e. exactly one for each relation). There will be a set of edges\\n\\ncorresponding to the local tree rooted at h with dependent nodes\\n\\n. The set of relation edges for the entire derivation is the union of these local edge sets.\\n\\nTo determine the probability of deriving a relation graph Cfor a phrase headed by h0 we make use of parameters (`dependency parameters')\\n\\nP(r(h,w) | h,r)for the probability, given a node h and a relation r, that wis an r-dependent of h. Under the assumption that the dependents of a head are chosen independently from each other, the probability of deriving C is:\\n\\nwhere\\n\\nP(Top(h0)) is the probability of choosing h0 to start the derivation.\\n\\nIf we now remove the assumption made earlier that there is exactly one r-dependent of a head, we need to elaborate the derivation model to include choosing the number of such dependents. We model this by parameters\\n\\nP(N(r,n) | h)that is, the probability that head h has n r-dependents. We will refer to this probability as a `detail parameter'. Our previous assumption amounted to stating that this was always 1 for n=1 or for n=0. Detail parameters allow us to model, for example, the number of adjectival modifiers of a noun or the `degree' to which a particular argument of a verb is optional. The probability of an expansion of h giving rise to local edges E(h) is now:\\n\\n...y estimates that are accurate enough for practical purposes.\\n\\nP(E(h)|h) = _r  P(N(r,n_r)|h) k(n_r)  _1 i n_r  P(r(h,w^r_i) | h,r). where r ranges over the set of relation labels and h has nr r-dependents\\n\\n. k(nr) is a combinatoric constant for taking account of the fact that we are not distinguishing permutations of the dependents (e.g. there are nr! permutations of the  r-dependents of h if these dependents are all distinct).\\n\\nSo if h0 is the root of a tree C, we have\\n\\nwhere heads(C) is the set of nodes in C and EC(h) is the set of edges headed by h in C.\\n\\nThe above formulation is only an approximation for relation graphs that are not trees because the independence assumptions which allow the dependency parameters to be simply multiplied together no longer hold for the general case. Dependency graphs with cycles do arise as the most natural analyses of certain linguistic constructions, but calculating their probabilities on a node by node basis as above may still provide probability estimates that are accurate enough for practical purposes.\\n\\nGeneration Model\\n\\nWe now return to the generation model P(W | C). As mentioned earlier, since C includes the words in W and a set of relations between them, the generation model is concerned only with surface order. One possibility is to use `bi-relation' parameters for the probability that an ri-dependent immediately follows an rj-dependent. This approach is problematic for our overall statistical model because such parameters are not independent from the `detail' parameters specifying the number of r-dependents of a head.\\n\\nWe therefore adopt the use of `sequencing' parameters, these being probabilities of particular orderings of dependents given that the multiset of dependency relations is known. We let the identity relation e stand for the head itself. Specifically, we have parameters P(s|M(s))where s is a sequence of relation labels including an occurrence of e and M(s) is the multiset for this sequence. For a head h in a relation graph C, let sWCh be the sequence of dependent relations induced by a particular word string Wgenerated from C. We now have\\n\\nwhere h ranges over all the heads in C, and nr is the number of occurrences of r in sWCh, assuming that all orderings of nr-dependents are equally likely. We can thus use these sequencing parameters directly in our overall model.\\n\\ntopmost head parameters  P(Top(h))\\n\\ndependency parameters\\n\\nP(r(h,w)|h,r)\\n\\ndetail parameters\\n\\nP(N(r,n)|h)\\n\\nsequencing parameters P(s|M(s))\\n\\nThe overall model splits the contributions of content P(C)and ordering P(W|C). However, we may also want a model for P(W), for example for pruning speech recognition hypotheses. Combining our content and ordering models we get:\\n\\nP(W) = _C  P(C)  P(W | C) = _C P(Top(h_C))   _h W  P(s_WCh|h) _r(h,w) E_C(h)  P(r(h,w)|h,r) The parameters P(s|h) can be derived by combining sequencing parameters with the detail parameters for h.\\n\\nTranslation Model\\n\\nMapping Relation Graphs\\n\\nAs already mentioned, the translation model defines mappings between relation graphs Cs for the source language and Ct for the target language. A direct (though incomplete) justification of translation via relation graphs may be based on a simple referential view of natural language semantics. Thus nominals and their modifiers pick out entities in a (real or imaginary) world, verbs and their modifiers refer to actions or events in which the entities participate in roles indicated by the edge relations. Under this view, the purpose of the translation mapping is to determine a target language relation graph that provides the best approximation to the referential function induced by the source relation graph. We call this approximating referential equivalence.\\n\\nThis referential view of semantics is not adequate for taking account of much of the complexity of natural language including many aspects of quantification, distributivity and modality. This means it cannot capture some of the subtleties that a theory based on logical equivalence might be expected to. On the other hand, when we proposed a logic based approach as our qualitative model, we had to restrict it to a simple first order logic anyway for computational reasons, and even then it did not appear to be practical. Thus using the more impoverished lexical relations representation may not be costing us much in practice.\\n\\nOne aspect of the representation that is particularly useful in the translation application is its convenience for partial and/or incremental representation of content - we can refine the representation by the addition of further edges. A fully specified denotation of the meaning of a sentence is rarely required for translation, and as we pointed out when discussing logic representations, a complete specification may not have been intended by the speaker. Although we have not provided a denotational semantics for sets of relation edges, we anticipate that this will be possible along the lines developed in monotonic semantics (Alshawi and Crouch 1992).\\n\\nTranslation Parameters\\n\\nTo be practical, a model for\\n\\nP(Ct|Cs) needs to decompose the source and target graphs Cs and Ct into subgraphs small enough that subgraph translation parameters can be estimated. We do this with the help of `node alignment relations' between the nodes of these graphs. These alignment relations are similar in some respects to the alignments used by Brown et al. (1990) in their surface translation model. The translation probability is then the sum of probabilities over different alignments f:\\n\\n. There are different ways to model\\n\\nP(Ct,f|Cs) corresponding to different kinds of alignment relations and different independence assumptions about the translation mapping.\\n\\nFor our quantitative design, we adopt a simple model in which lexical and relation (structural) probabilities are assumed to be independent. In this model the alignment relations are functions from the word occurrence nodes of Ct to the word occurrences of Cs. The idea is that\\n\\nf(vj)=wi means that the source word occurrence wi `gave rise' to the target word occurrence vj. The inverse relation f[-1] need not be a function, allowing different numbers of words in the source and target sentences.\\n\\nWe decompose\\n\\nP(Ct,f|Cs) into `lexical' and `structural' probabilities as follows:\\n\\nP(Ct,f|Cs) = P(Nt,f|Ns)  P(Et|Nt,f,Cs)where Nt and Ns are the node sets for Ct and Csrespectively, and Et is the set of edges for the target graph.\\n\\nThe first factor\\n\\nP(Nt,f|Ns) is the lexical component in that it does not take into account any of the relations in the source graph Cs. This lexical component is the product of alignment probabilities for each node of Ns:\\n\\nP(N_t,f|N_s)= _w_i N_s P(f^-1(w_i)= {v_i^1 ...v_i^n} | w_i). That is, the probability that f maps exactly the (possibly empty) subset\\n\\nof Nt to wi. These sets are assumed to be disjoint for different source graph nodes, so we can replace the factors in the above product with parameters: P(M | w)where w is a source language word and M is a multiset of target language words.\\n\\nWe will derive a target set of edges Et of Ct by k derivation steps which partition the set of source edges Es into subgraphs\\n\\n. These subgraphs give rise to disjoint sets of relation edges\\n\\nwhich together form Et. The structural component of our translation model will be the sum of derivation probabilities for such an edge set Et.\\n\\nFor simplicity, we assume here that the source graph Cs is a tree. This is consistent with our earlier assumptions about the source language model. We take our partitions of the source graph to be the edge sets for local trees. This ensures that the the partitioning is deterministic so the probability of a derivation is the product of the probabilities of derivation steps. More complex models with larger partitions rooted at a node are possible but these require additional parameters for partitioning. For the simple model it remains to specify derivation step probabilities.\\n\\nThe probability of a derivation step is given by parameters of the form:\\n\\nP(T'i|S'i,fi)where S'i and T'i are unlabeled graphs and fi is a node alignment function from T'i to S'i. Unlabeled graphs are just like our relation edge graphs except that the nodes are not labeled with words (the edges still have relation labels). To apply a derivation step we need a notion of graph matching that respects edge labels: g is an isomorphism (modulo node labels) from a graph G to a graph H if g is a one-one and onto function from the nodes of G to the nodes of H such that\\n\\niff\\n\\n.\\n\\nThe derivation step with parameter\\n\\nP(T'i|S'i,fi) is applicable to the source edges Si, under the alignment f, giving rise to the target edges Ti if (i) there is an isomorphism hi from S'i to Si (ii) there is an isomorphism gi from Ti to T'i (iii) for any node vof Ti it must be the case that\\n\\nhi(fi(gi(v))) = f(v). This last condition ensures that the target graph partitions join up in a way that is compatible with the node alignment f.\\n\\nThe factoring of the translation model into these lexical and structural components means that it will overgenerate because these aspects are not independent in translation between real natural languages. It is therefore appropriate to filter translation hypotheses by rescoring according to the version of the overall statistical model that included the factors\\n\\nP(Ct)P(Cs|Ct) so that the target language model constrains the output of the translation model. Of course, in this case we need to model the translation relation in the `reverse' direction. This can be done in a parallel fashion to the forward direction described above.\\n\\nConclusions\\n\\nThe quantitative model is in a much better position to cope with these problems. It is less brittle because statistical associations have replaced constraints (featural, selectional, etc.) that must be satisfied exactly. The probabilistic models give us a systematic and well motivated way of ranking alternative hypotheses. Computationally, the quantitative model lets us escape from the undecidability of logic-based reasoning. Because this model is highly lexical, we can hope that the input words will allow effective pruning by limiting the number of search paths having significantly high probabilities.\\n\\nWe retained some of the basic assumptions about the structure of language when moving to the quantitative model. In particular, we preserved the notion of hierarchical phrase structure. Relations motivated by dependency grammar made it possible to do this without giving up sensitivity to lexical collocations which underpin simple statistical models like N-grams. The quantitative model also reduced overall complexity in terms of the sets of symbols used. In addition to words, it only required symbols for dependency relations, whereas the qualitative model required symbol sets for linguistic categories and features, and a set of word sense symbols. Despite their apparent importance to translation, the quantitative system can avoid the use of word sense symbols (and the problems of granularity they give rise to) by exploiting statistical associations between words in the target language to filter implicit sense choices.\\n\\ninherent lexical sensitivity of dependency representations, facilitating parameter estimation;\\n\\nquantitative preference based on probabilistic derivation and translation;\\n\\nincremental and/or partial specification of the content of utterances, particularly useful in translation;\\n\\ndecomposition of complex utterances through recursive linguistic structure.\\n\\nAcknowledgements\\n\\nI am grateful to Fernando Pereira, Mike Riley, and Ido Dagan for valuable discussions on the issues addressed in this paper. Fernando Pereira and Ido Dagan also provided helpful comments on a draft of the paper.\\n\\nReferences\\n\\nAlshawi, H., D. Carter, B. Gamback and M. Rayner. 1992. ``Swedish-English QLF Translation''. In H. Alshawi (ed.) The Core Language Engine, Cambridge, Mass. : MIT Press.\\n\\nAlshawi, H. and R. Crouch. 1992. ``Monotonic Semantic Interpretation''. Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, Newark, Delaware.\\n\\nAlshawi, H. and D. Carter. 1994. ``Training and Scaling Preference Functions for Disambiguation''. To appear in Computational Linguistics.\\n\\nBrill, E. 1993. ``Automatic Grammar Induction and Parsing Free Text: A Transformation-Based Approach''.Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, 259-265.\\n\\nBrown, P., J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, J. Lafferty, R. Mercer and P. Rossin. 1990. ``A Statistical Approach to Machine Translation''. Computational Linguistics 16:79-85.\\n\\nChang, J., Y. Luo, and K. Su. 1992. ``GPSM: A Generalized Probabilistic Semantic Model for Ambiguity Resolution''. Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, 177-192.\\n\\nChang, J., K. Su. 1993. ``A Corpus-Based Statistics-Oriented Transfer and Generation Model for Machine Translation''. Proceedings of the 5th International Conference on Theoretical and Methodological Issues in Machine Translation.\\n\\nDagan I. and A. Itai. 1994. ``Word Sense Disambiguation Using a Second Language Monolingual Corpus''. To appear in Computational Linguistics.\\n\\nDagan, I., S. Marcus and S. Markovitch. 1993. ``Contextual Word Similarity and Estimation from Sparse Data''. Proceedings of the 31st meeting of the Association for Computational Linguistics, ACL, 164-171.\\n\\nGazdar, G., E. Klein, G.K. Pullum, and I.A.Sag. 1985. Generalised Phrase Structure Grammar. Oxford: Blackwell.\\n\\nHindle, D. and M. Rooth. 1993. ``Structural Ambiguity and Lexical Relations''. Computational Linguistics 19:103-120.\\n\\nHobbs, J.R., M. Stickel, P. Martin and D. Edwards. 1988. ``Interpretation as Abduction'', Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, Buffalo, New York, 95-103.\\n\\nHudson, R.A. 1984. Word Grammar. Oxford: Blackwell.\\n\\nIsabelle, P. and E. Macklovitch. 1986. ``Transfer and MT Modularity'', Eleventh International Conference on Computational Linguistics, Bonn, 115-117.\\n\\nJelinek, F., R.L. Mercer and S. Roukos. 1992. ``Principles of Lexical Language Modeling for Speech Recognition''. In S. Furui and M.M. Sondhi (eds. ), Advances in Speech Signal Processing, New York: Marcel Dekker Inc.\\n\\nMellish, C.S. 1988. ``Implementing Systemic Classification by Unification''. Computational Linguistics 14:40-51.\\n\\nMcCord, M. 1988. ``A Multi-Target Machine Translation System''. Proceedings of the International Conference on Fifth Generation Computer Systems, Tokyo, Japan, 1141-1149.\\n\\nPereira, F., N. Tishby and L. Lee. 1993. ``Distributional Clustering of English Words''. Proceedings of the 31st meeting of the Association for Computational Linguistics, ACL, 183-190.\\n\\nPollard, C.J. and I.A. Sag. 1987. Information Based Syntax and Semantics: Volume I -- Fundamentals. CSLI Lecture Notes, Number 13. Center for the Study of Language and Information, Stanford, California.\\n\\nRayner, M. and H.  Alshawi. 1992. ``Deriving Database Queries from Logical Forms by Abductive Definition Expansion''. Proceedings of the Third Conference on Applied Natural Language Processing, Trento, Italy.\\n\\nRichard, M.D. and R.P. Lippmann. 1991. ``Neural Network Classifiers Estimate Bayesian a posteriori Probabilities''. Neural Computation 3:461-483.\\n\\nShieber, S.M. 1986. An Introduction to Unification-Based Approaches to Grammar. CSLI Lecture Notes, Number 4. Center for the Study of Language and Information, Stanford, California.\\n\\nSmajda, F. and K. McKeown. 1990. ``Automatically Extracting and Representing Collocations for Language Generation''. In Proceedings of the 28th Annual Meeting  of the Association for Computational Linguistics, Pittsburgh.\\n\\nTaylor, L., C. Grover, and E.J. Briscoe. 1989. ``The Syntactic Regularity of English Noun Phrases''. Proceedings of the 4th European ACL Conference, 256-263.\\n\\nWeaver, W. 1955. ``Translation''. In W. Locke and A. Booth (eds. ), Machine Translation of Languages, Cambridge, Mass. : MIT Press.\", metadata={'source': '../data/raw/cmplg-xml/9408014.xml'}),\n",
       " Document(page_content=\"Experimentally Evaluating Communicative Strategies: The Effect of the Task\\n\\nEffective problem solving among multiple agents requires a better understanding of the role of communication in collaboration. In this paper we show that there are communicative strategies that greatly improve the performance of resource-bounded agents, but that these strategies are highly sensitive to the task requirements, situation parameters and agents' resource limitations. We base our argument on two sources of evidence: (1) an analysis of a corpus of 55 problem solving dialogues, and (2) experimental simulations of collaborative problem solving dialogues in an experimental world, Design-World, where we parameterize task requirements, agents' resources and communicative strategies.\\n\\nIntroduction\\n\\nCognitive effort is involved in processes such as making inferences and swapping items from long term memory into working memory. When agents have limited working memory, then only a limited number of items can be  SALIENT, i.e. accessible in working memory. Since other processes, e.g. inference, operate on salient items, an inference process may require the cognitive effort involved with retrieving items from long term memory, in addition to the effort involved with reasoning itself.\\n\\n[] (20) H: Right. The maximum amount of credit that you will be able to get will be 400 that they will be able to get will be 400 dollars on their tax return  (21) C: 400 dollars for the whole year? (22) H: Yeah it'll be 20%  (23) C: um hm (24) H: Now if indeed they pay the $2000 to your wife, that's great. (25) C: um hm  (26a) H: SO WE HAVE 400 DOLLARS. (26b) Now as far as you are concerned, that could cost you more.....\\n\\nUtterances such as 0-26a, that repeat, paraphrase or make inferences explicit, are collectively called  INFORMATIONALLY REDUNDANT UTTERANCES, IRUs. In 0, the utterances that originally added the belief that they will get 400 dollars  to the context are in italics and the IRU is given in CAPS.\\n\\nIn order to test the hypothesized relationship of communicative strategies to agents' resource limits we developed a test-bed environment, Design-World, in which we vary task requirements, agents' resources and communicative strategies. Our artificial agents are based on a cognitive model of attention and memory. Our experimental results show that communicative strategies that incorporate IRUs can help resource-limited cognitive agents coordinate, limit processing, and improve the quality and robustness of the problem solution. We will show that the task determines whether a communicative strategy is beneficial, depending on how the task is defined in terms of fault intolerance and the level of belief coordination required.\\n\\nDesign-World Task and Agent Architecture\\n\\nWhen an agent retrieves items from memory, search starts from the current pointer location and spreads out in a spherical fashion. Search is restricted to a particular search radius: radius is defined in Hamming distance. For example if the current memory pointer loci is (0 0 0), the loci distance 1 away would be (0 1 0) (0 -1 0) (0 0 1) (0 0 -1) (-1 0 0) (1 0 0). The actual locations are calculated modulo the memory size. The limit on the search radius defines the capacity of attention/working memory and hence defines which stored beliefs and intentions are  SALIENT.\\n\\nThe radius of the search sphere in the AWM model is used as the parameter for Design-World agents' resource-bound on attentional capacity. In the experiments below, memory is 16x16x16 and the radius parameter varies between 1 and 16, where AWM of 1 gives severely attention limited agents and AWM of 16 means that everything an agent knows is salient.\\n\\nDesign\\n\\n\\n\\nWorld Communicative Strategies\\n\\nA discourse act may be left implicit, or may be varied to consist of one or more communicative acts. Discourse acts are different from actions on the environment because they are actions whose intended effect is a change in the other agent's mental state. Because the other agent is an active intelligence, it is possible for it to supplement an underspecified discourse action with its own processing. The variation in the degree of explicitness of a discourse act is the basis of agents' communicative strategies. Here we will compare three communicative strategies: (1) All-Implicit; (2) Close-Consequence; and (3) Explicit-Warrant.\\n\\nThe All-Implicit strategy is a `bare bones' strategy, exemplified by the partial dialogue in 1. In 1 each utterance is shown both as a gloss in italics, and in the artificial language that the agents communicate with.\\n\\n[1:] BILL: Then, let's put the green rug in the study. (propose agent-bill agent-kim option-43:  put-act (agent-bill green rug room-1))\\n\\n[2:] KIM: Then, let's put the green lamp in the study. (propose agent-kim agent-bill option-61:  put-act (agent-kim green lamp room-1))\\n\\n[3:] BILL: No, instead let's put the green couch in the study. (reject agent-bill agent-kim option-75:  put-act (agent-bill green couch room-1))\\n\\n.....\\n\\nIn dialogue 1 agent CLC uses the Close-Consequence strategy. CLC makes explicit  CLOSING statements, such as 1-2, on the completion of the intention associated with a discourse segment. CLC's  CLOSING discourse act also includes IRUs as in 1-3; CLC makes the inference explicit that since they have agreed on putting the green rug in the study, Bill no longer has the green rug (act-effect inference).\\n\\n[1:] BILL: Then, let's put the green rug in the study. (propose agent-bill agent-clc option-30: put-act (agent-bill green rug room-1))\\n\\n[2:] CLC: So, we've agreed to put the green rug in the study. (close agent-clc agent-bill intended-30: put-act (agent-bill green rug room-1))\\n\\n[3:] CLC:  AGENT-BILL DOESN'T HAVE GREEN RUG. (say agent-clc agent-bill bel-48: has n't (agent-bill green rug))\\n\\nThe Explicit-Warrant strategy varies the proposal discourse act by including  WARRANT IRUs in each proposal. In general a WARRANT for an intention is a reason for adopting the intention, and here  WARRANTS are the score propositions that give the utility of the proposal, which are mutually believed at the outset of the dialogues. In 1, the  WARRANT IRU is in  CAPS.\\n\\n[1:] IEI:   PUTTING IN THE GREEN RUG IS WORTH  56 (say agent-iei agent-iei2 bel-265: score  (option-202:  put-act (agent-bill green rug room-1) 56))\\n\\n[2:] IEI: Then, let's put the green rug in the study. (propose agent-iei agent-iei2 option-202:  put-act (agent-bill green rug room-1))\\n\\nDesign World Task Variations\\n\\nStandard Task\\n\\nThe Standard task is defined so that the  RAW SCORE that agents achieve for a  DESIGN-HOUSE plan, constructed via the dialogue, is the sum of all the furniture items for each valid step in their plan. The point values for invalid steps in the plan are simply subtracted from the score so that agents are not heavily penalized for making mistakes.\\n\\nZero Invalids Task\\n\\nZero NonMatching Beliefs Task\\n\\nExperimental Results\\n\\nA strategy A is defined to be  BENEFICIAL as compared to a strategy B, for a set of fixed parameter settings, if the difference in distributions using the Kolmogorov-Smirnov two sample test is significant at p [ .05, in the positive direction, for two or more AWM settings. A strategy is  DETRIMENTAL if the differences go in the negative direction. Strategies may be neither  BENEFICIAL or  DETRIMENTAL, since there may be no difference between two strategies.\\n\\nIn the reminder of this section, we first compare within strategy, for each task definition and show that whether or not a strategy is beneficial depends on the task. Then we compare across strategies for a particular task, showing that the interaction of the strategy and task varies according to the strategy. The comparisons will show that what counts as a good collaborative strategy depends on cognitive limits on attention and the definition of success for the task.\\n\\nClose Consequence\\n\\nExplicit Warrant\\n\\nWhen the task is Zero-Invalid (no figure due to space), the benefits of the Explicit-Warrant strategy are dampened from the benefits of the Standard task, because Explicit-Warrant does nothing to address the reasons for agents making mistakes. In comparison with the All-Implicit strategy, it is detrimental at AWM of 1 and 2, but is still beneficial at AWM of 5,6,7, and 11.\\n\\nRelated Work\\n\\nConclusion\\n\\nIn this paper we showed that collaborative communicative behavior cannot be defined in the abstract: what counts as collaborative depends on the task, and the definition of success in the task. We used two empirical methods to support our argument: corpus based analysis and experimentation in Design-World. The methods and the focus of this work are novel; previous work on resource limited agents has not examined the role of communicative strategies in multi-agent interaction whereas work on communication has not considered the effects of resource limits.\\n\\nWe showed that strategies that are inefficient under assumptions of perfect reasoners with unlimited attention and retrieval are effective with resource limited agents. Furthermore, different tasks make different cognitive demands, and place different requirements on agents' collaborative behavior. Tasks which require a high level of belief coordination can benefit from communicative strategies that include redundancy. Fault intolerant tasks benefit from redundancy for rehearsing the effects of actions.\\n\\nBecause the communicative strategies that we tested were based on a corpus analysis of human human financial advice dialogues and because variations in the Design-World task were parametrized, we believe the results presented here may be domain independent, though clearly more research is needed.\\n\\nBibliography\\n\\nJames F. Allen and C. Raymond Perrault. Analyzing intention in utterances. Artificial Intelligence, 15:143-178, 1980.\\n\\nJ. R. Anderson and G. H. Bower. Human Associative Memory. V.H. Winston and Sons, 1973.\\n\\nAlan Baddeley.\\n\\nWorking Memory.\\n\\nOxford University Press, 1986.\\n\\nMichael Bratman, David Israel, and Martha Pollack. Plans and resource bounded practical reasoning. Computational Intelligence, 4:349-355, 1988.\\n\\nJean C. Carletta. Risk Taking and Recovery in Task-Oriented Dialogue. PhD thesis, Edinburgh University, 1992.\\n\\nAlison Cawsey, Julia Galliers, Steven Reece, and Karen Sparck Jones. Automating the librarian: A fundamental approach using belief revision. Technical Report 243, Cambridge Computer Laboratory, 1992.\\n\\nA. Chapanis, R.B. Ochsman, R.N. Parrish, and G.D. Weeks. Studies in interactive communication: The effects of four communication modes on the behavior of teams during cooperative problem-solving. Human Factors, 14:487-509, 1972.\\n\\nJon Doyle. Rationality and its roles in reasoning. Computational Intelligence, November 1992.\\n\\nTimothy W. Finin, Aravind K. Joshi, and Bonnie Lynn Webber. Natural language interactions with artificial experts. Proceedings of the IEEE, 74(7):921-938, 1986.\\n\\nJulia R. Galliers. Autonomous belief revision and communication. In P. Gardenfors, editor, Belief Revision, pages 220 - 246. Cambridge University Press, 1991.\\n\\nCurry I. Guinn. A computational model of dialogue initiative in collaborative discourse. In AAAI Technical Report FS-93-05, 1993.\\n\\nSteve Hanks, Martha E. Pollack, and Paul R. Cohen. Benchmarks, testbeds, controlled experimentation and the design of agent architectures. AI Magazine, December 1993.\\n\\nD. L. Hintzmann and R. A. Block. Repetition and memory: evidence for a multiple trace hypothesis. Journal of Experimental Psychology, 88:297-306, 1971.\\n\\nThomas K. Landauer. Memory without organization: Properties of a model with random storage and undirected retrieval. Cognitive Psychology, pages 495-531, 1975.\\n\\nDonald A. Norman and Daniel G. Bobrow. On data-limited and resource-limited processes. Cognitive Psychology, 7(1):44-6, 1975.\\n\\nMartha Pollack, Julia Hirschberg, and Bonnie Webber. User participation in the reasoning process of expert systems. In AAAI82, 1982.\\n\\nMartha E. Pollack and Marc Ringuette. Introducing the Tileworld: Experimentally Evaluating Agent Architectures. In AAAI90, pages 183-189, 1990.\\n\\nCandace Sidner. Using discourse to negotiate in collaborative activity: An artificial language. AAAI Workshop on Cooperation among Heterogeneous Agents, 1992.\\n\\nSidney Siegel. Nonparametric Statistics for the Behavioral Sciences. McGraw Hill, 1956.\\n\\nAdelheit Stein and Ulrich Thiel. A conversational model of multimodal interaction. In AAAI93, 1993.\\n\\nMarilyn Walker. Informational redundancy and resource bounds in dialogue. In AAAI Spring Symposium on Reasoning about Mental States, 1993. Also available as IRCS techreport IRCS-93-20, University of Pennsylvania.\\n\\nMarilyn A. Walker. Redundancy in collaborative dialogue. In Fourteenth International Conference on Computational Linguistics, pages 345-351, 1992.\\n\\nMarilyn A. Walker. Informational Redundancy and Resource Bounds in Dialogue. PhD thesis, University of Pennsylvania, 1993.\\n\\nMarilyn A. Walker. Testing collaborative strategies by computational simulation: Cognitive and task effects. Knowledge Based Systems, 1995. March.\\n\\nMarilyn A. Walker and Steve Whittaker. Mixed initiative in dialogue: An investigation into discourse segmentation. In Proc. 28th Annual Meeting of the ACL, pages 70-79, 1990.\\n\\nSteve Whittaker, Erik Geelhoed, and Elizabeth Robinson. Shared workspaces: How do they work and when are they useful? IJMMS, 39:813-842, 1993.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9408015.xml'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents_batch(docs: List[Document]) -> List[Document]:\n",
    "    embeded_docs = []\n",
    "    for doc in tqdm(docs):\n",
    "        embeded_docs.append(embeddings.embed_documents(doc.page_content))\n",
    "    return embeded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   docs_embeded = embed_documents_batch(docs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(docs_embeded[9][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_ENV = os.getenv('PINECONE_ENV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "def connect_index(index_name: str, API_KEY:str = PINECONE_API_KEY, ENV:str = PINECONE_ENV) -> pinecone.Index:\n",
    "    pinecone.init(api_key=API_KEY, environment=ENV)\n",
    "    index = pinecone.Index(index_name)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = connect_index('test-docs')\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../data/raw/common.txt'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/var/folders/69/n3p05xld14g7f0tk90l220fc0000gn/T/ipykernel_13734/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1410132893.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">9</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'/var/folders/69/n3p05xld14g7f0tk90l220fc0000gn/T/ipykernel_13734/1410132893.py'</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'record_texts'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/var/folders/69/n3p05xld14g7f0tk90l220fc0000gn/T/ipykernel_13734/\u001b[0m\u001b[1;33m1410132893.py\u001b[0m:\u001b[94m9\u001b[0m in \u001b[92m<module>\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'/var/folders/69/n3p05xld14g7f0tk90l220fc0000gn/T/ipykernel_13734/1410132893.py'\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'record_texts'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, record in enumerate(docs):\n",
    "    # first get metadata fields for this record\n",
    "    metadata = {\n",
    "    'id': uuid4().hex,\n",
    "    'source': record.metadata,\n",
    "    }\n",
    "    record_metadatas = [{\n",
    "            \"chunk\": j, \"text\": text, **metadata\n",
    "            } for j, text in enumerate(record_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_embedded_documents(documents: List[Document], embeddings, index: pinecone.Index, batch_limit: int =100, **metadata_dict: Optional[Dict[str, Any]]): \n",
    "    batch_limit = 100\n",
    "\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "\n",
    "    record_texts = documents\n",
    "    for i, record in enumerate(tqdm(documents)):\n",
    "        # first get metadata fields for this record\n",
    "        if len(metadata_dict)>0:\n",
    "            metadata = metadata_dict\n",
    "        else:\n",
    "            metadata = {\n",
    "            'id': uuid4().hex,\n",
    "            'source': record.metadata['source'],\n",
    "            }\n",
    "        # now we create chunks from the record text\n",
    "        record_texts = text_splitter.split_text(record.page_content)\n",
    "        # create individual metadata dicts for each chunk\n",
    "        record_metadatas = [{\n",
    "                    \"chunk\": j, \"text\": text, **metadata\n",
    "                    } for j, text in enumerate(record_texts)]\n",
    "        # # append these to current batches\n",
    "        # texts.extend(record_texts)\n",
    "        # metadatas.extend(record_metadatas)\n",
    "        # if we have reached the batch_limit we can add texts\n",
    "        if len(texts) >= batch_limit:\n",
    "            # ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "            # embeds = embeddings.embed_documents(texts)\n",
    "            index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "            texts = []\n",
    "            metadatas = []\n",
    "\n",
    "    if len(texts) > 0:\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = embeddings.embed_documents(texts)\n",
    "        index.upsert(vectors=zip(ids, embeds, metadatas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1. Is senior living the same as a nursing home? Think of it this way, senior living is a social model providing care when you need it while preserving your independence. Nursing homes primarily provide nursing services to the chronically ill. It’s important to note, nursing homes often provide a broader range of skilled nursing. Senior living, on the other hand, offers various lifestyle options for older adults who want to maintain their independence while living in their own apartments. 2. Will I lose my independence when I move in? No, in fact, quite the opposite. At Carlton, we take pride in our philosophy of independence with assistance. In our communities, you have the freedom to live life while knowing that a helping hand is always available when you need it. Whether it’s assistance with daily activities, medication management, or simply having someone there for peace of mind, we are here to provide the support that complements and enhances your independence. 3. Is senior living affordable? The affordability of senior living can vary depending on factors such as location, level of care required, amenities offered, and the specific community you choose. However, many are surprised to learn that the cost of senior living is often lower than the cost of staying in their current homes. To help you get a more accurate comparison, we have created a Cost Comparison Calculator so you can compare costs side by side. for your personalized comparison. 4.\\n\\nWhat are some benefits of senior living? Senior living offers numerous benefits, including: Socializing on your terms – You can socialize in the community whenever you want to, and retreat to the comfort and privacy of your own apartment to relax and unwind. A Supportive Environment – Trained staff are always happy and available to lend a helping hand while providing person-centered care. This involves taking a holistic approach to care that addresses physical, emotional, and social well-being. Safety, Security & Technology – Carlton is designed with safety in mind, with features like emergency response systems, secure premises, and 24/7 staff availability. Including innovations like voice-activated technology through Alexa, Artificial Intelligence enabled fall-prevention systems. Chore-free living – Say goodbye to the burdens of cleaning, laundry, home maintenance, preparing meals, and other unpleasant tasks; Carlton takes care of all these responsibilities. Entertainment at your doorstep – From exercise classes to art workshops, to educational seminars, you will have access to the best entertainment whenever you want it. 5. How do I know when to consider senior living? Asking yourself these questions can be an excellent place to start: Are you looking for good company, opportunities to socialize, and a sense of community? Do you have safety concerns like falls, navigating stairs, or driving?\\n\\nIs keeping up with household chores, home maintenance, and yardwork overwhelming or unmanageable? Is traveling to the store, shopping for groceries, and preparing healthy and nutritious meals a challenge or a worry? Could you use some help with daily tasks and personal care? If you answered “yes” to any of these questions, it may be time to consider Carlton Senior Living. Start with open conversations with friends, family, healthcare professionals, and senior living experts like us. We will guide and support you while you navigate through this decision-making process.', metadata={'source': '../data/raw/common.txt'}),\n",
       " Document(page_content='Distributional Part\\n\\n\\n\\nof\\n\\n\\n\\nSpeech Tagging\\n\\nThis paper presents an algorithm for tagging words whose part-of-speech properties are unknown. Unlike previous work, the algorithm categorizes word tokens in context instead of word types. The algorithm is evaluated on the Brown Corpus.\\n\\nIntroduction\\n\\nSince online text becomes available in ever increasing volumes and an ever increasing number of languages, there is a growing need for robust processing techniques that can analyze text without expensive and time-consuming adaptation to new domains and genres. This need motivates research on fully automatic text processing that may rely on general principles of linguistics and computation, but does not depend on knowledge about individual words.\\n\\nRelated Work\\n\\nWhat these approaches have in common is that they classify words instead of individual occurrences. Given the widespread part-of-speech ambiguity of words this is problematic. How should a word like ``plant\\'\\' be categorized if it has uses both as a verb and as a noun? How can a categorization be considered meaningful if the infinitive marker ``to\\'\\' is not distinguished from the homophonous preposition?\\n\\nTag induction\\n\\nWe start by constructing representations of the syntactic behavior of a word with respect to its left and right context. Our working hypothesis is that syntactic behavior is reflected in co-occurrence patterns. Therefore, we will measure the similarity between two words with respect to their syntactic behavior to, say, their left side by the degree to which they share the same neighbors on the left. If the counts of neighbors are assembled into a vector (with one dimension for each neighbor), the cosine can be employed to measure similarity. It will assign a value close to 1.0 if two words share many neighbors, and 0.0 if they share none. We refer to the vector of left neighbors of a word as its left context vector, and to the vector of right neighbors as its right context vector. The unreduced context vectors in the experiment described here have 250 entries, corresponding to the 250 most frequent words in the Brown corpus.\\n\\nThis basic idea of measuring distributional similarity in terms of shared neighbors must be modified because of the sparseness of the data. Consider two infrequent adjectives that happen to modify different nouns in the corpus. Their right similarity according to the cosine measure would be zero. This is clearly undesirable. But even with high-frequency words, the simple vector model can yield misleading similarity measurements. A case in point is ``a\\'\\' vs. ``an\\'\\'. These two articles do not share any right neighbors since the former is only used before consonants and the latter only before vowels. Yet intuitively, they are similar with respect to their right syntactic context despite the lack of common right neighbors.\\n\\nOur solution to these problems is the application of a singular value decomposition. We can represent the left vectors of all words in the corpus as a matrix C with n rows, one for each word whose left neighbors are to be represented, and kcolumns, one for each of the possible neighbors. SVD can be used to approximate the row and column vectors of C in a low-dimensional space. In more detail, SVD decomposes  a matrix C, the matrix of left vectors in our case, into three matrices T0, S0, and D0 such that:\\n\\nC= T0 S0 D0\\'\\n\\ninduction based on word type only\\n\\ninduction based on word type and context\\n\\ninduction based on word type and context, restricted to ``natural\\'\\' contexts\\n\\ninduction based on word type and context, using generalized left and right context vectors\\n\\nInduction based on word type only Induction based on word type and context The right context vector of the preceding word.\\n\\nThe left context vector of w.\\n\\nThe right context vector of w.\\n\\nThe left context vector of the following word.\\n\\nAgain, an SVD is applied to address the problems of sparseness and generalization. We randomly selected 20,000 word triplets from the corpus and formed concatenations of four context vectors as described above. The singular value decomposition of the resulting 20,000-by-1,000 matrix defines a mapping from the 1,000-dimensional space of concatenated context vectors to a 50-dimensional reduced space. Our tag set was then induced by clustering the reduced vectors of the 20,000 selected occurrences into 200 classes. Each of the 200 tags is defined by the centroid of the corresponding class (the sum of its members). Distributional tagging of an occurrence of a word w proceeds then by retrieving the four relevant context vectors (right context vector of previous word, left context vector of following word, both context vectors of w) concatenating them to one 1000-component vector, mapping this vector to 50 dimensions, computing the correlations with the 200 cluster centroids and, finally, assigning the occurrence to the closest cluster. This procedure was applied to all tokens of the Brown corpus.\\n\\nWe will see below that this method of distributional tagging, although partially successful, fails for many tokens whose neighbors are punctuation marks. The context vectors of punctuation marks contribute little information about syntactic categorization since there are no grammatical dependencies between words and punctuation marks, in contrast to strong dependencies between neighboring words.\\n\\nFor this reason, a second induction on the basis of word type and context was performed, but only for those tokens with informative contexts. Tokens next to punctuation marks and tokens with rare words as neighbors were not included. Contexts with rare words (less than ten occurrences) were also excluded for similar reasons: If a word only occurs nine or fewer times its left and right context vectors capture little information for syntactic categorization. In the experiment, 20,000 natural contexts were randomly selected, processed by the SVD and clustered into 200 classes. The classification was then applied to all natural contexts of the Brown corpus.\\n\\nGeneralized context vectors\\n\\nThe context vectors used so far only capture information about distributional interactions with the 250 most frequent words. Intuitively, it should be possible to gain accuracy in tag induction by using information from more words. One way to do this is to let the right context vector record which classes of left context vectors occur to the right of a word. The rationale is that words with similar left context characterize words to their right in a similar way. For example, ``seemed\\'\\' and ``would\\'\\' have similar left contexts, and they characterize the right contexts of ``he\\'\\' and ``the firefighter\\'\\' as potentially containing an inflected verb form. Rather than having separate entries in its right context vector for ``seemed\\'\\', ``would\\'\\', and ``likes\\'\\', a word like ``he\\'\\' can now be characterized by a generalized entry for ``inflected verb form occurs frequently to my right\\'\\'.\\n\\nAnother argument for the two-step derivation is that many words don\\'t have any of the 250 most frequent words as their left or right neighbor. Hence, their vector would be zero in the word-based scheme. The class-based scheme makes it more likely that meaningful representations are formed for all words in the vocabulary.\\n\\nThe generalized context vectors were input to the tag induction procedure described above for word-based context vectors: 20,000 word triplets were selected from the corpus, encoded as 1,000-dimensional vectors (consisting of four generalized context vectors), decomposed by a singular value decomposition and clustered into 200 classes. The resulting classification was applied to all tokens in the Brown corpus.\\n\\nResults\\n\\nIt is clear from the tables that incorporating context improves performance considerably. The F score increases for all tags except CD, with an average improvement of more than 0.20. The tag CD is probably better thought of as describing a word class. There is a wide range of heterogeneous syntactic functions of cardinals in particular contexts: quantificational and adnominal uses, bare NP\\'s (``is one of\\'\\'), dates and ages (``Jan 1\\'\\', ``gave his age as 25\\'\\'), and enumerations. In this light, it is not surprising that the word-type method does better on cardinals.\\n\\nEven for ``natural\\'\\' contexts, performance varies considerably. It is fairly good for prepositions, determiners, pronouns, conjunctions, the infinitive marker, modals, and the possessive marker. Tag induction fails for cardinals (for the reasons mentioned above) and for ``-ing\\'\\' forms. Present participles and gerunds are difficult because they exhibit both verbal and nominal properties and occur in a wide variety of different contexts whereas other parts of speech have a few typical and frequent contexts.\\n\\nIt may seem worrying that some of the tags are assigned a high number of clusters (e.g., 49 for N, 36 for ADN). A closer look reveals that many clusters embody finer distinctions. Some examples: Nouns in cluster 0 are heads of larger noun phrases, whereas the nouns in cluster 1 are full-fledged NPs. The members of classes 29 and 111 function as subjects. Class 49 consists of proper nouns. However, there are many pairs or triples of clusters that should be collapsed into one on linguistic grounds. They were separated on distributional criteria that don\\'t have linguistic correlates.\\n\\nAn analysis of the divergence between our classification and the manually assigned tags revealed three main sources of errors: rare words and rare syntactic phenomena, indistinguishable distribution, and non-local dependencies.\\n\\nRare words are difficult because of lack of distributional evidence. For example, ``ties\\'\\' is used as a verb only 2 times (out of 15 occurrences in the corpus). Both occurrences are miscategorized, since its context vectors do not provide enough evidence for the verbal use. Rare syntactic constructions pose a related problem: There are not enough instances to justify the creation of a separate cluster. For example, verbs taking bare infinitives were classified as adverbs since this is too rare a phenomenon to provide strong distributional evidence (``we do not DARE speak of\\'\\', ``legislation could HELP remove\\'\\').\\n\\nThe case of the tags ``VBN\\'\\' and ``PRD\\'\\' (past participles and predicative adjectives) demonstrates the difficulties of word classes with indistinguishable distributions. There are hardly any distributional clues for distinguishing ``VBN\\'\\' and ``PRD\\'\\' since both are mainly used as complements of ``to be\\'\\'. A common tag class was created for ``VBN\\'\\' and ``PRD\\'\\' to show that they are reasonably well distinguished from other parts of speech, even if not from each other. Semantic understanding is necessary to distinguish between the states described by phrases of the form ``to be adjective\\'\\'  and the processes described by phrases of the form ``to be past participle\\'\\'.\\n\\nFinally, the method fails if there are no local dependencies that could be used for categorization and only non-local dependencies are informative. For example, the adverb in ``Mc*N. Hester, CURRENTLY Dean of ...\\'\\' and the conjunction in ``to add that, IF United States policies ...\\'\\' have similar immediate neighbors (comma, NP). The decision to consider only immediate neighbors is responsible for this type of error since taking a wider context into account would disambiguate the parts of speech in question.\\n\\nFuture Work\\n\\nThere are three avenues of future research we are interested in pursuing. First, we are planning to apply the algorithm to an as yet untagged language. Languages with a rich morphology may be more difficult than English since with fewer tokens per type, there is less data on which to base a categorization decision.\\n\\nConclusion\\n\\nIn this paper, we have attempted to construct an algorithm for fully automatic distributional tagging, using unannotated corpora as the sole source of information. The main innovation is that the algorithm is able to deal with part-of-speech ambiguity, a pervasive phenomenon in natural language that was unaccounted for in previous work on learning categories from corpora. The method was systematically evaluated on the Brown corpus. Even if no automatic procedure can rival the accuracy of human tagging, we hope that the algorithm will facilitate the initial tagging of texts in new languages and sublanguages.\\n\\nAcknowledgments\\n\\nI am grateful for helpful comments to Steve Finch, Jan Pedersen and two anonymous reviewers (from ACL and EACL). I\\'m also indebted to Michael Berry for SVDPACK and to the Penn Treebank Project for the parsed Brown corpus.\\n\\nBibliography\\n\\nSteven Abney. 1991. Parsing by chunks. In Berwick, Abney, and Tenny, editors, Principle-Based Parsing. Kluwer Academic Publishers.\\n\\nMichael W. Berry. 1992. Large-scale sparse singular value computations. The International Journal of Supercomputer Applications, 6(1):13-49.\\n\\nDouglas Biber. 1993. Co-occurrence patterns among collocations: A tool for corpus-based lexical knowledge acquisition. Computational Linguistics, 19(3):531-538.\\n\\nEric Brill and Mitch Marcus. 1992a. Tagging an unfamiliar text with minimal human supervision. In Robert Goldman, editor, Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language. AAAI Press.\\n\\nEric Brill and Mitchell Marcus. 1992b. Automatically acquiring phrase structure using distributional analysis. In Proceedings of the DARPA workshop \"Speech and Natural Language\", pages 155-159.\\n\\nEric Brill, David Magerman, Mitch Marcus, and Beatrice Santorini. 1990. Deducing linguistic structure from the statistics of large corpora. In Proceedings of the DARPA Speech and Natural Language Workshop, pages 275-282.\\n\\nEric Brill. 1993. Automatic grammar induction and parsing free text: A transformation-based approach. In Proceedings of ACL 31, Columbus OH.\\n\\nEugene Charniak, Curtis Hendrickson, Neil Jacobson, and Mike Perkowitz. 1993. Equations for part-of-speech tagging. In Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 784-789.\\n\\nEugene Charniak, Glenn Carroll, John Adcock, Anthony Cassandra, Yoshihiko Gotoh, Jeremy Katz, Michael Littman, and John McCann. 1994. Taggers for parsers. Technical Report CS-94-06, Brown University.\\n\\nKenneth W. Church. 1989. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of ICASSP-89, Glasgow, Scotland.\\n\\nDoug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1991. A practical part-of-speech tagger. In The 3rd Conference on Applied Natural Language Processing, Trento, Italy.\\n\\nDouglas R. Cutting, Jan O. Pedersen, David Karger, and John W. Tukey. 1992. Scatter/gather: A cluster-based approach to browsing large document collections. In Proceedings of SIGIR \\'92, pages 318-329.\\n\\nC. G. deMarcken. 1990. Parsing the LOB corpus. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pages 243-259.\\n\\nJeffrey L. Elman.\\n\\n1990.\\n\\nFinding structure in time.\\n\\nCognitive Science, 14:179\\n\\n\\n\\n211.\\n\\nSteven Finch and Nick Chater. 1992. Bootstrapping syntactic categories using statistical methods. In Walter Daelemans and David Powers, editors, Background and Experiments in Machine Learning of Natural Language, pages 229-235, Tilburg University. Institute for Language Technology and AI.\\n\\nSteven Paul Finch. 1993. Finding Structure in Language. Ph.D. thesis, University of Edinburgh.\\n\\nW.N. Francis and F. Kucera. 1982. Frequency Analysis of English Usage. Houghton Mifflin, Boston.\\n\\nF. Jelinek. 1985. Robust part-of-speech tagging using a hidden markov model. Technical report, IBM, T.J. Watson Research Center.\\n\\nReinhard Kneser and Hermann Ney. 1993. Forming word classes by statistical clustering for statistical language modelling. In Reinhard Khler and Burghard B. Rieger, editors,   Contributions to Quantitative Linguistics, pages 221-226. Kluwer Academic Publishers, Dordrecht, The Netherlands.\\n\\nJulian Kupiec. 1992. Robust part-of-speech tagging using a hidden markov model. Computer Speech and Language, 6:225-242.\\n\\nJulian Kupiec. 1993. Murax: A robust linguistic approach for question answering using an on-line encyclopedia. In Proceedings of SIGIR \\'93, pages 181-190.\\n\\nJohn R. Ross. 1972. The category squish: Endstation Hauptwort. In Papers from the Eighth Regional Meeting. Chicago Linguistic Society.\\n\\nHinrich Schtze. 1993. Part-of-speech induction from scratch. In Proceedings of ACL 31, pages 251-258, Columbus OH.\\n\\nWhitney Tabor. 1994. Syntactic Innovation: A Connectionist Model. Ph.D. thesis, Stanford University.\\n\\nC. J. van Rijsbergen.\\n\\n1979.\\n\\nInformation Retrieval.\\n\\nButterworths, London.\\n\\nSecond Edition.\\n\\nFootnotes\\n\\nAlthough bib93 classifies collocations, these can also be ambiguous. For example, ``for certain\\'\\' has both senses of ``certain\\'\\': ``particular\\'\\' and ``sure\\'\\'. The small difference in overall frequency in the tables is due to the fact that some word-based context vectors consist entirely of zeros. There were about a hundred word triplets whose four context vectors did not have non-zero entries and could not be assigned a cluster. Because of phrases like ``I had sweet potatoes\\'\\', forms of ``have\\'\\' cannot serve as a reliable discriminator either.', metadata={'source': '../data/raw/cmplg-xml/9503009.xml'}),\n",
       " Document(page_content=\"A Note on the Complexity of Restricted Attribute-Value Grammars\\n\\nThe recognition problem for attribute-value grammars(AVGs) was shown to be undecidable by Johnson in 1988. Therefore, the general form of AVGs is of no practical use. In this paper we study a very restricted form of AVG, for which the recognition problem is decidable (though still\\n\\ncomplete), the R-AVG. We show that\\n\\nthe R-AVG formalism captures all of the context free languages and\\n\\nmore, and introduce a variation on the so-called off-line\\n\\nparsability constraint, the honest\\n\\nparsability constraint, which lets different types of\\n\\nR-AVG coincide precisely with well-known time complexity classes.\\n\\nIntroduction\\n\\nAlthough a universal feature theory does not exist, there is a general understanding of its objects. The objects of feature theories are abstract linguistic objects, e.g., an object ``sentence,'' an object ``masculine third person singular,'' an object ``verb,'' an object ``noun phrase.'' These abstract objects have properties like ``tense,'' ``number,'' ``predicate,'' ``subject.'' The values of these properties are either atomic, like ``present'' and ``singular,'' or abstract objects, like ``verb'' and ``noun-phrase.'' The abstract objects are fully described by their properties and their values. Multiple descriptions for the properties and values of the abstract linguistic objects are presented in the literature. Examples are:\\n\\n1. Feature graphs, which are labeled rooted directed acyclic graphs G=(V,A), where F is a collection of labels, a sink in the graph represents an atomic value and the labeling function is an injective function\\n\\n. 2. Attribute-value matrices, which are matrices in which the entries consist of an attribute and a value or a reentrance symbol. The values are either atomic or attribute-value matrices.\\n\\nhardness proof of the recognition problem.\\n\\n. Likewise, R-AVGL is a proper subset of the class of context sensitive languages, unless\\n\\nor\\n\\n. That is, for any language L that has an\\n\\nDefinitions and Notation\\n\\nAttribute\\n\\n\\n\\nValue Grammars\\n\\nDefinition thedefctr: An f-edge from x to s is a triple (x,f,s) such that x is a variable, f is an attribute, and s is a constant or a variable. A path, p, is a, possibly empty, sequence of f-edges\\n\\nin which the xi are variables and s is either a variable or a constant. Often a path is denoted by the sequence of its edges' attributes, in reversed order, e.g.,\\n\\n. Let p be a path, ps denotes the path that starts from s, where s is a constant only if p is the empty path. If the path is nonempty,\\n\\n, then s is a variable. For paths ps and qt we write\\n\\niff p and q start in sand t respectively and end in the same variable or constant. The expression\\n\\nis called a path equation. A feature graph is either a pair\\n\\n, or a pair (x,E) where x is the root and E a finite set of f-edges such that: 1. if (y,f,s) and (y,f,t) are in E, then s=t; 2. if (y,f,s) is in E, then there is a path from x to y in E.\\n\\nDefinition thedefctr:\\n\\nAn attribute\\n\\n\\n\\nvalue language\\n\\nAssume a finite set\\n\\n(of lexical forms) and a finite set\\n\\n(of categories).\\n\\nwill play the role of the set of terminals and\\n\\nwill play the role of the set of nonterminals in the productions.\\n\\nDefinition thedefctr: A constituent structure tree (CST) is a labeled tree in which the internal nodes are labeled with elements of Cat and the leaves are labeled with elements of Lex.\\n\\nDefinition thedefctr: Let T be a constituent structure tree and F be a set of formulas in an attribute-value language\\n\\n. An annotated constituent structure tree is a triple\\n\\n, where h is a function that maps internal nodes in T onto variables in F.\\n\\nDefinition thedefctr: A lexicon is a finite subset of\\n\\nA set of syntactic rules is a finite subset of\\n\\n. An attribute-value grammar is a triple\\n\\n>, where lexicon is a lexicon, rules is a set of syntactic rules and start is an element of\\n\\n.\\n\\nof sets is recursively presentable iff there is an effective enumeration\\n\\nof deterministic Turing machines which halt on all their inputs, and such that\\n\\n2. We say that a class of grammars\\n\\nis recursively presentable iff the class of sets\\n\\nis recursively presentable.\\n\\nRestricted Attribute\\n\\n\\n\\nValue Grammars\\n\\nThe only formulas that are allowed in the attribute-value language of restricted attribute-value grammars (R-AVGs) are path-equations and conjunctions of path-equations (i.e. disjunctions and negations are out). We will denote the attribute-value language of an R-AVG by\\n\\nto make the distinction clear. The CST of an R-AVG is produced by a chain- and\\n\\nrule free\\n\\nregular grammar. The CST of an R-AVG can be either a left-branching\\n\\nor a right-branching tree, since the grammar contains\\n\\nat most one nonterminal in each rule.\\n\\nDefinition thedefctr:\\n\\nThe set of syntactic rules of a restricted attribute-value\\n\\ngrammar is a subset of\\n\\n>. A restricted attribute-value grammar is a pair\\n\\n>, where rules is a set of syntactic rules and start is an element of\\n\\n.\\n\\nDefinition thedefctr:\\n\\nAn R\\n\\n\\n\\nAVG\\n\\n> generates an annotated constituent structure tree\\n\\niff 1. the root node of T is start, and 2. every internal node of T is licensed by a syntactic rule, and 3. the set F is consistent, i.e., describes a feature graph. Let\\n\\nstand for the formula\\n\\nin which all variable y is substituted for variable x. An internal node v of an annotated constituent structure tree is licensed by a syntactic rule\\n\\niff 1. the node v is labeled with category c0,\\n\\nh(v) = n0, and 2. all daughters of v are leaves, which are labeled with\\n\\n,\\n\\nand\\n\\n3.\\n\\nis in the set F. An internal node v of an annotated constituent structure tree is licensed by a syntactic rule\\n\\niff 1. the node v is labeled with category c0,\\n\\nh(v) = n0, and 2. one of v's daughters is an internal node, v1, which is labeled with category c1, and\\n\\nh(v1) = n1, and 3. the daughters of v that are leaves are labeled with\\n\\n,\\n\\nand\\n\\n4.\\n\\nis in the set F.\\n\\nWeak Generative Capacity\\n\\nLet L be a context free language. There exists an R-AVG G such that L=L(G).\\n\\nProof.If L is a context free language, then there exists a context free grammar G' in Greibach normal form such that L=L(G'). From this grammar G', we can construct a pushdown store M that accepts exactly the words in L(G')=L. Such a pushdown store M is actually a finite state automaton M' with a stack S. The finite state automaton M' may be simulated by a chain- and\\n\\nrule free regular grammar.\\n\\nFurthermore, we can construct an attribute-value language\\n\\nthat simulates the stack S. Thus it should be clear that there exists an R-AVG G that produces word w iff\\n\\nThe Honest Parsability Constraint and Consequences\\n\\nAs a more liberal constraint on R-AVGs we propose an analogous variation on the OLP Definition thedefctr: A grammar G satisfies the Honest Parsability Constraint(HPC) iff there exists a polynomial p s.t. for each win L(G) there exists a derivation with at most\\n\\nsteps.\\n\\nFrom Smolka's algorithm and Trautwein's observation it trivially follows that any attribute-value grammar that satisfies the HPC (HP-AVG) has an\\n\\nrecognition algorithm. The problem with the HPC is of course that it is not a syntactic property of grammars. The question whether a given AVG satisfies the HPC (or the OLP for that matter) may well be undecidable. Nonetheless, we can produce a set of rules that, when added to an attribute-value grammar enforces the HPC. The newly produced language is then a subset of the old produced language with an\\n\\nrecognition algorithm. Because of the fact that our addition may simulate any polynomial restriction, we regain the full class of AVG's that satisfy the HPC. In fact\\n\\nTheorem  4.1 The class, P-AVGL, of languages produced by the HP-AVGs is recursively presentable.\\n\\nFor any language L that has an\\n\\nrecognition algorithm, there exists a restricted attribute-value grammar G that respects the HPC and such that L=L(G).\\n\\nProof. (Sketch) Let M be the Turing machine that decides\\n\\n. Use a variation of Johnson's construction of a Turing machine to create an R-AVG that can produce any string w that is recognized by M. Add the set of rules that guarantee that only strings that can be produced with a polynomial number of rules can be produced by the grammar.\\n\\nVeer out the HPC\\n\\nTheorem  5.1 The class of languages generated by R-AVGs obeying the LDP condition is exactly\\n\\n.\\n\\nAcknowledgements\\n\\nWe are indebted to E. Aarts and W.C. Rounds for their valuable suggestions on an early presentation of this work.\\n\\nBibliography\\n\\nJ. Balczar, J. Daz, and J. Gabarr. Structural Complexity I. Springer-Verlag, New York, 1988.\\n\\nP. Blackburn and E. Spaan. A modal perspective on the computational complexity of attribute value grammar. Journal of Logic, Language and Information, 2(2):129-169, 1993.\\n\\nN. Chomsky. Three models for the description of language. IRE Transactions on Information Theory, 2(3):113-124, 1956.\\n\\nJ. Earley. An efficient context-free parsing algorithm. Communications of the Association for Computing Machinery, 13(2):94-102, February 1970.\\n\\nJ. Hopcroft and J. Ullman. Introduction to Automata Theory, Languages, and Computation. Addison Wesley, Reading, MA, 1979.\\n\\nM. Johnson. Attribute-Value Logic and the Theory of Grammar, volume 16 of CSLI Lecture Notes. CSLI, Stanford, 1988.\\n\\nC. Perrault. On the mathematical properties of linguistic theories. Computational Linguistics, 10(3-4):165-176, 1984.\\n\\nG. Smolka. Feature-constraint logics for unification grammars. Journal of Logic Programming, 12(1):51-87, 1992.\\n\\nT. Sudkamp. Languages and Machines: An introduction to the Theory of Computer Science. Addison Wesley, Reading, MA, 1988.\\n\\nM. Trautwein. Assessing complexity results in feature theories. ILLC Research Report and Technical Notes Series LP-95-01, University of Amsterdam, Amsterdam, 1995. Submitted to the CMP-LG archive.\\n\\nSimulating a Context Free Grammar in GNF\\n\\nA context free grammar (CFG) is a quadruple\\n\\n, where N is a set of nonterminals,\\n\\nis a set of terminals, P is a set of productions, and\\n\\nis the start nonterminal. A CFG is in Greibach normalform (GNF) if, and only if, the productions are of one of the following forms, where\\n\\nand\\n\\nGiven a GNF\\n\\nThe three syntactic abbreviations below are used to clarify the simulation. We use represent a stack by a Greek letter, or a string of symbols; the top of the stack is the leftmost symbol of the string. Let x0 encode a stack\\n\\n,\\n\\nthen the formulas in\\n\\nthe abbreviation\\n\\nexpress that x1 encodes\\n\\na stack\\n\\n. Likewise, the formulas in the abbreviation\\n\\nexpress that x0 encodes a stack\\n\\n, and X1 encodes the stack\\n\\n. The abbreviation EMPTY-STACK expresses that x0 encodes an empty stack.\\n\\nWe have to prove that GNF Gand its simulation by R-AVG G'generate (almost) the same language. Obviously, R-AVG G'cannot generate the empty string. However, for all non-empty strings the following theorem holds.\\n\\nTheorem  6.1 Start nonterminal S of GNF Gderives string\\n\\n(\\n\\n) if, and only if, start nonterminal S of R-AVG G'derives string\\n\\nwith the empty stack.\\n\\nProof.There are two cases to consider. First, S derives string\\n\\nin one step. Second, S derives string\\n\\nin more than one step. The lemma below is needed in the proof of the second case.\\n\\nCase I Let start nonterminal S derive string\\n\\nin one step. GNF G contains a production\\n\\niff R-AVG G'   contains a production\\n\\nwith the equation\\n\\nEMPTY\\n\\n\\n\\nSTACK.\\n\\nSo, S derives\\n\\nin a derivation of GNF G iff S derives\\n\\nwith an empty stack in the derivation of R-AVG G'. Case II Initial nonterminal S of GNF G derives string\\n\\nin more than one step iff there is a left-most derivation\\n\\n. GNF G contains production\\n\\niff R\\n\\n\\n\\nAVG G' contains production\\n\\nwith the equation EMPTY-STACK. By the next lemma:\\n\\niff\\n\\nwith\\n\\nthe empty stack.\\n\\nHence S derives\\n\\nfor GNF G iff\\n\\nS derives\\n\\nwith empty stack for R-AVG G'.\\n\\nConstructing an Honestly Parsable Attribute-Value Grammar\\n\\nArithmetic by AVGs\\n\\nWe start with a little bit of arithmetic.\\n\\nNatural numbers.\\n\\nThe AVMs below encode natural numbers in binary notation. The sequences of attributes  0 and  1 in these AVMs encode natural numbers, from least- to most-significant bit. The attribute  V has value 1 (or 0) if, and only if, it has a sister attribute  1 (or  0). 1. The AVMs\\n\\nand\\n\\nencode the natural numbers zero and one. 2. The AVMs\\n\\nand\\n\\nencode natural numbers iff the AVM [F] encodes a natural number.\\n\\nSyntactic rules that tests two numbers for equality.\\n\\nAssume a nonterminal A with some AVM\\n\\n, where [F] and [H] encode natural number x and y, respectively. We present one syntactic rule that derives from this nonterminal A a nonterminal B with AVM\\n\\nif x = y.\\n\\nClearly, this simple test takes one step. A more sophisticated test, which also tests for inequality, would compare [F]and [G] bit-by-bit. Such a test would take\\n\\nderivation steps.\\n\\nSyntactic rules that multiply by two.\\n\\nAssume a nonterminal A with some AVM\\n\\n, where [F] encodes natural number x. We present one syntactic rule that derives from this nonterminal A a nonterminal B with the AVM\\n\\n, where [H] encodes natural number 2x.\\n\\nThe number  N in [H] equals two times  N in [F]if, and only if, the least-significant bit of  N in [H]is 0, and the remaining bits form the same sequence as the number  N in [F]. Multiplication by two takes one derivation step.\\n\\nSyntactic rules that increments by one.\\n\\nAssume a nonterminal A with some AVM\\n\\n, where [F] encodes natural number x. We present five syntactic rules that derive from this nonterminal A a nonterminal C with AVM\\n\\n, where [H] encodes natural number x+1.\\n\\nThe increment of  N requires two additional pointers in the AVM of A: attribute  P points to the next bit that has to be incremented; attribute  Q points to the most-significant bit of the (intermediate) result. These additional pointers are hidden from the AVMs of the nonterminals A and C.\\n\\ntakes\\n\\nderivation steps.\\n\\nRules, similar to the ones above, can be given that decrement the attribute  N by one. We only have to take a little extra care that the number 0 cannot be decremented.\\n\\nSyntactic rules that sum two numbers.\\n\\nIn this section we use the previous test and increment rules (indicated by =). Assume a nonterminal A with some AVM\\n\\n, where [F'] encodes the natural number x + y.\\n\\nThe increment of  N by  M is similar to the increment by one. Here, three additional pointers are required: the attributes  P and  Q point to the bits in  N and M respectively that have to be summed next; attribute  R points to the most-significant bit of the (intermediate) result. In the addition two states are distinguished. In the one state, the carry bit is zero, indicated by nonterminal A'. In the other state, the carry bit is one, indicated by nonterminal B. We claim that\\n\\ntakes\\n\\nderivation steps.\\n\\nSyntactic rules that sum a sequence of numbers.\\n\\nIn this section we use the previous summation rules (indicated by =). Assume a nonterminal A with some AVM\\n\\n, where [F'] encodes a list of numbers. To wit\\n\\n, where [F] encodes the natural number\\n\\n.\\n\\nThe summation requires an additional pointer in the AVM [F']: attribute  P points to the next element in the list that has to be summed. We claim that\\n\\ntakes\\n\\nderivation steps.\\n\\nCreating a counter of logarithmic size\\n\\nCreate an AVM of the following form:\\n\\nAttribute  COUNTER is used to distinguish the AVMs that encodes the counter from those in the original attribute-value grammar. We will neglect the attribute  COUNTER in the remainder of this section, because it is not essential here. The attributes  SIZE,  N,  M and  POLY encode natural numbers. The attribute  SIZE records the size of the string that will be generated. The attribute  POLY records the maximum number of derivation steps that is allowed for a string of size  SIZE. The attributes  N and  M are auxiliary numbers.\\n\\nThe construction of the counter starts with an initiation-step. The further construction of the counter consists of cycles of two phases. Each cycle starts in nonterminal A.\\n\\nInitiation step and first phase.\\n\\nThe initiation-step sets the numbers  SIZE and  N to 0, and the numbers  M and  POLY to 1. In the first phase of each cycle, the numbers  SIZE and  N are incremented by 1.\\n\\nThe second phase of the cycle.\\n\\nIn this phase the numbers  N and  M are compared. If  N is twice  M, then (i) number POLY is extended by k bits, (ii) number  M is doubled, and (iii) number  N is set to 0. If  N is less than twice  M, nothing happens.\\n\\nThe left rule of the second phase doubles the number  M in the second and the third equation. The test ``Is  N equal to 2 M?'' therefore reduces to one (the first) equation. The fourth equation extend the number  POLY with k bits. The fifth and sixth equations set the number  N to 0.\\n\\nThe right rule is always applicable. If the right rule is used where the left rule was applicable, then the number  N will never be equal to\\n\\nin the rest of the derivation. Thus  POLY will not be extended any more.\\n\\nWe claim that the left rule appears\\n\\ntimes and the right rule O(n) times in a derivation for input of size n. Obviously, the number  POLY is\\n\\nwhen the number  SIZE is i.\\n\\nFrom AVG to HP\\n\\n\\n\\nAVG\\n\\nIn this section we show how to transform an AVG into an AVG that satisfies the HPC (HP-AVG). Since all computation steps of the HP-AVG only require a linear amount of derivation steps, total derivations of HP-AVGs have polynomial length.\\n\\nWe can divide the attributes of the HP-AVG into two groups. The attributes that encode the counters, and the attributes of the original AVG. The former will be embedded under the attribute COUNTER, the latter under the attribute  GRAMMAR. In the sequel, we mean by\\n\\nthe formula\\n\\nembedded under the attribute  GRAMMAR, i.e., the formula obtained from\\n\\nby substituting the variables xi by\\n\\n.\\n\\nSecond, the HP-AVG contains an extension of the lexicon of the AVG. The entries of the lexicon are extended in the following way. The size of the lexical form is set to one, and the amount of derivation steps is zero. Thus, if\\n\\nis the lexicon of the AVG, then\\n\\nis the lexicon of the HP-AVG, where\\n\\nThird, the HP-AVG contains extensions of the syntactic rules of the AVG. The syntactic rules are extended in the following way. The numbers POLY and  SIZE of the daughter nonterminals are collected in the lists  PLIST and  SLIST. Both lists are summed. The number  SIZE of the mother nonterminal is equal to the sum of  SIZE's, and the number  POLY of the mother nonterminal is one more than the sum of  POLY's. Thus, if\\n\\nis a syntactic rule of the AVG, then\\n\\nis a syntactic rule of the HP-AVG, where\\n\\nNow, a derivation for the HP-AVG starts with a nondeterministic construction of a counter  SIZE with value n and a counter POLY with value O(n[k]). Then, the derivation of the original AVG is simulated, such that (i) the mother nonterminal produces a string of size n if, and only if the daughter nonterminals together produce a string of size n, and (ii) the mother nonterminal makes n[k]+1 derivation steps if, and only if the daughter nonterminals together make n[k] derivation steps.\\n\\nFootnotes\\n\\nThe author was supported in part by HCM grant ERB4050PL93-0516. The author was supported by the Foundation for language, speech and logic (TSL), which is funded by the Netherlands organization for scientific research (NWO)\", metadata={'source': '../data/raw/cmplg-xml/9503021.xml'}),\n",
       " Document(page_content=\"Text Chunking using Transformation\\n\\n\\n\\nBased Learning\\n\\nEric Brill introduced transformation-based learning and showed that it can do part-of-speech tagging with fairly high accuracy. The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ``baseNP'' chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that partition the sentence. Some interesting adaptations to the transformation-based learning approach are also suggested by this application.\\n\\nIntroduction\\n\\nText Chunking\\n\\nExisting Chunk Identification Techniques\\n\\nExisting efforts at identifying chunks in text have been focused primarily on low-level noun group identification, frequently as a step in deriving index terms, motivated in part by the limited coverage of present broad-scale parsers when dealing with unrestricted text. Some researchers have applied grammar-based methods, combining lexical data with finite state or other grammar constraints, while others have worked on inducing statistical models either directly from the words or from automatically assigned part-of-speech classes.\\n\\nRunning Church's program on test material, however, reveals that the definition of NP embodied in Church's program is quite simplified in that it does not include, for example, structures or words conjoined within NP by either explicit conjunctions like ``and'' and ``or'', or implicitly by commas. Church's chunker thus assigns the following NP chunk structures:\\n\\n[a Skokie] , [Ill.] , [subsidiary] [newer] , [big-selling prescriptions drugs] [the inefficiency] , [waste] and [lack] of [coordination] [Kidder] , [Peabody]   [Co]\\n\\nIt is difficult to compare performance figures between studies; the definitions of the target chunks and the evaluation methodologies differ widely and are frequently incompletely specified. All of the cited performance figures above also appear to derive from manual checks by the investigators of the system's predicted output, and it is hard to estimate the impact of the system's suggested chunking on the judge's determination. We believe that the work reported here is the first study which has attempted to find NP chunks subject only to the limitation that the structures recognized do not include recursively embedded NPs, and which has measured performance by automatic comparison with a preparsed corpus.\\n\\nDeriving Chunks from Treebank Parses\\n\\nThe goal of the ``baseNP'' chunks was to identify essentially the initial portions of non-recursive noun phrases up to the head, including determiners but not including postmodifying prepositional phrases or clauses. These chunks were extracted from the Treebank parses, basically by selecting NPs that contained no nested NPs. The handling of conjunction followed that of the Treebank annotators as to whether to show separate baseNPs or a single baseNP spanning the conjunction. Possessives were treated as a special case, viewing the possessive marker as the first word of a new baseNP, thus flattening the recursive structure in a useful way. The following sentences give examples of this baseNP chunk structure: During [N the third quarter N] , [N Compaq N] purchased [N a former Wang Laboratories manufacturing facility N] in [N Sterling N] , [N Scotland N] , which will be used for [N international service and repair operations N] .\\n\\n[N The government N] has [N other agencies and instruments N] for pursuing [N these other objectives N] .\\n\\nEven [N Mao Tse-tung N] [N 's China N] began in [N 1949 N] with [N a partnership N] between [N the communists N] and [N a number N] of [N smaller , non-communist parties N] .\\n\\nThe chunks in the partitioning chunk experiments were somewhat closer to Abney's model, where the prepositions in prepositional phrases are included with the object NP up to the head in a single N-type chunk. This created substantial additional ambiguity for the system, which had to distinguish prepositions from particles. The handling of conjunction again follows the Treebank parse with nominal conjuncts parsed in the Treebank as a single NP forming a single N chunk, while those parsed as conjoined NPs become separate chunks, with any coordinating conjunctions attached like prepositions to the following N chunk.\\n\\nThe portions of the text not involved in N-type chunks were grouped as chunks termed V-type, though these ``V'' chunks included many elements that were not verbal, including adjective phrases. The internal structure of these V-type chunks loosely followed the Treebank parse, though V chunks often group together elements that were sisters in the underlying parse tree. Again, the possessive marker was viewed as initiating a new N-type chunk. The following sentences are annotated with these partitioning N and V chunks: [N Some bankers N]  [V are reporting V] [N more inquiries than usual N] [N about CDs N] [N since Friday N] .\\n\\n[N Eastern Airlines N] [N ' creditors N] [V have begun exploring V] [N alternative approaches N] [N to a Chapter 11 reorganization N] [V because V] [N they N] [V are unhappy V] [N with the carrier N] [N 's latest proposal N] .\\n\\n[N Indexing N] [N for the most part N] [V has involved simply buying V] [V and then holding V] [N stocks N] [N in the correct mix N] [V to mirror V] [N a stock market barometer N] .\\n\\nThese two kinds of chunk structure derived from the Treebank data were encoded as chunk tags attached to each word and provided the targets for the transformation-based learning.\\n\\nThe Transformation\\n\\n\\n\\nbased Learning Paradigm\\n\\nTo learn a model, one first applies the baseline heuristic to produce initial hypotheses for each site in the training corpus. At each site where this baseline prediction is not correct, the templates are then used to form instantiated candidate rules with patterns that test selected features in the neighborhood of the word and actions that correct the currently incorrect tag assignment. This process eventually identifies all the rule candidates generated by that template set that would have a positive effect on the current tag assignments anywhere in the corpus.\\n\\nThose candidate rules are then tested against the rest of corpus, to identify at how many locations they would cause negative changes. One of those rules whose net score (positive changes minus negative changes) is maximal is then selected, applied to the corpus, and also written out as the first rule in the learned sequence. This entire learning process is then repeated on the transformed corpus: deriving candidate rules, scoring them, and selecting one with the maximal positive effect. This process is iterated, leading to an ordered sequence of rules, with rules discovered first ordered before those discovered later. The predictions of the model on new text are determined by beginning with the baseline heuristic prediction and then applying each rule in the learned rule sequence in turn.\\n\\nTransformational Text Chunking\\n\\nThis section discusses how text chunking can be encoded as a tagging problem that can be conveniently addressed using transformational learning. We also note some related adaptations in the procedure for learning rules that improve its performance, taking advantage of ways in which this task differs from the learning of part-of-speech tags.\\n\\nEncoding Choices\\n\\nIn the baseNP experiments aimed at non-recursive NP structures, we use the chunk tag set {I, O, B}, where words marked I are inside some baseNP, those marked O are outside, and the B tag is  used to mark the left most item of a baseNP which immediately follows another baseNP. In these tests, punctuation marks were tagged in the same way as words.\\n\\nIn the experiments that partitioned text into N and V chunks, we use the chunk tag set {BN, N, BV, V, P}, where BN marks the first word and N the succeeding words in an N-type group while BV and V play the same role for V-type groups. Punctuation marks, which are ignored in Abney's chunk grammar, but which the Treebank data treats as normal lexical items with their own part-of-speech tags, are unambiguously assigned the chunk tag P. Items tagged P are allowed to appear within N or V chunks; they are irrelevant as far as chunk boundaries are concerned, but they are still available to be matched against as elements of the left hand sides of rules.\\n\\nEncoding chunk structure with tags attached to words rather than non-recursive bracket markers inserted between words has the advantage that it limits the dependence between different elements of the encoded representation. While brackets must be correctly paired in order to derive a chunk structure, it is easy to define a mapping that can produce a valid chunk structure from any sequence of chunk tags; the few hard cases that arise can be handled completely locally. For example, in the baseNP tag set, whenever a B tag immediately follows an O, it must be treated as an I, and, in the partitioning chunk tag set, wherever a V tag immediately follows an N tag without any intervening BV, it must be treated as a BV.\\n\\nBaseline System\\n\\nRule Templates\\n\\nIn transformational learning, the space of candidate rules to be searched is defined by a set of rule templates that each specify a small number of particular feature sets as the relevant factors that a rule's left-hand-side pattern should examine, for example, the part-of-speech tag of the word two to the left combined with the actual word one to the left. In the preliminary scan of the corpus for each learning pass, it is these templates that are applied to each location whose current tag is not correct, generating a candidate rule that would apply at least at that one location, matching those factors and correcting the chunk tag assignment.\\n\\nThe same 10 patterns can also be used to match against part-of-speech tags, encoded as P0, P-1, etc. (In other tests, we have explored mixed templates, that match against both word and part-of-speech values, but no mixed templates were used in these experiments.) These 20 word and part-of-speech patterns were then combined with each of the 5 different chunk tag patterns shown on the right side of the table. The cross product of the 20 word and part-of-speech patterns with the 5 chunk tag patterns determined the full set of 100 templates used.\\n\\nAlgorithm Design Issues\\n\\nThe large increase in the number of rule templates in the text chunking application when compared to part-of-speech tagging pushed the training process against the available limits in terms of both space and time, particularly when combined with the desire to work with the largest possible training sets. Various optimizations proved to be crucial to make the tests described feasible.\\n\\nOrganization of the Computation\\n\\nThe power of that approach is dependent on the fact that the confusion matrix for part-of-speech tagging partitions the space of candidate rules into a relatively large number of classes, so that one is likely to be able to exclude a reasonably large portion of the search space. In a chunk tagging application, with only 3 or 4 tags in the effective tagset, this approach based on the confusion matrix offers much less benefit.\\n\\nHowever, even though the confusion matrix does not usefully subdivide the space of possible rules when the tag set is this small, it is still possible to apply a similar optimization by sorting the entire list of candidate rules on the basis of their positive scores, and then processing the candidate rules (which means determining their negative scores and thus their net scores) in order of decreasing positive scores. By keeping track of the rule with maximum benefit seen so far, one can be certain of having found one of the globally best rules when one reaches candidate rules in the sorted list whose positive score is not greater than the net score of the best rule so far.\\n\\nIndexing Static Rule Elements\\n\\nHowever, it is possible to construct a limited index that lists for each candidate rule those locations in the corpus at which the static portions of its left-hand-side pattern match. Because this index involves only the stable word identity and part-of-speech tag values, it does not require updating; thus it can be stored more compactly, and it is also not necessary to maintain back pointers from corpus locations to the applicable rules. This kind of partial static index proved to be a significant advantage in the portion of the program where candidate rules with relatively high positive scores are being tested to determine their negative scores, since it avoids the necessity of testing such rules against every location in the corpus.\\n\\nHeuristic Disabling of Unlikely Rules\\n\\nWe also investigated a new heuristic to speed up the computation: After each pass, we disable all rules whose positive score is significantly lower than the net score of the best rule for the current pass. A disabled rule is then reenabled whenever enough other changes have been made to the corpus that it seems possible that the score of that rule might have changed enough to bring it back into contention for the top place. This is done by adding some fraction of the changes made in each pass to the positive scores of the disabled rules, and reenabling rules whose adjusted positive scores came within a threshold of the net score of the successful rule on some pass.\\n\\nNote that this heuristic technique introduces some risk of missing the actual best rule in a pass, due to its being incorrectly disabled at the time. However, empirical comparisons between runs with and without rule disabling suggest that conservative use of this technique can produce an order of magnitude speedup while imposing only a very slight cost in terms of suboptimality of the resulting learned rule sequence.\\n\\nResults\\n\\nThe first line in each table gives the performance of the baseline system, which assigned a baseNP or chunk tag to each word on the basis of the POS tag assigned in the prepass. Performance is stated in terms of recall (percentage of correct chunks found) and precision (percentage of chunks found that are correct), where both ends of a chunk had to match exactly for it to be counted. The raw percentage of correct chunk tags is also given for each run, and for each performance measure, the relative error reduction compared to the baseline is listed. The partitioning chunks do appear to be somewhat harder to predict than baseNP chunks. The higher error reduction for the former is partly due to the fact that the part-of-speech baseline for that task is much lower.\\n\\nAnalysis of Initial Rules\\n\\nContribution of Lexical Templates\\n\\nFrequent Error Classes\\n\\nA rough hand categorization of a sample of the errors from a baseNP run indicates that many fall into classes that are understandably difficult for any process using only local word and part-of-speech patterns to resolve. The most frequent single confusion involved words tagged VBG and VBN, whose baseline prediction given their part-of-speech tag was O, but which also occur frequently inside baseNPs. The system did discover some rules that allowed it to fix certain classes of VBG and VBN mistaggings, for example, rules that retagged VBNs as I when they preceded an NN or NNS tagged I. However, many also remained unresolved, and many of those appear to be cases that would require more than local word and part-of-speech patterns to resolve.\\n\\nThe second most common class of errors involved conjunctions, which, combined with the former class, make up half of all the errors in the sample. The Treebank tags the words ``and'' and frequently ``,'' with the part-of-speech tag CC, which the baseline system again predicted would fall most often outside of a baseNP. However, the Treebank parses do also frequently classify conjunctions of Ns or NPs as a single baseNP, and again there appear to be insufficient clues in the word and tag contexts for the current system to make the distinction. Frequently, in fact, the actual choice of structure assigned by the Treebank annotators seemed largely dependent on semantic indications unavailable to the transformational learner.\\n\\nFuture Directions\\n\\nWe are planning to explore several different paths that might increase the system's power to distinguish the linguistic contexts in which particular changes would be useful. One such direction is to expand the template set by adding templates that are sensitive to the chunk structure. For example, instead of referring to the word two to the left, a rule pattern could refer to the first word in the current chunk, or the last word of the previous chunk. Another direction would be to enrich the vocabulary of chunk tags, so that they could be used during the learning process to encode contextual features for use by later rules in the sequence.\\n\\nWe would also like to explore applying these same kinds of techniques to building larger scale structures, in which larger units are assembled or predicate/argument structures derived by combining chunks. One interesting direction here would be to explore the use of chunk structure tags that encode a form of dependency grammar, where the tag ``N+2'' might mean that the current word is to be taken as part of the unit headed by the N two words to the right.\\n\\nConclusions\\n\\nBy representing text chunking as a kind of tagging problem, it becomes possible to easily apply transformation-based learning. We have shown that this approach is able to automatically induce a chunking model from supervised training that achieves recall and precision of 92% for baseNP chunks and 88% for partitioning N and V chunks. Such chunking models provide a useful and feasible next step in textual interpretation that goes beyond part-of-speech tagging, and that serve as a foundation both for larger-scale grouping and for direct extraction of subunits like index terms. In addition, some variations in the transformation-based learning algorithm are suggested by this application that may also be useful in other settings.\\n\\nAcknowledgments\\n\\nWe would like to thank Eric Brill for making his system widely available, and Ted Briscoe and David Yarowsky for helpful comments, including the suggestion to test the system's performance without lexical rule templates.\\n\\nBibliography\\n\\nAbney, Steven. 1991. Parsing by chunks. In Berwick, Abney, and Tenny, editors, Principle-Based Parsing. Kluwer Academic Publishers.\\n\\nBourigault, D. 1992. Surface grammatical analysis for the extraction of terminological noun phrases. In Proceedings of the Fifteenth International Conference on Computational Linguistics, pages 977-981.\\n\\nBrill, Eric. 1993a. Automatic grammar induction and parsing free text: A transformation-based approach. In Proceedings of the DARPA Speech and Natural Language Workshop, 1993, pages 237-242.\\n\\nBrill, Eric. 1993b. A Corpus-Based Approach to Language Learning. Ph.D. thesis, University of Pennsylvania.\\n\\nBrill, Eric. 1993c. Rule based tagger, version 1.14. Available from ftp.cs.jhu.edu in the directory /pub/brill/programs/.\\n\\nBrill, Eric. 1994. Some advances in transformation-based part of speech tagging. In Proceedings of the Twelfth National Conference on Artificial Intelligence, pages 722-727. (cmp-lg/9406010).\\n\\nBrill, Eric and Philip Resnik. 1994. A rule-based approach to prepositional attachment disambiguation. In Proceedings of the Sixteenth International Conference on Computational Linguistics. (cmp-lg/9410026).\\n\\nChurch, Kenneth. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Second Conference on Applied Natural Language Processing. ACL.\\n\\nEjerhed, Eva I. 1988. Finding clauses in unrestricted text by finitary and stochastic methods. In Second Conference on Applied Natural Language Processing, pages 219-227. ACL.\\n\\nGee, James Paul and Franois Grosjean. 1983. Performance structures: A psycholinguistic and linguistic appraisal. Cognitive Psychology, 15:411-458.\\n\\nKupiec, Julian. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proceedings of the 31st Annual Meeting of the ACL, pages 17-22.\\n\\nMarcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: A revised corpus design for extracting predicate argument structure. In Human Language Technology, ARPA March 1994 Workshop. Morgan Kaumann.\\n\\nRamshaw, Lance A. and Mitchell P. Marcus. 1994. Exploring the statistical derivation of transformational rule sequences for part-of-speech tagging. In Proceedings of the ACL Balancing Act Workshop on Combining Symbolic and Statistical Approaches to Language, pages 86-95. (cmp-lg/9406011).\\n\\nVoutilainen, Atro. 1993. NPTool, a detector of English noun phrases. In Proceedings of the Workshop on Very Large Corpora, pages 48-57. ACL, June. (cmp-lg/9502010).\\n\\nFootnotes\\n\\nThis heuristic fails in some cases. For example, Treebank uses the label NAC for some NPs functioning as premodifiers, like ``Bank of England'' in ``Robin Leigh-Pemberton, Bank of England governor, conceded..''; in such cases, ``governor'' is not included in any baseNP chunk. Non-constituent NP conjunction, which Treebank labels NX, is another example that still causes problems. Note that this is one of the cases where Church's chunker allows separate NP fragments to count as chunks.\", metadata={'source': '../data/raw/cmplg-xml/9505040.xml'}),\n",
       " Document(page_content=\"Free-ordered CUG on Chemical Abstract Machine\\n\\nWe propose a paradigm for concurrent natural language generation. In order to represent grammar rules distributively, we adopt categorial unification grammar (CUG) where each category owns its functional type. We augment typed lambda calculus with several new combinators, to make the order of lambda-conversions free for partial / local processing. The concurrent calculus is modeled with Chemical Abstract Machine. We show an example of a Japanese causative auxiliary verb that requires a drastic rearrangement of case domination.\\n\\nIntroduction\\n\\nParallel and distributed computation is expected to be the main stream of information processing. In the conventional generation, the rules for composition are given from the outside and those rules control all the behavior of the symbols or the objects, for assembling a hierarchical tree structure. For example, all the linguistic objects, such as words and phrases must be applied to so-called grammar rules to form grammatical structures or rational semantic representations, under a strict controller process. However, this kind of formalization obviously contradicts the partial / distributed processing that would be required in parallel architecture in future.\\n\\nIn order to represent grammar rules distributively, we adopt categorial grammar, where we can an attach local grammar rule to each word and phrase. What we aim in this paper is to propose a paradigm that enables partial / local generation through decompositions and reorganizations of tentative local structures.\\n\\nIn the following section, we introduce the extended lambda-calculus. Thereafter we introduce the ChAM model and we reinterpret the model in terms of natural language processings. Then we show the model of membrane interaction model with the example of Japanese causative sentence that requires drastic change of domination of cases. Finally we will discuss the future of the model.\\n\\nExtended typed lambda\\n\\n\\n\\ncalculus\\n\\nLambda\\n\\n\\n\\ncalculus of polymorphic type\\n\\nWe use greek letters, for type schemas. For type constants we use\\n\\nwhile for type variables we use\\n\\nrepresents that the object a is of type\\n\\n.\\n\\nIf\\n\\nand\\n\\nare types, then\\n\\nis a type.\\n\\nThe purpose of type inference is to infer the type of an object from a set of objects whose types are known. We presuppose that two type variables\\n\\nand\\n\\nare unified with a unifier\\n\\n.\\n\\nWe\\n\\nuse\\n\\nfor this set of type-known objects. The most important two rules are as follows:\\n\\nExtended combinators\\n\\nC-combinator changes the order of lambda-variables as follows:\\n\\nAnother requirement for exchanges of the order of lambda-conversion is the following case. Suppose that we are required to compose all the following typed objects:\\n\\nIn such a case, we need to concatenate g and a first, and then g(a)becomes applicable to f. However, with the help of the following B-combinator:\\n\\nCost of unification\\n\\nwhere\\n\\nrepresents a unifier of two DAG's: one's syntactic case is x and the other's semantic role is y. k is some constant larger than 1 (k ] 1).\\n\\nChemical Abstract Machine\\n\\nWe assume the process of natural language recognition as follows. Whenever a linguistic object is recognized, it is thrown into the solution of ChAM, and acts as a  molecule. Verbs and some other auxiliary verbs introduces  membranes. These membranes becomes their scopes for case (or role) domination; namely, each verb searches for molecules (noun phrases) that are necessary to satisfy each verb's case (role) frame, within its membrane. In some occasions, if multiple verbs exist in one sentence, they may conflict as to which verb dominates which noun phrase. In such a case, two membranes can interact and can exchange some molecules.\\n\\nWe use\\n\\nfor membranes. When a membrane si contains a molecule\\n\\n,\\n\\nwe\\n\\ndenote as\\n\\nThe supporting relation (\\n\\n) can be interpreted as an inclusion relation (\\n\\n) in this case. Two membranes can interact when they contact with the notation `\\n\\n', as\\n\\n. If there is a floating molecule (that which is not yet concatenated with other molecules) on one side, it can move through the porous membranes. Valences for concatenation of each molecule are represented by typed lambda-variables. If one membrane contains only one composite structure, and it still has surplus valences, we can regard that whole the membrane has those surplus valences as follows.\\n\\nNow, we will apply our notions above to the actual problem of sentence generation.\\n\\nExample: Japanese causative sentence\\n\\n' means that each word is recognized in the general world however a verb `yomu' introduced a special membrane s1 as a subworld of W. Each DAG means a polymorphic type of the lexical item.\\n\\n's are the records of unification, that contain the costs and the original types; they become necessary when they are backtracked, and in that meaning, those bindings are transitive.\\n\\nNow, let us recapitulate what has occurred in the membrane s1. There were four lexical items in the set, and they are duly organized to a sentence and s1 becomes a singleton.\\n\\nthat it bit agent, so that the comparison should be made between\\n\\nand\\n\\n.\\n\\nHowever,\\n\\nbecause both of\\n\\nand\\n\\nConclusion\\n\\nIntroducing free-ordered typed lambda-calculus, together with the notion of unification costs in types, we have shown the structuring of natural language syntax, by distributively represented types in random orders. We adopted a model of Chemical Abstract Machine for the partial/ concurrent computation model.\\n\\nAlthough we introduced the concept of costs and termination was assured, the efficiency of constructing a parsing tree would be far slower than sequential processing. However our objective is not to propose a faster algorithm, but is to show the possibility of distributed processing of natural languages. We could show that natural language syntax is self-organizable, in that each linguistic objects do not need to be poured into `molds', viz., externally given grammar.\\n\\nBibliography\\n\\nG. Berry and G. Boudol. The chemical abstract machine. In 17th Annual ACM Symposium on Principles of Programming Languages, pages 81-93, 1990.\\n\\nH. B. Curry and R. Feys. Combinatory Logic, volume 1. North Holland, Amsterdam, Netherlands, 1968.\\n\\nD. Dowty. Type Raising, Functional Composition, and Non-constituent Conjunction - in Categorial Grammars and Natural Language Structures, pages 153-197. D. Reidel, 1988.\\n\\nJ. R. Hindley and Seldin J. P. Introduction to Combination and\\n\\nCalculus.\\n\\nCambridge University Press, 1986.\\n\\nS. M. Shieber.\\n\\nAn Introduction to Unification\\n\\n\\n\\nBased Approaches to Grammar.\\n\\nCSLI, Stanford University, 1986.\\n\\nM. Steedman. Combinators and grammars - in Categorial Grammars and Natural Language Structures, pages 417-442. D. Reidel, 1988.\\n\\nS. Tojo. Categorial analysis of sentence generation. In The Logic Programming Conference (LPC '91), pages 229-238. Institute of New Generation Computer (ICOT), 1991.\\n\\nH. Uszkoreit. Categorial unification grammars. In Proc. of COLING '86, pages 187-194, 1986.\\n\\nT. Gunji.\\n\\nJapanese Phrase Structure Grammar.\\n\\nD. Reidel, 1987.\\n\\nFootnotes\\n\\nare for\\n\\nand for\\n\\n,\\n\\nrespectively.\\n\\nunifies\\n\\nwhich appears in both type declarations.\\n\\nare for\\n\\nand for\\n\\n,\\n\\nrespectively.\\n\\nunifies\\n\\nwhich appears in both type declarations.\", metadata={'source': '../data/raw/cmplg-xml/9411021.xml'}),\n",
       " Document(page_content='Resolving Anaphors in Embedded Sentences\\n\\nWe propose an algorithm to resolve anaphors, tackling mainly the problem of intrasentential antecedents. We base our methodology on the fact that such antecedents are likely to occur in embedded sentences. Sidner\\'s focusing mechanism is used as the basic algorithm in a more complete approach. The proposed algorithm has been tested and implemented as a part of a conceptual analyser, mainly to process pronouns. Details of an evaluation are given.\\n\\nIntroduction\\n\\nWe first present how intrasentential antecedents occur in embedded sentences. We recall the main ideas of the focusing approach, then expand on the main hypotheses which led to the design of the anaphora resolution algorithm.\\n\\nIntrasentential Antecedents Embedded sentences and elementary events\\n\\nAn embedded sentence contains either more than one verb or a verb and derivations of other verbs (see sentence 1 with verbs said and forming). 1) Three of the world\\'s leading advertising groups, Agence Havas S.A. of France, Young  Rubicam of the U.S. and Dentsu Inc. of Japan, said they are forming a global advertising joint venture. Broadly speaking embedded sentences concern more than one fact. In sentence 1 there is the fact of saying something and that of forming a joint venture. We call such a fact an elementary event (EE hereafter). Thus an embedded sentence will contain several EEs.\\n\\nFactors that influence embedded sentences are mainly semantic features of verbs. For example the verb to say, that takes a sentence complement favours an introduction of a new fact, i.e., ``to say something\\'\\', and the related fact. There are other classes of verbs such as want to, hope that, and so on. In the following, subordinate phrases, like relative or causal sentences, will be also considered as embedded ones.\\n\\nEmbedded sentences with intrasentential antecedents\\n\\nFirst of all, we will distinguish the Possessive, Reciprocal and Reflexive pronouns (PRR hereafter) from the other pronouns (non-PRR hereafter).\\n\\nOn the basis of 120 articles, of 4 sentences on average, containing 332 pronouns altogether, we made the following assumption (1): Assumption : non-PRR pronouns can have intrasentential antecedents, only if these pronouns occur in an embedded sentence. The statistics below show that of 262 non-PRR pronouns, there are 244 having intrasentential antecedents, all of which occur in embedded sentences and none in a ``simple\\'\\' sentence. The remaining 18 non-PRR pronouns have intersentential antecedents. Pronouns \\t\\t                      332non-PRR \\t\\t                      262With intrasentential antecedents \\t\\t   244in an embedded sentence \\t\\t With intrasentential in a simple \\t\\t      0sentence \\t\\t With intersentential antecedents \\t\\t       18\\n\\nOur assumption means that, while the PRR pronouns may find their antecedents in an non embedded sentence (e.g., sentences 2 and 3) the non-PRR pronouns can not. 2) Vulcan made  its initial Investment in Telescan in May, 1992. 3) The agencies HCM and DYR are themselves joint ventures. Without jumping to conclusions, we cannot avoid making a parallel with the topological relations defined in the binding theory (Chomsky, 1980), between two coreferring phrases in the syntactic tree level. Assumption 1 redefines these relations in an informal and less rigorous way, at the semantic level, i.e., considering semantic parameters such as the type of verbs that introduce embedded sentences.\\n\\nUsing Sidner\\'s Focusing Approach\\n\\nTo resolve anaphors one of the most suitable existing approaches when dealing with anaphor issues in a conceptual analysis process is the focusing approach proposed by Sidner. However, this mechanism is not suitable for intrasentential cases. We propose to exploit its main advantages in order to build our anaphora resolution mechanism extending it to deal also with intrasentential antecedents.\\n\\nThe expected focus algorithm that selects an initial focus called the \"expected focus\". This selection may be ``confirmed\\'\\' or ``rejected\\'\\' in subsequent sentences. The expected focus is generally chosen on the basis of the verb semantic categories. There is a preference in terms of thematic position: the ``theme\\'\\' (as used by Gruber and Anderson, 1976 for the notion of the object case of a verb) is the first, followed by the goal, the instrument and the location ordered according to their occurrence in the sentence; the final item is the agent that is selected when no other role suits.\\n\\nThe anaphora interpreter uses the state of the focus and a set of algorithms associated with each anaphor type to determine which element of the data structures is the antecedent. Each algorithm is a filter containing several interpretation rules (IR). Each IR in the algorithm appropriate to an anaphor suggests one or several antecedents depending on the focus and on the anaphor type.\\n\\nAn evaluation of the proposed antecedents is performed using different kinds of criteria (syntactic, semantic, inferential, etc.)\\n\\nThe focusing algorithm makes use of data structures, i.e., the focus registers that represent the state of the focus: the current focus (CF) representation, alternate focus list (AFL) that contains the other phrases of the sentence and the focus stack (FS). A parallel structure to the CF is also set to deal with the agentive pronouns. The focusing algorithm updates the state of the focus after each sentence anaphor (except the first sentence). After the first sentence, it confirms or rejects the predicted focus taking into account the results of anaphor interpretation. In the case of rejection, it determines which phrase is to move into focus.\\n\\nThis is a brief example (Sidner 1983) : a Alfred and Zohar liked to play baseball. b They played it every day after school before dinner. c After their game, Alfred and Zohar had ice cream cones. d They tasted really good.\\n\\nIn a) the expected focus is ``baseball\\'\\' (the theme)\\n\\nIn b) ``it\\'\\' refers to ``baseball\\'\\' (CF). ``they\\'\\' refers to Alfred and Zohar (AF)\\n\\nThe focusing algorithm confirms the CF.\\n\\nIn d) ``they\\'\\' refers to ``ice cream cones\\'\\' in AFL.\\n\\nThe focusing algorithm decides that since no anaphor refers to the CF, the CF is stacked and ``ice cream cones\\'\\' is the new CF (focus movement).\\n\\nthe focusing algorithm\\n\\nfollowed by the interpretation of anaphors,\\n\\nthen by the evaluation of the proposed antecedents.\\n\\nWhat needs to be improved in the focusing approach? Intrasentential antecedents Initial Anaphors\\n\\nThe focusing mechanism fails in the expected focus algorithm when encountering anaphors occurring in the first sentence of a text, which we call initial anaphors, such as They in sentence (1). The problem with initial anaphors is that the focus registers cannot be initialised or may be wrongly filled if there are anaphors inside the first sentence of the text. It is clear that taking the sentence in its classical meaning as the unit of processing in the focusing approach, is not suitable when sentences are embedded.\\n\\nWe will focus on the mechanisms and algorithmic aspects of the resolution (how to fill the registers, how to structure algorithms, etc.) and not on the rule aspects, like how IRs decide to choose Bill and not John (sentence 4).\\n\\nOur Solution\\n\\nAs stated above, embedded sentences include several elementary events (EEs). EEs are represented as conceptual entities in our work. We consider that such successive EEs involve the same context that is introduced by several successive short sentences. Moreover, our assumption states that when non-PRR anaphors have intrasentential antecedents, they occur in embedded sentences. Starting with these considerations, the algorithm is governed by the hypotheses expanded below.\\n\\nMain hypotheses\\n\\nFirst hypothesis : EE is the unit of processing in the basic focusing cycle. An EE is the unit of processing in our resolution algorithm instead of the sentence. The basic focusing cycle is applied on each EE in turn and not sentence by sentence. Notice that a simple sentence coincides with its EE.\\n\\nSecond hypothesis : The ``initial\\'\\'  EE of a well formed first sentence does not contain non-PRR pronouns just as an initial simple sentence cannot.\\n\\nFor example, in the splitting of sentence 1 into two EEs (see below), EE1 does not contain non-PRR pronouns because it is the initial EE of the whole discourse.\\n\\nEE1) ``Three of the world\\'s leading advertising groups, Agence Havas S.A. of France, Young  Rubicam of the U.S. and Dentsu Inc. of Japan, said\\'\\' EE2) ``they are forming a global advertising joint venture.\\'\\'\\n\\nThird hypothesis : PRR pronouns require special treatment.\\n\\nPRR could refer to intrasentential antecedents in simple sentences (such as in those of sentences 3 and 4). An initial EE could then contain an anaphor of the PRR type. Our approach is to add a special phase that resolves first the PRRs occurring in the initial EE before applying the expected focusing algorithm on the same initial EE. In all other cases, PRRs are treated equally to other pronouns.\\n\\nThis early resolution relies on the fact that the PRR pronouns may refer to the agent, as in sentence 3, as well as to the complement phrases. However the ambiguity will not be huge at this first level of the treatment. Syntactic and semantic features can easily be used to resolve these anaphors. This relies also on the fact that the subject of the initial EE cannot be a pronoun (second hypothesis).\\n\\nHaving mentioned this particular case of PRR in initial EE, we now expand on the whole algorithm of resolution.\\n\\nThe Algorithm\\n\\napplying the resolution rules,\\n\\napplying the focusing algorithm, i.e., updating the focus registers\\n\\nthe evaluation of the proposed antecedents for each anaphor.\\n\\nThe algorithm is based on the decomposition of the sentence into EEs and the application of the basic focusing cycle on each EE in turn and not sentence by sentence.\\n\\nsplit the sentence into EEs\\n\\napply Step 3 then Step 4.\\n\\nMain Results : 1. Intrasentential antecedents are taken into account when applying the focusing algorithm. For example, in sentence 1, the intrasentential antecedent Bill will be taken into account, because EE1 would be processed beforehand by the expected focusing algorithm. 2. The problem of initial anaphors is then resolved. The expected focusing algorithm is applied only on the initial EE which must not contain anaphors.\\n\\nExamples and results\\n\\nTo illustrate the algorithm, let\\'s consider the following sentence : Lafarge Coppee said it would buy 10 percent in National Gypsum, the number two plasterboard company in the US, a purchase which allows it to be present on the world\\'s biggest plasterboard market. At the conceptual level, there are 3 EEs. They are involved respectively by the said, buy, and allows verbs. They correspond respectively to the following surface sentences: EE1 ``Lafarge Coppee said\\'\\' EE2 ``it would buy 10 percent in National Gypsum, the number two plasterboard company in the US\\'\\' EE3 ``a purchase which allows it to be present on the world\\'s biggest plasterboard market.\\'\\'\\n\\nthe expected focusing algorithm is applied to the first EE, EE1, which contains non-PRR anaphors.\\n\\nEE2 contains only one pronoun it, which is resolved by the basic focusing cycle\\n\\nit in EE3 will be resolved in the same way.\\n\\n5) Mary sacked out in his apartment before Sam could kick her out. 6) Girls who he has dated say that Sam is charming.\\n\\nOur algorithm fails in resolving his in 5, because the algorithm searches only for the entities that preceed the anaphor in the text. The same applies for he in 6. However improving our algorithm to process classical cases of cataphors, such as those in sentence 6, should not require major modifications, only change in the order in which the EEs are searched through.\\n\\nFor example, to process pronouns of the sentence 6 split into two EES (see below), the algorithm must consider EE2 before EE1. This means applying the step 2 of the algorithm to EE2, then step 3 to EE1. The sentence 5 should require specific treatment, though.\\n\\nEE1) ``that Sam is charming\\'\\' EE2) ``Girls who he has dated say\\'\\'\\n\\nHobbs also pointed out the cases of ``picture noun\\'\\' examples, as in sentences 7 and 8: 7) John saw a picture of him. 8) John\\'s father\\'s portrait of him. In 7 our algorithm is successful, i.e., it will not identify him with John because of our previous assumption (section 2.2). However our algorithm would fail in 8 because the non-PRR pronoun him could refer to John which occurs in the same EE.\\n\\nFor example in the embedded sentence 9 where either the reflexive (himself) or non reflexive pronouns (him) may be used, it is more natural to make use of him. 9) John claimed that the picture of him hanging in the post office was a fraud.\\n\\nThe Conceptual Level\\n\\nWe comment here on the main aspects of the conceptual analysis that are related to the anaphora resolution process. They concern mainly the way of splitting embedded sentences and the problems of determining the theme and of managing the other ambiguities and the several readings.\\n\\nThe conceptual analyser\\'s strategy consists of a continuous step-by-step translation of the original natural language sentences into conceptual structures (CS hereafter). This translation uses the results of the syntactic analysis (syntactic tree). It is a progressive substitution of the NL terms located in the syntactic tree with concepts and templates of the conceptual representation language. Triggering rules are evoked by words of the sentence and allow the activation of well-formed CS templates when the syntactico-semantic filter is unified with the syntactic tree. The values caught by the filter variables are the arguments of the CS roles, i.e., they fill the CS roles. If they are anaphors, they are considered to be unbound variables and result in unfilled roles in the CS. The anaphora resolution aims therefore at filling the unfilled roles with the corresponding antecedents.\\n\\nSplitting into EE move information (from ``to say\\'\\'),\\n\\nproduce an agreement (from ``to agree\\'\\'),\\n\\nproduce a joint venture (from ``to form\\'\\').\\n\\nDetermining the theme Managing other ambiguities Multiple readings The set of conceptual structures for the current reading Ri on which the resolution is performed, given that several readings could arise from previous ambiguity processing.\\n\\nThe set of conceptual structures of the current sentence Si where the anaphor occurs;\\n\\nThe set of conceptual structures of the current elementary event EEi where the anaphor occurs after the Si splitting.\\n\\nThe state of the focus (content of the registers), SFi\\n\\nConclusion\\n\\nWe have proposed a methodology to resolve anaphors occurring in embedded sentences. The main idea of the methodology is the use of other kinds of restrictions between the anaphor and its antecedents than the syntactic ones. We demonstrated that anaphors with intrasentential antecedents are closely related to embedded sentences and we showed how to exploit this data to design the anaphora resolution methodology. Mainly, we exploited Sidner\\'s focusing mechanism, refining the classical unit of processing, that is the sentence, to that of the elementary event. The algorithm has been implemented (in Common Lisp, Sun Sparc) to deal with pronouns as a part of a deep analyser. The main advantages of the proposed algorithm is that it is independent from the knowledge representation language used and the deep understanding approach in which it is integrated. Thus, it could be set up in any conceptual analyser, as long as a semantic representation of the text is available. Moreover Sidner\\'s approach does not impose its own formalisms (syntactic or semantic) for its application. The improvment of the proposed algorithm requires dealing with special cases of anaphors such as cataphors and also with specific cases which are not easily handled in the literature. For example, we saw that a solution to processing cataphors could be to reconsider the order in which the conceptual structures (elementary events beforehand) are searched.\\n\\nAcknowledgements\\n\\nThis work has been supported by the European Community Grant LE1-2238 (AVENTINUS project).\\n\\nBibliography\\n\\nAnderson, S.R. 1977. Formal syntax. In Wasow and Akmajian, editors, Comment on the paper by Wasow in Culicover. Academic Press, pages 361-376.\\n\\nAzzam, Saliha. 1995a. Computation of Ambiguities (Anaphors and PPs) in NL texts. CLAM : The prototype. Ph.D. thesis, Paris Sorbonne University.\\n\\nAzzam, Saliha. 1995b. Anaphors, pps and disambiguation process for conceptual analysis. In 14th International Joint Conference on Artificial Intelligence (IJCAI\\'95). San Mateo (CA): Morgan Kaufmann.\\n\\nCarter, David. 1987. Interpreting Anaphors in natural language texts. Chichester : Ellis Horwood.\\n\\nGruber, J.S. 1976. Lexical structures in syntax and semantics. North-Holland.\\n\\nHobbs, Jerry. 1985. Resolving pronoun references. In B. Grosz K. Sparck-Jones B. Webber, editor, Readings in Natural Language, volume 44. Morgan Kaufmann Publishers Los Altos California, pages 311-338.\\n\\nLappin, S. and H.J. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20:535-561.\\n\\nMerlo, P. 1993. For an incremental computation of intrasentential coreference. In the 13th International Joint Conference on Artificial Intelligence (IJCAI\\'93), pages 1216-1221. San Mateo (CA): Morgan Kaufmann.\\n\\nSidner, C. 1979. Toward a computation of intrasentential coreference. Technical Report TR-537, MIT. Artificial Intelligence Labratory.\\n\\nSidner, C. 1981. Focusing for interpretation of pronouns. American Journal of Computational Linguistics, 7:217-231.\\n\\nSidner, C. 1983. Focusing in the comprehension of definite anaphora. In Brady. M and Berwick R.C, editors, Computational Models of Discourse. Cambridge (MA) : The MIT Press.', metadata={'source': '../data/raw/cmplg-xml/9605007.xml'}),\n",
       " Document(page_content=\"Learning Dependencies between Case Frame Slots\\n\\nWe address the problem of automatically acquiring case frame patterns (selectional patterns) from large corpus data. In particular, we propose a method of learning dependencies between case frame slots. We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution, where random variables represent case slots. We then formalize the dependencies between case slots as the     probabilistic dependencies between these random variables. Since the number of parameters in a multi-dimensional joint distribution is exponential in general, it is infeasible to accurately estimate them in practice. To overcome this difficulty, we settle with approximating the target joint distribution by the product of     low order component distributions, based on corpus data. In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task. Our experimental results indicate that for certain classes of verbs, the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies.\\n\\nIntroduction\\n\\nWe address the problem of automatically acquiring case frame patterns (selectional patterns) from large corpus data. The acquisition of case frame patterns normally involves the following three subproblems: 1) Extracting case frames from corpus data, 2) Generalizing case frame slots within these case frames, 3) Learning dependencies that exist between these generalized case frame slots.\\n\\nIn this paper, we propose a method of learning dependencies between case frame slots. By `dependency' is meant the relation that exists between case slots which constrains the possible values assumed by each of those case slots. As illustrative examples, consider the following sentences.\\n\\nWe see that an `airline company' can be the subject of verb `fly' (the value of slot `arg1'), when the direct object (the value of slot `arg2') is an `airplane' but not when it is an `airline company'. These examples indicate that the possible values of case slots depend in general on those of the other case slots: that is, there exist `dependencies' between different case slots.\\n\\nThe knowledge of such dependencies is useful in various tasks in natural language processing, especially in analysis of sentences involving multiple prepositional phrases, such as\\n\\nNote in the above example that the slot of `from' and that of `to' should be considered dependent and the attachment site of one of the prepositional phrases (case slots) can be determined by that of the other with high accuracy and confidence.\\n\\nIn this paper, we view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution, where random variables represent case slots. We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables. Since the number of parameters that exist in a multi-dimensional joint distribution is exponential if we allow n-ary dependencies in general, it is infeasible to estimate them with high accuracy with a data size available in practice. It is also clear that relatively few of these random variables (case slots) are actually dependent on each other with any significance. Thus it is likely that the target joint distribution can be approximated reasonably well by the product of component distributions of low order, drastically reducing the number of parameters that need to be considered. This is indeed the approach we take in this paper.\\n\\nProbabilistic Models for Case Frame Patterns\\n\\nis given a specific probability value by a word-based model. In contrast,\\n\\nis assigned a specific probability by a slot-based model.\\n\\nWe then formulate the dependencies between case slots as the   probabilistic dependencies between the random variables in each of these three models. In the absence of any constraints, however, the number of parameters in each of the above three models is exponential (even the slot-based model has O(2[n]) parameters ), and thus it is infeasible to estimate them in practice. A simplifying assumption that is often made to deal with this difficulty is that random variables (case slots) are mutually independent.\\n\\nSuppose for example that in the analysis of the sentence\\n\\nthe following alternative interpretations are given.\\n\\nWe wish to select the more appropriate of the two interpretations. A heuristic word-based method for disambiguation, in which the random variables (case slots) are assumed to be dependent, is to calculate the following values of word-based likelihood and to select the interpretation corresponding to the higher likelihood value.\\n\\nand\\n\\nThe independence assumption can also be made in the case of a class-based model or a slot-based model. For slot-based models, with the independence assumption, the following probabilities\\n\\nAssuming that random variables (case slots) are mutually independent would drastically reduce the number of parameters. (Note that under the independence assumption the number of parameters in a slot-based model becomes O(n).) As illustrated in Section 1, this assumption is not necessarily valid in practice.\\n\\nWhat seems to be true in practice is that some case slots are in fact dependent but overwhelming majority of them are independent, due partly to the fact that usually only a few case slots are obligatory and most others are optional. Thus the target joint distribution is likely to be approximable by the product of several component distributions of low order, and thus have in fact a reasonably small number of parameters. We are thus lead to the approach of approximating the target joint distribution by such a simplified model, based on corpus data.\\n\\nApproximation by Dendroid Distribution\\n\\nWithout loss of generality, any n-dimensional joint distribution can be written as\\n\\nfor some permutation (\\n\\nm1,m2,...mn) of 1,2,..,n, where we let\\n\\nP(Xm1|Xm0) denote\\n\\nP(Xm1).\\n\\nA plausible assumption on the dependencies between random variables is intuitively that each variable directly depends on at most one other variable. (Note that this assumption is the simplest among those that relax the independence assumption.) For example, if a joint distribution\\n\\nP(X1,X2,X3) over 3 random variables\\n\\nX1,X2,X3can be written (approximated) as follows, it (approximately) satisfies such an assumption.\\n\\nExperimental Results\\n\\nWe conducted some experiments to test the performance of the proposed method as a method of acquiring case frame patterns. In particular, we tested to see how effective the patterns acquired by our method are in a structural disambiguation experiment. We will describe the results of this experimentation in this section.\\n\\nExperiment 1: Slot\\n\\n\\n\\nbased Model\\n\\nExperiment 2: Class\\n\\n\\n\\nbased Model\\n\\nOur experimental result verifies the validity in practice of the assumption widely made in statistical natural language processing that class-based case slots (and also word-based case slots) are mutually independent, at least when the data size available is that provided by the current version of the Penn Tree Bank. This is an empirical finding that is worth noting, since up to now the independence assumption was based solely on human intuition, to the best of our knowledge.\\n\\nConclusions\\n\\nWe conclude this paper with the following remarks. 1. The primary contribution of research reported in this paper is that we have proposed a method of learning dependencies between case frame slots, which is theoretically sound and efficient, thus providing an effective tool for acquiring case dependency information. 2. For the slot-based model, sometimes case slots are found to be dependent. Experimental results demonstrate that using the dependency information, when dependency does exist, structural disambiguation results can be improved. 3. For the word-based or class-based models, case slots are judged independent, with the data size currently available in the Penn Tree Bank. This empirical finding verifies the independence assumption widely made in practice in statistical natural language processing.\\n\\nWe proposed to use dependency forests to represent case frame patterns. It is possible that more complicated probabilistic dependency graphs like Bayesian networks would be more appropriate for representing case frame patterns. This would require even more data and thus the problem of how to collect sufficient data would be a crucial issue, in addition to the methodology of learning case frame patterns as probabilistic dependency graphs. Finally the problem of how to determine obligatory/optional cases based on dependencies acquired from data should also be addressed.\\n\\nAcknowledgement\\n\\nWe thank Mr.K.Nakamura, Mr.T.Fujita, and Dr.K.Kobayashi of NEC CC Res. Labs. for their constant encouragement. We thank Mr.R.Isotani of NEC Information Technology Res. Labs. for his comments. We thank Ms. Y.Yamaguchi of NIS for her programming effort.\\n\\nBibliography\\n\\nHiyan Alshawi and David Carter. 1995. Training and scaling preference functions for disambiguation. Computational Linguistics, 20(4):635-648.\\n\\nLalit R. Bahl, Frederick Jelinek, and Robert Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transaction on Pattern Analysis and Machine Intelligence, 5(2):179-190.\\n\\nEric Brill and Philip Resnik. 1994. A rule-based approach to prepositional phrase attachment disambiguation. Proceedings of the 15th COLING, pages 1198-1204.\\n\\nJing-Shin Chang, Yih-Fen Luo, and Keh-Yih Su. 1992. GPSM: A generalized probabilistic semantic model for ambiguity resolution. Proceedings of the 30th ACL, pages 177-184.\\n\\nC.K. Chow and C.N. Liu. 1968. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 14(3):462-467.\\n\\nMichael Collins and James Brooks. 1995. Prepositional phrase attachment through a backed-off model. Proceedings of the 3rd Workshop on Very Large Corpora.\\n\\nThomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley  Sons Inc.\\n\\nWilliams A. Gale and Kenth W. Church. 1990. Poor estimates of context are worse than none. Proceedings of the DARPA Speech and Natural Language Workshop, pages 283-287.\\n\\nRalph Grishman and John Sterling. 1994. Generalizing automatically generated selectional patterns. Proceedings of the 15th COLING, pages 742-747.\\n\\nDonald Hindle and Mats Rooth. 1991. Structural ambiguity and lexical relations. Proceedings of the 29th ACL, pages 229-236.\\n\\nHang Li and Naoki Abe. 1995. Generalizing case frames using a thesaurus and the MDL principle. Proceedings of Recent Advances in Natural Language Processing, pages 239-248.\\n\\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The penn treebank. Computational Linguistics, 19(1):313-330.\\n\\nJudea Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers Inc.\\n\\nJ. Ross Quinlan and Ronald L. Rivest. 1989. Inferring decision trees using the minimum description length principle. Information and Computation, 80:227-248.\\n\\nAdwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994. A maximum entropy model for prepositional phrase attachment. Proceedings of ARPA Workshop on Human Language Technology, pages 250-255.\\n\\nPhilip Resnik. 1992. Semantic classes and syntactic ambiguity. Proceedings of ARPA Workshop on Human Language Technology.\\n\\nJorma Rissanen. 1978. Modeling by shortest data description. Automatic, 14:37-38.\\n\\nJorma Rissanen. 1983. A universal prior for integers and estimation by minimum description length. The Annals of Statistics, 11(2):416-431.\\n\\nJorma Rissanen. 1984. Universal coding, information, predication and estimation. IEEE Transaction on Information Theory, 30(4):629-636.\\n\\nJorma Rissanen. 1986. Stochastic complexity and modeling. The Annals of Statistics, 14(3):1080-1100.\\n\\nJorma Rissanen. 1989. Stochastic Complexity in Statistical Inquiry. World Scientific Publishing Co.\\n\\nSatoshi Sekine, Jeremy J. Carroll, Sofia Ananiadou, and Jun'ichi Tsujii. 1992. Automatic learning for semantic collocation. Proceedings of the 3rd Conference on Applied Natural Language Processing, pages 104-110.\\n\\nFrank Smadja. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143-177.\\n\\nJoe Suzuki. 1993. A construction of bayesian networks from databases based on an MDL principle. Proceedings of Uncertainty in AI '93.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9605013.xml'}),\n",
       " Document(page_content='A Support Tool for Tagset Mapping Motivation\\n\\nTagsets used in existing corpora have usually been designed to satisfy the needs of specific projects. A tagset used for robust parsing will tend to stress distributional properties, whereas a corpus within a lexical resource specially designed for human interaction (which might include a human oriented dictionary) will most likely distinguish word classes along traditional linguistic lines.\\n\\nObjectivisation and standardisation of similar information: Millions of words have been analysed in the past, using different annotation schemes. Especially the manually analysed linguistic data is expensive to produce and extremely valuable. With a standardised tagset, linguistic information from different corpora of the same language can be reused and thus merged into a large data base. Such data bases improve the performance of statistical methods and are a useful resource for the production of balanced corpora.\\n\\nShared use of language resources: Corpus manipulation tools such as retrieval tools can be applied to merged resources in a uniform format without much customisation. As well, users of these tools will find it easier to work with a corpus tagged in a standardised tagset. Now,  they have to memorize only one scheme of tag classes (class names, class semantics, exceptions), as opposed to several schemes for several corpora before.\\n\\nComparison of annotation schemes: A comparison of the granularity and degree of similarity of tagsets can be carried out more objectively, once the mapping results are available. The validation of the suggestions of the LRE-project EAGLES is an application in this field.\\n\\nWe believe that standards are important for the linguistic community, especially from the point of view of reusablility.\\n\\nOf course, there are limits: proposals for standard tagsets should be regarded as approaches towards a neutral platform between projects and different theories, rather than as ready-made tagsets that will never be changed. It is important indeed that standards and their support tools be flexible about possible extensions and improvements.\\n\\nA standardised tagset As the tagset is constraint-based, a flexible generalisation is possible over all atomic constraints and combinations of constraints. As a formal grammar is   used to define syntactically well-formed specifications of word forms, we can regard our standard tagset as a specification language. Example: The specification [pos = v  vtype = aux  pers = 3] denotes 3rd person auxiliary verbs.\\n\\nThe tagset is also typed, which adds to the naturalness of the specifications of wordforms and helps discover semantic errors in specifications (inconsistent combinations of features, wrong values for features). In our implementation, we follow the closed-world-assumption, which leads to a coherent interpretation for underspecified and/or negated descriptions. Example: [pos = v  (vform = fin |  case != gen)] is a syntactically correct, but ill-typed specification, as the Types v (Verb) and gen (Genitive) are not type compatible.\\n\\nThe tagset can be easily modified because its manually written definition is compiled into a system internal format. As the design of a tagset involves a cycle with feedback phases, including manual tagging and the writing of guidelines, there will be frequent   modifications to the tagset, especially in the initial phase.\\n\\nTag mapping: the problems\\n\\nMapping tags of an existing, flat-labeled tagset or source annotation scheme to tags of a specification language (target annotation scheme) is an instance of the retagging problem. It is straightforward only in the trivial cases 1:1 (renaming) and n:1. In the latter case, the physical tagset makes finer distinctions than the target annotation scheme. This case introduces no problem for the mapping itself even if not all information contained in the corpus can be accessed. Unfortunately, what we usually find in the mapping business is a mixture of two more problematic cases:\\n\\nMapping Rules Class coverage rules describe a correspondence of source and target annotation classes. The rule format is as follows:   for each physical tag, the equivalent expression in the specification language is named. Example:  [pos = \\'NN\\']    =]   [n  ( common sg  |mass ) ]. The word forms that are annotated with the physical tag NN are ``common singular nouns or mass nouns\\'\\' in the terms of the specification language.\\n\\nMtree: Internal representation\\n\\ndefinition holes: Either target or source annotation schemes are not covered by a mapping rule (classes have been forgotten by the person writing the mapping rules).\\n\\nnondisjunctiveness of classes: A target annotation class has several source annotation correspondences. Although this might be an instance of case n:1, a warning is issued, because most such cases occur due to a conceptual error.\\n\\nSystem Support Compilation of the tagset definition: useful for tagsets with many non-hierarchical, i.e. combinatory features (which would have to be multiplied out manually otherwise.)\\n\\nResults and Outlook\\n\\nFor test purposes, we wrote mapping rules for the UPenn and SUSANNE tagsets. The number of coverage rules is equivalent to the number of physical tags. Rules are easy to formulate, once users have got used to the class semantics of the standard tag set. Information input are tagging guidelines, if the source annotation scheme comes with a comprehensive description of the intended class semantics, or corpus queries otherwise, which is more time consuming.\\n\\nWe wrote exemplary exception lexicon entries for auxiliary verbs and some for noun exceptions, but more work can be put into the exception lexicon to improve the accuracy in the lexically determinable cases of discrepancies.\\n\\nApart from being used for the validation of the EAGLES standard for English and German, the tool has been integrated into a corpus query system (Christ 94, Schulze 94) to allow for ``more abstract\\'\\' and corpus independent queries. A typical query (content verbs in infinitive or primary auxiliaries in past tense) to a specific corpus (here: UPenn) looks like this:\\n\\nQuery] [(vtype=con  vform=inf) | (vtype=prim  tense=past)]. %% warning:  Noise from [con  fin  imp] %%             and from [con  fin  sub] %%              (Due to tag \"VB\")! [((pos = \"VB\"  word != \"be|do|have\")| (pos = \"VBD\"  word = \"was|were|had|did\")| (pos = \"VBN\"  word = \"been|had|done\"))] We get the information that the system will query for tags VB, VBD, VBN (with lexical constraints) in the UPenn corpus; however, we must expect to find finite content verbs (namely imperative and subjunctive forms) in our output (1:n case).\\n\\nIt would be particularly interesting to explore ways of how to use an MRD to build an exception lexicon automatically, which is especially useful for closed word classes.\\n\\nAnother interesting case are multi-word tags and discrepancies with respect to the assignment of word boundaries (tokenising). Compare the following cases (UPenn tokenising and tagging):\\n\\nPeter/NP   \\'s/POS  house\\n\\nhe/PP   \\'s/VBZ  not at home\\n\\nIn our opinion, Peter\\'s should be regarded as one nominal item (with genitive as value for the case attribute), whereas he and \\'s should be kept as two words. We are thinking about designing a rule construct to express this kind of word bundelling with conditional features.\\n\\nBibliography\\n\\nERIC ATWELL, JOHN HUGHES, CLIVE SOUTER: AMALGAM: Automatic Mapping Among Lexico-Grammatical Annotation Models, Internal Paper,  CCALAS, Leeds University, Aug. 1994.\\n\\nOLIVER CHRIST: A modular and flexible architecture for an integrated corpus query system. In:   Proceedings of COMPLEX\\'94 (3rd Conference on Computational Lexicography and Text Research). Budapest, Hungary, Jul. 1994.\\n\\nEAGLES LEXICON WORKING GROUP, Interim Report, draft version, ILC Pisa, Oct. 94, to appear Feb. 95.\\n\\nROGER GARSIDE, GEOFFREY LEECH, GEOFFREY SAMPSON (EDS. ): The Computational Analysis of English - A Corpus-based Approach, Longman, London, 1987.\\n\\nSIDNEY GREENBAUM, RANDOLPH QUIRK: A Student\\'s Grammar of the English Language, Longman, London,  1990.\\n\\nGEOFFREY LEECH, ANDREW WILSON: Morphosyntactic Corpus Annotation, EAGLES, Text Corpora Working Group, Subtask 3.1: Invitation draft. University of Lancester, Dec. 1993.\\n\\nS.A. MAMRAK, C. S. O\\'CONNELL: Technical Documentation for The Integrated Chameleon Architecture, Ohio State University, Columbus, Mar. 1992.\\n\\nMITCH MARCUS, BEATRICE SANTORINI, MARY ANN MARCINKIEWICZ: Building a large natural language corpus of English: The Penn Treebank. - Computational Linguistics 19, 313-330, 1993.\\n\\nMONICA MONACHINI, NICOLETTA CALZOLARI: Synopsis and Comparison of Morphosyntactic Phenomena Encoded in Lexicon and Corpora, Internal Document, EAGLES Lexicon Group, ILC, Universit Pisa, Oct. 1994.\\n\\nBEATRICE SANTORINI:   Part-of-Speech Tagging Guidelines for the Penn Treebank Project. Technical Report. Department of Computer and Information Science, University of Pennsylvania, Mar. 1991.\\n\\nBRUNO MAXIMILIAN SCHULZE:   Entwurf und Implementierung eines Anfragesystems fr Textcorpora, Diplomarbeit Nr. 1059, IMS, Universitt Stuttgart, Feb. 1994.\\n\\nSIMONE TEUFEL: Linguistisch motivierte Corpuserschlieung: Spezifikationssprache und Anfrageinterpreter, Diplomarbeit Nr. 1058, Institut fr Maschinelle Sprachverarbeitung, Universitt Stuttgart, Jun. 1994.\\n\\nTEXT ENCODING INITIATIVE: List of Common Morphological Features. - TEI-AI1W2, Working Paper, Draft Version, Chicago, Jun. 1991.\\n\\nFootnotes', metadata={'source': '../data/raw/cmplg-xml/9506005.xml'}),\n",
       " Document(page_content=\"Qualitative and Quantitative Models of Speech Translation\\n\\nThis paper compares a qualitative reasoning model of translation with a quantitative statistical model. We consider these models within the context of two hypothetical speech translation systems, starting with a logic-based design and pointing out which of its characteristics are best preserved or eliminated in moving to the second, quantitative design. The quantitative language and translation models are based on relations between lexical heads of phrases. Statistical parameters for structural dependency, lexical transfer, and linear order are used to select a set of implicit relations between words in a source utterance, a corresponding set of relations between target language words, and the most likely translation of the original utterance.\\n\\nIntroduction\\n\\nIn recent years there has been a resurgence of interest in statistical approaches to natural language processing. Such approaches are not new, witness the statistical approach to machine translation suggested by Weaver (1955), but the current level of interest is largely due to the success of applying hidden Markov models and N-gram language models in speech recognition. This success was directly measurable in terms of word recognition error rates, prompting language processing researchers to seek corresponding improvements in performance and robustness. A speech translation system, which by necessity combines speech and language technology, is a natural place to consider combining the statistical and conventional approaches and much of this paper describes probabilistic models of structural language analysis and translation. Our aim will be to provide an overall model for translation with the best of both worlds. Various factors will lead us to conclude that a lexicalist statistical model with dependency relations is well suited to this goal.\\n\\nAs well as this quantitative approach, we will consider a constraint/logic based approach and try to distinguish characteristics that we wish to preserve from those that are best replaced by statistical models. Although perhaps implicit in many conventional approaches to translation, a characterization in logical terms of what is being done is rarely given, so we will attempt to make that explicit here, more or less from first principles.\\n\\nQualitative and Quantitative Models\\n\\nOne contrast often taken for granted is the identification of a `statistical-symbolic' distinction in language processing as an instance of the empirical vs. rational debate. I believe this contrast has been exaggerated though historically it has had some validity in terms of accepted practice. Rule based approaches have become more empirical in a number of ways: First, a more empirical approach is being adopted to grammar development whereby the rule set is modified according to its performance against corpora of natural text (e.g. Taylor, Grover, and Briscoe 1989). Second, there is a class of techniques for learning rules from text, a recent example being Brill 1993. Conversely, it is possible to imagine building a language model in which all probabilities are estimated according to intuition without reference to any real data, giving a probabilistic model that is not empirical.\\n\\nMost language processing labeled as statistical involves associating real-number valued parameters to configurations of symbols. This is not surprising given that natural language, at least in written form, is explicitly symbolic. Presumably, classifying a system as symbolic must refer to a different set of (internal) symbols, but even this does not rule out many statistical systems modeling events involving nonterminal categories and word senses. Given that the notion of a symbol, let alone an `internal symbol', is itself a slippery one, it may be unwise to build our theories of language, or even the way we classify different theories, on this notion.\\n\\nInstead, it would seem that the real contrast driving the shift towards statistics in language processing is a contrast between qualitative systems dealing exclusively with combinatoric constraints, and quantitative systems that involve computing numerical functions. This bears directly on the problems of brittleness and complexity that discrete approaches to language processing share with, for example, reasoning systems based on traditional logical inference. It relates to the inadequacy of the dominant theories in linguistics to capture `shades' of meaning or degrees of acceptability which are often recognized by people outside the field as important inherent properties of natural language. The qualitative-quantitative distinction can also be seen as underlying the difference between classification systems based on feature specifications, as used in unification formalisms (Shieber 1986), and clustering based on a variable degree of granularity (e.g. Pereira, Tishby and Lee 1993).\\n\\nIt seems unlikely that these continuously variable aspects of fluent natural language can be captured by a purely combinatoric model. This naturally leads to the question of how best to introduce quantitative modeling into language processing. It is not, of course, necessary for the quantities of a quantitative model to be probabilities. For example, we may wish to define real-valued functions on parse trees that reflect the extent to which the trees conform to, say, minimal attachment and parallelism between conjuncts. Such functions have been used in tandem with statistical functions in experiments on disambiguation (for instance Alshawi and Carter 1994). Another example is connection strengths in neural network approaches to language processing, though it has been shown that certain networks are effectively computing probabilities (Richard and Lippmann 1991).\\n\\nNevertheless, probability theory does offer a coherent and relatively well understood framework for selecting between uncertain alternatives, making it a natural choice for quantitative language processing. The case for probability theory is strengthened by a well developed empirical methodology in the form of statistical parameter estimation. There is also the strong connection between probability theory and the formal theory of information and communication, a connection that has been exploited in speech recognition, for example using the concept of entropy to provide a motivated way of measuring the complexity of a recognition problem (Jelinek et al. 1992).\\n\\nEven if probability theory remains, as it currently is, the method of choice in making language processing quantitative, this still leaves the field wide open in terms of carving up language processing into an appropriate set of events for probability theory to work with. For translation, a very direct approach using parameters based on surface positions of words in source and target sentences was adopted in the Candide system (Brown et al. 1990). However, this does not capture important structural properties of natural language. Nor does it take into account generalizations about translation that are independent of the exact word order in source and target sentences. Such generalizations are, of course, central to qualitative structural approaches to translation (e.g. Isabelle and Macklovitch 1986, Alshawi et al. 1992).\\n\\nDissecting a Logic\\n\\n\\n\\nBased System\\n\\nWe now consider a hypothetical speech translation system in which the language processing components follow a conventional qualitative transfer design. Although hypothetical, this design and its components are similar to those used in existing database query (Rayner and Alshawi 1992) and translation systems (Alshawi et al 1992). More recent versions of these systems have been gradually taking on a more quantitative flavor, particularly with respect to choosing between alternative analyses, but our hypothetical system will be more purist in its qualitative approach.\\n\\nThe overall design is as follows. We assume that a speech recognition subsystem delivers a list of text strings corresponding to transcriptions of an input utterance. These recognition hypotheses are passed to a parser which applies a logic-based grammar and lexicon to produce a set of logical forms, specifically formulas in first order logic corresponding to possible interpretations of the utterance. The logical forms are filtered by contextual and word-sense constraints, and one of them is passed to the translation component. The translation relation is expressed by a set of first order axioms which are used by a theorem prover to derive a target language logical form that is equivalent (in some context) to the source logical form. A grammar for the target language is then applied to the target form, generating a syntax tree whose fringe is passed to a speech synthesizer.\\n\\nTaking the various components in turn, we make a note of undesirable properties that might be improved by quantitative modeling.\\n\\nAnalysis and Generation\\n\\nA grammar, expressed as a set of syntactic rules (axioms) Gsyn and a set of semantic rules (axioms) Gsem is used to support a relation form holding between strings s and logical forms\\n\\nexpressed in first order logic:\\n\\n. The relation form is many-to-many, associating a string with linguistically possible logical form interpretations. In the analysis direction, we are given s and search for logical forms\\n\\n, while in generation we search for strings s given\\n\\n.\\n\\nFor analysis and generation, we are treating strings sand logical forms\\n\\nas object level entities. In interpretation and translation, we will move down from this meta-level reasoning to reasoning with the logical forms as propositions.\\n\\nThe list of text strings handed by the recognizer to the parser can be assumed to be ordered in accordance with some acoustic scoring scheme internal to the recognizer. The magnitude of the scores is ignored by our qualitative language processor; it simply processes the hypotheses one at a time until it finds one for which it can produce a complete logical form interpretation that passes grammatical and interpretation constraints, at which point it discards the remaining hypotheses. Clearly, discarding the acoustic score and taking the first hypothesis that satisfies the constraints may lead to an interpretation that is less plausible than one derivable from a hypothesis further down in the recognition list. But there is no point in processing these later hypotheses since we will be forced to select one interpretation essentially at random.\\n\\nSyntax\\n\\nThe syntactic rules in Gsyn relate `category' predicates\\n\\nc0, c1, c2holding of a string and two spanning substrings (we limit the rules here to two daughters for simplicity):\\n\\n(Here, and subsequently, variables like s0 and s1 are implicitly universally quantified.) Gsyn also includes lexical axioms for particular strings wconsisting of single words: c1(w),   ...  cm(w). For a feature-based grammar, these rules can include conjuncts constraining the values,\\n\\n, of discrete-valued functions fon the strings:\\n\\n.\\n\\nThe main problem here is that such grammars have no notion of a degree of grammatical acceptability - a sentence is either grammatical or ungrammatical. For small grammars this means that perfectly acceptable strings are often rejected; for large grammars we get a vast number of alternative trees so the chance of selecting the correct tree for simple sentences can get worse as the grammar coverage increases. There is also the problem of requiring increasingly complex feature sets to describe idiosyncrasies in the lexicon.\\n\\nSemantics\\n\\nSemantic grammar axioms belonging to Gsem specify a `composition' function g for deriving a logical form for a phrase from those for its subphrases:\\n\\nThe interpretation rules for strings bottom out in a set of lexical semantic rules associating words with predicates (\\n\\n) corresponding to `word senses'. For a particular word and syntactic category, there will be a (small, possibly empty) finite set of such word sense predicates:\\n\\n...\\n\\n.\\n\\nFirst order logic was assumed as the semantic representation language because it comes with well understood, if not very practical, inferential machinery for constraint solving. However, applying this machinery requires making logical forms fine grained to a degree often not warranted by the information the speaker of an utterance intended to convey. An example of this is explicit scoping which leads (again) to large numbers of alternatives which the qualitative model has difficulty choosing between. Also, many natural language sentences cannot be expressed in first order logic without resort to elaborate formulas requiring complex semantic composition rules. These rules can be simplified by using a higher order logic but at the expense of even less practical inferential machinery.\\n\\nIn applying the grammar in generation we are faced with the problem of balancing over and under-generation by tweaking grammatical constraints, there being no way to prefer fully grammatical target sentences over more marginal ones. Qualitative approaches to grammar tend to emphasize the ability to capture generalizations as the main measure of success in linguistic modeling. This might explain why producing appropriate lexical collocations is rarely addressed seriously in these models, even though lexical collocations are important for fluent generation. The study of collocations for generation fits in more naturally with statistical techniques, as illustrated by Smajda and McKeown (1990).\\n\\nInterpretation\\n\\nIn the logic-based model, interpretation is the process of identifying from the possible interpretations\\n\\nof\\n\\ns for which\\n\\nhold, ones that are consistent with the context of interpretation. We can state this as follows:\\n\\n. Here, we have separated the context into a contingent set of contextual propositions S and a set R of (monolingual) `meaning postulates', or selectional restrictions, that constrain the word sense predicates in all contexts. A is a set of assumptions sufficient to support the interpretation\\n\\ngiven S and R. In other words, this is `interpretation as abduction' (Hobbs et al. 1988), since abduction, not deduction, is needed to arrive at the assumptions A.\\n\\nThe most common types of meaning postulates in R are those for restriction, hyponymy, and disjointness, expressed as follows:\\n\\nrestriction;\\n\\nhyponymy;\\n\\ndisjointness. Although there are compilation techniques (e.g. Mellish 1988) which allow selectional constraints stated in this fashion to be implemented efficiently, the scheme is problematic in other respects. To start with, the assumption of a small set of senses for a word is at best awkward because it is difficult to arrive at an optimal granularity for sense distinctions. Disambiguation with selectional restrictions expressed as meaning postulates is also problematic because it is virtually impossible to devise a set of postulates that will always filter all but one alternative. We are thus forced to under-filter and make an arbitrary choice between remaining alternatives.\\n\\nLogic based translation\\n\\nIn both the quantitative and qualitative models we take a transfer approach to translation. We do not depend on interlingual symbols, but instead map a representation with constants associated with the source language into a corresponding expression with constants from the target language. For the qualitative model, the operable notion of correspondence is based on logical equivalence and the constants are source word sense predicates\\n\\nand\\n\\ntarget sense predicates\\n\\n.\\n\\nMore specifically, we will say the translation relation between a source logical form\\n\\nand a target logical form\\n\\nholds\\n\\nif we have\\n\\nwhere B is a set of monolingual and bilingual meaning postulates, and S is a set of formulas characterizing the current context. A' is a set of assumptions that includes the assumptions A which supported\\n\\n. Here bilingual meaning postulates are first order axioms relating source and target sense predicates. A typical bilingual postulate for translating between p1 and q1 might be of the form:\\n\\n.\\n\\nThe need for the assumptions A' arises when a source language word is vaguer that its possible translations in the target language, so different choices of target words will correspond to translations under different assumptions. For example, the condition p5(x1) above might be proved from the input logical form, or it might need to be assumed.\\n\\nIn the general case, finding solutions (i.e.\\n\\npairs) for the abductive schema is an undecidable theorem proving problem. This can be alleviated by placing restrictions on the form of meaning postulates and input formulas and using heuristic search methods. Although such an approach was applied with some success in a limited-domain system translating logical forms into database queries (Rayner and Alshawi 1992), it is likely to be impractical for language translation with tens of thousands of sense predicates and related axioms.\\n\\nSetting aside the intractability issue, this approach does not offer a principled way of choosing between alternative solutions proposed by the prover. One would like to prefer solutions with `minimal' sets of assumptions, but it is difficult to find motivated definitions for this minimization in a purely qualitative framework.\\n\\nQuantitative Model Components\\n\\nMoving to a Quantitative Model A transfer organization with analysis, transfer, and generation components.\\n\\nMonolingual models that can be used for both analysis and generation.\\n\\nTranslation models that exclusively code contrastive (cross-linguistic) information.\\n\\nHierarchical phrases capturing recursive linguistic structure.\\n\\nInstead of feature based syntax trees and first-order logical forms we will adopt a simpler, monostratal representation that is more closely related to those found in dependency grammars (e.g. Hudson 1984). Dependency representations have been used in large scale qualitative machine translation systems, notably by McCord (1988). The notion of a lexical `head' of a phrase is central to these representations because they concentrate on relations between such lexical heads. In our case, the dependency representation is monostratal in that the relations may include ones normally classified as belonging to syntax, semantics or pragmatics.\\n\\nOne salient property of our language model is that it is strongly lexical: it consists of statistical parameters associated with relations between lexical items and the number and ordering of dependents of lexical heads. This lexical anchoring facilitates statistical training and sensitivity to lexical variation and collocations. In order to gain the benefits of probabilistic modeling, we replace the task of developing large rule sets with the task of estimating large numbers of statistical parameters for the monolingual and translation models. This gives rise to a new cost trade-off in human annotation/judgement versus barely tractable fully automatic training. It also necessitates further research on lexical similarity and clustering (e.g. Pereira, Tishby and Lee 1993, Dagan, Marcus and Markovitch 1993) to improve parameter estimation from sparse data.\\n\\nTranslation via Lexical Relation Graphs\\n\\nThe model associates phrases with relation graphs. A relation graph is a directed labeled graph consisting of a set of relation edges. Each edge has the form of an atomic proposition\\n\\nr(wi,wj)where r is a relation symbol, wi is the lexical head of a phrase and wj is the lexical head of another phrase (typically a subphrase of the phrase headed by wi). The nodes wi and wj are word occurrences representable by a word and an index, the indices uniquely identifying particular occurrences of the words in a discourse or corpus. The set of relation symbols is open ended, but the first argument of the relation is always interpreted as the head and the second as the dependent with respect to this relation. The relations in the models for the source and target languages need not be the same, or even overlap. To keep the language models simple, we will mainly restrict ourselves here to dependency graphs that are trees with unordered siblings. In particular, phrases will always be contiguous strings of words and dependents will always be heads of subphrases.\\n\\nIgnoring algorithmic issues relating to compactly representing and efficiently searching the space of alternative hypotheses, the overall design of the quantitative system is as follows. The speech recognizer produces a set of word-position hypotheses (perhaps in the form of a word lattice) corresponding to a set of string hypotheses for the input. The source language model is used to compute a set of possible relation graphs, with associated probabilities, for each string hypothesis. A probabilistic graph translation model then provides, for each source relation graph, the probabilities of deriving corresponding graphs with word occurrences from the target language. These target graphs include all the words of possible translations of the utterance hypotheses but do not specify the surface order of these words. Probabilities for different possible word orderings are computed according to ordering parameters which form part of the target language model.\\n\\nIn the following section we explain how the probabilities for these various processing stages are combined to select the most likely target word sequence. This word sequence can then be handed to the speech synthesizer. For tighter integration between generation and synthesis, information about the derivation of the target utterance can also be passed to the synthesizer.\\n\\nIntegrated Statistical Model\\n\\nThe probabilities associated with phrases in the above description are computed according to the statistical models for analysis, translation, and generation. In this section we show the relationship between these models to arrive at an overall statistical model of speech translation. We are not considering training issues in this paper, though a number of now familiar techniques ranging from methods for maximum likelihood estimation to direct estimation using fully annotated data are applicable.\\n\\nAs: (acoustic evidence for) source language speech\\n\\nWs: source language word string\\n\\nWt: target language word string\\n\\nCs: source language relation graph\\n\\nCt: target language relation graph\\n\\nGiven a spoken input in the source language, we wish to find a target language string that is the most likely translation of the input. We are thus interested in the conditional probability of Wt given As. This conditional probability can be expressed as follows (cf. Chang and Su 1993):\\n\\nP(W_t | A_s) = _W_s,C_s,C_t  P(W_s | A_s)  P(C_s | W_s,A_s) P(C_t | C_s,W_s,A_s)  P(W_t | C_t,C_s,W_s,A_s).\\n\\nWe now apply some simplifying independence assumptions concerning relation graphs. Specifically, that their derivation from word strings is independent of acoustic information; that their translation is independent of the original words and acoustics involved; and that target word string generation from target relation edges is independent of the source language representations. The extent to which these (Markovian) assumptions hold depend on the extent to which relation edges represent all the relevant information for translation. In particular it means they should express aspects of surface relevant to meaning, such as topicalization, as well as predicate argument structure. In any case, the simplifying assumptions give the following:\\n\\nP(W_t | A_s) _W_s,C_s,C_t P(W_s | A_s)  P(C_s | W_s) P(C_t | C_s)  P(W_t | C_t). This can be rewritten with two applications of Bayes rule:\\n\\n_W_s,C_s,C_t  P(A_s | W_s)  (1/P(A_s))  P(W_s | C_s) P(C_s)  P(C_t | C_s)  P(W_t | C_t).\\n\\nSince As is given, 1/P(As) is a constant which can be ignored in finding the maximum of\\n\\nP(Wt | As). Determining\\n\\nWt that maximizes\\n\\nP(Wt | As) therefore involves the following factors:\\n\\nP(As | Ws): source language acoustics\\n\\nP(Ws | Cs): source language generation\\n\\nP(Cs):       source content relations\\n\\nP(Ct | Cs): source to target transfer\\n\\nP(Wt | Ct): target language generation\\n\\nFinally note that by another application of Bayes rule we can replace the two factors\\n\\nP(Cs) P(Ct | Cs) by\\n\\nP(Ct) P(Cs | Ct) without changing other parts of the model. This latter formulation allows us to apply constraints imposed by the target language model to filter inappropriate possibilities suggested by analysis and transfer. In some respects this is similar to Dagan and Itai's (1994) approach to word sense disambiguation using statistical associations in a second language.\\n\\nLanguage Models\\n\\nLanguage Production Model\\n\\nOur language model can be viewed in terms of a probabilistic generative process based on the choice of lexical `heads' of phrases and the recursive generation of subphrases and their ordering. For this purpose, we can define the head word of a phrase to be the word that most strongly influences the way the phrase may be combined with other phrases. This notion has been central to a number of approaches to grammar for some time, including theories like dependency grammar (Hudson 1976, 1990) and HPSG (Pollard and Sag 1987). More recently, the statistical properties of associations between words, and more particularly heads of phrases, has become an active area of research (e.g. Chang, Luo, and Su 1992; Hindle and Rooth 1993).\\n\\nThe language model factors the statistical derivation of a sentence with word string W as follows:\\n\\nwhere C ranges over relation graphs. The content model, P(C), and generation model, P(W | C), are components of the overall statistical model for spoken language translation given earlier. This decomposition of P(W) can be viewed as first deciding on the content of a sentence, formulated as a set of relation edges according to a statistical model for P(C), and then deciding on word order according to P(W | C).\\n\\nOf course, this decomposition simplifies the realities of language production in that real language is always generated in the context of some situation S (real or imaginary), so a more comprehensive model would be concerned with P(C | S), i.e. language production in context. This is less important, however, in the translation setting since we produce Ct in the context of a source relation graph Cs and we assume the availability of a model for\\n\\nP(Ct | Cs).\\n\\nContent Derivation Model\\n\\nThe model for deriving the relation graph of a phrase is taken to consist of choosing a lexical head h0 for the phrase (what the phrase is `about') followed by a series of `node expansion' steps. An expansion step takes a node and chooses a possibly empty set of edges (relation labels and ending nodes) starting from that node. Here we consider only the case of relation graphs that are trees with unordered siblings.\\n\\nTo start with, let us take the simplified case where a head word h has no optional or duplicated dependents (i.e. exactly one for each relation). There will be a set of edges\\n\\ncorresponding to the local tree rooted at h with dependent nodes\\n\\n. The set of relation edges for the entire derivation is the union of these local edge sets.\\n\\nTo determine the probability of deriving a relation graph Cfor a phrase headed by h0 we make use of parameters (`dependency parameters')\\n\\nP(r(h,w) | h,r)for the probability, given a node h and a relation r, that wis an r-dependent of h. Under the assumption that the dependents of a head are chosen independently from each other, the probability of deriving C is:\\n\\nwhere\\n\\nP(Top(h0)) is the probability of choosing h0 to start the derivation.\\n\\nIf we now remove the assumption made earlier that there is exactly one r-dependent of a head, we need to elaborate the derivation model to include choosing the number of such dependents. We model this by parameters\\n\\nP(N(r,n) | h)that is, the probability that head h has n r-dependents. We will refer to this probability as a `detail parameter'. Our previous assumption amounted to stating that this was always 1 for n=1 or for n=0. Detail parameters allow us to model, for example, the number of adjectival modifiers of a noun or the `degree' to which a particular argument of a verb is optional. The probability of an expansion of h giving rise to local edges E(h) is now:\\n\\n...y estimates that are accurate enough for practical purposes.\\n\\nP(E(h)|h) = _r  P(N(r,n_r)|h) k(n_r)  _1 i n_r  P(r(h,w^r_i) | h,r). where r ranges over the set of relation labels and h has nr r-dependents\\n\\n. k(nr) is a combinatoric constant for taking account of the fact that we are not distinguishing permutations of the dependents (e.g. there are nr! permutations of the  r-dependents of h if these dependents are all distinct).\\n\\nSo if h0 is the root of a tree C, we have\\n\\nwhere heads(C) is the set of nodes in C and EC(h) is the set of edges headed by h in C.\\n\\nThe above formulation is only an approximation for relation graphs that are not trees because the independence assumptions which allow the dependency parameters to be simply multiplied together no longer hold for the general case. Dependency graphs with cycles do arise as the most natural analyses of certain linguistic constructions, but calculating their probabilities on a node by node basis as above may still provide probability estimates that are accurate enough for practical purposes.\\n\\nGeneration Model\\n\\nWe now return to the generation model P(W | C). As mentioned earlier, since C includes the words in W and a set of relations between them, the generation model is concerned only with surface order. One possibility is to use `bi-relation' parameters for the probability that an ri-dependent immediately follows an rj-dependent. This approach is problematic for our overall statistical model because such parameters are not independent from the `detail' parameters specifying the number of r-dependents of a head.\\n\\nWe therefore adopt the use of `sequencing' parameters, these being probabilities of particular orderings of dependents given that the multiset of dependency relations is known. We let the identity relation e stand for the head itself. Specifically, we have parameters P(s|M(s))where s is a sequence of relation labels including an occurrence of e and M(s) is the multiset for this sequence. For a head h in a relation graph C, let sWCh be the sequence of dependent relations induced by a particular word string Wgenerated from C. We now have\\n\\nwhere h ranges over all the heads in C, and nr is the number of occurrences of r in sWCh, assuming that all orderings of nr-dependents are equally likely. We can thus use these sequencing parameters directly in our overall model.\\n\\ntopmost head parameters  P(Top(h))\\n\\ndependency parameters\\n\\nP(r(h,w)|h,r)\\n\\ndetail parameters\\n\\nP(N(r,n)|h)\\n\\nsequencing parameters P(s|M(s))\\n\\nThe overall model splits the contributions of content P(C)and ordering P(W|C). However, we may also want a model for P(W), for example for pruning speech recognition hypotheses. Combining our content and ordering models we get:\\n\\nP(W) = _C  P(C)  P(W | C) = _C P(Top(h_C))   _h W  P(s_WCh|h) _r(h,w) E_C(h)  P(r(h,w)|h,r) The parameters P(s|h) can be derived by combining sequencing parameters with the detail parameters for h.\\n\\nTranslation Model\\n\\nMapping Relation Graphs\\n\\nAs already mentioned, the translation model defines mappings between relation graphs Cs for the source language and Ct for the target language. A direct (though incomplete) justification of translation via relation graphs may be based on a simple referential view of natural language semantics. Thus nominals and their modifiers pick out entities in a (real or imaginary) world, verbs and their modifiers refer to actions or events in which the entities participate in roles indicated by the edge relations. Under this view, the purpose of the translation mapping is to determine a target language relation graph that provides the best approximation to the referential function induced by the source relation graph. We call this approximating referential equivalence.\\n\\nThis referential view of semantics is not adequate for taking account of much of the complexity of natural language including many aspects of quantification, distributivity and modality. This means it cannot capture some of the subtleties that a theory based on logical equivalence might be expected to. On the other hand, when we proposed a logic based approach as our qualitative model, we had to restrict it to a simple first order logic anyway for computational reasons, and even then it did not appear to be practical. Thus using the more impoverished lexical relations representation may not be costing us much in practice.\\n\\nOne aspect of the representation that is particularly useful in the translation application is its convenience for partial and/or incremental representation of content - we can refine the representation by the addition of further edges. A fully specified denotation of the meaning of a sentence is rarely required for translation, and as we pointed out when discussing logic representations, a complete specification may not have been intended by the speaker. Although we have not provided a denotational semantics for sets of relation edges, we anticipate that this will be possible along the lines developed in monotonic semantics (Alshawi and Crouch 1992).\\n\\nTranslation Parameters\\n\\nTo be practical, a model for\\n\\nP(Ct|Cs) needs to decompose the source and target graphs Cs and Ct into subgraphs small enough that subgraph translation parameters can be estimated. We do this with the help of `node alignment relations' between the nodes of these graphs. These alignment relations are similar in some respects to the alignments used by Brown et al. (1990) in their surface translation model. The translation probability is then the sum of probabilities over different alignments f:\\n\\n. There are different ways to model\\n\\nP(Ct,f|Cs) corresponding to different kinds of alignment relations and different independence assumptions about the translation mapping.\\n\\nFor our quantitative design, we adopt a simple model in which lexical and relation (structural) probabilities are assumed to be independent. In this model the alignment relations are functions from the word occurrence nodes of Ct to the word occurrences of Cs. The idea is that\\n\\nf(vj)=wi means that the source word occurrence wi `gave rise' to the target word occurrence vj. The inverse relation f[-1] need not be a function, allowing different numbers of words in the source and target sentences.\\n\\nWe decompose\\n\\nP(Ct,f|Cs) into `lexical' and `structural' probabilities as follows:\\n\\nP(Ct,f|Cs) = P(Nt,f|Ns)  P(Et|Nt,f,Cs)where Nt and Ns are the node sets for Ct and Csrespectively, and Et is the set of edges for the target graph.\\n\\nThe first factor\\n\\nP(Nt,f|Ns) is the lexical component in that it does not take into account any of the relations in the source graph Cs. This lexical component is the product of alignment probabilities for each node of Ns:\\n\\nP(N_t,f|N_s)= _w_i N_s P(f^-1(w_i)= {v_i^1 ...v_i^n} | w_i). That is, the probability that f maps exactly the (possibly empty) subset\\n\\nof Nt to wi. These sets are assumed to be disjoint for different source graph nodes, so we can replace the factors in the above product with parameters: P(M | w)where w is a source language word and M is a multiset of target language words.\\n\\nWe will derive a target set of edges Et of Ct by k derivation steps which partition the set of source edges Es into subgraphs\\n\\n. These subgraphs give rise to disjoint sets of relation edges\\n\\nwhich together form Et. The structural component of our translation model will be the sum of derivation probabilities for such an edge set Et.\\n\\nFor simplicity, we assume here that the source graph Cs is a tree. This is consistent with our earlier assumptions about the source language model. We take our partitions of the source graph to be the edge sets for local trees. This ensures that the the partitioning is deterministic so the probability of a derivation is the product of the probabilities of derivation steps. More complex models with larger partitions rooted at a node are possible but these require additional parameters for partitioning. For the simple model it remains to specify derivation step probabilities.\\n\\nThe probability of a derivation step is given by parameters of the form:\\n\\nP(T'i|S'i,fi)where S'i and T'i are unlabeled graphs and fi is a node alignment function from T'i to S'i. Unlabeled graphs are just like our relation edge graphs except that the nodes are not labeled with words (the edges still have relation labels). To apply a derivation step we need a notion of graph matching that respects edge labels: g is an isomorphism (modulo node labels) from a graph G to a graph H if g is a one-one and onto function from the nodes of G to the nodes of H such that\\n\\niff\\n\\n.\\n\\nThe derivation step with parameter\\n\\nP(T'i|S'i,fi) is applicable to the source edges Si, under the alignment f, giving rise to the target edges Ti if (i) there is an isomorphism hi from S'i to Si (ii) there is an isomorphism gi from Ti to T'i (iii) for any node vof Ti it must be the case that\\n\\nhi(fi(gi(v))) = f(v). This last condition ensures that the target graph partitions join up in a way that is compatible with the node alignment f.\\n\\nThe factoring of the translation model into these lexical and structural components means that it will overgenerate because these aspects are not independent in translation between real natural languages. It is therefore appropriate to filter translation hypotheses by rescoring according to the version of the overall statistical model that included the factors\\n\\nP(Ct)P(Cs|Ct) so that the target language model constrains the output of the translation model. Of course, in this case we need to model the translation relation in the `reverse' direction. This can be done in a parallel fashion to the forward direction described above.\\n\\nConclusions\\n\\nThe quantitative model is in a much better position to cope with these problems. It is less brittle because statistical associations have replaced constraints (featural, selectional, etc.) that must be satisfied exactly. The probabilistic models give us a systematic and well motivated way of ranking alternative hypotheses. Computationally, the quantitative model lets us escape from the undecidability of logic-based reasoning. Because this model is highly lexical, we can hope that the input words will allow effective pruning by limiting the number of search paths having significantly high probabilities.\\n\\nWe retained some of the basic assumptions about the structure of language when moving to the quantitative model. In particular, we preserved the notion of hierarchical phrase structure. Relations motivated by dependency grammar made it possible to do this without giving up sensitivity to lexical collocations which underpin simple statistical models like N-grams. The quantitative model also reduced overall complexity in terms of the sets of symbols used. In addition to words, it only required symbols for dependency relations, whereas the qualitative model required symbol sets for linguistic categories and features, and a set of word sense symbols. Despite their apparent importance to translation, the quantitative system can avoid the use of word sense symbols (and the problems of granularity they give rise to) by exploiting statistical associations between words in the target language to filter implicit sense choices.\\n\\ninherent lexical sensitivity of dependency representations, facilitating parameter estimation;\\n\\nquantitative preference based on probabilistic derivation and translation;\\n\\nincremental and/or partial specification of the content of utterances, particularly useful in translation;\\n\\ndecomposition of complex utterances through recursive linguistic structure.\\n\\nAcknowledgements\\n\\nI am grateful to Fernando Pereira, Mike Riley, and Ido Dagan for valuable discussions on the issues addressed in this paper. Fernando Pereira and Ido Dagan also provided helpful comments on a draft of the paper.\\n\\nReferences\\n\\nAlshawi, H., D. Carter, B. Gamback and M. Rayner. 1992. ``Swedish-English QLF Translation''. In H. Alshawi (ed.) The Core Language Engine, Cambridge, Mass. : MIT Press.\\n\\nAlshawi, H. and R. Crouch. 1992. ``Monotonic Semantic Interpretation''. Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, Newark, Delaware.\\n\\nAlshawi, H. and D. Carter. 1994. ``Training and Scaling Preference Functions for Disambiguation''. To appear in Computational Linguistics.\\n\\nBrill, E. 1993. ``Automatic Grammar Induction and Parsing Free Text: A Transformation-Based Approach''.Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, 259-265.\\n\\nBrown, P., J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, J. Lafferty, R. Mercer and P. Rossin. 1990. ``A Statistical Approach to Machine Translation''. Computational Linguistics 16:79-85.\\n\\nChang, J., Y. Luo, and K. Su. 1992. ``GPSM: A Generalized Probabilistic Semantic Model for Ambiguity Resolution''. Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, 177-192.\\n\\nChang, J., K. Su. 1993. ``A Corpus-Based Statistics-Oriented Transfer and Generation Model for Machine Translation''. Proceedings of the 5th International Conference on Theoretical and Methodological Issues in Machine Translation.\\n\\nDagan I. and A. Itai. 1994. ``Word Sense Disambiguation Using a Second Language Monolingual Corpus''. To appear in Computational Linguistics.\\n\\nDagan, I., S. Marcus and S. Markovitch. 1993. ``Contextual Word Similarity and Estimation from Sparse Data''. Proceedings of the 31st meeting of the Association for Computational Linguistics, ACL, 164-171.\\n\\nGazdar, G., E. Klein, G.K. Pullum, and I.A.Sag. 1985. Generalised Phrase Structure Grammar. Oxford: Blackwell.\\n\\nHindle, D. and M. Rooth. 1993. ``Structural Ambiguity and Lexical Relations''. Computational Linguistics 19:103-120.\\n\\nHobbs, J.R., M. Stickel, P. Martin and D. Edwards. 1988. ``Interpretation as Abduction'', Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, Buffalo, New York, 95-103.\\n\\nHudson, R.A. 1984. Word Grammar. Oxford: Blackwell.\\n\\nIsabelle, P. and E. Macklovitch. 1986. ``Transfer and MT Modularity'', Eleventh International Conference on Computational Linguistics, Bonn, 115-117.\\n\\nJelinek, F., R.L. Mercer and S. Roukos. 1992. ``Principles of Lexical Language Modeling for Speech Recognition''. In S. Furui and M.M. Sondhi (eds. ), Advances in Speech Signal Processing, New York: Marcel Dekker Inc.\\n\\nMellish, C.S. 1988. ``Implementing Systemic Classification by Unification''. Computational Linguistics 14:40-51.\\n\\nMcCord, M. 1988. ``A Multi-Target Machine Translation System''. Proceedings of the International Conference on Fifth Generation Computer Systems, Tokyo, Japan, 1141-1149.\\n\\nPereira, F., N. Tishby and L. Lee. 1993. ``Distributional Clustering of English Words''. Proceedings of the 31st meeting of the Association for Computational Linguistics, ACL, 183-190.\\n\\nPollard, C.J. and I.A. Sag. 1987. Information Based Syntax and Semantics: Volume I -- Fundamentals. CSLI Lecture Notes, Number 13. Center for the Study of Language and Information, Stanford, California.\\n\\nRayner, M. and H.  Alshawi. 1992. ``Deriving Database Queries from Logical Forms by Abductive Definition Expansion''. Proceedings of the Third Conference on Applied Natural Language Processing, Trento, Italy.\\n\\nRichard, M.D. and R.P. Lippmann. 1991. ``Neural Network Classifiers Estimate Bayesian a posteriori Probabilities''. Neural Computation 3:461-483.\\n\\nShieber, S.M. 1986. An Introduction to Unification-Based Approaches to Grammar. CSLI Lecture Notes, Number 4. Center for the Study of Language and Information, Stanford, California.\\n\\nSmajda, F. and K. McKeown. 1990. ``Automatically Extracting and Representing Collocations for Language Generation''. In Proceedings of the 28th Annual Meeting  of the Association for Computational Linguistics, Pittsburgh.\\n\\nTaylor, L., C. Grover, and E.J. Briscoe. 1989. ``The Syntactic Regularity of English Noun Phrases''. Proceedings of the 4th European ACL Conference, 256-263.\\n\\nWeaver, W. 1955. ``Translation''. In W. Locke and A. Booth (eds. ), Machine Translation of Languages, Cambridge, Mass. : MIT Press.\", metadata={'source': '../data/raw/cmplg-xml/9408014.xml'}),\n",
       " Document(page_content=\"Experimentally Evaluating Communicative Strategies: The Effect of the Task\\n\\nEffective problem solving among multiple agents requires a better understanding of the role of communication in collaboration. In this paper we show that there are communicative strategies that greatly improve the performance of resource-bounded agents, but that these strategies are highly sensitive to the task requirements, situation parameters and agents' resource limitations. We base our argument on two sources of evidence: (1) an analysis of a corpus of 55 problem solving dialogues, and (2) experimental simulations of collaborative problem solving dialogues in an experimental world, Design-World, where we parameterize task requirements, agents' resources and communicative strategies.\\n\\nIntroduction\\n\\nCognitive effort is involved in processes such as making inferences and swapping items from long term memory into working memory. When agents have limited working memory, then only a limited number of items can be  SALIENT, i.e. accessible in working memory. Since other processes, e.g. inference, operate on salient items, an inference process may require the cognitive effort involved with retrieving items from long term memory, in addition to the effort involved with reasoning itself.\\n\\n[] (20) H: Right. The maximum amount of credit that you will be able to get will be 400 that they will be able to get will be 400 dollars on their tax return  (21) C: 400 dollars for the whole year? (22) H: Yeah it'll be 20%  (23) C: um hm (24) H: Now if indeed they pay the $2000 to your wife, that's great. (25) C: um hm  (26a) H: SO WE HAVE 400 DOLLARS. (26b) Now as far as you are concerned, that could cost you more.....\\n\\nUtterances such as 0-26a, that repeat, paraphrase or make inferences explicit, are collectively called  INFORMATIONALLY REDUNDANT UTTERANCES, IRUs. In 0, the utterances that originally added the belief that they will get 400 dollars  to the context are in italics and the IRU is given in CAPS.\\n\\nIn order to test the hypothesized relationship of communicative strategies to agents' resource limits we developed a test-bed environment, Design-World, in which we vary task requirements, agents' resources and communicative strategies. Our artificial agents are based on a cognitive model of attention and memory. Our experimental results show that communicative strategies that incorporate IRUs can help resource-limited cognitive agents coordinate, limit processing, and improve the quality and robustness of the problem solution. We will show that the task determines whether a communicative strategy is beneficial, depending on how the task is defined in terms of fault intolerance and the level of belief coordination required.\\n\\nDesign-World Task and Agent Architecture\\n\\nWhen an agent retrieves items from memory, search starts from the current pointer location and spreads out in a spherical fashion. Search is restricted to a particular search radius: radius is defined in Hamming distance. For example if the current memory pointer loci is (0 0 0), the loci distance 1 away would be (0 1 0) (0 -1 0) (0 0 1) (0 0 -1) (-1 0 0) (1 0 0). The actual locations are calculated modulo the memory size. The limit on the search radius defines the capacity of attention/working memory and hence defines which stored beliefs and intentions are  SALIENT.\\n\\nThe radius of the search sphere in the AWM model is used as the parameter for Design-World agents' resource-bound on attentional capacity. In the experiments below, memory is 16x16x16 and the radius parameter varies between 1 and 16, where AWM of 1 gives severely attention limited agents and AWM of 16 means that everything an agent knows is salient.\\n\\nDesign\\n\\n\\n\\nWorld Communicative Strategies\\n\\nA discourse act may be left implicit, or may be varied to consist of one or more communicative acts. Discourse acts are different from actions on the environment because they are actions whose intended effect is a change in the other agent's mental state. Because the other agent is an active intelligence, it is possible for it to supplement an underspecified discourse action with its own processing. The variation in the degree of explicitness of a discourse act is the basis of agents' communicative strategies. Here we will compare three communicative strategies: (1) All-Implicit; (2) Close-Consequence; and (3) Explicit-Warrant.\\n\\nThe All-Implicit strategy is a `bare bones' strategy, exemplified by the partial dialogue in 1. In 1 each utterance is shown both as a gloss in italics, and in the artificial language that the agents communicate with.\\n\\n[1:] BILL: Then, let's put the green rug in the study. (propose agent-bill agent-kim option-43:  put-act (agent-bill green rug room-1))\\n\\n[2:] KIM: Then, let's put the green lamp in the study. (propose agent-kim agent-bill option-61:  put-act (agent-kim green lamp room-1))\\n\\n[3:] BILL: No, instead let's put the green couch in the study. (reject agent-bill agent-kim option-75:  put-act (agent-bill green couch room-1))\\n\\n.....\\n\\nIn dialogue 1 agent CLC uses the Close-Consequence strategy. CLC makes explicit  CLOSING statements, such as 1-2, on the completion of the intention associated with a discourse segment. CLC's  CLOSING discourse act also includes IRUs as in 1-3; CLC makes the inference explicit that since they have agreed on putting the green rug in the study, Bill no longer has the green rug (act-effect inference).\\n\\n[1:] BILL: Then, let's put the green rug in the study. (propose agent-bill agent-clc option-30: put-act (agent-bill green rug room-1))\\n\\n[2:] CLC: So, we've agreed to put the green rug in the study. (close agent-clc agent-bill intended-30: put-act (agent-bill green rug room-1))\\n\\n[3:] CLC:  AGENT-BILL DOESN'T HAVE GREEN RUG. (say agent-clc agent-bill bel-48: has n't (agent-bill green rug))\\n\\nThe Explicit-Warrant strategy varies the proposal discourse act by including  WARRANT IRUs in each proposal. In general a WARRANT for an intention is a reason for adopting the intention, and here  WARRANTS are the score propositions that give the utility of the proposal, which are mutually believed at the outset of the dialogues. In 1, the  WARRANT IRU is in  CAPS.\\n\\n[1:] IEI:   PUTTING IN THE GREEN RUG IS WORTH  56 (say agent-iei agent-iei2 bel-265: score  (option-202:  put-act (agent-bill green rug room-1) 56))\\n\\n[2:] IEI: Then, let's put the green rug in the study. (propose agent-iei agent-iei2 option-202:  put-act (agent-bill green rug room-1))\\n\\nDesign World Task Variations\\n\\nStandard Task\\n\\nThe Standard task is defined so that the  RAW SCORE that agents achieve for a  DESIGN-HOUSE plan, constructed via the dialogue, is the sum of all the furniture items for each valid step in their plan. The point values for invalid steps in the plan are simply subtracted from the score so that agents are not heavily penalized for making mistakes.\\n\\nZero Invalids Task\\n\\nZero NonMatching Beliefs Task\\n\\nExperimental Results\\n\\nA strategy A is defined to be  BENEFICIAL as compared to a strategy B, for a set of fixed parameter settings, if the difference in distributions using the Kolmogorov-Smirnov two sample test is significant at p [ .05, in the positive direction, for two or more AWM settings. A strategy is  DETRIMENTAL if the differences go in the negative direction. Strategies may be neither  BENEFICIAL or  DETRIMENTAL, since there may be no difference between two strategies.\\n\\nIn the reminder of this section, we first compare within strategy, for each task definition and show that whether or not a strategy is beneficial depends on the task. Then we compare across strategies for a particular task, showing that the interaction of the strategy and task varies according to the strategy. The comparisons will show that what counts as a good collaborative strategy depends on cognitive limits on attention and the definition of success for the task.\\n\\nClose Consequence\\n\\nExplicit Warrant\\n\\nWhen the task is Zero-Invalid (no figure due to space), the benefits of the Explicit-Warrant strategy are dampened from the benefits of the Standard task, because Explicit-Warrant does nothing to address the reasons for agents making mistakes. In comparison with the All-Implicit strategy, it is detrimental at AWM of 1 and 2, but is still beneficial at AWM of 5,6,7, and 11.\\n\\nRelated Work\\n\\nConclusion\\n\\nIn this paper we showed that collaborative communicative behavior cannot be defined in the abstract: what counts as collaborative depends on the task, and the definition of success in the task. We used two empirical methods to support our argument: corpus based analysis and experimentation in Design-World. The methods and the focus of this work are novel; previous work on resource limited agents has not examined the role of communicative strategies in multi-agent interaction whereas work on communication has not considered the effects of resource limits.\\n\\nWe showed that strategies that are inefficient under assumptions of perfect reasoners with unlimited attention and retrieval are effective with resource limited agents. Furthermore, different tasks make different cognitive demands, and place different requirements on agents' collaborative behavior. Tasks which require a high level of belief coordination can benefit from communicative strategies that include redundancy. Fault intolerant tasks benefit from redundancy for rehearsing the effects of actions.\\n\\nBecause the communicative strategies that we tested were based on a corpus analysis of human human financial advice dialogues and because variations in the Design-World task were parametrized, we believe the results presented here may be domain independent, though clearly more research is needed.\\n\\nBibliography\\n\\nJames F. Allen and C. Raymond Perrault. Analyzing intention in utterances. Artificial Intelligence, 15:143-178, 1980.\\n\\nJ. R. Anderson and G. H. Bower. Human Associative Memory. V.H. Winston and Sons, 1973.\\n\\nAlan Baddeley.\\n\\nWorking Memory.\\n\\nOxford University Press, 1986.\\n\\nMichael Bratman, David Israel, and Martha Pollack. Plans and resource bounded practical reasoning. Computational Intelligence, 4:349-355, 1988.\\n\\nJean C. Carletta. Risk Taking and Recovery in Task-Oriented Dialogue. PhD thesis, Edinburgh University, 1992.\\n\\nAlison Cawsey, Julia Galliers, Steven Reece, and Karen Sparck Jones. Automating the librarian: A fundamental approach using belief revision. Technical Report 243, Cambridge Computer Laboratory, 1992.\\n\\nA. Chapanis, R.B. Ochsman, R.N. Parrish, and G.D. Weeks. Studies in interactive communication: The effects of four communication modes on the behavior of teams during cooperative problem-solving. Human Factors, 14:487-509, 1972.\\n\\nJon Doyle. Rationality and its roles in reasoning. Computational Intelligence, November 1992.\\n\\nTimothy W. Finin, Aravind K. Joshi, and Bonnie Lynn Webber. Natural language interactions with artificial experts. Proceedings of the IEEE, 74(7):921-938, 1986.\\n\\nJulia R. Galliers. Autonomous belief revision and communication. In P. Gardenfors, editor, Belief Revision, pages 220 - 246. Cambridge University Press, 1991.\\n\\nCurry I. Guinn. A computational model of dialogue initiative in collaborative discourse. In AAAI Technical Report FS-93-05, 1993.\\n\\nSteve Hanks, Martha E. Pollack, and Paul R. Cohen. Benchmarks, testbeds, controlled experimentation and the design of agent architectures. AI Magazine, December 1993.\\n\\nD. L. Hintzmann and R. A. Block. Repetition and memory: evidence for a multiple trace hypothesis. Journal of Experimental Psychology, 88:297-306, 1971.\\n\\nThomas K. Landauer. Memory without organization: Properties of a model with random storage and undirected retrieval. Cognitive Psychology, pages 495-531, 1975.\\n\\nDonald A. Norman and Daniel G. Bobrow. On data-limited and resource-limited processes. Cognitive Psychology, 7(1):44-6, 1975.\\n\\nMartha Pollack, Julia Hirschberg, and Bonnie Webber. User participation in the reasoning process of expert systems. In AAAI82, 1982.\\n\\nMartha E. Pollack and Marc Ringuette. Introducing the Tileworld: Experimentally Evaluating Agent Architectures. In AAAI90, pages 183-189, 1990.\\n\\nCandace Sidner. Using discourse to negotiate in collaborative activity: An artificial language. AAAI Workshop on Cooperation among Heterogeneous Agents, 1992.\\n\\nSidney Siegel. Nonparametric Statistics for the Behavioral Sciences. McGraw Hill, 1956.\\n\\nAdelheit Stein and Ulrich Thiel. A conversational model of multimodal interaction. In AAAI93, 1993.\\n\\nMarilyn Walker. Informational redundancy and resource bounds in dialogue. In AAAI Spring Symposium on Reasoning about Mental States, 1993. Also available as IRCS techreport IRCS-93-20, University of Pennsylvania.\\n\\nMarilyn A. Walker. Redundancy in collaborative dialogue. In Fourteenth International Conference on Computational Linguistics, pages 345-351, 1992.\\n\\nMarilyn A. Walker. Informational Redundancy and Resource Bounds in Dialogue. PhD thesis, University of Pennsylvania, 1993.\\n\\nMarilyn A. Walker. Testing collaborative strategies by computational simulation: Cognitive and task effects. Knowledge Based Systems, 1995. March.\\n\\nMarilyn A. Walker and Steve Whittaker. Mixed initiative in dialogue: An investigation into discourse segmentation. In Proc. 28th Annual Meeting of the ACL, pages 70-79, 1990.\\n\\nSteve Whittaker, Erik Geelhoed, and Elizabeth Robinson. Shared workspaces: How do they work and when are they useful? IJMMS, 39:813-842, 1993.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9408015.xml'}),\n",
       " Document(page_content=\"Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs Introduction CCG\\n\\nCCG is a grammatical formalism in which there is a one-to-one correspondence between the rules of composition at the level of syntax and logical form. Each word is (perhaps ambiguously) assigned a category and LF, and when the syntactical operations assign a new category to a constituent, the corresponding semantic operations produce a new LF for that constituent as well. The CCG rules\\n\\nshown in Figure 1 are implemented in the system described in this paper. Each of the three operations have both a forward and backward variant.\\n\\nImplementation of Coordination\\n\\nConclusion\\n\\nWe have shown how higher-order logic programming can be used to elegantly implement the semantic theory of CCG, including the previously difficult case of its handling of coordination constructs. The techniques used here should allow similar advantages for a variety of such theories.\\n\\nAcknowledgments\\n\\nThis work is supported by ARO grant DAAL03-89-0031, DARPA grant N00014-90-J-1863, and ARO grant DAAH04-94-G-0426. I would like to thank Aravind Joshi, Dale Miller, Jong Park, and Mark Steedman for valuable discussions and comments on earlier drafts.\\n\\nBibliography\\n\\nDavid Dowty. 1988. Type raising, functional composition, and non-constituent conjunction. In Richard T. Oehrle, Emmon Bach, and Deirdre Wheeler, editors, Categorial Grammars and Natural Language Structures. Reidel, Dordrecht, pages 153-198.\\n\\nGerald Gazdar. 1988. Applicability of indexed grammars to natural languages. In U. Reyle and C. Rohrer, editors, Natural language parsing and linguistic theories. Reidel, Dordrecht, pages 69-94.\\n\\nJerry R. Hobbs and Stuart M. Shieber. 1987. An algorithm for generating quantifier scopings. Computational Linguistics, 13:47-63.\\n\\nEinar Jowsey. 1990. Constraining Montague Grammar for Computational Applications. PhD thesis, University of Edinburgh.\\n\\nDale Miller. 1990. A logic programming language with lambda abstraction, function variables and simple unification. In P. Schroeder-Heister, editor, Extensions of Logic Programming, Lecture Notes in Artifical Intelligence, Springer-Verlag, 1990.\\n\\nDale Miller. 1991. Abstract syntax and logic programming. In Proceedings of the Second Russian Conference on Logic Programming, September 1991.\\n\\nDale Miller and Gopalan Nadathur. 1986. Some uses of higher-order logic in computational linguistics. In 24th Annual Meeting of the Association for Computational Linguistics, pages 247-255.\\n\\nDale Miller, Gopalan Nadathur, Frank Pfenning, Andre Scedrov. 1991. Uniform proofs as a foundation for logic programming. In Annals of Pure and Applied Logic, 51:125-157.\\n\\nRobert C. Moore. 1989. Unification-based semantic interpretation. In 27th Annual Meeting of the Association for Computational Linguistics, pages 33-41.\\n\\nRemo Pareschi. 1989. Type-Driven Natural Language Aanalysis. PhD thesis, University of Edinburgh.\\n\\nJong C. Park. 1992. A unification-based semantic interpretation for coordinate constructs. In 30th Annual Meeting of the Association for Computational Linguistics, pages 209-215.\\n\\nJong C. Park. 1995. Quantifier scope and constituency. In 33rd Annual Meeting of the Association for Computational Linguistics (this volume).\\n\\nBarbara Partee and Mats Rooth. 1983. Generalized conjunction and type ambiguity. In Rainer Bauerle, Christoph Schwarze, and Arnim von Stechow, editors, Meaning, Use, and Interpretation of Language. W. de Gruyter, Berlin, pages 361-383.\\n\\nFernando C.N. Pereira. 1990. Semantic interpretation as higher-order deduction. In Jan van Eijck, editor, Logics in AI: European Workshop JELIA'90, Lecture Notes in Artificial Intelligence number 478, pages 78-96. Springer-Verlag, Berlin, Germany.\\n\\nFernando C.N. Pereira and Stuart M. Shieber. 1987. Prolog and Natural-Language Analysis. Number 10 in CSLI Lecture Notes. Center for the Study of Language and Information, Stanford, California, 1985. Distributed by the University of Chicago Press.\\n\\nFrank Pfenning and Conal Elliot. 1988. Higher-order abstract syntax. In Proceedings of the ACM-SIGPLAN Conference on Programming Language Design and Implementation, 1988.\\n\\nMark J. Steedman. 1990. Gapping as constituent coordination. In Linguistics and Philosophy 13, pages 207-263\\n\\nDavid Weir. 1988. Characterizing Mildly Context-sensitive Grammar Formalism. CIS-88-74, PhD thesis, University of Pennsylvania.\\n\\nDavid Weir and Aravind Joshi. 1988. Combinatory categorial grammars: generative power and relation to linear CF rewriting systems. In 26th Annual Meeting of the Association for Computational Linguistics, pages 278-285.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9506004.xml'}),\n",
       " Document(page_content='On Descriptive Complexity, Language Complexity, and GB1\\n\\nWe introduce\\n\\n, a monadic second-order language for reasoning about trees which characterizes the strongly Context-Free Languages in the sense that a set of finite trees is definable in\\n\\niff it is (modulo a projection) a Local Set--the set of derivation trees generated by a CFG. This provides a flexible approach to establishing language-theoretic complexity results for formalisms that are based on systems of well-formedness constraints on trees. We demonstrate this technique by sketching two such results for Government and Binding Theory. First, we show that free-indexation, the mechanism assumed to mediate a variety of agreement and binding relationships in GB, is not definable in\\n\\nand therefore not enforcible by CFGs. Second, we show how, in spite of this limitation, a reasonably complete GB account of English can be defined in\\n\\n. Consequently, the language licensed by that account is strongly context-free. We illustrate some of the issues involved in establishing this result by looking at the definition, in\\n\\n, of chains. The limitations of this definition provide some insight into the types of natural linguistic principles that correspond to higher levels of language complexity. We close with some speculation on the possible significance of these results for generative linguistics.\\n\\nIntroduction\\n\\nOne of the more significant developments in generative linguistics over the last decade has been the development of constraint-based formalisms--grammar formalisms that define languages not in terms of the derivations of the strings in the language, but rather in terms of well-formedness conditions on the structures analyzing their syntax. Because traditional notions of language complexity are generally defined in terms of rewriting mechanisms, complexity of the languages licensed by these formalisms can be difficult to determine.\\n\\nBut formal language theory still has much to offer to generative linguistics. Language complexity provides one of the most useful measures with which to compare languages and language formalisms. We have an array of results establishing the boundaries of these classes, and, while many of the results do not seem immediately germane to natural languages, even seemingly artificial diagnostics (like the copy language\\n\\n, capable of encoding such constraints lucidly. The key merit of such an encoding is the fact that sets of trees are definable in\\n\\nif and only if\\n\\nthey are strongly context\\n\\n\\n\\nfree. Thus definability in\\n\\nL[2]K,P\\n\\nWe have, then, two conflicting criteria for our language. It must be expressive enough to capture the relationships that define the trees licensed by the theory, but it must be restricted sufficiently to be no more expressive than Context-Free Grammars. In keeping with the first of these our language is intended to support, as transparently as possible, the kinds of reasoning about trees typical of linguistic applications. It includes binary predicates for  the usual structural relationships between the nodes in the trees--parent (immediate domination), domination (reflexive), proper domination (irreflexive), left-of (linear precedence) and equality. In addition, it includes an arbitrary array of monadic predicate constants--constants naming specific subsets of the nodes in the tree. These can be thought of as atomic labels. The formula\\n\\n, for instance, is true at every node labeled N.  It includes, also, a similar array of individual constants--constants naming specific individuals in the tree--although these prove to be of limited usefulness. There are two sorts of variables as well--those that range over nodes in the tree and those that range over arbitrary subsets of those nodes (thus this is is monadic second-order language). Crucially, though, this is all the language includes. By restricting ourselves to this language we restrict ourselves to working with properties that can be expressed in terms of these basic predicates.\\n\\nTo be precise, the actual language we use in a given situation depends on the sets of constants in use in that context. We are concerned then with a family of languages, parameterized by the sets of individual and set constants they employ.\\n\\nWe use infix notation for the fixed predicate constants\\n\\n,\\n\\n,\\n\\n,\\n\\n,\\n\\nand\\n\\n. We use lower-case for individual variables and constants, and upper-case for set variables and predicate constants. Further, we will say X(x) to assert that the individual assigned to the variable x is included in the set assigned to the variable X. So, for instance,\\n\\nasserts that the set assigned to X includes every node dominated by the node assigned to x.\\n\\nTruth, for these languages, is defined relative to a specific class of models. The basic models are just ordinary structures interpreting the individual and predicate constants.\\n\\nIf the domain of\\n\\nis empty (i.e., the model is for a language\\n\\n) we will generally omit it. Models for\\n\\n,\\n\\nthen, are tuples\\n\\n.\\n\\nThe intended class of these models are, in essence, labeled tree domains. A tree domain is the set of node addresses generated by giving the address\\n\\nto the root and giving the children of the node at address w addresses (in order, left to right)\\n\\n, where the centered dot denotes concatenation. Tree domains, then, are particular subsets of\\n\\n.\\n\\n(\\n\\nis the\\n\\nset of natural numbers.)\\n\\n>\\n\\nEvery tree domain has a natural interpretation as a model for\\n\\n(which interprets only the fixed predicate symbols.)\\n\\n>\\n\\nThe structures of interest to us are just those models that are the natural interpretation of a tree domain, augmented with interpretations of additional individual and predicate constants.\\n\\nIn general, satisfaction is relative to an assignment mapping each individual variable into a member of\\n\\nand each predicate variable\\n\\ninto a subset of\\n\\n.\\n\\nWe use\\n\\nto denote that a model M satisfies a formula\\n\\nwith an assignment s.  The notation\\n\\nasserts that M models\\n\\nwith any assignment. When\\n\\nis a sentence (has no unquantified variables) we will usually use this form.\\n\\nProper domination is a defined predicate:\\n\\nDefinability in L[2]K,P\\n\\nWe are interested in the subsets of the class of intended models which are definable in\\n\\nusing any sets K and P. If\\n\\nis a set of\\n\\nsentences in a language\\n\\n, we will use the notation\\n\\nto denote the set of trees, i.e., intended models, that satisfy all of the sentences in\\n\\n. We are interested, then, in the sets of trees that are\\n\\nfor some such\\n\\n. In developing our definitions we can use individual and monadic predicates freely (since K and P can always be taken to be the sets that actually occur in our definitions) and we can quantify over individuals and sets of individuals. We will also use non-monadic predicates and even higher-order predicates, e.g., properties of subsets, but only those that can be explicitly defined, that is, those which can be eliminated by a simple syntactic replacement of the predicate by its definition.\\n\\nThis use of explicitly defined predicates is crucial to the transparency of definitions in\\n\\n. We might, for instance, define a simplified version of government in three steps:\\n\\nin words, x governs y iff it c-commands y and no barrier intervenes between them. It c-commands y iff neither x nor y dominates the other and every branching node that properly dominates x also properly dominates y.\\n\\nis just a monadic predicate; it is within the language of\\n\\n(for suitable P) and its definition is simply a biconditional\\n\\nformula. In contrast, C-Command and Governs are non-monadic and do not occur in\\n\\n. Their definitions, however, are ultimately in terms of monadic predicates and the fixed predicates (parent, etc.) only. One can replace each of their occurrences in a formula with the right hand side of their definitions and eventually derive a formula that is in\\n\\n. We will reserve the use of\\n\\n(in\\n\\ncontrast to\\n\\n) for explicit definitions of non-monadic predicates.\\n\\nDefinitions can also use predicates expressing properties of sets and relations between sets, as long as those properties can be explicitly defined. The subset relation, for instance can be defined:\\n\\nWe can also capture the stronger notion of one set being partitioned by a collection of others:\\n\\nHere\\n\\nis a some sequence of set variables and\\n\\nis shorthand for the disjunction\\n\\nfor all Xi in\\n\\n, etc. There is a distinct instance of\\n\\nfor each sequence\\n\\n, although we can ignore distinctions between sequences of the same length. Finally, we note that finiteness is a definable property of subsets in our intended models. This follows from the fact that these models are linearly ordered by the  lexicographic order relation:\\n\\nand that every non-empty subset of such a model has a least element with respect to that order. A set of nodes, then, is finite iff each of its non-empty subsets has an upper-bound with respect to lexicographic order as well.\\n\\nThese three second-order relations will play a role in the next section.\\n\\nCharacterizing the Local Sets\\n\\nWe can now give an example of a class of sets of trees that is definable in\\n\\n--the local sets (i.e., the sets of derivation trees generated by Context-Free Grammars). The idea behind the definition is simple. Given an arbitrary Context-Free Grammar, we can treat its terminal and non-terminal symbols as monadic predicate constants. The productions of the grammar, then, relate the label of a node to the number and labels of its children. If the set of productions for a non-terminal A, for instance, is\\n\\nwe can translate this as\\n\\nwhere\\n\\nWe can collect such translations of all the productions of the grammar together with sentences requiring nodes labeled with terminal symbols to have no children, requiring the root to be labeled with the start symbol, requiring the sets of nodes labeled with the terminal and non-terminal symbols to partition the set of all nodes in the tree, and requiring that set of nodes to be finite. It is easy to show that the models of this set of sentences are all and only the derivation trees of the grammar. In this way we get the first half of our characterization of the local sets.\\n\\nIt is, perhaps, not surprising that we can define the local sets with\\n\\n. This is superficially quite a powerful language, allowing, as it does, a certain amount of second-order quantification. It is maybe more remarkable that, modulo a projection, the only sets of finite trees (with bounded branching) that are definable in\\n\\nare the local sets.\\n\\nThe proof hinges on the fact that one can translate formulae in\\n\\ninto the language of SnS--the monadic second-order theory of multiple successor functions. This is the monadic second-order theory of the structure\\n\\na generalization of the natural numbers with successor and less-than. The universe, Tn, is the complete n-branching tree domain. The relation\\n\\nis domination,\\n\\nis lexicographic order, and the functions\\n\\nare the successor functions, each taking nodes into their\\n\\nchild (\\n\\nOne way of understanding his proof is via the observation that satisfying assignments for a formula\\n\\n,\\n\\nwith free variables among\\n\\ncan be understood as trees labeled with (subsets of) the variables in\\n\\n. A node is in the set assigned to Xi in\\n\\niff it is labeled with Xi. Rabin showed that, for any\\n\\nin the language of SnS, the set of trees encoding the satisfying assignments for\\n\\nin\\n\\nis accepted by a particular type of finite-state automaton on infinite trees. We say that the set is Rabin recognizable. He goes on to show that emptiness of these sets is decidable. It follows that satisfiability of these formulae, and hence the theory SnS, is decidable.\\n\\nFor us, the key point is the fact that the sets encoding satisfying assignments are Rabin recognizable. It is not difficult to exhibit a syntactic transformation which, given any\\n\\nin\\n\\n,\\n\\nproduces a\\n\\nformula\\n\\nin the language of SnS, where XU is a new variable and\\n\\nis a sequence of new variables (one for each of the finitely many predicates in P that occur in\\n\\n)\\n\\nsuch that,\\n\\niff\\n\\nthat is, the set AU and the sequences of sets\\n\\nand\\n\\nform\\n\\na satisfying assignment for\\n\\nin\\n\\niff the structure consisting of the universe AU along with the natural interpretation of\\n\\n,\\n\\n,\\n\\nand\\n\\non AU, and the sets\\n\\n,\\n\\nsatisfies\\n\\nwith the assignment taking\\n\\ninto\\n\\n. It follows that a set of trees is definable in\\n\\niff\\n\\nthey are Rabin recognizable.\\n\\nare simply recognizable. One can think of these automata as traversing the tree, top down, assigning states to the children of a node on the basis of a transition function that depends on the state of the node, its label, and the position of the child among its siblings. A tree is accepted if it can be labeled by the automaton in such a way that the root is labeled with a start state and the set of states labeling the leaves is one of a set of accepting sets of states. Every set of trees that is accepted in this way is the projection of a local set. To see this, suppose that\\n\\nis a tree accepted by a tree automaton. Then there is some assignment of states to the nodes in\\n\\nthat witnesses this fact.\\n\\nSuppose, for\\n\\ninstance,\\n\\nin which each node is labeled with a pair consisting of the label from\\n\\nand the state assigned to that node. It is easy to show that, given a recognizable set of trees, one can construct a CFG to generate the corresponding set of trees labeled with pairs as in\\n\\n. In the example, for instance, this would include, among others, the productions\\n\\nThe original set of trees is then the first projection of the set generated by the CFG.\\n\\nTogether, these two theorems give us our primary result.\\n\\nNon\\n\\n\\n\\nDefinability of Free Indexation\\n\\nThis characterization provides a powerful tool for establishing strong context-freeness of  classes of languages that are defined by constraints on the structure of the trees analyzing the strings in the language. If one can show that the constraints defining such a set, or perhaps that any constraints in the class employed by a given formalism, can be defined within\\n\\nthen the corresponding language or class of languages is strongly context-free. Much of the value of standard language complexity classes, on the other hand, comes from results that allow one to show that a given language or class of languages is not included in a particular complexity class. Such results are available here as well, in the form of non-definability results for\\n\\nallows definition of known non-CF languages, then clearly that predicate properly extends the power of the language and cannot be definable. In this way, one can show that the predicate\\n\\nwhich holds between two nodes iff the yields of the subtrees rooted at those nodes are labeled identically wrt P is not definable in\\n\\n, for if it were one could define the copy language\\n\\n.\\n\\nIn this section we will explore an approach that is more difficult but is one of the most  general--reduction from the monadic second-order theory of the grid--and will use it to demonstrate non-definability of free-indexation--a mechanism which shows up in a number of modules of GB.\\n\\nThe grid is the structure\\n\\nwhere\\n\\nThis is the structure of the (discrete) first quadrant. Note the similarity to\\n\\n, the structure of two successor functions. The key distinction is the fact that G satisfies the property\\n\\nthat is, the horizontal successor of the vertical successor of a point is the same as the vertical successor of its horizontal successor. Let\\n\\nNow, the monadic second-order theory of any of our intended structures is decidable (by reduction to SnS), as is the monadic second-order theory of any of our intended structures augmented with any predicate that is definable in\\n\\n(since we can reduce this to the theory of the original structure via that definition). Our approach to showing that a predicate is not definable in\\n\\nis to show that the theory of one of our structures augmented with that predicate is not decidable. In particular, we will show that the theory of such a structure includes an undecidable fragment of the monadic second-order theory of the grid.\\n\\nOur focus, in this section, is the mechanism known as free-indexation. In the Government and Binding Theory framework this is the mechanism that is generally assumed to mediate issues like agreement, co-reference of nominals, and identification of moved elements with their traces. In its most general form this operates by assigning indices to the nodes of the tree randomly and then filtering out those assignments that do not meet various constraints on agreement, co-reference, etc. In essence, the indexation is an equivalence relation, one that distinguishes unboundedly many equivalence classes among the nodes of the tree. That is, each value of the index identifies an equivalence class and there is no a priori bound on its maximum value. Free-indexation views constraints on the indexation as a filter that admits only those equivalence relations that meet specific conditions on the relationships between the individuals in these classes.\\n\\nTo see that we cannot define such equivalence relations in\\n\\n,\\n\\nconsider the\\n\\nclass of structures\\n\\nwhere T2 is the complete binary-branching tree domain,\\n\\n,\\n\\n,\\n\\nand\\n\\nare the natural interpretations of parent, domination, and left-of on that domain, and\\n\\nis any arbitrary equivalence relation. Let S2S+CI be the monadic second-order theory of this class of structures. Our claim is that this is an undecidable theory.\\n\\nLewis\\'s proof of the non-decidability of\\n\\nis based on a construction that takes any given Turning Machine M into a formula\\n\\nsuch that\\n\\niff M halts (when started, say, on the empty tape). The idea behind our proof of the non-decidability of S2S+CI is that there is a natural correspondence between points in T2 and those in\\n\\nthat is induced by interpreting node addresses in T2 as paths (non-decreasing in both x and y) from the origin in\\n\\n. Of course, in general, there will be many points in T2 that correspond to the same point in\\n\\n, but we can restrict the interpretation of\\n\\nin such a way that all points in T2 that correspond to the same point in\\n\\nwill be co-indexed. We then restrict the interpretation of the variables in\\n\\nin such a way that it does not break the classes of\\n\\n. In more typically linguistic terms, we require co-indexed nodes to agree on the features in\\n\\n.\\n\\nThe formula\\n\\nof Lewis\\' proof\\n\\ninvolves only the constant\\n\\n,\\n\\nthe successor functions\\n\\nand\\n\\n, some set of (bound) individual variables, the (free) monadic predicate variables in\\n\\n,\\n\\nand the logical connectives.\\n\\nLet\\n\\nThen\\n\\nis true only at the root,\\n\\nis true iff y is the leftmost child of x and\\n\\nis true iff y is the rightmost child of x. These translations are sufficient for us to translate\\n\\ninto a\\n\\nformula\\n\\nthat, when combined with an axiom\\n\\nconstraining the interpretation of\\n\\nand\\n\\nas sketched above, will be satisfiable by a model in the class\\n\\niff\\n\\nis satisfied by G.  That is:\\n\\niff\\n\\nThis in turn implies that\\n\\nDecidability of S2S+CI, then, would imply decidability of the halting problem.\\n\\nIt remains only to define\\n\\n.\\n\\nLet\\n\\nALIGN=\"right\"\\n\\nwhere\\n\\nThis requires that every node is co-indexed with itself, that the left children of co-indexed nodes are co-indexed as are the right children of co-indexed nodes, and that the left child of the right child and right child of the left child of co-indexed nodes are co-indexed. Finally all co-indexed nodes are forced, by\\n\\n, to agree on all predicates in\\n\\n. That this is sufficient to carry the reduction of the halting problem to membership in S2S+CI depends on the fact that\\n\\nforces all points in T2 equivalent in the sense that they correspond to the same point in G as sketched above, to agree on the predicates in\\n\\n. Thus we (roughly) can take the quotient with respect to this equivalence without affecting satisfiability of\\n\\n. The resulting structure is isomorphic to G and satisfies\\n\\niff G satisfies\\n\\nThe non-definability of free-indexation is a significant obstacle to capturing GB accounts of language in\\n\\n. We thus establish that this account licenses a strongly context-free language. It seems noteworthy that GB theorists have been led, by purely linguistic considerations, to precisely the kind of re-interpretation of the theory we require in order to establish our language-theoretic results.\\n\\nDefining Chains\\n\\nWe turn now to an example that is particularly relevant to the issue of capturing a Government and Binding Theory account of English in\\n\\nIdentifying Antecedents of Traces\\n\\nGovernment and Binding Theory analyzes sentences with four distinct syntactic representations which are related by the general transformation move-\\n\\n. These are D-Structure--corresponding to the deep-structure of earlier transformational theories, S-Structure--roughly corresponding to the surface-structure of those theories, Phonetic Form--the actual phonetic structure of the sentence, and Logical Form--a more or less direct representation of the sentence\\'s semantic content. The principles embodying a GB theory of language are collected into modules which apply at various levels of this analysis. The principles we capture include basic X-bar Theory, Theta Theory, the Case Filter, Binding Theory, Control Theory and various constraints on movement, in particular the Empty Category Principle. In this section we focus on the Empty Category Principle and the definition of chains.\\n\\nAs we noted in the introduction, we prefer to regard GB theories as a set of constraints on structures rather than a mechanism for constructing them. We take this a step further by assuming that those constraints apply to a single tree which includes S-Structure and D-Structure as submodels, rather than having some constraints apply to one structure, others to the other, and others still to the relationship between them. In this view, D-Structure and move-\\n\\ntransforms this structure by cutting out the subtrees rooted at Ij and Ni, leaving phonetically empty traces (tj and ti), and re-attaching them a higher positions in the tree. In the case of Whom the movement occurs in two steps, with traces being left at each intermediate position. The original position of the moved element is referred to as the base position, and its final resting place is the target position. The moved element is identified with its traces by co-indexation. Together, an element and the traces co-indexed with it form a chain. Chains can be broken up into a sequence of links each consisting of a trace and its antecedent--the next higher element of the chain.\\n\\nThe fundamental issue we must address in defining chains within\\n\\nis how to identify the antecedent of a trace without reference to indices. Our key idea is that, if we can limit the portion of the tree in which an antecedent can occur, then we can possibly bound the number of potential antecedents a trace may have. Such a bound would suffice since, while we cannot capture indexations with an unbounded range of index, we can capture any indexation in which there is a constant bound on the total number of distinct indices.\\n\\nA non-pronominal empty category must be properly head-governed. (Formal Licensing)\\n\\nOperators must be identified with their variables. (Identification)\\n\\nTo a first approximation, government is simply a relation between an element and those elements occurring in a specifically limited region of the tree dominated by the phrase in which that element (the governor) occurs. Its definition has three components. First, for the class of government relations we are considering here, the governor must c-command the elements it governs, that is, those elements must be dominated by a sibling of the governor. Second, there must be no intervening barrier. For Rizzi, the notion of barrier is much weaker than it is in the Barriers account. Here, this constraint simply forbids the government relation from crossing certain phrasal boundaries (in particular specifiers, adjuncts and complements of nouns or prepositions). The final component of the government relation requires a governor to be the minimal potential governor of the elements it governs, that is, no potential governor can fall properly between a governor and the elements it governs. There are a range of types of government relations that fall under this general category. In Rizzi\\'s theory only potential governors of the same type count for the minimality requirement. (This is the relativized aspect of his theory.) For antecedent-government there is an additional requirement that the governor be co-indexed with the trace.\\n\\nAs we will see, we can drop the co-indexation requirement on the grounds that, when it exists, the antecedent-governor is unique.\\n\\nIn this way, minimality suffices to pick out the unique antecedent of traces in chains that are identified by antecedent-government. But under Rizzi\\'s criteria chains can also be identified by referential indices. These are just indices assigned to elements that receive what are termed referential Theta roles. Again to a first approximation, we can take these simply to be elements that are the objects of verbs.\\n\\nDefining Antecedent\\n\\n\\n\\nGovernment, Links, and Chains\\n\\nRelativized Minimality theory distinguishes a number of distinct varieties of antecedent-government, one for each class of movement. We look at one representative case -antecedent-government. This is defined, in\\n\\nas follows:\\n\\nIn words, this says simply that x is an -antecedent-governor of y iff x is in a non-argument () position, it c-commands y, no barrier intervenes between x and y, and no non-argument specifier falls between xand y. The actual definitions of\\n\\n,\\n\\n,\\n\\n,\\n\\n,\\n\\nand\\n\\nis unimportant here. The predicate\\n\\nis used to check the compatibility of the features of the trace with those of its antecedent.\\n\\nUsing this, we can define the link relation.\\n\\nThis is just antecedent-government with certain additional configurational requirements. We can extend the notion of links based on Rizzi\\'s antecedent-government to include antecedents and traces that Rizzi  identifies with a referential index (which we refer to as -referential links), and links formed by rightward movement. This gives us five distinct link relations. As they are mutually exclusive, we can take their disjunction to form a single link relation which must be satisfied by every trace and its antecedent.\\n\\nThe idea, now, is to define chains as any set of nodes that are linearly ordered by\\n\\n. Before we can do this, though, we have one more issue to resolve. The problem is that, while we can identify a unique antecedent for each trace, nothing assures us that there will be a unique trace for each antecedent, that is, nothing prevents us from identifying the same node as the antecedent of more than one trace.\\n\\nWe rule out such structures by requiring that chains not only be linearly ordered by\\n\\n, but that they are also closed under the link relation, that is,  every chain includes every node that is related by\\n\\nFormalizing this, we get:\\n\\nDefining the ECP\\n\\nWe can now capture Rizzi\\'s version of the Empty Category Principle:\\n\\nLicensing\\n\\nIdentification\\n\\nNote, in particular, that in our definition the identification requirement is reduced simply to a requirement that every trace is a member of some well-formed chain. As we admit the notion of trivial chains--chains with a single element, formed by zero movements--we can generalize this to a global requirement that every element of the tree is a member of a (possibly trivial) well-formed chain.\\n\\nIdentification (Generalized)\\n\\nRecall that identification is the component of Rizzi\\'s definition  that accounts for most of the effects attributed to ECP in the Barrier\\'s account of movement. Thus we have reduced a variety of effects to a single simple global principle. Of course we have paid for this with a complex definition of chains, but much of this complexity lies in the definition of antecedent-government and Rizzi argues, on linguistic grounds, for essentially this definition in any case. It is satisfying that we can recover its added complexity in the form of a greatly simplified ECP.\\n\\nLimits of the Definition\\n\\nThe fact that we can exhibit a definition in\\n\\nof the class of trees licensed by a specific GB account of English provides a strong complexity result for that class of trees--it is strongly context-free. We don\\'t, on the other hand, expect this formalization to work for GB theories in general, and, in particular we don\\'t expect it to work for a GB account of Universal Grammar.\\n\\n, and, in fact, the definition we give fails to license these structures. Examining why this is the case provides some insight into the kinds of natural properties of linguistic structures that correspond to increased language-theoretic complexity.\\n\\nand\\n\\n) are indistinguishable. Any attempt to form a chain including any of these nodes will be required to include all four and the result will not be linearly ordered.\\n\\nConclusion\\n\\nIn this paper we have introduced a kind of descriptive complexity result for the strongly Context-Free Languages--a language is strongly context-free iff the set of trees analyzing the syntax of its strings is definable in\\n\\n(modulo a projection). Using this result we have sketched a couple of language complexity results relevant to GB, namely, that free-indexation cannot, in general, be enforced by CFGs, and that a specific GB account of English licenses a strongly context-free language. The first of these results is not likely to come as a surprise to the GB community. The appropriateness of free-indexation as a fundamental component in linguistic theories has been questioned in the more recent GB literature on purely linguistic (rather than complexity theoretic) grounds.\\n\\n. As expected, our definitions fail for these constructions. The fact that the definition works for English is a consequence of the fact that, in the account of English we capture, it is possible to classify chains into finitely many categories in such a way that no two chains from a given category ever overlap. GB-style analyses of the constructions studied by Shieber and by Miller include positions in which an unbounded number of chains can overlap. Our definition is unable to identify any well-formed chains including these positions; indeed, there is unlikely to be any way to distinguish these chains without the equivalent of unbounded indices.\\n\\nAs it stands, this result speaks only of the particular account of English we capture. The fact that this is context-free says nothing about the nature of human language faculty, since the principle it depends upon is unlikely to be a principle of Universal Grammar. It does, however, raise the prospect of wider results. Extensions of our descriptive complexity result to larger language complexity classes could provide formal restrictions on the principles employed by GB theories that would be sufficient to provide non-trivial generative capacity results for those theories without losing the ability to capture the full range of human language. With such extended characterizations one might establish upper bounds on the complexity of human language in general. The possibility that such results might be obtainable is suggested by the fact that we find numerous cases in which the issues arising in our studies for definability reasons, and ultimately for language complexity reasons, have parallels that arise in the GB literature motivated by more purely linguistic concerns. This suggests that the regularities of human languages that are the focus of the linguistic studies are perhaps reflections of properties of the human language faculty that can be characterized, at least to some extent, by language complexity classes.\\n\\nBibliography\\n\\nRobert C. Berwick. Strong generative capacity, weak generative capacity, and modern linguistic theories. Computational Linguistics, 10:189-202, 1984.\\n\\nJoan Bresnan, Ronald M. Kaplan, Stanley Peters, and Annie Zaenen. Cross-serial dependencies in Dutch. Linguistic Inquiry, 13:613-635, 1982.\\n\\nMichael Brody.\\n\\ntheory and arguments.\\n\\nLinguistic Inquiry, 24(1):1-23, 1993.\\n\\nRobert C. Berwick and Amy S. Weinberg. The Grammatical Basis of Linguistic Performance. MIT Press, 1984.\\n\\nNoam Chomsky.\\n\\nBarriers.\\n\\nMIT Press, 1986.\\n\\nNoam Chomsky. A minimalist program for linguistic theory. In The View from Building 20, pages 1-52. MIT Press, 1993.\\n\\nJohn Doner. Tree acceptors and some of their applications. Journal of Computer and System Sciences, 4:406-451, 1970.\\n\\nFerenc Gcseg and Magnus Steinby. Tree Automata. Akadmiai Kiad, Budapest, 1984.\\n\\nMark Johnson. The use of knowledge of language. Journal of Psycholinguistic Research, 18(1):105-128, 1989.\\n\\nJan Koster. Domains and Dynasties. Foris Publications, Dordrecht, Holland, 1987.\\n\\nMarcus Kracht. Syntactic codes and grammar refinement. Journal of Logic, Language, and Information, to appear.\\n\\nSteven G. Lapointe.\\n\\nRecursiveness and deletion.\\n\\nLinguistic Analysis, 3(3):227\\n\\n\\n\\n265, 1977.\\n\\nHarry R. Lewis. Unsolvable Classes of Quantificational Formulas. Addison-Wesley, 1979.\\n\\nMaria Rita Manzini. Locality: A Theory and Some of Its Empirical Consequences. MIT Press, Cambridge, Ma, 1992.\\n\\nPhilip H. Miller. Scandinavian extraction phenomena revisited: Weak and strong generative capacity. Linguistics and Philosophy, 14:101-113, 1991.\\n\\nGeoffrey K. Pullum and Gerald Gazdar. Natural language and context-free languages. Linguistics and Philosophy, 4:471-504, 1982.\\n\\nGeoffery K. Pullum. On two recent attempts to show that English is not a CFL. Computational Linguistics, 10:182-188, 1984.\\n\\nMichael O. Rabin. Decidability of second-order theories and automata on infinite trees. Transactions of the American Mathematical Society, 141:1-35, July 1969.\\n\\nLuigi Rizzi.\\n\\nRelativized Minimality.\\n\\nMIT Press, 1990.\\n\\nJames Rogers. Studies in the Logic of Trees with Applications to Grammar Formalisms. Ph.D. dissertation, Univ. of Delaware, 1994.\\n\\nStuart M. Shieber.\\n\\nEvidence against the context\\n\\n\\n\\nfreeness of natural language.\\n\\nLinguistics and Philosophy, 8:333\\n\\n\\n\\n343, 1985.\\n\\nEdward P. Stabler, Jr. The Logical Approach to Syntax. Bradford, 1992.\\n\\nEdward P. Stabler. The finite connectivity of linguistic structure. In C. Clifton, L. Frazier, and K. Rayner, editors, Perspectives on Sentence Processing, chapter 13, pages 303-336. Lawrence Erlbaum, Hillsdale, NJ, 1994.\\n\\nJ. W. Thatcher. Characterizing derivation trees of context-free grammars through a generalization of finite automata theory. Journal of Computer and System Sciences, 1:317-322, 1967.\\n\\nFootnotes\\n\\n, our result is one way of showing that', metadata={'source': '../data/raw/cmplg-xml/9505041.xml'}),\n",
       " Document(page_content=\"Ellipsis and Higher\\n\\n\\n\\nOrder Unification\\n\\nWe present a new method for characterizing the interpretive possibilities generated by elliptical constructions in natural language. Unlike previous analyses, which postulate ambiguity of interpretation or derivation in the full clause source of the ellipsis, our analysis requires no such hidden ambiguity. Further, the analysis follows relatively directly from an abstract statement of the ellipsis interpretation problem. It predicts correctly a wide range of interactions between ellipsis and other semantic phenomena such as quantifier scope and bound anaphora. Finally, although the analysis itself is stated nonprocedurally, it admits of a direct computational method for generating interpretations.\\n\\nIntroduction\\n\\nIn this paper, we present a new method for characterizing the interpretive possibilities generated by elliptical constructions in natural language. Unlike previous analyses, which postulate ambiguity of interpretation or derivation in the full clause source of the ellipsis, our analysis requires no such hidden ambiguity. For example, the ambiguity typically characterized as enabling ``strict'' versus ``sloppy'' readings of elliptical constructions does not arise from a corresponding ambiguity as to whether the pronoun in the antecedent clause is given a strict or sloppy interpretation; instead, the ambiguity follows from the process of interpreting the elided phrase on the basis of its unambiguous antecedent. Further, the analysis follows relatively directly from an abstract statement of the ellipsis interpretation problem and applies to the interpretation of a wide variety of elliptical constructions, including VP ellipsis, ``do so'' and ``do it'' anaphora, gapping, stripping, and related constructions involving recovery of implicit relations such as ``only'' modification and cleft constructions. It predicts correctly a wide range of interactions between ellipsis and other semantic phenomena such as quantifier scope and bound anaphora. Finally, although the analysis itself is stated nonprocedurally, it admits of a direct computational method for generating interpretations.\\n\\nThe analysis we present is intended to characterize the semantics of constructions involving ellipsis. Many interesting issues arise regarding the syntax of ellipsis, and we will touch on some of these issues; our main goal, though, is to characterize a method for ellipsis interpretation.\\n\\nBasics An Abstract Statement of the Ellipsis Problem\\n\\nWe can provide an abstract and reasonably theory-neutral characterization of ellipsis phenomena and their interpretation as follows. An elliptical construction involves two phrases (usually clauses) that are parallel in structure in some sense. The antecedent or source clause is complete, whereas the target clause is missing (or contains only vestiges of) material found overtly in the source. As a concrete example, which we will use as our primary source of data in the paper, consider the verb phrase (VP) ellipsis phenomenon, as in (1).\\n\\nDan likes golf, and George does too.\\n\\nThe sentence is interpreted as meaning that Dan and George both like golf. The source clause, `Dan likes golf', parallels the target `George does too', with the subjects `Dan' and `George' being parallel elements, and the VP of the target sentence being vestigially represented by the target phrase `does too'.\\n\\nIn general, then, the abstract problem of ellipsis can be stated as the problem of recovering solutions to the equation\\n\\nNot only is this an abstract characterization of the ellipsis problem, it is essentially the entire analysis proposed in this paper. It constitutes an analysis because the equational statement of the problem, together with some reasonable assumptions, determines rigorously the sets of interpretations for target clauses, which interpretations, we will see, correspond to the actual possible interpretations of the target.\\n\\nPrevious Analyses of Ellipsis\\n\\nIt is important that ellipsis analyses (including the equational one outlined above) allow for ambiguity in the target clause, that is, for a set of relations to be made available by the source clause. The availability of multiple relations is attested in various phenomena in which the target clause has multiple readings; it can be seen most clearly in the distinction between strict and sloppy readings. In the sentence\\n\\nDan likes his wife, and George does too.\\n\\n(under the reading in which the pronoun `his' refers to Dan) the property predicated of George might be the property of liking Dan's wife or the property of liking one's own wife. In lambda notation, these two properties are given by\\n\\nand\\n\\ncorresponding to the strict and sloppy readings of the sentence, respectively. As we will see, the possibility of several available properties arises in other cases as well.\\n\\nUnder an identity-of-relations analysis, ambiguity of interpretation in a target clause comes about because the source clause is ambiguous. However, only a single relation is available from the source clause in any given instance. The relation that the source clause makes available is, in most previous work, that associated with its VP, though we emphasize that this is not a necessary condition for an identity-of-relations analysis, nor do we believe it is a tenable stance.\\n\\nThe solutions therefore form a continuum, the ambiguity arising at more or less superficial levels. All of the analyses, however, share a reliance on semantic ambiguity in the source clause.\\n\\nOur solution to the question of what properties are made available can be seen as lying at the far end of this continuum. We eschew not only syntactic ambiguity in the source clause, but semantic ambiguity arising from any source, as a generator of the multiple readings of elliptical constructions. Instead, multiple solutions come about as a natural result of directly stating the definition of the relation to be applied in the target clause.\\n\\nThe New Analysis and an Example\\n\\nAs described earlier, the problem of extracting a relation from the source clause can be stated equationally as\\n\\nIn cases of VP ellipsis where the subjects of the source and target are parallel, the equation is simply\\n\\nP(s1) = s\\n\\nwhere s1 is the interpretation of the subject of the source clause. By solving this equation for the unknown, P, we generate the relation (or relations, if multiple solutions exist) that the resolution of the ellipsis requires.\\n\\nDan likes golf, and George does too.\\n\\nRecall that `Dan' and `George' are the parallel elements in this example, and the semantic interpretation of `Dan likes golf' is\\n\\nWe have underlined what we will call primary occurrences of the parallel element's interpretation, for reasons to be clarified later. In this case, the single occurrence of dan is primary. Any other occurrences will be referred to as secondary.\\n\\nTo form an interpretation for the second conjunct, we require a property P that, when applied to the interpretation of the subject of the first conjunct, will yield the interpretation of the first conjunct as a whole. This property will serve to generate the interpretation of the target clause. It will be applied to the interpretation of the parallel element, that of the subject `George', in the second clause.\\n\\nThe latter term is the interpretation for the source sentence; the equation requires P to be a property that, when predicated of the subject interpretation dan, yields the first term.\\n\\nIn summary, our analysis of the abstract problem of ellipsis resolution, that is, generating appropriate properties to be used in interpreting the target clause, is to state the problem equationally based on the parallel structure in the two clauses and to solve the equation using higher-order unification (under the constraint requiring abstraction of primary occurrences). The properties that are generated as solutions to the equation are predicated of the parallel elements in the target clause to generate the target clause interpretation.\\n\\nStrict and Sloppy Readings\\n\\nDan likes his wife, and George does too.\\n\\nLet us assume the following interpretation for the first conjunct:\\n\\nAgain, the first two solutions fail the constraint on abstraction of primary occurrences. Either of the other two remaining properties yields a possible interpretation of the target clause. The first gives rise to what has been called the strict reading of the second conjunct, while the second gives rise to the sloppy reading:\\n\\n(George likes Dan's wife.)\\n\\n(George likes George's wife.)\\n\\nConstraints on Relation Formation\\n\\nIf the constraint were not in force, the following readings would be produced for `... and George does too':\\n\\nThese are just the readings where the parallelism between the clauses has been disregarded. Thus, the constraint is a reflex of the inherent parallelism in elliptical constructions.\\n\\nThe existence of this constraint means, not surprisingly, that it is necessary to retain a connection between the syntactic and the semantic representation of the source sentence. By maintaining this connection, we can ensure that the solutions produced by higher-order unification satisfy the constraint that parallelism must be maintained by abstracting out of parallel positions.\\n\\nConstraints on Parallelism\\n\\nOne of the distinguishing features of our analysis is that the ellipsis resolution problem is separated into two subtasks: a prior determination of the parallel structure of source and target, and consequent formation of the implicit relation to be used in the target. We have been addressing the latter subtask primarily, and will continue to do so, but we digress to mention some perhaps obvious facts about the parallelism determination that might get lost in the sequel.\\n\\nDepth of embedding imposes constraints as well.\\n\\n(We ignore the nonsensical reading in which the city of New York was the agent of the leaving action.) Similarly, the sentence\\n\\nIt is obvious that Dan is happy, and George is too.\\n\\n(pointed out to us by Mats Rooth) can only be interpreted as meaning that George is happy, not that it is obvious that George is happy. If the obviousness is included, the parallelism would have to hold between `George' and `it', not `Dan'.\\n\\nSuch constraints hold not only in VP ellipsis, but also in gapping, stripping, comparative deletion and other elliptical constructions. Thus, not all constraints on readings of elliptical sentences follow from the relation formation issues that are the primary topic of this paper.\\n\\nJohn greeted every person that Bill did. * John greeted every person that Bill did so. ? John greeted every person that Bill did too.\\n\\nThese fine syntactic distinctions are not addressed by the present analysis, which attempts to make clear only the space of semantic interpretations.\\n\\nOf course, not all elements in the target clause must be analyzed as parallel to some element in the source. For instance, adverbial phrases can be viewed as modifying the target directly. This possibility is exemplified by the following sentence:\\n\\nJim couldn't open the door, but Polly did with her blowtorch.\\n\\nFormal Semantic Background\\n\\nWe outline here the formal machinery underlying the semantic analyses used in this paper.\\n\\nInterpretation of Ellipsis Resolution  Equations\\n\\nWe have claimed that the meaning of\\n\\nDan likes golf, and George does too.\\n\\nis\\n\\nwhere the equation\\n\\nmust be satisfied. It is not obvious that the equation in 0 is semantically interpretable, as opposed to being a recipe for invoking a formal procedure, higher-order unification, with no underlying meaning in and of itself. Clearly, the combined meaning of -1 and 0 is not equivalent to\\n\\nThis would merely require that P be such that it is true of Dan if Dan likes golf. Since the first conjunct states that Dan does in fact like golf, P need only be a true property of Dan. The entire formula then requires only that George possess some property, any property, that Dan possesses, which would give an incorrect interpretation for the target sentence. The equation is not to be interpreted, then, as codenotation in a model.\\n\\nInstead, we want ellipsis to be more content-independent, in that the property should be such that the equation holds whether or not Dan happens to like golf. It should be independent of the particulars of a given model, that is, it should hold in any model in a suitable class of models. But we have to be careful about the choice of model class. Even necessary truths should not codenote over the class of models in which the ellipsis equation is interpreted. Otherwise, the sentence\\n\\nEvery square has four sides, and every rhombus does too.\\n\\nThus, the semantic invariants in ellipsis resolution are those that follow from the type structure--the function-argument relationships--of natural language, and not from any contingent or even necessary truths. This accords with intuition, in that the felicity of ellipsis does not depend on the meaning of the words in the source sentence (though the elided property does), but does depend on their type structure. An ellipsis equation is not merely a recipe for a syntactic process. It has a meaning, but the meaning must be taken in a different, and much more profligate, model than that of the interpretation of the sentence itself. The ellipsis equation reflects semantic facts about the sentence, but just at the gross level of function-argument structure.\\n\\nThere is one remaining problem, however, for this view of ellipsis equations--the issue of primary occurrences. The primary occurrence notation serves to couple the ellipsis equations to the choice of parallel elements, and provides a way of forcing abstraction over the meanings of the parallel elements. Intuitively, the distinction between primary and secondary occurrences is clear: a primary occurrence corresponds to a distinguished semantic role in the situations described by the source and target clauses. At present, however, we have no way of making precise the intuitions that led to the primary occurrence notation; that is, we know of no semantic correlate to the equational system with primary occurrence notation under the related constraint on abstraction. It remains for future work to reconstruct the semantical foundations of this variant of higher-order unification. Although we have some ideas as to how such a reconstruction might proceed, it is premature to discuss them here.\\n\\nHigher\\n\\n\\n\\norder unification\\n\\nFurthermore, many of the equations we will be interested in solving are of the schematic form\\n\\nInterpretation of quantification and long-distance dependencies\\n\\nBefore characterizing the interaction between our analysis of ellipsis and various other semantic phenomena, we must first lay out an approach to semantic interpretation--quantifier scoping in particular--in which to couch the discussion. For the most part, the particulars of the method for characterizing quantifier scoping are relatively unimportant; the analysis could be stated in terms of Cooper storage, say, or even quantifier raising. We will use here a variation of a method for interpreting quantifier scoping and long-distance dependencies developed by Pereira Pereira:SemComp. For those readers unfamiliar with this method, we provide some examples later in this section.\\n\\nJohn greeted every person.\\n\\nwill be for us\\n\\nwhich we will abbreviate as\\n\\nThe quantified noun phrase `every person' is given the interpretation\\n\\nthat is, a sentence meaning free of undischarged assumptions. When several quantifier assumptions are introduced, there is the option of discharging them in several different orders, leading to alternative quantifier scopings for the sentence.\\n\\nThe treatment of the semantics of long-distance dependencies is handled similarly by introducing and discharging assumptions. Again, the form of these introduction and discharge rules could be justified on the basis of functional application and abstraction. As an example of a derivation involving a long-distance dependency, we consider the example\\n\\nJohn greeted every person that arrived.\\n\\nThe trace in the subject position of the relative clause can be thought of as introducing a bind assumption for a new variable\\n\\nThe relative clause being completed, we can now discharge the bind assumption. We do so by forming a higher-order predicate by abstracting the matrix, conjoined with a place-holder for the modified nominal, and abstracted by the bound variable.\\n\\nFinally, the rule for combining a quantifier every with a predicate forms a quantifier assumption over a new variable.\\n\\nFrom here, the derivation continues as before, yielding the sentence meaning (before the quantifier is discharged)\\n\\nand the scoped meaning is\\n\\nThis completes the background information on the formal semantics we will presume in the remainder of the paper.\\n\\nInteractions with Other Phenomena\\n\\nThe approach to ellipsis resolution that is advocated here displays differences from previous approaches in its handling of various phenomena. We will discuss how our analysis differs from identity-of-relations analyses in general, and certain particular instances thereof, by briefly examining the predictions of the analyses with respect to the following phenomena:\\n\\nNon-constituent abstractions: There are many cases in which the relation constructed from the source clause does not correspond in any straightforward fashion to the interpretation of some syntactic constituent: for example, when it must take more than one argument. For instance, the tense and aspect as well as the subject are abstracted in the sentence Dan is running for president, and George did last term. Examples demonstrate other nonstandard abstractions as well. Such cases are especially problematic for identity-of-relations analyses in which the relation  is necessarily associated with some constituent such as the VP in the source clause.\\n\\nMultiple property extraction: In some cases, a single sentence serves as the antecedent for two subsequent instances of ellipsis involving different parallel elements: John finished reading the poem before Bill did, and the short story too. This sentence has a reading on which John finished reading both the poem and the short story before Bill finished reading the poem. This is problematic for identity-of-relations analyses in which only a single property is available in any given instance for the interpretation of subsequent elided phrases.\\n\\nCascaded ellipsis: Analyses differ as to what readings are predicted for sentences containing multiple elliptical clauses in which the interpretation of one elided constituent depends partially or entirely on the interpretation of another elided constituent. An example is: John realizes that he is a fool, but Bill does not, even though his wife does.\\n\\nInteraction with quantifier scoping: As is well known, the ambiguities following from varying quantifier scope possibilities interact with ellipsis resolution possibilities. For instance, in the sentence John greeted every person when Bill did. two readings are possible, depending on whether the universal quantifier has wide scope over both the main and subordinate clause, or quantifies separately in each clause. But in John greeted every person that Bill did. only a wide scope reading is available.\\n\\nWe discuss each of these phenomena below, and demonstrate that our approach constructs appropriate solutions. In the succeeding section, we discuss in detail an example sentence which illustrates differences among a number of analyses of ellipsis that have been proposed in the past. Finally, we turn to problematic cases for this and other analyses.\\n\\nNon\\n\\n\\n\\nConstituent Abstractions\\n\\nSloppy readings with embedded antecedents\\n\\nThe primary argument given by Reinhart Reinhart:Anaph for the distinction between bound variable and referential pronouns is the requirement that bound variable pronouns must be c-commanded by their antecedents. She uses this requirement to predict that the following example has only a strict reading:\\n\\nPeople from LA adore it and so do people from NY. [Reinhart's (17a), page 150]\\n\\nReinhart proposes a requirement that a pronoun must be c-commanded by its antecedent if the antecedent is a quantifier; further, she claims that a pronoun giving rise to a sloppy interpretation must be c-commanded by its antecedent. For Reinhart, then, the availability of a sloppy reading correlates with the possibility of a bound-variable interpretation of a pronoun, and she requires a c-command relation for this interpretation to be possible. This restriction simplifies the task of an identity-of-relations analysis because it reduces the number of cases in which a sloppy reading is available. An analysis postulating ambiguity of pronoun interpretation for only this restricted set of cases seems methodologically more plausible.\\n\\nFelixi's mother thinks hei's a genius and so does Siegfried's mother. [Reinhart's (8a)]\\n\\nWe'll discuss Rosai's problems with heri parents and Sonya's problems too. [Reinhart's (8b)]\\n\\nWescoat:StrictSloppy notes a number of more extreme cases of sloppy readings involving non-c-commanding, embedded constituent antecedents, such as:\\n\\nThe policeman who arrested John failed to read him his rights, and so did the one who arrested Bill.\\n\\nThe person who introduced Mary to John would not give her his phone number, nor would the person who introduced Sue to Bill.\\n\\nWescoat claims, and we agree, that sloppy readings are possible with these sentences; that is, that the following readings are available--perhaps even preferred--for them:\\n\\nThe policeman who arrested John failed to read John John's rights, and the one who arrested Bill failed to read Bill Bill's rights.\\n\\nThe person who introduced Mary to John would not give Mary John's phone number, and the person who introduced Sue to Bill would not give Sue Bill's phone number.\\n\\nwhere pwi(x,y) is the person who introduced x to y, and phone(x) is x's phone number. The parallel elements in the construction are, respectively, `the person who introduced Mary to John' and `the person who introduced Sue to Bill'; `Mary' and `Sue'; `John' and `Bill'. Thus, the appropriate equation to solve is\\n\\nThe sloppy reading is engendered by the following unifying substitution for P:\\n\\nwhich, when applied to the interpretations of the parallel elements in the target, yields the target interpretation\\n\\nNon\\n\\n\\n\\nsubject abstraction\\n\\nThere exist many cases of multiple parallel elements in the source and target clause; it is very common for ellipsis to involve relations formed by abstraction of elements other than the interpretation of the subject noun phrase.\\n\\nFor example, the tense and aspect of the target clause might differ from that in the source clause:\\n\\nDan is running for president, and George did last term.\\n\\nThe mood can also differ:\\n\\n``I want to leave.'' ``Well, do.''\\n\\n``Eat your dinner.'' ``I did.''\\n\\nThe two clauses may differ in polarity:\\n\\nDan didn't leave, but George did.\\n\\nThese examples show that relations of varying arity must be available as interpretations for elided phrases. The consequence of this for theories where the relation available for interpretation of subsequent ellipsis must be available in the source sentence is, again, that every sentence which can be the antecedent for subsequent ellipsis must be many ways ambiguous; an interpretation must be available for each relation that might be needed to interpret ellipsis in subsequent discourse.\\n\\nOn the equational analysis, however, such ambiguity is not required. A single interpretation for the source clause can give rise to any required interpretation for the target, since there is no inherent restriction as to the number or nature of the parallel elements involved in the ellipsis.\\n\\ncan be solved yielding\\n\\nand applied to the target parallel elements:\\n\\nYou can keep Rosa in her room for the whole afternoon, but not Zelda. [Reinhart's (18c)]\\n\\nMaxwell killed the judge with a silver hammer, and I'd like to do the same thing to that cop, with a cudgel. [Jackendoff's (6.196)]\\n\\nFred hung Tessie up in a tree and poured paint on her, but I bet he wouldn't do it to Sue with glue. [Jackendoff's (6.197)]\\n\\nMultiple Property Abstraction\\n\\nA difficulty in any identity-of-relations analysis, which makes available only one interpretation for subsequent clauses exhibiting ellipsis, is seen when a single sentence is the antecedent for the ellipsis of two different noun phrases. Consider the following:\\n\\nJohn finished reading the poem before Bill did, and the short story too.\\n\\nThis sentence has a reading on which John finished reading both the poem and the short story before Bill finished reading the poem. On this reading, the source for both elliptical clauses is the same clause, `John finished reading the poem.' To produce a relation which can be the interpretation for the elided VP whose subject is Bill, the interpretation for the sentence `John finished reading the poem' must be derived as:\\n\\nUnder an identity-of-relations analysis, the source clause is deemed ambiguous between the two derivations. They do not simultaneously exist in a given analysis; only one or the other may be chosen. Thus, the reading noted above would not be generable. On the other hand, an analysis such as ours allows for the formation of two different relations from the semantic representation of the first sentence; the representation of the first sentence does not constrain the possibilities for construction of such relations. The next section provides an example of a similar problem and its derivation in our framework.\\n\\nCascaded Ellipsis\\n\\nWe use the term ``cascaded ellipsis'' to refer to cases of multiple ellipsis in which one of the elided constituents depends on another elided constituent for its interpretation. Analyses dependent on an identity-of-relations approach generally make available fewer readings in cascaded ellipsis cases than the analysis presented here; we believe that the greater number of readings available with our analysis is in fact warranted.\\n\\nDahl:SI provides the following example (Dahl's (12), an English paraphrase of Scheibe's (58a) Scheibe:Problem):\\n\\nJohn realizes that he is a fool, but Bill does not, even though his wife does.\\n\\nDahl claims that this sentence has, among other readings, the following one:\\n\\nJohn realizes that John is a fool but\\n\\nBill does not realize that Bill is a fool, even though\\n\\nBill's wife realizes that Bill is a fool\\n\\nOn our analysis, this reading is readily available. Assume that the interpretation for `John realizes that he is a fool' on the reading under discussion is:\\n\\nThis sentence serves as the antecedent for the elided phrase in the second conjunct, `Bill does not'. `Bill' and `John' are parallel elements; for the reading under discussion, second-order matching solves the equation\\n\\nproducing, among others, the following property (corresponding to the sloppy option):\\n\\nwhich is applied to `Bill'. The interpretation for the second conjunct as a whole is, then, the following:\\n\\nWe assume that the second clause may serve as an antecedent for the elided portion of the third conjunct. The parallel elements are `Bill' and `his wife'; the ellipsis equation is\\n\\nOn the reading under discussion, the strict option is chosen; the property Q applied to the interpretation of `his wife' is:\\n\\nThe resulting interpretation for the third conjunct is:\\n\\nAlthough we have described the derivation of the meaning for this example in terms of temporal ordering (we resolve the first ellipsis, using its result to resolve the second), it is important to note that the analysis is truly order-free. In essence, we merely set up two equations in two unknowns and solve them using unification. The result, as is typical with declarative, equational methods, does not depend on solving the equations in a particular order.\\n\\nUnder an identity-of-relations analysis, such as Sag's, the existence of this reading is problematic, as he notes. The problem is that there are conflicting requirements on the form of the semantic representation of the second clause.\\n\\nSag obtains strict and sloppy readings under ellipsis by optionally applying a rule that replaces the interpretation of a pronoun (which has an invariant referent and induces a strict reading) by a lambda-bound variable (inducing a sloppy reading). The representation Sag provides for the first two conjuncts is:\\n\\nCrucially, the interpretation for the pronoun `he' which is reconstructed in the second conjunct is represented by a bound variable.\\n\\nIn contrast, for the reading under discussion, the representation for the second and third conjuncts is:\\n\\nThe strict reading is only available when the option of replacing the pronoun interpretation with a lambda-bound variable is not taken. These conflicting requirements make it impossible for Sag's analysis--or any identity-of-relations analysis, where the difference between a strict and a sloppy reading corresponds to a difference in the form of the semantic representation--to obtain the reading for this sentence that we (and Dahl) assume exists.\\n\\nInteractions with Quantifier Scope\\n\\nQuantification and antecedent\\n\\n\\n\\ncontained ellipsis\\n\\nJohn greeted every person when Bill did. John greeted every person that Bill did.\\n\\nWe might discharge the assumption at this point, but we choose not to in this first scenario. Consequently, the interpretation of the full sentence is\\n\\nwhere P is constrained equationally by virtue of the interpretation of the source clause:\\n\\nThis equation has a single (most general) solution\\n\\nIt is this value for P that we will apply to bill. Thus, the interpretation of the full sentence, with ellipsis resolved is\\n\\nThe assumption may now be discharged, yielding the full interpretation\\n\\nThis interpretation corresponds to a necessarily distributive reading, the `individual' reading, in which each person is simultaneously greeted by John and Bill.\\n\\nAlternatively, the assumption can be discharged before the ellipsis is resolved. Under this scenario, the interpretation of the source clause is\\n\\nThe full sentence, then, is interpreted as\\n\\nwhere, again, the interpretation of the source clause is used to constrain the property P:\\n\\nThe single value for Pis\\n\\nleading to the final interpretation\\n\\nThis interpretation yields a `group' reading paraphrasable as `John greeted every person when Bill greeted every person.' The two derivations, then, correspond to just the interpretations noted by Sag.\\n\\nAs before, we will resolve the ellipsis by finding solutions for the equation\\n\\nwhich, discharging the assumption, reduces to\\n\\nAlternatively, we might attempt to discharge the assumption before resolving the ellipsis. Recall the starting point for the previous derivation:\\n\\nDischarging the assumption yields\\n\\nResolving the ellipsis, then, involves finding solutions to the equation\\n\\nNotice that the variable P appears on both sides of this equation. For this reason, the derivation runs into problems, for there simply are no solutions to this equation; no unifier exists for this pair of typed terms. Thus, the derivation fails at this point; the sentence has only the `individual' reading, in agreement with our judgments.\\n\\nThe computational reflex of the above lack of a solution is a violation of the so-called ``occurs check'' in the unification algorithm. This check prevents the construction of unifiers in which the substitution for a variable contains that variable. In other words, the occurs check blocks the creation of a circularity (a value for P containing P itself). It is interesting to note that this is formally analogous to the syntactic argumentation which Williams Williams:DLF uses to eliminate such readings.\\n\\nIn sum, the ellipsis characterization described here allows for antecedent-contained ellipsis, and predicts correctly the interactions with quantifier scope.\\n\\nQuantification parallelism\\n\\nJohn gave every student a test, and Bill did too.\\n\\nwe predict (correctly, we take it) no reading of the form\\n\\nwhere the two quantifiers take different scopes in the two clauses. Consider the possible orderings of ellipsis resolution and discharging of quantifier assumptions. If ellipsis resolution occurs before some of the quantifiers have been discharged, these quantifiers will scope over both clauses. Thus, the only way for both quantifiers to scope separately is for ellipsis resolution to occur after both quantifier assumptions are discharged. But in that case, the ellipsis equation will include the quantifier order manifest in the source clause interpretation, and this will be carried over to the target interpretation.\\n\\nQuantification and type raising\\n\\nIn general, the semantic types of parallel elements must be identical. In the case of a quantified NP parallel to a non-quantified NP, this implies that the type of the non-quantified NP must be raised to that of the quantified type. As an example, we consider the sentence\\n\\nEvery student revised his paper, and then Bill did.\\n\\nThis sentence is ambiguous, depending on whether Bill revises his own paper or each student's paper.\\n\\nAs usual, the ellipsis resolution can occur before or after the quantifier assumption is discharged. If the quantifier is discharged first, we have the meaning given in 1 for the first clause.\\n\\nTo resolve the ellipsis, we need to set up an equation involving the interpretation of the parallel element in the source, ``every student''. However, for this purpose we cannot directly use the stored noun phrase interpretation given in 1.\\n\\nThis interpretation only makes sense as part of the derivation of the meaning of the whole source clause. Instead, we calculate the contribution of ``every student'' to the meaning of the source clause by examining the effect of applying it to an arbitrary property S.  Combining S with 0, we obtain first the interpretation in 1.\\n\\nThe assumption can then be discharged to yield the sentence meaning in 1.\\n\\nevery(x, student(x), S(x))\\n\\nIt is easy to show that this is equivalent to the usual generalized quantifier meaning of ``every student''.\\n\\nThis equation has the solution\\n\\naccording to which Bill revises his own paper.\\n\\nOn the other hand, if ellipsis resolution occurs first, the derivation yields\\n\\njust before resolution. The equation\\n\\nadmits of a strict interpretation for P:\\n\\nThe meaning for the conjoined sentence before ellipsis resolution\\n\\nreduces, after ellipsis resolution, to\\n\\nwhich, following discharging of the quantifier assumption, becomes\\n\\nOn this reading, Bill revises each student's paper after the student revises it. A sloppy reading, on which Bill revises his own paper, is generable in this way as well; in this particular case, though, it is logically equivalent to the reading described above, in which type-lifting of `Bill' is involved.\\n\\nOther Phenomena\\n\\nWe defer discussion of several other important interactions of ellipsis and scoping phenomena to a companion paper in preparation. In that paper we intend to discuss, in addition to a more detailed explication of the mainstream quantifier cases:\\n\\nScope ambiguities with indefinites: Indefinites give rise to several readings under ellipsis, depending on their scope and the relative order of ellipsis resolution and discharge of the indefinite. This sentence, for example, has three readings:\\n\\nJohn lost a book he owned, and so did Bill.\\n\\nOn the first reading, John and Bill lost the same book; on the second reading, John and Bill each lost one of John's books, possibly distinct; and on the third reading Bill lost one of his own books.\\n\\nDe dicto/de re ambiguities: Similar ambiguities are found in sentences with opaque verbs. This sentence has three readings:\\n\\nBill wants to read a good book and John does too.\\n\\nWhere the first conjunct has a de dicto reading, the second conjunct does also; where the first conjunct has a de re reading, however, there are two readings for the sentence as a whole, depending on whether or not Bill and John want to read the same book.\\n\\n`Canadian flag' examples: Hirshbuhler:Ellipsis discusses examples such as the following:\\n\\nA Canadian flag was hanging in front of each window, and an American one was too.\\n\\nA Comparison of Approaches\\n\\nIn order to more fully explicate the differences between our approach to ellipsis resolution and other approaches, we analyze a single example in detail from the perspective of several previous proposals. Rather than make the comparison in the respective notations of the original proposals, we normalize those notations by using lambda terms uniformly. When the analyses are viewed in this way, several apparently different analyses are seen to generate the same set of readings in the same manner, despite their having originally been stated in differing notations.\\n\\nWe will use the following example, discussed at length by Gawron and Peters GP:Anaph:\\n\\nJohn revised his paper before the teacher did, and Bill did too.\\n\\nWe follow Gawron and Peters in directing attention to the reading of the first conjunct where `John' and `his' corefer, and of the second elliptical clause where its source is the entire previous sentence. The following six readings exhaust the set of readings generated by any of the analyses (including our own) that we discuss. We present them with paraphrases for reference.\\n\\nEach person revised his own paper.\\n\\nEach person revised John's paper.\\n\\nJohn and then the teacher revised John's paper; Bill and then the teacher revised Bill's paper.\\n\\nJohn and Bill both revised John's paper before the teacher revised the teacher's paper.\\n\\nJohn and Bill revised their own papers before the teacher revised John's paper.\\n\\nJohn and then the teacher revised John's paper; Bill revised John's paper before the teacher revised Bill's paper.\\n\\nAs we will see, the equational analysis that we propose in this paper is the most profligate of the analyses, potentially generating all six of these readings, though restrictions might eliminate certain of these.\\n\\nZero\\n\\n\\n\\nReading Analyses\\n\\nThe strictest version of an identity-of-relations analysis requires that a lambda term used in the derivation of the meaning of the source clause be used in the derivation of the target clause meaning (either by copying or deletion under identity). Under such an analysis, the pertinent level of semantic representation of the source clause to use in the target clause derivation is that before beta reduction has occurred, as beta reduction eliminates the function-typed lambda terms. For the sentence `John revised his paper', the unreduced meaning representation is one of:\\n\\nor\\n\\ncorresponding to the strict and sloppy readings, respectively. In forming the meaning of the first target clause `before the teacher did', we use whichever term P is made available by the first clause, generating\\n\\nP(teacher). An issue remains as to how the two clause meanings are then combined to form a single sentence meaning. The most natural method, direct coordination, would yield (for the sloppy reading):\\n\\ncorresponding to a syntactic analysis under which the adverbial clause is attached at the S level. However, this is not itself of the form appropriate for being the source of a later ellipsis, that is, a function applied to the subject meaning. Thus, under this analysis, the second ellipsis, `and Bill did too', would be uninterpretable. (Recall that we are ignoring the readings in which the source for the second ellipsis is merely `John revised his paper'. Such a reading would be possible, although it would be strict or sloppy dependent on the interpretation  of the other two clauses.)\\n\\nTwo\\n\\n\\n\\nReading Analyses\\n\\nThe second ellipsis is not, of course, uninterpretable, so we attempt to design a meaning representation for its source that is of the appropriate form. A first method is to place the meaning of the `before' clause within the meaning of its source VP:\\n\\nThe analyses of Sag:PhD and Williams:DLF, although they do not consider cases such as these, might be reasonably viewed as generating these readings in much this way. Similarly, the analysis presented by Roberts:PhD and phrased in terms of DRT generates these two readings (as discussed by Gawron and Peters).\\n\\nThree\\n\\n\\n\\nReading Analyses\\n\\nThis analysis, which generates three readings for the example sentence, is essentially the analysis developed by Gawron and Peters within a situation-theoretic framework. (The final reading corresponds to the second argument of revise being ``absorbed'' by the lambda operator.) Our transliteration makes clear that, for this case at least, situation-semantics machinery is not necessary to yield the readings in question; an extrapolation of Sag's or Williams's analyses might achieve the same result. Of course, other aspects of the Gawron and Peters analysis depend intrinsically on the situation-theoretic foundation.\\n\\nSix\\n\\n\\n\\nReading Analyses\\n\\nThe three readings provided by the Gawron and Peters analysis seem to exhaust the possibilities for an identity-of-relations approach. Our analysis produces six readings for the example sentence.\\n\\nThe first conjoined sentence then will have the meaning\\n\\nunder the constraint\\n\\nThe second elliptical clause takes its source to be the whole first conjunction. Thus, its interpretation will be\\n\\nunder the constraint\\n\\nThese two equations in two unknowns (P and Q) are solved, as usual, by higher-order unification; we will take the equation for Pfirst. One solution is to take the strict reading for P,\\n\\nleading to the following interpretation for the second equation:\\n\\nThis equation, in turn, has a solution\\n\\nThe semantics for this reading of the sentence as a whole is:\\n\\nOur analysis allows for readings that are missing under the analyses discussed above because it is not an identity-of-relations analysis; interpretation of ellipsis does not involve copying the interpretation of a constituent in the source.\\n\\nFive\\n\\n\\n\\nReading Analyses\\n\\nFour\\n\\n\\n\\nReading Analyses\\n\\nAn unpublished analysis attributed to Hans Kamp (personal communication to Mark Gawron and Stanley Peters, cited by Gawron and Peters GP:Anaph) and couched in DRT assigns four readings to the sentence, and does so by eliminating the identity-of-relations assumption. In Kamp's analysis, as in our own, ambiguities between strict and sloppy readings do not arise from ambiguity in the source clause; the source has only a single interpretation. Essentially, Kamp makes a copy of the discourse representation structure of the source, and then imposes constraints identifying the participants in the source and target copies. These constraints must be applied in a symmetric manner. If a sloppy interpretation constraint applies to one copied discourse entity, it must apply to all; similarly for a strict interpretation constraint. Gawron and Peters mention a possible extension to Kamp's analysis that allows for the generation of all six of the readings listed above by relaxing the symmetry requirement. We refer the reader to the discussion by Gawron and Peters for a fuller description of Kamp's proposal.\\n\\nInsofar as Kamp's analysis can be fleshed out, his analysis and ours make the same predictions as to the class of readings available in cases of cascaded ellipsis. Readings missing under other analyses are available for our analysis and his. The particular syntactic operations that Kamp (under Gawron and Peters's reconstruction) presupposes have no particular foundation other than efficacy. Our analysis can be seen as providing an argument for the operational view implicit in Kamp's analysis based on the underlying equational characterization of elliptical constructions. This equational foundation, as we have seen, articulates with other semantic phenomena in ways not appreciated in the previous research.\\n\\nSummary\\n\\nIn summary, each analysis differs in the number of analyses that are predicted for the given sentence. Here is a scorecard.\\n\\nRather, the pertinent distinction in differentiating the first four readings from the last two is that the resolution of the second elliptical construction in the last two readings must treat the parallel structures that ellipsis applies to in a non-parallel fashion. We conjecture that such non-parallel cases are highly dispreferred, if not disallowed entirely.\\n\\nA reasonable, historically accurate reading for this sentence may be represented as:\\n\\nOpinions differ as to the acceptability of this reading. One's opinion in this case can be seen as a litmus determining whether parallelism of the sort violated here is required, or merely preferred.\\n\\nProblematic Cases\\n\\nThe following issues are problematic for most analyses of ellipsis interpretation. We present them, along with our conjectured solutions, to codify the range of phenomena that analyses of ellipsis might account for and to provide a preliminary guess as to their possible solutions in our framework.\\n\\nNon\\n\\n\\n\\nSyntactic Parallelism\\n\\nOur analysis of ellipsis resolution presupposes identification of the source of the ellipsis and the parallel structuring of the source and target. This division of labor between identification of parallelism and resolution of ellipsis is purposeful, as the factors involved in the solution of the two problems are quite different. Although determining the parallelism may seem to be a purely syntactic operation, much like the matching that goes on at the semantic level, this similarity is illusory. Cases of semantic or pragmatic parallelism also exist. These cases are particularly problematic for theories of ellipsis in which the interpretation of an elided phrase is presumed to correspond to the interpretation of some syntactic constituent in the source clause, as is the case in most identity-of-relations analyses.\\n\\nSemantic parallelism\\n\\nExamples of ellipsis exist in which the parallelism is between the ``logical subject'' (i.e., passive agent) in the source clause and the surface subject in the target clause:\\n\\nSimilar examples involving ``so\\n\\n\\n\\nanaphora'' are also\\n\\nfound:\\n\\nExamples of this type are ubiquitous, but seem to be confined pragmatically to cases where the source clause states a general fact or rule, and the target clause provides a specific instance of this fact or rule.\\n\\nThe policeman who arrested John failed to read him his rights, and so, for that matter, did the one Bill got collared by.\\n\\nOur analysis does not require that the property provided as the interpretation for the elided portion of the target clause in examples like those above correspond to the interpretation of any constituent in the source clause. It is not clear that there is any analysis available for examples of this sort within a theory in which the interpretation for elided phrases must be that of some constituent in the source clause, as is the case in most identity-of-relations analyses.\\n\\nOther cases of semantic/thematic parallelism can also arise; sentence 1 is from instructions on a bottle of Agree shampoo:\\n\\nAvoid getting shampoo in eyes--if it does, flush thoroughly with water.\\n\\nSyntactically, the parallel elements are the object of the source clause and the subject of the target; thematically, these elements are the ``theme'' arguments of the intransitive/causative get verb pair.\\n\\nOther combinations of logical-subject/surface-subject parallelism do not seem to arise. The following examples, where the parallel elements are intended to be the surface subject in the source clause and the logical subject in the target, are ungrammatical:\\n\\nHowever, the following sentence (due to Peter Sells) has a similar structure, yet seems to be more acceptable:\\n\\nJohn completed the assignment faster than it ever had been in the history of the school.\\n\\nTo the extent that this sentence is grammatical, it illustrates that either the source or target clause can contain a logical subject which is parallel to a surface subject in the other clause. Although examples such as these are often restricted in their distribution, they demonstrate that the parallelism between elements in the source and target clause need not be confined to surface syntactic parallelism.\\n\\nPragmatic parallelism\\n\\nIrv and Mary want to dance together but Mary can't since her husband is here.\\n\\nMary wants to go to Spain and Fred wants to go to Peru, but because of limited resources, only one of them can.\\n\\nFortunately, the first person to die in 1990 and the first couple to file for divorce in 1990 were allowed to do so anonymously. []\\n\\nAmid applause at the Congress of the   Russian Federation (RSFSR), Mr. Yeltsin put forward a bill setting Russian law above the law of the Soviet Union - something Mr. Gorbachev, as Soviet president, declared unconstitutional when Estonia, Latvia and Lithuania did it last year. []\\n\\nSentences of this sort illustrate that, to a greater or lesser extent, relations involved in the resolution of anaphoric processes such as ellipsis can be made available contextually. Identity-of-relations analyses allow for only the simplest cases of resolution of elided constituents, since the only mechanism that is available to provide an interpretation for the target is that of copying an interpretation from the source. Our approach goes beyond identity-of-relations analyses by allowing for the construction of new relations on the basis of old ones; the use of unification to construct relations is, as we have seen, more powerful and more flexible than copying.\\n\\nFurther Constraints on Relation Formation\\n\\nCases in which sloppy but not strict readings are available might seem to be problematic for an analysis like the one presented here. Below we will examine cases of control and reflexivization in which exclusively sloppy readings are available. Solutions will be proposed which do not involve constraining the process by which relations are formed as interpretations for elided constituents.\\n\\nHowever, there are other cases in which readings are unexpectedly unavailable; these cases generally involve multiple occurrences of pronouns whose antecedent is a parallel element in the source clause. It seems that a constraint is necessary on the possibilities for forming relations in cases such as these.\\n\\nObligatory sloppy readings\\n\\nControl\\n\\nIn general, only sloppy readings are available for sentences involving control. The following sentence is not ambiguous:\\n\\nJohn tried to run, and Bill did too.\\n\\nThere is no reading according to which Bill tries to bring it about that John runs.\\n\\n[b.]\\n\\ntry(john,run(john))\\n\\nIf Chierchia's hypothesis is correct, the lack of a strict reading is predicted; in the representation in (a), there is only a single occurrence of ``john'', and a strict reading is impossible to produce.\\n\\nHowever, there are reasons to doubt the adequacy of an analysis like this one. First, anaphors whose antecedent is the subject of a controlled VP can give rise to both a sloppy and a strict reading under ellipsis. Consider this sentence:\\n\\nJohn tried to kill himself before Bill did.\\n\\nSince the reflexive ``himself'' cannot be bound to a higher clause subject, its antecedent must be the subject of ``kill''. On the property analysis, the interpretation for the first conjunct would then be:\\n\\nHowever, we find this sentence to have two readings, corresponding to the following paraphrases:\\n\\n[a.] John tried to kill himself before Bill tried to kill himself.\\n\\n[b.] John tried to kill himself before Bill tried to kill John.\\n\\nPetarPetar jeAux pokusaotried dathat postanebecome predsednikpresident aand toit jeAux pokusalatried itoo MarijaMarija ``Petar tried to become president and Marija tried it too.''\\n\\nThis sentence means that Marija tried to bring it about that Marija (not Petar) become president.\\n\\nAnother option, and the one that Zec takes, is to posit an obligatory coreference relation between the subject of ``try'' and the subject of its complement clause; this relation would presumably be induced by the control verb. If this option is taken, it would presumably force the abstraction of both arguments at the same time under second-order matching.\\n\\nwhich, when taken to be a source for ellipsis, would generate only two solutions to the equation\\n\\nmanifesting a sloppy reading for the controlled subject occurrence and either a strict or a sloppy reading for the reflexive occurrence, as required.\\n\\nReflexivization\\n\\nZijShe verdedigdedefended zichherself beterbetter danthan PeterPeter ``She defended herself better than Peter.''\\n\\nSells et al. characterize reflexive constructions involving only sloppy readings as ``closed predicate'' constructions. They discuss only examples in which the reflexive appears in object position, with its antecedent being the subject of the same clause.\\n\\nWe might assume, then, that for the examples they discuss, the presence of the reflexive correlates with the operation of a semantic relation-reducing rule, one which semantically ``intransitivizes'' the verb. In the Dutch case, then, the presence of zich signals a change in the meaning of the verb from the meaning in (a) to the one in (b):\\n\\nA solution of this type would work for all cases of obligatory sloppy readings for reflexives that are described by Sells et al., since they consider only cases where the reflexive and its antecedent are arguments of the same predicate, in which a relation-reducing operation can apply.\\n\\nHowever, this solution would be inappropriate in cases where the reflexive and its antecedent are clearly arguments of different predicates, where a relation-reducing operation cannot apply. Although such cases are difficult to find, it may be that the Serbo-Croatian reflexive sebe (genitive svoje) is such a case.\\n\\nThe following sentence has only a sloppy reading (Draga Zec, personal communication):\\n\\nPetarPetar jeAux sakriohid stoone hiljadahundred dolaradollars ispodunderneath svojeself's kucehouse aand tothat jeAux uciniodid ialso PavlePavle ``Petar hid one hundred dollars underneath self's house, and Paul did (that) too.''\\n\\nThe only reading available for this sentence is that Paul hid one hundred dollars under his own house.\\n\\nWe postulate that the reflexive sebe in Serbo-Croatian engenders primary as opposed to secondary occurrences, which would then be subject to the primary occurrence constraint. This would also be true of the English reflexive for those speakers who find that strict readings with reflexives are unacceptable.\\n\\nIn short, a variety of syntactic constructions give rise to multiple primary occurrences of parallel elements: control, both of the type seen in English and of the type seen in Serbo-Croatian, and reflexivization in some dialects of English and in Serbo-Croatian.\\n\\nAntecedent\\n\\n\\n\\nanaphor constraints\\n\\nMissing readings with multiple occurrences of anaphora\\n\\nIn cases where there are two pronouns coreferent with the parallel element in the source, one might expect that each pronoun would give rise to either a strict or a sloppy reading, giving a total of four readings for the target clause. This does not seem to be the case, however; one of the readings is systematically missing.\\n\\n[a.] Bill believed that he loved his wife, and Harry did too.\\n\\n[b.] Harry believed that Bill loved Bill's wife.\\n\\n[b''.] Harry believed that Harry loved Harry's wife.\\n\\n[b''.] Harry believed that Harry loved Bill's wife.\\n\\n[a.] Edith said that finding her husband nude had upset her, and Martha did too.\\n\\n[b.] Martha said that finding Martha's husband nude had upset Martha.\\n\\n[b'.] Martha said that finding Edith's husband nude had upset Edith.\\n\\n[b''.] Martha said that finding Edith's husband nude had upset Martha.\\n\\nThe interpretation paraphrased in 0b''' is missing. The unifier for P for the missing reading is:\\n\\nJohn revised his paper before the teacher did, and Bill did too.\\n\\nJohn and then the teacher revised John's paper; Bill revised John's paper before the teacher revised Bill's paper.\\n\\nOn the excluded reading, the source clause is ``John revised his paper before the teacher did'', and the target clause is ``Bill did too''. ``John'' and ``Bill'' are the parallel elements. The unifier for P for this reading is:\\n\\nThese various missing readings can be captured by positing a linking relationship between the semantics of pronouns and that of their antecedents, and generalizing it to include the relation between the semantics of terms induced by ellipsis and that of their source parallel element. Under a suitable definition of this generalized antecedent linking, all of the cases here can be captured by requiring that if an occurrence is abstracted over, so must its generalized antecedent.\\n\\nApparent Syntactic Constraints\\n\\nFinally, we turn to some simple examples that seem to lack any readings whatsoever. Consider the following examples, where ``Mary'' is taken to be the antecedent of ``she'':\\n\\nThese judgments would follow from an analysis on which syntactic structure is copied from the source to the target, as the sentences with copies in place violate constraints on binding.\\n\\nHowever, a simple copying analysis faces problems in accounting for grammatical examples of similar structure:\\n\\nJohn got to Suei's apartment before shei did.\\n\\nJohn voted for Suei because shei told him to.\\n\\nOn a copying analysis, these examples would be incorrectly predicted to be ungrammatical, just as their copied versions are:\\n\\nJohn got to Suei's apartment before shei got to Suei's\\n\\napartment.\\n\\nJohn voted for Suei because shei told him to vote for Suei.\\n\\nAnother example that seems to argue for a quite superficial analysis of ellipsis is the following, due to Yoshihisa Kitagawa (personal communication to Peter Sells):\\n\\nJohn thinks that Mary will revise his paper before Bill will.\\n\\non the reading in which Mary revises John's paper and Bill revises his own paper. We find the intuition questionable, but it is clearly problematic for our (and many others') analysis if the reading is deemed to be available.\\n\\nFinally, examples which seem to argue for the presence of a gap in the ellipsis site include the following:\\n\\nOn the assumption that long-distance dependencies are syntactically constrained and that subjacency violations involve an improper syntactic relation between a filler and a gap, these examples indicate that the ellipsis site contains a gap at syntactic structure.\\n\\nConclusion\\n\\nThe underlying idea in the analysis of ellipsis that we have presented here--namely, the construction of higher-order equations on the basis of parallel structures, and their solution by unification--has been exemplified primarily by the verb-phrase ellipsis construction. However, many other elliptical phenomena and related phenomena subject to multiple readings akin to the strict and sloppy readings discussed here may be analyzed using the same techniques. The ambiguities in cleft sentences such as\\n\\nIt is Dan who loves his wife.\\n\\nand interpretation of ``only'' with respect to its focus, as in\\n\\nOnly Dan loves his wife.\\n\\nas well as more standard elliptical phenomena such as stripping and comparative deletion can be analyzed in this way as well, making a broad range of predictions as to the space of possible readings and their interaction with other semantic phenomena. It remains for future work to test these potential applications more fully.\\n\\nWe adduce three advantages of the analysis of elliptical constructions presented here over previous alternatives. First, it is in certain respects simpler, in that it requires no postulation of otherwise unmotivated ambiguities in the source clause. Second, it is more accurate in its predictions, especially in allowing readings disallowed in identity-of-relations analyses. Third, it is methodologically preferable in that the analysis follows directly from a semantic statement of the ellipsis problem with little stipulation. The operation on which it relies, higher-order unification, is semantically sound in that the results it produces are determined by the meanings of phrases directly rather than by the form of the representations encoding those meanings, as operations of deletion or copying of portions of such representations are.\\n\\nAcknowledgements\\n\\nWe would like to thank the following people for helpful discussion: Hiyan Alshawi, Sam Bayer, Joan Bresnan, Mark Gawron, Kris Halvorsen, Dan Hardt, Julia Hirschberg, David Israel, Mark Johnson, Ron Kaplan, Lauri Karttunen, Shalom Lappin, Richard Larson, Peter Ludlow, John Maxwell, Richard Oehrle, Stanley Peters, Hub Prust, Steve Pulman, Mats Rooth, Ivan Sag, Peter Sells, Gregory Ward, Michael Wescoat, Annie Zaenen, Draga Zec, and two anonymous reviewers. The written comments alone that we received prior to publication ran to well over 50 pages, longer, in fact, than the paper itself. We regret that we could not include discussion of all the important issues that they raised.\\n\\n=\\n\\nBibliography\\n\\nJohan van Benthem. 1989. Categorial grammar and type theory. Journal of Philosophical Logic. To appear.\\n\\nGennaro Chierchia. 1983. Outline of a semantic theory of (obligatory) control. In Michael Barlow, Daniel Flickinger, and Michael Wescoat, editors, Proceedings of the West Coast Conference on Formal Linguistics 2, pages 19-31. Stanford Linguistics Association, Stanford University.\\n\\nGennaro Chierchia. 1984. Anaphoric properties of infinitives and gerunds. In Mark Cobler, Susannah MacKaye, and Michael Wescoat, editors, Proceedings of the West Coast Conference on Formal Linguistics 3, pages 28-39. Stanford Linguistics Association, Stanford University.\\n\\nGennaro Chierchia. 1988. Dynamic generalized quantifiers and donkey anaphora. In M. Krifka, editor, Proceedings of the 1988 Tbingen Conference. Seminar fr Natrliche-Sprachiche Systeme der Universitt Tbingen, November.\\n\\nRobin Cooper. 1983. Quantification and Syntactic Theory, volume 21 of Synthese Language Library. D. Reidel, Dordrecht.\\n\\nsten Dahl. 1972. On so-called `sloppy identity'. In Gothenburg Papers in Theoretical Linguistics, volume 11. University of Gteborg.\\n\\nsten Dahl. 1974. How to open a sentence: Abstraction in natural language. In Logical Grammar Reports, No. 12. University of Gteborg.\\n\\nRobert Fiengo and Robert May. 1990. Anaphora and ellipsis. MS, City University of New York and University of California, Irvine.\\n\\nHarvey Friedman. 1975. Equality between functionals. In R. Parikh, editor, Lecture Notes in Mathematics 453, pages 22-37. Springer-Verlag, Berlin, Germany.\\n\\nMark Gawron and Stanley Peters. 1990. Anaphora and quantification in Situation Semantics. CSLI/University of Chicago Press, Stanford University. CSLI Lecture Notes, Number 19.\\n\\nGerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and Ivan A. Sag. 1985. Generalized Phrase Structure Grammar. Harvard University Press, Cambridge, MA.\\n\\nJ. Groenendijk and M. Stockhof. 1987. Dynamic Montague Grammar. Paper presented at the Workshop on Discourse Representation Theory, Stuttgart, West Germany, December.\\n\\nIsabelle Hak.\\n\\n1985.\\n\\nThe Syntax of Operators.\\n\\nPh.D. thesis, MIT.\\n\\nIsabelle Hak. 1987. Bound VPs that need to be. Linguistics and Philosophy, 10:503-530.\\n\\nJorge Hankamer and Ivan A. Sag. 1976. Deep and surface anaphora. Linguistic Inquiry, 7(3):391-428.\\n\\nIrene Heim. 1982. The Semantics of Definite and Indefinite Noun Phrases. Ph.D. thesis, University of Massachusetts-Amherst.\\n\\nLars Hellan. 1988. Anaphora in Norwegian and the Theory of Grammar. Foris Publications, Dordrecht.\\n\\nJulia Hirschberg and Gregory Ward. 1991. Accent and bound anaphora. Cognitive Linguistics. To appear.\\n\\nPaul Hirshbhler. 1982. VP deletion and across-the-board quantifier scope. In James Pustejovsky and Peter Sells, editors, Proceedings of NELS 12. GLSA, University of Massachusetts-Amherst.\\n\\nJerry R. Hobbs and Stuart M. Shieber. 1987. An algorithm for generating quantifier scopings. Computational Linguistics, 13:47-63.\\n\\nGrard Huet and Bernard Lang. 1978. Proving and applying program transformations expressed with second-order patterns. Acta Informatica, 11(1):31-55.\\n\\nRay S. Jackendoff. 1972. Semantic Interpretation in Generative Grammar. MIT Press, Cambridge, MA.\\n\\nHans Kamp. 1981. A theory of truth and semantic representation. In Jeroen Groenendijk, Theo Janssen, and Martin Stokhof, editors, Formal Methods in the Study of Language, pages 277-321, Amsterdam. Mathematical Centre.\\n\\nYoshihisa Kitagawa. 1991. Copying identity. Natural Language and Linguistic Theory. To appear.\\n\\nShalom Lappin. 1984. VP anaphora, quantifier scope, and logical form. Linguistic Analysis, 13(4):273-315.\\n\\nFernando C. N. Pereira.\\n\\n1990.\\n\\nCategorial semantics and scoping.\\n\\nComputational Linguistics, 16(1):1\\n\\n\\n\\n10.\\n\\nS. G. Pulman. 1988. A contextual reasoning and cooperative response framework for the Core Language Engine. Internal report, SRI Cambridge, Cambridge, England.\\n\\nTanya Reinhart. 1983. Anaphora and Semantic Interpretation. University of Chicago Press, Chicago.\\n\\nCraige Roberts. 1987. Modal Subordination, Anaphora, and Distributivity. Ph.D. thesis, University of Massachusetts-Amherst.\\n\\nIvan A. Sag.\\n\\n1976.\\n\\nDeletion and Logical Form.\\n\\nPh.D. thesis, MIT.\\n\\nTraugott Scheibe. 1973. Zum Problem der grammatisch relevanten Identitt. In F. Kiefer and N. Ruwet, editors, Generative Grammar in Europe, pages 482-527. D. Reidel Publishing Company, Dordrecht.\\n\\nLen Schubert and F J. Pelletier. 1982. From English to logic: Context-free computation of `conventional' logical translations. American Journal of Computational Linguistics, 10:165-176. Reprinted in Grosz et al., 1986.\\n\\nPeter Sells, Annie Zaenen, and Draga Zec. 1987. Reflexivization variation: Relations between syntax, semantics, and lexical structure. In Masayo Iida, Stephen Wechsler, and Draga Zec, editors,   Working Papers in Grammatical Theory and Discourse Structure, pages 169-238. CSLI/University of Chicago Press, Stanford University. CSLI Lecture Notes, Number 11.\\n\\nMark J. Steedman.\\n\\n1990.\\n\\nGapping as constituent coordination.\\n\\nLinguistics and Philosophy, 13(2):207\\n\\n\\n\\n263.\\n\\nBonnie Lynn Webber. 1978. A Formal Approach to Discourse Anaphora. Ph.D. thesis, Harvard University.\\n\\nMichael Wescoat. 1989. Sloppy readings with embedded antecedents. MS, Stanford University.\\n\\nEdwin Williams.\\n\\n1977.\\n\\nDiscourse and logical form.\\n\\nLinguistic Inquiry, 8(1):101\\n\\n\\n\\n139.\\n\\nDraga Zec. 1987. On obligatory control in clausal complements. In Masayo Iida, Stephen Wechsler, and Draga Zec, editors,   Working Papers in Grammatical Theory and Discourse Structure, pages 139-168. CSLI/University of Chicago Press, Stanford University. CSLI Lecture Notes, Number 11.\\n\\nFootnotes\\n\\n[(a)]Dan likes Dan's wife.\\n\\n[(b)]Dan likes Dan's wife, and Bill does too.\\n\\nhas only a strict reading, which accords with conventional wisdom. Solutions involving vacuous abstraction, such as\\n\\nare ruled out where necessary as special cases of this more general constraint. A direct prohibition against vacuous abstraction might be too strong, since verb phrase ellipsis is possible even in cases where the subject of the source clause is pleonastic and makes no semantic contribution (examples due to Ivan Sag):\\n\\nJohn said it would rain, and it did. John said there would be trouble, and there was.\\n\\nSuppose the interpretation of the former example (ignoring tense and aspect as usual) were\\n\\nThen the second-order matching problem induced by the ellipsis would be\", metadata={'source': '../data/raw/cmplg-xml/9503008.xml'}),\n",
       " Document(page_content=\"A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances\\n\\nDrawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics. This paper provides a theoretical framework, called stratified logic, that can accommodate defeasible pragmatic inferences. The framework yields an algorithm that computes the conversational, conventional, scalar, clausal, and normal state implicatures; and the presuppositions that are associated with utterances. The algorithm applies equally to simple and complex utterances and sequences of utterances.\\n\\nPragmatics and Defeasibility\\n\\nIt is our aim to provide such an approach here. In doing this, we assume the existence, for each type of pragmatic inference, of a set of necessary conditions that must be true in order for that inference to be triggered. Once such a set of conditions is met, the corresponding inference is drawn, but it is assigned a defeasible status. It is the role of context and knowledge of the conversants to ``decide'' whether that inference will survive or not as a pragmatic inference of the structure. We put no boundaries upon the time when such a cancellation can occur, and we offer a unified explanation for pragmatic inferences that are inferable when simple utterances, complex utterances, or sequences of utterances are considered.\\n\\nStratified logic\\n\\nTheoretical foundations\\n\\nThe algorithm\\n\\nA set of examples\\n\\nWe present a set of examples that covers a representative group of pragmatic inferences. In contrast with most other approaches, we provide a consistent methodology for computing these inferences and for determining whether they are cancelled or not for all possible configurations: simple and complex utterances and sequences of utterances.\\n\\nSimple pragmatic inferences\\n\\nLexical pragmatic inferences\\n\\nScalar implicatures\\n\\nSimple cancellation\\n\\nComplex utterances\\n\\nWe now study how stratified logic and  the model-ordering relation capture one's intuitions.\\n\\nOr -- non-cancellation Or -- cancellation Pragmatic inferences in sequences of utterances\\n\\nConversational implicatures in indirect replies Conclusions\\n\\nUnlike most research in pragmatics that focuses on certain types of presuppositions or implicatures, we provide a global framework in which one can express all these types of pragmatic inferences. Each pragmatic inference is associated with a set of necessary conditions that may trigger that inference. When such a set of conditions is met, that inference is drawn, but it is assigned a defeasible status. An extended definition of satisfaction and a notion of ``optimism'' with respect to different interpretations yield the preferred interpretations for an utterance or sequences of utterances. These interpretations contain the pragmatic inferences that have not been cancelled by context or conversant's knowledge, plans, or intentions. The formalism yields an algorithm that has been implemented in Common Lisp with Screamer. This algorithm computes uniformly pragmatic inferences that are associated with simple and complex utterances and sequences of utterances, and allows cancellations of pragmatic inferences to occur at any time in the discourse.\\n\\nAcknowledgements\\n\\nThis research was supported in part by a grant from the Natural Sciences and Engineering Research Council of Canada.\\n\\nBibliography\\n\\nG. Frege. 1892. ber sinn und bedeutung. Zeitschrift fr Philos. und Philos. Kritik, 100:373-394. reprinted as: On Sense and Nominatum, In Feigl H. and Sellars W., editors, Readings in Philosophical Analysis, pages 85-102, Appleton-Century-Croft, New York, 1947.\\n\\nG.J.M. Gazdar. 1979. Pragmatics: Implicature, Presupposition, and Logical Form. Academic Press.\\n\\nN. Green and S. Carberry. 1994. A hybrid reasoning model for indirect answers. In Proceedings 32nd Annual Meeting of the Association for Computational Linguistics, pages 58-65.\\n\\nN. Green. 1990. Normal state implicature. In Proceedings 28th Annual Meeting of the Association for Computational Linguistics, pages 89-96.\\n\\nN. Green. 1992. Conversational implicatures in indirect replies. In Proceedings 30th Annual Meeting of the Association for Computational Linguistics, pages 64-71.\\n\\nJ.B. Hirschberg. 1985. A theory of scalar implicature. Technical Report MS-CIS-85-56, Department of Computer and Information Science, University of Pennsylvania. Also published by Garland Publishing Inc., 1991.\\n\\nG. Hirst, S. McRoy, P. Heeman, P. Edmonds, and D. Horton. 1994. Repairing conversational misunderstandings and non-understandings. Speech Communication, 15:213-229.\\n\\nG. Hirst. 1991. Existence assumptions in knowledge representation. Artificial Intelligence, 49:199-242.\\n\\nL. Karttunen and S. Peters. 1979. Conventional implicature. In Oh C.K. and Dinneen D.A, editors, Syntax and Semantics, Presupposition, volume 11, pages 1-56. Academic Press.\\n\\nL. Karttunen.\\n\\n1974.\\n\\nPresupposition and linguistic context.\\n\\nTheoretical Linguistics, 1:3\\n\\n\\n\\n44.\\n\\nP. Kay.\\n\\n1992.\\n\\nThe inheritance of presuppositions.\\n\\nLinguistics  Philosophy, 15:333\\n\\n\\n\\n379.\\n\\nD. Marcu. 1994. A formalism and an algorithm for computing pragmatic inferences and detecting infelicities. Master's thesis, Dept. of Computer Science, University of Toronto, September. Also published as Technical Report CSRI-309, Computer Systems Research Institute, University of Toronto.\\n\\nD. Marcu and G. Hirst. 1994. An implemented formalism for computing linguistic presuppositions and existential commitments. In H. Bunt, R. Muskens, and G. Rentier, editors, International Workshop on Computational Semantics, pages 141-150, December.\\n\\nS. McRoy and G. Hirst. 1993. Abductive explanation of dialogue misunderstandings. In Proceedings, 6th Conference of the European Chapter of the Association for Computational Linguistics, pages 277-286, April.\\n\\nR.E. Mercer. 1987. A Default Logic Approach to the Derivation of Natural Language Presuppositions. Ph.D. thesis, Department of Computer Science, University of British Columbia.\\n\\nW.V.O. Quine. 1949. Designation and existence. In Feigl H. and Sellars W., editors, Readings in Philosophical Analysis, pages 44-51. Appleton-Century-Croft, New York.\\n\\nR. Reiter. 1980. A logic for default reasoning. Artificial Intelligence, 13:81-132.\\n\\nB. Russell. 1905. On denoting. Mind n.s., 14:479-493. reprinted in: Feigl H. and Sellars W. editors, Readings in Philosophical Analysis, pages 103-115. Appleton-Century-Croft, New York, 1949.\\n\\nR.A. van der Sandt. 1992. Presupposition projection as anaphora resolution. Journal of Semantics, 9:333-377.\\n\\nJ.M. Siskind and D.A. McAllester. 1993. Screamer: A portable efficient implementation of nondeterministic Common Lisp. Technical Report IRCS-93-03, University of Pennsylvania, Institute for Research in Cognitive Science, July 1.\\n\\nR.M. Weischedel. 1979. A new semantic computation while parsing: Presupposition and entailment. In Oh C.K. and Dinneen D.A, editors, Syntax and Semantics, Presupposition, volume 11, pages 155-182. Academic Press.\\n\\nH. Zeevat. 1992. Presupposition and accommodation in update semantics. Journal of Semantics, 9:379-412.\", metadata={'source': '../data/raw/cmplg-xml/9504017.xml'}),\n",
       " Document(page_content=\"Restricting the Weak\\n\\n\\n\\nGenerative Capacity of Synchronous\\n\\nTree\\n\\n\\n\\nAdjoining Grammars\\n\\nThe formalism of synchronous tree-adjoining grammars, a variant of standard tree-adjoining grammars (TAG), was intended to allow the use of TAGs for language transduction in addition to language specification. In previous work, the definition of the transduction relation defined by a synchronous TAG was given by appeal to an iterative rewriting process. The rewriting definition of derivation is problematic in that it greatly extends the expressivity of the formalism and makes the design of parsing algorithms difficult if not impossible. We introduce a simple, natural definition of synchronous tree-adjoining derivation, based on isomorphisms between standard tree-adjoining derivations, that avoids the expressivity and implementability problems of the original rewriting definition. The decrease in expressivity, which would otherwise make the method unusable, is offset by the incorporation of an alternative definition of standard tree-adjoining derivation, previously proposed for completely separate reasons, thereby making it practical to entertain using the natural definition of synchronous derivation. Nonetheless, some remaining problematic cases call for yet more flexibility in the definition; the isomorphism requirement may have to be relaxed. It remains for future research to tune the exact requirements on the allowable mappings.\\n\\nIntroduction The Rewriting Definition of Derivation\\n\\nThe original definition of derivation for synchronous TAGs was based on the iterative rewriting of one derived tree pair into another. In this section, we provide a more precise description of this approach to derivation, along with a discussion of its problems. First, however, we digress to discuss some notational issues.\\n\\nNotation\\n\\nWe assume in this and later sections a general familiarity with tree-adjoining grammars and their formal foundations, as described, for instance, by v87.\\n\\nThe Rewriting Process\\n\\n3. Modify the current derived tree pair by adjoining the chosen trees at the end of the chosen link, yielding the modified derived tree pair\\n\\nThis becomes the new current derived tree pair.\\n\\nThe operation I[A/t] used above takes a tree I, an auxiliary tree A, and an address t in I and yields the result of adjoining Aat address t in I. (The generalization to allow for substitution as well as adjunction as a primitive operation -- both in this notation and the definition of derivation -- should be clear.) A formal definition for this operation is given by Vijay-Shanker [page 15]v87 and by Shieber and Schabes [appendix]full-alt-deriv.\\n\\nAn Example of Rewriting\\n\\nBy way of example, we present a sample synchronous TAG that transduces between a tiny fragment of English and a corresponding ``logical form'' semantic representation.\\n\\nProblems with the Rewriting Definition\\n\\nThere are two problems with the rewriting definition of synchronous TAGs, having to do with the expressivity and implementability of the formalism under that definition.\\n\\nExpressivity\\n\\nSynchronous TAGs under this definition may specify non-tree-adjoining languages. More precisely stated, given a grammar G, although, by definition, L(GL) is a tree-adjoining language, L(G)L may not be.\\n\\nImplementability\\n\\nUnfortunately, the structure of a synchronous derivation bears no uniform relationship to the kind of derivation postulated for standard TAGs. (This point is discussed further in the next section.) Thus, if a standard TAG parsing algorithm is used for the first step in the process (so that DL is a traditional TAG derivation tree), the second step is not well defined. It is therefore not clear how synchronous TAGs can be effectively used under this definition of derivation.\\n\\nNote that this point is independent of whether the three conceptual phases of processing are interleaved in time. The possibility to interleave the computations of the phases does not make their definition any simpler.\\n\\nThe Natural Definition of Derivation\\n\\nThe notion of derivation just presented for synchronous TAGs is quite nonstandard for the TAG literature in being ``flat'' and rewriting oriented. Recall that the standard definition of TAG derivation, due to v87, is hierarchically structured in terms of derivation trees, trees that serve to characterize the operations required to construct a particular derived tree, and hence its yield.\\n\\n1. DL is a well-formed derivation tree relative to GL.\\n\\n2. DR is a well-formed derivation tree relative to GR.\\n\\nThis, then, is the most natural definition of synchronous tree-adjoining derivation, as it is the natural generalization of the definition of derivation for standard TAGs. It merely requires that there be two derivations that are separately well-formed and appropriately synchronized as specified by the links.\\n\\nAdvantages of the Natural Definition\\n\\nExpressivity\\n\\nAlternatively, the TAL nature of synchronous TAGs under this definition can be easily shown by reduction to tree-set-local multicomponent TAGs (MCTAG), which are known to generate only tree-adjoining languages. Each elementary tree pair in the synchronous TAG corresponds to an elementary tree set in the MCTAG. To ensure that left-hand trees are not adjoined into right-hand trees and vice versa, the node labels on the left- and right-hand trees are uniformly renamed apart. Each node in a left-hand tree is marked with a selective adjoining constraint that allows adjunction only of certain elementary tree sets. For each link that impinges on the node, and each tree pair that can operate on that link, the corresponding tree set is allowed by the SA constraint. Similar constraints are added to each right-hand node. Finally, for each pair of nonterminals that root the trees in an initial tree pair, a new elementary tree is constructed rooted in a new nonterminal symbol not used elsewhere with two nonterminal children labeled by the left and right root nonterminals of the initial tree pair and which are to be filled by substitution.\\n\\nSince any synchronous TAG can be reduced to a tree-set-local MCTAG, the languages generated by synchronous TAGs are at most the tree-adjoining languages. The converse inclusion is obvious.\\n\\nImplementability\\n\\nAnother advantage of the new definition of synchronous derivation is in its utility for implementation of synchronous TAG transducers. Recall that under the rewriting definition, the structure of a synchronous derivation bears no uniform relationship to the kind of derivation postulated for standard TAGs and therefore recovered by standard TAG parsing algorithms. Thus, the second step in the schematic process\\n\\nProblems with the Natural Definition\\n\\nAlong with the advantages of the new definition of synchronous TAG derivation, new problems are introduced as well. First, the exclusion of multiple adjunctions at a single address is problematic for synchronous TAG derivations. Second, the isomorphism requirement between the derivation trees may be too strong as well. The former problem admits of a straightforward solution, which we describe below. The latter does not; we describe the symptoms of the problem but leave its resolution as an open issue for further research.\\n\\nMultiple Adjunction\\n\\nThe Isomorphism Requirement\\n\\nA potentially more severe (and certainly more subtle) problem results from the requirement of isomorphism between DL and DR. There seem to be certain applications of synchronous TAGs for which this requirement is too strong. In this section, we present a taxonomy of potential counterexamples to isomorphism, organized by the ``shape'' of the nonisomorphic part of the mapping between the derivation trees. The examples are drawn from both technological application of synchronous TAG to the problem of defining translations between languages and application of synchronous TAG to the modeling of natural language semantics. It may turn out that different applications provide different amounts of pressure to loosen the isomorphism requirement in differing ways. Although we discuss several possible approaches to resolving this issue, we leave to further work whether a satisfactory solution for a given application can be found, and if so, what that solution might be.\\n\\nMany\\n\\n\\n\\nto\\n\\n\\n\\nOne Mappings\\n\\nThe simplest examples are cases in which an atomic construction in one language is compound in another. For example, asj90 point out that the English adverbial `hopefully' is translated by the French phrase `on espre que'. Whereas the English corresponds to a single elementary tree, the French corresponds to a tree derived by substituting the elementary tree for `on' as the NP argument of `espre'. Such examples argue for the ability to allow the mapping between the left and right derivation trees to be relaxed from a strict isomorphism.\\n\\nOne might think (as indeed the present author did before penetrating discussions with Anthony Kroch) that a mismatch such as this shows that the isomorphism requirement must also be too strong for the purpose of modeling natural language semantics, for if these two constructions -- `hopefully' and `on espre que' -- have the same semantics, then at least one of the two (if not both) must exhibit a mismatch between the natural language derivation and a derivation of its logical form. The error in this reasoning follows from the assumption that the relationship of ``corresponds as an appropriate translation'' (in the sense in which bilingual dictionaries record such facts) is tantamount to ``means the same as''. This assumption is highly suspect. Bilingual dictionaries do not codify perfect translations in any sense, if such a notion is even coherent.\\n\\nHowever, mismatches of this variety may also be found in applications to directly modeling natural-language semantics. For instance, the transduction relationship between a compound idiom (such as `kick the bucket') and its atomic semantics (given, e.g., by a simple predication of die) might be thought to be of this form.\\n\\nElimination of Dominance\\n\\nEven when the number of nodes in the paired derivation trees is the same, they may exhibit different structure. Nodes participating in a domination relationship in one tree may be mapped to nodes neither of which dominates the other.\\n\\nAbeill (personal communication) has noted a potential example of such a mismatch. For instance, in the sentence\\n\\nLe docteur soigne les dents de Jean. The doctor treats Jean's teeth.\\n\\nthe subphrase `de Jean' is substituted into the `dents' tree syntactically, and arguably modifies the semantics of that tree as well. However, the cliticized version of the sentence\\n\\nLe docteur lui soigne les dents. The doctor treats his teeth.\\n\\nAgain, examples may be found in the arena of semantic interpretation. Although the argumentation is much more complex, and well beyond the scope of this paper, similar relationships arise in the context of modeling quantifier scope ambiguity.\\n\\nInversion of Dominance\\n\\nJean monte la rue en courant. John runs up the street.\\n\\nWe know of no example of inversion of dominance in applications to natural-language semantics.\\n\\nRelaxing Isomorphism\\n\\nIf further relaxation of the isomorphism requirement is to be allowed, some method of controlling the relationship between the pair derivations will be needed. Owen Rambow and Giorgio Satta (personal communication) have conjectured that an approach along the lines of control grammars might be useful. This possibility, though tantalizing, remains to be explored.\\n\\nThe exact nature of the relationship between paired derivation trees must remain for future work.\\n\\nConclusion\\n\\nWe have introduced a simple, natural definition of synchronous tree-adjoining derivation, based on isomorphisms between standard tree-adjoining derivations, that avoids the expressivity and implementability problems of the original rewriting definition. The decrease in expressivity, which would otherwise make the method unusable, is offset by the incorporation of an alternative definition of standard tree-adjoining derivation, previously proposed for completely separate reasons, that allows for multiple adjunctions at a single node in an elementary tree. The increased flexibility from the ability to perform such multiple adjunctions makes it conceivable to entertain using the natural definition of synchronous derivation. Nonetheless, some remaining problematic cases call for yet more flexibility in the definition; the isomorphism requirement may have to be relaxed. It remains for future research to tune the exact requirements on the allowable mappings.\\n\\nAcknowledgements\\n\\nThe research described in this paper was made possible in part by a Presidential Young Investigator grant IRI-91-57996 from the National Science Foundation and matching funds from Xerox Corporation. An early version of this paper was presented at the Second Workshop on Tree-Adjoining Grammars, in Philadelphia, PA, in August 1992. The author is indebted to the following people for helpful discussions on the subject matter of this paper: Anne Abeill, Aravind Joshi, Owen Rambow, Giorgio Satta, Yves Schabes, K. Vijay-Shanker, David Weir, and Peter Whitelock.\\n\\nBibliography\\n\\nAbeill, Anne, Yves Schabes, and Aravind K. Joshi. 1990. Using lexicalized tree adjoining grammars for machine translation. In Proceedings of the 13th International Conference on Computational Linguistics (COLING'90), Helsinki, August.\\n\\nSchabes, Yves and Stuart M. Shieber. 1994. An alternative conception of tree-adjoining derivation. Computational Linguistics, 20(1):91-124, March. Also available as cmp-lg/9404001.\\n\\nShieber, Stuart M. and Yves Schabes. 1990. Synchronous tree-adjoining grammars. In Proceedings of the 13th International Conference on Computational Linguistics, Helsinki.\\n\\nVijay-Shanker, K. 1987. A Study of Tree Adjoining Grammars. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania.\\n\\nWhitelock, Peter. 1992. Shake-and-bake translation. In Proceedings of the 14th International Conference on Computational Linguistics (COLING '92), Nantes, France, July.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9404003.xml'}),\n",
       " Document(page_content=\"Assessing Complexity Results in Feature Theories\\n\\nIn this paper, we assess the complexity results of formalisms that describe the feature theories used in computational linguistics. We show that from these complexity results no immediate conclusions can be drawn about the complexity of the recognition problem of unification grammars using these feature theories. On the one hand, the complexity of feature theories does not provide an upper bound for the complexity of such unification grammars. On the other hand, the complexity of feature theories need not provide a lower bound. Therefore, we argue for formalisms that describe actual unification grammars instead of feature theories. Thus the complexity results of these formalisms judge upon the hardness of unification grammars in computational linguistics.\\n\\nIntroduction\\n\\nIn this paper, we will focus on the complexity results that are obtained from formalizing feature theories. We will prove that these complexity results do not hold if we consider unification grammars that use these feature theories in addition to a constituent structure component. First we will show, that the complexity of a unification grammar theory may be higher than the complexity of its feature theory and constituent structure components. Second we will explain, that the complexity of a unification grammar may be lower than the complexity of the formalized feature theory.\\n\\nBoth proofs put the complexity results that have been achieved in a different perspective. The first proof implies that the complexity of a feature theory does not provide an upper bound for the complexity of grammars using that feature theory. The second proof implies that the  complexity of a feature theory might not provide a lower bound for the complexity of grammars using that feature theory. Therefore, we argue that if one is interested in the complexity of unification grammars that are used in grammars, one should look at the complexity of these unification grammars as a whole. No insight in the complexity of a unification grammar is gained by looking only at the complexity of its components in isolation.\\n\\nPreliminaries\\n\\nComplexity Theory.\\n\\nIn complexity theory one tries to determine the complexity of problems. The complexity is measured by the amount of time and space needed to solve a problem. Usually, one considers decision problems: problems that are answered  `Yes' or `No'. Often we are interested in the distinction between tractable and intractable problems. A problem is tractable if its solution requires an amount of steps that is polynomial in the size of the input: we say that the problem requires polynomial time. Likewise, we speak of linear time, etcetera. The tractable problems are also called `P problems'. The intractable problems are called `NP-hard problems'. The easiest intractable problems are the `NP-complete problems'. It is unknown whether NP-complete problems have polynomial time solutions. However we know, that solutions for NP-complete problems can be guessed and checked in polynomial time. It is strongly believed that the class of P problems and the class of NP-complete problems are different, although this is yet unproven.\\n\\nThere is a direct manner to determine the upper bound complexity of a problem, if there is an algorithm that solves the problem: determine the complexity of that algorithm. An indirect way to determine the lower bound complexity of a problem is the reduction. A reduction from some problem A to some problem B maps instances of problem A onto instances of problem B.\\n\\nThe reductions that we will consider are known as polynomial time, many-one reductions. These many-one reductions are subject to two conditions: (1) the reductions are easy to compute, and (2) the reductions preserve the answers. A reduction from A to B is easy to compute, if the mapping takes polynomial time. A reduction preserves answers if the answer to the instance of A is the same as the answer to the instance of B. That is, the answer to the instance of A is `Yes' if, and only if, the answer to the instance of B is also `Yes'.\\n\\nA reduction is an elegant way to classify a problem as intractable. Suppose problem B is a problem with unknown complexity. Let there be a reduction f from an NP-hard problem A to problem B. Furthermore, let f conform to the two conditions above. By an indirect proof, it follows from this reduction that B is at least as hard as A. Hence B is also an NP-hard problem. If we also prove that we can guess a solution for B and check that guessed solution in polynomial time, then B is an NP-complete problem.\\n\\nA well-known NP-complete problem is  SATISFIABILITY (SAT).\\n\\nDefinition  2.1\\n\\nSATISFIABILITY\\n\\nINSTANCE:\\n\\nA formula\\n\\n, from propositional logic, in conjunctive normalform. QUESTION: Is there an assignment of truth-values to the propositional variables of\\n\\n,\\n\\nsuch that\\n\\nis true?\\n\\nThe instances of  SATISFIABILITY are formulas in conjunctive normalform, i.e., the formulas are conjunctions of clauses. The clauses are disjunctions of literals, and the literals are positive and negative occurrences of propositional variables. We call formula\\n\\na satisfiable formula if an assignment exists that makes formula\\n\\ntrue.\\n\\nAn assignment assigns either the value true or the value false to each propositional variable. Given such an assignment, we can determine the truth-value of a formula. The formula\\n\\nis true if, and only if, each clause,\\n\\n,\\n\\nis true.\\n\\nA clause\\n\\nis true if, and only if, at least one literal, li, is true. A positive (negative) literal, li = pj (\\n\\n), is true if, and only if, the variable pj is assigned the value true (false).\\n\\nFeature theory.\\n\\nAlthough there is no such thing as a universal feature theory, there is a general understanding of its abstract objects. These abstract objects describe the internal information or properties of words and phrases. Properties that these abstract objects typically have are the case, the gender, the number, and the tense of words and phrases.\\n\\nThe properties of abstract objects can be combined to form new abstract objects. This operation is called unification. The unification of abstract objects combines all the properties of these abstract objects, provided that the properties are not contradictory.\\n\\nA simple feature theory\\n\\nIn the first part of this section, we will formalize the notion of a feature theory. In the second part of this section, we will present an algorithm that solves the unification problem in an amount of time that is quadratic in the size of its input. This part should convince the reader that the feature theory is indeed simple.\\n\\nThe feature theory formally.\\n\\nAlthough a universal feature theory does not exist, there is a general understanding of its objects. The object of feature theories are abstract linguistic objects, e.g., an object `sentence', an object `masculine third person singular', an object `verb', an object `noun phrase'. These abstract objects have properties, like, tense, number, predicate, subject. The values of these properties are either atomic, like, present and singular, or abstract objects, like, verb and noun phrase.\\n\\nThe abstract objects can be represented as rooted graphs (`feature-graphs'). The nodes of these graphs stand for abstract objects, and the edges represent the properties. More formally, a feature-graph is either a pair\\n\\n,\\n\\nwhere a is an\\n\\natomic value and\\n\\nis the empty set, or a pair (x, E), where x is a root node, and E is a finite, possibly empty set of edges such that (1) for each property and all nodes there is at most one edge that represents the property departing from the node, and (2) if there is an edge in E from node y to node z, then there is a path in E leading from node x to node y.\\n\\nAs an example consider the following abstract objects and simplified feature-graph.\\n\\nSentence: A man walks This abstract object has property  TENSE with value present, property  SUBJECT with value Noun phrase: A man, and property  PREDICATE with value Verb: walks.\\n\\nNoun phrase: A man has property  NUMBER with value singular.\\n\\nVerb: walks also has property  NUMBER with value singular.\\n\\nAssume three pair-wise disjunct sets of symbols: the set of constants A, the set of variables V, and the set of attributes L. The attributes (denoted by f, g, h or capitalized strings) correspond to the properties of the abstract objects, the variables (denoted by x, y, z) correspond to the abstract objects, and the constants (denoted by a, b, c or italicized strings) correspond to the atomic values. Let s, t denote variables or constants, and let a path (denoted by p, q) be a finite, possible empty sequence of attributes.\\n\\nDefinition  3.1 The terms of the description language FL are the elements from V and A. The formulas of the description language (FL-formulas) are equations, and conjunctions:\\n\\nif\\n\\nare formulas, p, q are paths, and s, t are terms. The formulas of the following form are called primitive formulas:\\n\\nis interpreted as: the terms s and t denote the same node in the feature-graph. The formula\\n\\nis interpreted as: there is an edge with label ffrom the node denoted by s to the node denoted by tin the feature-graph.\\n\\nAnother familiar, intuitive description is the attribute-value matrix notation. An attribute-value matrix (AVM) is a set of attribute-value pairs. The values of the attribute-value pairs are boxlabels, and atomic values or AVMs, where equal boxlabels denote equal values. The elements of an AVM are written below one another. The total set is written between squared brackets.\\n\\nare used to denote that the two attributes NUMBER have the same value.\\n\\nThe AVM notation is intuitive because AVMs strongly resemble feature-graphs. We can view the opening brackets and the atomic values of an AVM as nodes. The outermost bracket is the root-node. The attributes of the AVM can be view as edges with the attribute as their label. The box-labels identify nodes in the feature-graph.\\n\\nUnification in FL.\\n\\nLet A and B be abstract linguistic objects, or feature-graphs, that are described by the FL-formulas\\n\\nand\\n\\n, respectively. The unification of A and B is described by FL-formula\\n\\nif and only if\\n\\ndescribes a feature-graph. In the final part of this section we will present an efficient algorithm that determines whether an FL-formula describes a feature-graph. Hence we can view the algorithm as a unification algorithm.\\n\\nUnification in AVM.\\n\\nLet A and B be abstract linguistic objects, or feature-graphs, that are described by the AVMs [F] and [G], respectively. The unification of A and B is denoted by\\n\\n. The algorithm of the final part of this section can be used to compute the AVM\\n\\nefficiently, in the following way.\\n\\nFirst, there is a linear time algorithm that transforms AVMs into FL formulas. Second, the algorithm of the final part of this section can easily be modified such that it also outputs the feature-graph that is described by an FL-formula. Since the modified algorithm will remain efficient, the feature-graph will be small. Finally, there is a trivial, linear time, algorithm that transforms feature-graphs into AVMs.\\n\\nThis feature theory is simple.\\n\\nThe algorithm  FEATUREGRAPHSAT can be used to determine whether two abstract objects can be unified: if the formulas\\n\\nand\\n\\ndescribe abstract objects, then\\n\\ndescribes their unification if, and only if, the unification exists. So we may say that the algorithm solves the unification problem.\\n\\nThe algorithm  FEATUREGRAPHSAT below determines syntactically whether a formula is satisfiable in some feature algebra. Because there is a 1-1 correspondence between satisfiable formulas and feature-graphs, the algorithm determines whether a formula describes a feature-graph. The algorithm first transforms any formula by means of syntactic simplification rules into a normal form. Then this normal form is checked syntactically in order to see whether the formula is satisfiable.\\n\\n. Hence the algorithm  FEATUREGRAPHSAT takes quadratic time, and thus shows that the feature theory is indeed simple.\\n\\nALGORITHM FEATUREGRAPHSAT  INPUT: \\t\\tFormula\\n\\nfrom the                description language. OUTPUT: \\t\\t1) `Yes' if\\n\\ndescribes an acyclic                feature-graph, or         2) `No' otherwise.Begin AlgorithmEach\\n\\nis of the form\\n\\n, where p, q are paths,s,t are terms. TRANSFORM\\n\\ninto a set of primitive formulas:\\n\\n. SIMPLIFY the set P, yielding set S, until no further                simplification is possible. If set S is clash-free and acyclic, then\\t\\tExit with answer `Yes', else \\t\\tExit with answer `No'. End Algorithm\\n\\nFUNCTION TRANSFORM         INPUT: \\t\\tFormula\\n\\nfrom the                description language. OUTPUT: \\t\\tA set of primitive formulas\\n\\n.\\n\\nBegin Function\\n\\n,\\n\\nwhere Step 0.\\n\\nStep 1.\\n\\n, where y is a        fresh variable Step 2.\\n\\n, where         yi (\\n\\n) are fresh variables, and y is a variable        introduced in step 1. End Function\\n\\nIn the procedure  SIMPLIFY we will use the following notations. We use [x/s]P to denote the set that is obtained from P by replacing every occurrence of variable x by term s, and\\n\\nto denote the set\\n\\n,\\n\\nprovided that\\n\\n.\\n\\nif x occurs in P and\\n\\n2.\\n\\n3.\\n\\n4.\\n\\nEnd while    Exit with the simplified form of set P, S. End Procedure\\n\\nLemma  3.1 A simplified set of primitive formulas S is clash-free if 1. S contains no formula\\n\\n,\\n\\nand\\n\\n2.\\n\\nS contains no formula\\n\\nsuch that\\n\\n.\\n\\nLemma  3.2 A simplified set of primitive formulas S is acyclic if, and only if, S does not contain a sequence of formulas\\n\\nand\\n\\n(\\n\\n).\\n\\nProofBy induction on the length of a cycle.\\n\\nNo upper bound\\n\\nFrom Johnson's work, we see that combining problems may change the complexity from decidable to undecidable. We claim that combining problems may change also the complexity from tractable to intractable. Hence, even when we confine ourselves to decidable problems, the complexity of the recognition problem of a unification grammar that uses some feature theory may be higher than the complexity of the satisfiability problem of that feature theory. The claim shows that even under the Off-line Parsability Constraint the complexity of the feature theory still does not provide an upper bound on the complexity of the unification grammar.\\n\\nA fixed regular grammar\\n\\nThe regular language that we want to recognize is\\n\\n.\\n\\nMany other regular grammars could be given for the same language. However, the one presented, as will be seen later, is sufficient for our purposes here: that is, the reduction from  SATISFIABILITY. Obviously, the recognition problem of fixed regular grammar takes linear time.\\n\\nCombining a regular grammar and a feature theory\\n\\nthe set of atomic values is\\n\\n. The linear rewrite rules describe how constituents are formed. The formulas indicate how nodes of the feature-graphs are related to the non-terminals of the rewrite rules.\\n\\nExample(s)We will show the potential derivation of the string\\n\\ncannot be generated.\\n\\ncannot be generated by G.\\n\\nFact  4.2 The language recognized by the unification grammar Gis a proper subset of the regular language\\n\\n.\\n\\nin\\n\\nd steps\\n\\n(\\n\\n), then there are two intermediate stages. First, S derives\\n\\nin a steps.\\n\\nThis T derives\\n\\nin b steps.\\n\\nFinally, this A derives\\n\\nin csteps.\\n\\nIf\\n\\n,\\n\\nwhere\\n\\n,\\n\\nand\\n\\n,\\n\\nthen\\n\\nthere is a\\n\\nsuch that\\n\\n(d = a + b + c)and the feature structure\\n\\nis associated with T,\\n\\nwhere\\n\\nif l = p,\\n\\nand\\n\\nif\\n\\n.\\n\\nThe reduction from SAT.\\n\\nFirst, we will give the reduction from the NP-complete problem SAT to the recognition problem of G. Then we will show that this reduction is computable in polynomial time and answer preserving. Thus we have proven that the recognition problem of the unification grammar G is NP-hard.\\n\\nThe reduction from SAT to the recognition problem of Gmaps propositional logical formulas onto strings. We assume, without loss of generality, that the indices of the propositional logical variables are in binary representation. This reduction, f, is defined by the following four equations:\\n\\nThe reduction f maps formula\\n\\nonto string\\n\\n,\\n\\nwhere\\n\\n, and v[i]j is a string of the form\\n\\n.\\n\\nThe reduction f is computable in linear time.\\n\\nProofBy induction on the construction of SAT formulas.\\n\\nLet\\n\\nbe a propositional logical formula in conjunctive normalform, and f the reduction stated above. Formula\\n\\nis a satisfiable formula if, and only if, string\\n\\nis in the\\n\\nlanguage generated by G.\\n\\nProofThe proof of this lemma is split in two subproofs. First, we will prove that if\\n\\nis satisfiable, then w is in the language generated by G. Second, we will prove that if\\n\\nis in the language generated by G, then\\n\\nis satisfiable.\\n\\nOnly if:\\n\\nlet\\n\\n(1) if g assigns a truth-value to one occurrence of a variable, then g assigns that truth-value to all occurrences of that variable in the formula. In other words, g is consistent.\\n\\n(2) g assigns truth to the formula. That is, in each clause, g assigns truth to some literal.\\n\\n. This string w is generated by G if, and only if, the string\\n\\nis\\n\\nderived by S. Moreover,\\n\\nif and\\n\\nonly if\\n\\n,\\n\\nhas the\\n\\nfollowing intermediate steps:\\n\\nLet us assume that\\n\\n, only if the assignment g assigns truth to the k-th literal in the i-th clause of\\n\\n. This k-th literal in the i-th clause, is either\\n\\nor\\n\\n.\\n\\nIn the first\\n\\ncase g assigns truth\\n\\n\\n\\nvalue true to variable\\n\\n, in the second case g assigns truth-value false to variable\\n\\n. By induction on the number of substrings wi, we will prove that under the above made assumption S derives\\n\\n.\\n\\nOne substring wm: Let S0 = S derive wm S (\\n\\n), where k depends on the assignment g:\\n\\nThe non-terminal S derives the empty string in one step. Thus the feature structure associated with S is\\n\\n. The feature structure associated with T is the unification of\\n\\nand the feature structure associated with S:\\n\\nwhere\\n\\nif\\n\\n,\\n\\nand\\n\\nif\\n\\n. The feature structure associated with S0 is\\n\\nNone of the unifications fails, and thus S derives wm.\\n\\nMore than one substring wi: Let S0 = S derive wi S (\\n\\n):\\n\\nBy the induction hypothesis, we assume that S derives\\n\\nMoreover, the feature structure associated with S is\\n\\n=\\n\\n,\\n\\nwhere\\n\\nis a feature structure of the form\\n\\nor a unification of such feature structures. The feature structure associated with T is the unification of\\n\\nand the feature structure associated with S:\\n\\nis the unification of\\n\\nand\\n\\n.\\n\\ncontains\\n\\n,\\n\\nand\\n\\nfails. But,\\n\\nfails only if g assigns both truth-value true and truth-value false to variable\\n\\n.\\n\\nHence\\n\\nwould fail only if g would be inconsistent, which g is not. Hence there is a derivation for string\\n\\nif\\n\\nis satisfiable.\\n\\nIf:\\n\\nsuppose that\\n\\n,\\n\\nwhere\\n\\nObviously,\\n\\nif, and only if,\\n\\nProofAn NP-hard lower bound is proven above. An NP upper bound is proven when we can guess a solution, and check that solution in polynomial time. The NP upper bound is proven as follows.\\n\\nGiven a string w and a grammar G, we can guess a sequence of O(|w|) rules that encode the derivation for w. The guessed rules describe a constituent structure tree and a set of formulas. First, we must check that the constituent structure tree described by the rules has yield w. Second, we have to check that the set of formulas describes some feature-graph.\\n\\nOn lower bounds\\n\\nThe previous section shows the complexity of a feature theory does not provide an upper bound for the complexity of a unification grammar that uses this feature theory. The question that arises is whether the complexity of a feature theory provides a lower bound for the complexity of such a unification grammar.\\n\\nIn general, it seems that the complexity of the combination of two problems is at least as hard as the complexity of these two problems in isolation. So one would be tempted to answer the question above in the affirmative. However, if a problem A contains information about solutions for a problem B, and vice versa, then the combination of A and B may have lower complexity than Aand B in isolation. For instance, let problem A be the complement of problem B. Then the combinations `A or B' and `A and B' have the trivial solutions `always answer yes' and `always answer no', respectively.\\n\\nTo be more specific, in the case of unification grammars, there seem to be easy reductions from the unification problem of a feature theory to the recognition problem of arbitrary unification grammars that use this feature theory. In some specific situations, however, these reductions do not exist. Below, we will present some examples of situations in which the feature theory does not provide a lower bound for the recognition problem.\\n\\nThe feature theory does not provide a lower bound if the complexity of the recognition problem of the grammar component provides a lower bound for the complexity of the recognition problem of the unification grammar. Consider for instance the class of grammars that generate a finite language. The combination of a feature theory with a grammar from this class yields a unification grammar that generates a finite language. Obviously, the recognition problem of this unification grammar does not depend on the unification problem of the feature theory. Hence the lower bound complexity of this class of unification grammars is not provided by the complexity of the feature theory.\\n\\nThe feature theory does not provide a lower bound if the unification grammar uses only a fragment of the feature theory. This happens when the unification grammar formalism restricts the unification. For instance, the unification grammar formalism may demand that feature structures are unified at the outermost attributes. This demand implies that the size of the feature structures that appear in the fixed unification grammar is bounded. Consequently, there have to be feature structures in the feature theory that cannot be encoded by the unification grammar. One may object that the obligatory unification at the outermost attribute should be incorporated in the formalization of the feature theory. Thus reducing the complexity of the unification problem of the feature theory. However, there is no predefined way to construct unification grammars from a feature theory and a grammar component. So, there may be many blurred restrictions on the unification. These blurred restrictions are the cause that the formalization of the feature theory may be too expressive and that the unification grammar uses only a fragment of the feature theory.\\n\\nThe two examples show that not in all situations the complexity of the unification problem of the feature theory provides a lower bound for the complexity of the recognition problem of the unification grammar. In some special cases the complexity of the unification grammar may be lower than the complexity of the feature theory. Hence care has to be taken for drawing overhasty conclusions about the lower bound complexity of the unification grammar from the complexity of the feature theory.\\n\\nConclusions\\n\\nIn this paper, we have assessed the complexity results of formalizations that intend to describe feature theories in computational linguistics. These formalizations do not take the constituent structure component of unification grammars into account. As a result, the complexity of the unification problem of feature theories does not provide an upper bound, and need not provide a lower bound, for the complexity of the recognition problem of unification grammars using these theories.\\n\\nThus the complexity results that have been achieved in the formalisms of feature theories are not immediately relevant for unification grammars used in computational linguistics. Complexity analyses will only contribute to computational linguistics if the analyzed formalizations are connected closely with actual unification grammars. Therefore, we argue for formalisms that describe unification grammars as a whole instead of bare feature theories.\\n\\nBibliography\\n\\nFranz Baader, Hans-Jrgen Brckert, Bernhard Nebel, Werner Nutt, and Gert Smolka. On the expressivity of feature logics with negation, functional uncertainty, and sort equations. Journal of Logic, Language and Information, 2(1):1-18, 1993.\\n\\nPatrick Blackburn and Edith Spaan. A modal perspective on the computational complexity of attribute value grammar. Journal of Logic, Language and Information, 2(2):129-169, 1993.\\n\\nMark Johnson. Attribute-Value Logic and the Theory of Grammar, volume 16 of CSLI Lecture Notes. CSLI, Stanford, 1988.\\n\\nRobert T. Kasper and William C. Rounds. The logic of unification in grammar. Linguistics and Philosophy, 13:35-58, 1990.\\n\\nGert Smolka. Feature-constraint logics for unification grammars. Journal of Logic Programming, 12(1):51-87, 1992.\\n\\nLeen Torenvliet and Marten Trautwein. Features that count. Presented at CLIN V (Fifth Computational Linguistics in the Netherlands Meeting), December 1994.\\n\\nFootnotes\\n\\nThis research was supported by the Linguistic Research Foundation, which is funded by the Netherlands organisation for scientific research, NWO\", metadata={'source': '../data/raw/cmplg-xml/9503022.xml'}),\n",
       " Document(page_content=\"Using Decision Trees for Coreference Resolution1\\n\\nThis paper describes  RESOLVE, a system that uses decision trees to learn how to classify coreferent phrases in the domain of business joint ventures. An experiment is presented in which the performance of  RESOLVE is compared to the performance of a manually engineered set of rules for the same task. The results show that decision trees achieve higher performance than the rules in two of three evaluation metrics developed for the coreference task. In addition to achieving better performance than the rules, RESOLVE provides a framework that facilitates the exploration of the types of knowledge that are useful for solving the coreference problem.\\n\\nIntroduction\\n\\nThe goal of an Information Extraction (IE) system is to identify information of interest from a collection of texts. Within a particular text, objects of interest are often referenced in different places and in different ways. One of the many challenges facing an IE system is to determine which references refer to which objects. This problem can be recast as a classification problem:  given two references, do they refer to the same object or different objects.\\n\\nIn an effort to address these problems, a new approach to coreference resolution was begun after the MUC-5 evaluation: a system named RESOLVE was created to build decision trees that can be used to classify pairs of phrases as coreferent or not coreferent. The errors generated by the sentence analyzer were eliminated by using a special tool - the Coreference Marking Interface, or CMI - to extract a set of phrases from the MUC-5 English Joint Venture (EJV) corpus. In order to minimize the difficulties involved with creating and maintaining complex sets of rules, a machine learning approach was adopted, in which a decision tree determines the order and relative weight of different pieces of evidence.\\n\\nDecision Trees vs. Rules\\n\\nAn experiment was conducted to compare the performance of the decision trees generated by  RESOLVE with the performance of manually engineered rules used for coreference classification in the UMass/Hughes MUC-5 IE system. A set of references, along with the coreference links among these, were extracted from a group of texts via  CMI. All possible pairings of references from each text were generated, and these pairings were used to create a set of feature vectors used by  RESOLVE. The pairings that contained coreferent phrases formed positive instances, while those that contained two non-coreferent phrases formed negative instances. RESOLVE was then iteratively trained and tested on different partitions of this set of feature vectors.\\n\\nData\\n\\nThe articles in the EJV corpus describe business joint ventures among two or more entities (companies, governments and/or people). The task definition provided for MUC-5 required IE systems to extract information about the entities involved, the relationships among these entities, the facilities associated with the joint venture, the products or services offered by the joint venture, its capitalization and revenue projections, and a variety of other related information. Since the entities involved in these joint ventures were the main focus of most of these articles, references to entities were much more numerous than references to other types of object classes, e.g., people. Therefore, entity references were selected as the focus of the experiments reported in this paper.\\n\\nCMI is a graphical user interface that permits the user to mark phrases in a text; for each phrase, the user can indicate the object(s) with which the phrase is coreferent and some additional information about the phrase that can be inferred either from the phrase itself or its local context. This additional information is parameterized and can be modified easily for use in different domains. The data used in this experiment was based on a set of phrases extracted using  CMI.\\n\\nAs an example, consider the following sentence, from text 0970 from the MUC-5 EJV corpus:\\n\\nFAMILYMART CO. OF SEIBU SAISON GROUP WILL OPEN A CONVENIENCE STORE IN TAIPEI FRIDAY IN A JOINT VENTURE WITH TAIWAN'S LARGEST CAR DEALER, THE COMPANY SAID WEDNESDAY.\\n\\nThe phrases underlined in this sentence contain relevant information that must be extracted by an IE system. The phrases in boldface refer to entity objects that are important to the MUC-5 task. As an example of the types of information collected about each phrase, consider the first phrase in the sentence:\\n\\n(:string ``FAMILYMART CO.''\\t\\t:slots (ENTITY\\t\\t \\t\\t(name ``FAMILYMART CO.'')\\t\\t \\t\\t(type COMPANY)\\t\\t \\t\\t(relationship JV-PARENT CHILD)))\\n\\nInformation collected about each phrase includes the string itself, the character position of the string in the source text (not shown), the index of the sentence within which the string is found (also not shown), and some slot information that can be inferred from either the string itself or its local context - the same kind of information that was contained in the memory tokens used by the MUC-5 system. In this example, the name of the entity and the fact that it is a company entity can both be inferred from the string itself. The fact that Familymart Company plans to open a store in ``A JOINT VENTURE'' with another entity is considered adequate evidence that the company is the parent of a joint venture (jv-parent); the fact that the sentence contains the pattern ``company-name-1 OF company-name-2'' is evidence that company-name-1, in this case Familymart Co., is a subsidiary (child) of company-name-2, in this case Seibu Saison Group.\\n\\nA second example of output from  CMI can be seen below, where nationality information has been extracted from the reference to the car dealer:\\n\\n(:string ``TAIWAN'S LARGEST CAR DEALER''\\t\\t:slots (ENTITY\\t\\t \\t\\t(type COMPANY)\\t\\t \\t\\t(relationship JV-PARENT)\\t\\t \\t\\t(nationality ``Taiwan (COUNTRY)'')))\\n\\nIn principle, much of the information gathered about a particular string could be found automatically: there are numerous proper name recognizer programs, programs that extract location information, and sentence analyzers that can infer relationship information - any system that exhibited good performance in MUC-5 must be good at inferring such relationships.\\n\\nFor the purposes of our experiment, however, this information was specified by a user via  CMI. The primary motivation for this was to minimize the noise in the data; coreference resolution often occurs at a late processing stage in an IE system, and earlier errors such as incorrect part-of-speech tags, incorrectly delimited sentences and semantic tagging errors can create significant noise for a coreference classifier.\\n\\nCMI was used to mark references to a variety of relevant object types (entity, facility, person and product-or-service) in 50 randomly selected texts. Since references to entity objects were most numerous, this was the object class chosen for the experiment. In the 50 texts, 472 references to a total of 205 entity objects were marked using  CMI.\\n\\nRules used in the MUC-5 System\\n\\nAnother factor influencing the coreference module was the short time allotted to developing and testing this system component. Since coreference resolution was a late stage in processing, upstream components had to be stabilized before serious development could take place on coreference. Several late-stage components were being developed in parallel, so it is difficult to assess the time devoted exclusively to developing the coreference module, but we estimate it was two person-weeks.\\n\\nThe UMass/Hughes MUC-5 IE system used a variety of mechanisms to identify phrases referring to joint ventures (the entity formed by two or more parent entities for some particular business purpose), to identify company names within a phrase (if they exist), and to determine whether one phrase was an alias (an abbreviation or shortened form), as well as the ability to identify trigger families and partitions in the text.\\n\\nOne of the many difficulties in developing the rule set for coreference classification was in ordering the rules. Several different orderings were tested during the development period, and the order shown above was the ordering of the rule set used for final evaluation. This difficulty in rule ordering was one of the motivations behind using a machine learning approach - we wanted to develop a system that could learn how to combine the positive and negative evidence.\\n\\nFeatures Used By RESOLVE\\n\\nA decision tree requires data to be represented by feature vectors, i.e., vectors of attribute/value pairs. For the task of coreference classification, references were paired up, and features were extracted from the pair of references as well as from the individual references themselves. Since this experiment involved a comparison between  RESOLVE and a manually engineered rule set, the features used in this experiment were based on the antecedents of the coreference rules used in the UMass/ Hughes MUC-5 IE system.\\n\\nNAME-i: Does reference i contain a name? Possible values: { YES, NO}.\\n\\nJV-CHILD-i: Does reference i refer to a joint venture child, i.e., a company formed as the result of a tie-up among two or more entities? Possible values: { YES, NO, UNKNOWN}.\\n\\nThe last four features focus on the pair of references.\\n\\nALIAS: Does one reference contain an alias of the other, i.e., does each reference contain a name and is one name a substring of the other name? Possible values: { YES, NO}.\\n\\nCOMMON-NP: Do the references share a common noun phrase? Some references contain non-simple noun phrases, e.g., appositions and relative clauses. This feature compares the simple constituent noun phrases of each reference. Possible values: { YES, NO}.\\n\\nSAME-SENTENCE: Do the references come from the same sentence? RESOLVE does not use CIRCUS output, and thus has no notion of a trigger family as it was used in the MUC-5 system; the  SAME-SENTENCE feature is a very weak attempt to extract this sort of information. Possible values: { YES, NO}.\\n\\nEvaluation Methodology\\n\\nCoreference is a symmetrical and transitive relation that holds among a set of two or more references, e.g., if we know that A is coreferent with B, and B is coreferent with C, then there is an implicit coreference ``link'' between A and C.  Any coreference classification for two references has implications beyond the determination of whether that particular classification was correct or incorrect. For example, if A and B are correctly classified as coreferent, but B and C are incorrectly classified as not coreferent, a system may also incorrectly conclude that A and C are not coreferent. Thus, simply measuring the accuracy of a coreference classifier is inadequate for evaluating how well the classifier performs its task.\\n\\nResults\\n\\nDiscussion\\n\\nWhen we first began applying decision trees to the coreference resolution problem, we were hoping to achieve performance that was comparable to the manually engineered rules we had used in MUC-5. We were greatly encouraged to discover that we could achieve performance that surpassed the performance of the rules from our MUC-5 system in both recall and F-measure scores.\\n\\nAs was noted earlier, the MUC-5 coreference rules were designed to minimize false positives. The effect of this bias can be seen in the higher precision score achieved by the rule set in comparison with both the unpruned and pruned decision trees. The difference in precision scores between the unpruned and pruned versions of the decision trees might be explained by the prevalence of negative instances (74%) in the data set, which may lead to a stronger bias to classify pairs of phrases as not coreferent in the smaller trees.\\n\\nConclusions\\n\\nOne of the original goals of this new approach was to develop a system that achieved good performance in resolving references - performance that was at least as good as the performance achieved using manually engineered rules in our MUC-5 system. However, as we continue to pursue this approach, we find that there is another advantage to using decision trees: they allow us to focus on determining which features work well for resolving references.\\n\\nWe are encouraged by the performance of the decision trees on the coreference resolution problem. The features we have used in the experiment described above are not considered comprehensive by any means. While they have proved sufficient for attaining a certain level of performance, an examination of specific errors made by the trees shows that additional features will be needed to attain higher levels.\\n\\nUltimately, we hope to understand better which features are important for coreference classification, across different objects and different domains. Such an understanding would benefit people involved with IE system development, and should be of interest to people outside the IE community as well. We think that decision trees are an important tool in a systematic study of coreference resolution.\\n\\n0pt\\n\\nBibliography\\n\\nJ. Aberdeen, J. Burger, D. Connolly, S. Roberts, and M. Vilain. MITRE-Bedford ALEMBIC: MUC-4 test results and analysis. In Proceedings of the Fourth Message Understanding Conference (MUC-4), pages 116-123, 1992.\\n\\nD. E. Appelt, J. Bear, J. R. Hobbs, D. Israel, and M. Tyson. SRI International FASTUS system: MUC-4 test results and analysis. In Proceedings of the Fourth Message Understanding Conference (MUC-4), pages 143-147, 1992.\\n\\nD. Ayuso, S. Boisen, H. Fox, H. Gish, R. Ingria, and R. Weischedel. BBN: Description of the PLUM system as used in MUC-4. In Proceedings of the Fourth Message Understanding Conference (MUC-4), pages 169-176, 1992.\\n\\nJ. Burger, M. Vilain, J. Aberdeen, D. Connolly, and L. Hirschman. A model-theoretic coreference scoring scheme. Technical report, The MITRE Corporation, Bedford, MA, 1994.\\n\\nN. Chinchor and B. Sundheim. MUC-5 evaluation metrics. In Proceedings of the Fourth Message Understanding Conference (MUC-5), pages 22-29, 1993.\\n\\nN. Chinchor. MUC-3 evaluation metrics. In Proceedings of the Third Message Understanding Conference (MUC-3), pages 17-24, 1991.\\n\\nN. Chinchor. MUC-4 evaluation metrics. In Proceedings of the Fourth Message Understanding Conference (MUC-4), pages 22-29, 1992.\\n\\nB. J. Grosz, A. K. Joshi, and S. Weinstein. Providing a unified account of definite noun phrases in discourse. In Proceedings of the 21st Annual Meeting of the ACL, pages 44-50, 1983.\\n\\nL. Iwanska, D. Appelt, D. Ayuso, K. Dahlgren, B. Glover Stalls, R. Grishman, G. Krupka, C. Montgomery, and E. Riloff. Computational aspects of discourse in the context of MUC-3. In Proceedings of the Third Message Understanding Conference (MUC-3), pages 256-282, 1992.\\n\\nW. Lehnert, C. Cardie, D. Fisher, E. Riloff, and R. Williams. University of Massachusetts: Description of the CIRCUS system as used for MUC-3. In Proceedings of the Third Message Understanding Conference (MUC-3), pages 223-233, 1991.\\n\\nW. Lehnert, C. Cardie, D. Fisher, J. McCarthy, E. Riloff, and S. Soderland. University of Massachusetts: Description of the CIRCUS system as used for MUC-4. In Proceedings of the Fourth Message Understanding Conference (MUC-4), pages 282-288, 1992.\\n\\nW. Lehnert, J. McCarthy, S. Soderland, E. Riloff, C. Cardie, J. Peterson, F. Feng, C. Dolan, and S. Goldman. University of Massachusetts/Hughes: Description of the CIRCUS system as used for MUC-5. In Proceedings of the Fourth Message Understanding Conference (MUC-5), pages 277-290, 1993.\\n\\nW. Lehnert. Symbolic/subsymbolic sentence analysis: Exploiting the best of two worlds. In J. Barnden and J. Pollack, editors, Advances in Connectionist and Neural Computation Theory, Vol. 1, pages 135-164. Ablex Publishers, Norwood, NJ, 1991.\\n\\nR. H. Merchant. Tipster program overview. In Proceedings of the TIPSTER Text Program (Phase I), pages 1-2, 1993.\\n\\nD. Moldovan, S. Cha, M. Chung, K. Hendrickson, J. Kim, and S. Kowalski. USC: MUC-4 test results and analysis. In Proceedings of the Fourth Message Understanding Conference (MUC-4), pages 164-166, 1992.\\n\\nJ. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA, 1993.\\n\\nC. L. Sidner. Towards a computational theory of definite anaphora comprehension in English discourse. TR 537, M.I.T. Artificial Intelligence Laboratory, 1979.\\n\\nB. M. Sundheim. Overview of the third message understanding evaluation and conference. In Proceedings of the Third Message Understanding Conference (MUC-3), pages 3-16, 1991.\\n\\nB. M. Sundheim. Overview of the fourth message understanding evaluation and conference. In Proceedings of the Fourth Message Understanding Conference (MUC-4), pages 3-21, 1992.\\n\\nB. M. Sundheim. TIPSTER/MUC-5 information extraction system evaluation. In Proceedings of the Fourth Message Understanding Conference (MUC-5), pages 27-44, 1993.\\n\\nC. J. van Rijsbergen.\\n\\nInformation Retrieval.\\n\\nButterworths, London, 1979.\\n\\nC. Weir and R. Fritzson. UNISYS: Description of the CBAS system used for MUC-5. In Proceedings of the Fourth Message Understanding Conference (MUC-5), pages 249-261, 1993.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9505043.xml'}),\n",
       " Document(page_content=\"Best\\n\\n\\n\\nFirst Surface Realization\\n\\nCurrent work in surface realization concentrates on the use of general, abstract algorithms that interpret large, reversible grammars. Only little attention has been paid so far to the many small and simple applications that require coverage of a small sublanguage at different degrees of sophistication. The system TG/2 described in this paper can be smoothly integrated with deep generation processes, it integrates canned text, templates, and context-free rules into a single formalism, it allows for both textual and tabular output, and it can be parameterized according to linguistic preferences. These features are based on suitably restricted production system techniques and on a generic backtracking regime.\\n\\nMotivation can be smoothly integrated with 'deep' generation processes,\\n\\nintegrates canned text, templates, and context-free rules into a single formalism,\\n\\nallows for both textual and tabular output,\\n\\nefficiently reuses generated substrings for additional solutions, and\\n\\ncan be parameterized according to linguistic properties (regarding style, grammar, fine-grained rhetorics etc. ).\\n\\ninvolve a direct mapping to surface forms (canned text),\\n\\nrequire to fill in some missing portion from a surface text (template), or\\n\\ninduce the application of other rules (classical grammar rules)\\n\\nEarly template-based generation methods have correctly been criticized for beeing too inflexible to account adequately for the communicative and rhetorical demands of many applications. On the other hand, templates have been successfully used when these demands could be hard-wired into the rules. In TG/2 the rule writer can choose her degree of abstraction according to the task at hand. She can freely intermix all kinds of rules.\\n\\nThe Generation Interface Language (GIL)\\n\\nAlthough the level of logical form is considered a good candidate for an interface to surface realization, practice shows that notational idosyncrasies can pose severe translation problems. TG/2 has an internal language, GIL, that corresponds to an extended predicate argument structure. GIL is the basis for the precondition test predicates and the selector functions of TGL. Any input to TG/2 is first translated into GIL before being processed. It is of considerable practical benefit to keep the rule basis as independent as possible from external conditions (such as changes to the output specification of the feeding system).\\n\\nThe top level consists of a speech act predicate and arguments for author, addressee and theme (the speechact proper).\\n\\nDiscourse objects can be assigned unique constants (ID) that denote SETs of discourse objects.\\n\\nSMOOD expresses sentence modalities including sentence type, time, a specification of which constituents to topicalize in a German declarative sentence, etc.\\n\\nThe predicate argument structure is reflected by corresponding features; ARGS contains a list of arguments.\\n\\nArguments and, in part, adjuncts are specified for their role, for cardinality, for quantificational force (under CONTENT.QFORCE), and further details such as name strings and natural gender.\\n\\nTemporal adjuncts relate to some context (e.g. tomorrow) or are indexical (e.g. on Wednesday, February 7, 1996). All common combinations in German are covered.\\n\\nThe Template Generation Language (TGL)\\n\\nCategory: The categories can be defined as in a context-free grammar. Correspondingly, categories are used for rule selection (see below). They ensure that a set of TGL rules possesses a context-free backbone.\\n\\nTest: The Lisp code under :TEST is a boolean predicate (usually about properties of the portion of input structure under investigation or about the state of some memory). In the sample rule, an argument is required that fills the patient role.\\n\\nTemplate: Actions under :TEMPLATE  include the selection of other rules (:RULE, :OPTRULE), executing a function (:FUN), or returning an ASCII string as a (partial) result.\\n\\nSide effects: The Lisp code under :SIDE-EFFECTS is a function whose value is ignored. It accounts for non-local dependencies between substructures, such as  updates of a discourse memory. Note that these effects can be traced and undone in the case of backtracking.\\n\\nAn interpreter with generic backtracking\\n\\nThe processing strategy is top-down and depth-first. The set of actions is fired from left to right. Failure of executing some action causes the rule to be backtracked.\\n\\nThe interpreter yields all solutions the grammar can generate. It attempts to generate and output a first solution, producing possible alternatives only on external demand. Any alternative is based on backtracking at least one rule. Backtrack points correspond to conflict sets containing more than one element.\\n\\nBacktracking may turn out to be inefficient if it involves recomputation of previously generated substrings. In TG/2 this effort is reduced considerably because it is only necessary to recompute the part licensed by the newly selected rule. What has been generated before or after it remains constant (modulo some word forms that need to agree with new material) and can thus be reused for subsequent solutions. This is possible due to the design properties of TGL: rules cannot irrevocably influence other parts of the solution. In particular, the context-free backbone implicit in any solution and the restrictions to side effects mentioned above keep the structural effects of TGL rules local.\\n\\nIn the sequel, technical aspects of the backtracking regime are discussed. Let us assume that the interpreter compute a backtrack point. Let us call the sequence of strings generated by previous actions its pre-context, the set of string sequences generated from the elements of the conflict set its ego, and the sequence of strings generated from subsequent actions its post-context. For every ego, the pre- or the post context may be empty.\\n\\nParameterization the element in the conflict set to be processed next, and\\n\\nthe backtrack point to be processed next.\\n\\nThe criteria are defined in terms of rule names, and a criterion is fulfilled if some corresponding rule is successfully applied. We call such a rule c-rule. TG/2 implements a simple strategy that processes those backtrack points first that have conflict sets containing c-rules, and preferrably choses a c-rule from a conflict set. When applied incrementally, this procedure yields all solutions fulfilling (some of) the criteria first.\\n\\nIt would be desirable to see the solution fulfilling most criteria first. However, incremental application enforces decisions to be taken locally for each conflict set. Any c-rule chosen may be the last one in a derivation, whereas chosing a non-c-rule may open up further opportunities of chosing c-rules. These limits are due to a lack of look-ahead information: it is not known in general which decisions will have to be taken until all solutions have been generated. Clearly, sacrificing incrementality is not what should be desired although it may be acceptable for some applications. The drawbacks include a loss of efficiency and run-time. This leaves us with two possible directions that can lead to improved results.\\n\\nAnalyzing dependencies of criteria: The solution fulfilling most criteria is generated first if sets of mutually independent criteria are applied: fulfilling one criterion must not exclude the applicability of another one, unless two criteria correspond to rules of the same conflict set. In this case, they must allow for the the application of the same subset of criteria. If these conditions are met, chosing a c-rule from every conflict set, if possible, will lead to a globally best solution first. There is, however, the practical problem that the conditions on the criteria can only be fulfilled by analyzing, and possibly modifying, the TGL grammar used. This contradicts the idea of having the user specify her preferences independent of TG/2 properties.\\n\\nLearning dependencies of criteria: Missing look-ahead information could be acquired  automatically by exploiting the derivational history of previously generated texts. For every applied rule, the set of c-rules applied later in the current subtree of a derivation is  stored. From this information, we can derive off-line for any set of criteria which c-rules have applied in the corpus and how often each c-rule has applied within a derivation. Computing such information from the context-free backbone of TGL grammars instead would be less effective since it neglects the drastic filtering effects of preconditions. However, checking the grammar this way indicates which c-rules will not appear in some subtree.\\n\\nDuring processing, TG/2 can then judge the global impact of chosing the locally best c-rule and decide to fulfill or violate a criterion. The success of this method depends on how well the derivation under construction fits with the sample data. The more examples the system observes, the more reliable will be its decisions.\\n\\nThe latter approach is in fact independent on how the criteria influence each other. In addition, it can be extended to cope with weighted criteria. A weight is specified by the user (e.g. a feeding system) and expresses the relative importance of the criterion being fulfilled in a solution. TG/2 would give preference to derivations leading to the maximum global weight. The global weight of a solution is the sum of the c-rule weights, each divided by the number of times the c-rule occurs.\\n\\nHowever, different GIL structures may, for a TGL rule, lead to different sets of follow-up c-rules. This causes the decision to be nondeterministic unless the reasons for the difference are learned and applied to the case at hand. We must leave it to future research to identify and apply suitable learning algorithms to solving this problem.\\n\\nConclusion\\n\\nIn this contribution, we have introduced TG/2, a production-rule based surface generator that can be parameterized to generate the best solutions first. The rules are encoded in TGL, a language that allows the definition of canned text items, templates, and context-free rules within the same formalism. TGL rules can, and should, be written with generation in mind, i.e. the goal of reversibility of grammars pursued with many constraint-based approaches has been sacrificed. This is justified because of the limited usefulness of large reversible grammars for generation.\\n\\nTGL is particularly well suited for the description of limited sublanguages  specific to the domains and the tasks at hand. Partial reuse of such descriptions depends on whether the grammar writer keeps general, reusable definitions independent from the specific, non-reusable parts of the grammar. For instance, time and date descriptions encoded for the  COSMA domain can be reused in other TG/2 applications. On the other hand, TGL sublanguage grammars can be developed using existing resources. For instance, suitable fragments of context-free grammars translated into TGL could be augmented by the domain and task specific properties needed. Practical experience must show whether this approach saves effort.\\n\\nThe system is fully implemented in Allegro Common Lisp and runs on different platforms (SUN workstations, PC, Macintosh). Computing the first solution of average-length sentences (10-20 words) takes between one and three seconds on a SUN SS 20. TG/2 is being used in the domain of appointment scheduling within DFKI's COSMA system. In the near future, the system will be used within an NL-based information kiosk, where information about environmental data must be provided in both German and French language, including tabular presentations if measurements of several substances are involved.\\n\\nFootnotes\\n\\nThis work has been supported by a grant from The Federal Ministry for Research and Technology (FKZ ITW 9402). I am grateful to Michael Wein, who implemented the interpreter, and to Jan Alexandersson for influential work on a previous version of the system. Finally, I wish to thank two anonymous reviewers for useful suggestions. All errors contained in this paper are my own. The notion of template is preserved for historical reasons: the predecessor system TG/1 was strictly template-based. In the case at hand, the grammar writer preferred to ensure availability of the substructure by virtue of the test predicate. In fact, it is preterminal rather than terminal elements that are stored in the table in order to account for modified constraints. This can be neglected in the present discussion, but will be taken up again below. Note that this conclusion  does not depend on the processing strategy chosen.\", metadata={'source': '../data/raw/cmplg-xml/9605010.xml'}),\n",
       " Document(page_content=\"Higher-Order Coloured Unification and Natural Language Semantics Introduction\\n\\nHowever, it is also well known that the HOU approach to NL semantics systematically over-generates and that some general theory of the interface between the interpretation process and other sources of linguistic information is needed in order to avoid this.\\n\\nIn their treatment of VP-ellipsis, DSP introduce an informal restriction to avoid over-generation: the Primary Occurrence Restriction (POR). Although this restriction is intuitive and linguistically well-motivated, it does not provide a general theoretical framework for extra-semantic constraints.\\n\\nHigher-Order Unification and NL semantics\\n\\nNote that in (1), unification yields a linguistically valid solution (1d) but also an invalid one: (1e). To remedy this shortcoming, DSP propose an informal restriction, the Primary Occurrence Restriction: Given a labeling of occurrences as either primary or secondary, the POR excludes of the set of linguistically valid solutions, any solution which contains a primary occurrence. Here, a primary occurrence is an occurrence that is   directly associated with a source parallel element. Neither the notion of direct association, nor that of parallelism is given a formal definition; but given an intuitive understanding of these notions, a source parallel element is an element of the source (i.e. antecedent) clause which has a parallel counterpart in the target (i.e. elliptic or anaphoric) clause.\\n\\nTo see how this works, consider example (1) again. In this case,   dan is taken to be a primary occurrence because it represents a source parallel element which is neither anaphoric nor controlled i.e. it is directly associated with a source parallel element. Given this, equation (1c) becomes (2a) with solutions (2b) and (2c) (primary occurrences are underlined). Since (2c) contains a primary occurrence, it is ruled out by the POR and is thus excluded from the set of linguistically valid solutions.\\n\\nAlthough the intuitions underlying the POR are clear, two main objections can be raised. First, the restriction is informal and as such provides no good basis for a mathematical and computational evaluation. As DSP themselves note, a general theory for the POR is called for. Second, their method is a generate-and-test method: all logically valid solutions are generated before those solutions that violate the POR and are linguistically invalid are eliminated. While this is sufficient for a theoretical analysis, for actual computation it would be preferable never to produce these solutions in the first place. In what follows, we present a unification framework which solves both of these problems.\\n\\nHigher-Order Coloured Unification (HOCU) Modeling the Primary Occurrence Restriction HOCU theory\\n\\nSome of the equations in the examples have multiple most general solutions, and indeed this multiplicity corresponds to the possibility of multiple different interpretations of the focus constructions. The role of colours in this is to restrict the logically possible solutions to those that are linguistically sound.\\n\\nLinguistic Applications of the POR\\n\\nFurthermore, we will see that what counts as a primary occurrence differs from one phenomenon to the other (for instance, an occurrence directly associated with focus counts as primary w.r.t focus semantics but not w.r.t to VP-ellipsis interpretation). To account for these differences, some machinery is needed which turns DSP's intuitive idea into a fully-blown theory. Fortunately, the HOCU framework is just this: different colours can be used for different types of primary occurrences and likewise for different types of free variables. In what follows, we show how each phenomenon is dealt with. We then illustrate by an example how their interaction can be accounted for.\\n\\nFocus\\n\\nSecond Occurrence Expressions\\n\\nGiven this proposal and some further assumptions about the semantics of only, the analysis of (7b) involves the following equations:\\n\\nResolution of the first equation then yields two solutions:\\n\\nAn_(j_) = P [P {x. like(x,y) y _e } P(j_) P = x. like(x,m) ]\\n\\nwith the unique well\\n\\n\\n\\ncoloured solution\\n\\nAdverbial quantification\\n\\nConsider the first equation. If An is the semantics shared by target and source clause, then the only possible value for An is\\n\\nwhere both occurrences of the parallel element m have been abstracted over. In contrast, the following solutions for An are incorrect.\\n\\nOnce again, we see that the POR is a necessary restriction: by labeling as primary, all occurrences representing a parallel element, it can be ensured that only the first solution is generated.\\n\\nInteraction of constraints\\n\\nPerhaps the most convincing way of showing the need for a theory of colours (rather than just an informal constraint) is by looking at the interaction of constraints between various phenomena. Consider the following discourse\\n\\nSuch a discourse presents us with a case of interaction between ellipsis and focus thereby raising the question of how DSP' POR for ellipsis should interact with our POR for focus.\\n\\nThe equations resulting from the interpretation of (9b) are:\\n\\nUnder the indicated colour constraints, three solutions are possible:\\n\\nAnother constraint\\n\\nAn additional argument in favour of a general theory of colours lies in the fact that constraints that are distinct from the POR need to be encoded to prevent HOU analyses from over-generating. In this section, we present one such constraint (the so-called   weak-crossover constraint) and show how it can be implemented within the HOCU framework.\\n\\nIn essence, the main function of the POR is to ensure that some occurrence occuring in an equation appears as a bound variable in the term assigned by substitution to the free variable occurring in this equation. However, there are cases where the dual constraint must be enforced: a term occurrence appearing in an equation must appear unchanged in the term assigned by substitution to the free variable occurring in this equation. The following example illustrates this.\\n\\nwhere the presence of the pronoun hei gives rise to two possible FSVs\\n\\nthus allowing two different readings: the corefential or strict reading\\n\\nand the bound\\n\\n\\n\\nvariable or sloppy reading.\\n\\nIn contrast, if the quantified/wh/focused NP does not precede and c-command the pronoun, as in\\n\\nthere is no ambiguity and the pronoun can only give rise to a co-referential interpretation. For instance, given (11) only one reading arises\\n\\nwith only one well\\n\\n\\n\\ncoloured solution\\n\\nCalculating Coloured Unifiers\\n\\nExample\\n\\nTo fortify our intuition on calculating higher-order coloured unifiers let us reconsider examples (10) and (11) with the equations\\n\\ncan be decomposed to the equations\\n\\nConclusion\\n\\nHigher-Order Unification has been shown to be a powerful tool for constructing the interpretation of NL. In this paper, we have argued that Higher-Order Coloured Unification allows a precise specification of the interface between semantic interpretation and other sources of linguistic information, thus preventing over-generation. We have substantiated this claim by specifying the linguistic, extra-semantic constraints regulating the interpretation of VP-ellipsis, focus, SOEs, adverbial quantification and pronouns whose antecedent is a focused NP.\\n\\nOther phenomena for which the HOCU approach seems particularly promising are phenomena in which the semantic interpretation process is obviously constrained by the other sources of linguistic information. In particular, it would be interesting to see whether coloured unification can appropriately model the complex interaction of constraints governing the interpretation and acceptability of gapping on the one hand, and sloppy/strict ambiguity on the other.\\n\\nAnother interesting research direction would be the development and implementation of a monostratal grammar for anaphors whose interpretation are determined by coloured unification. Colours are tags which decorate a semantic representation thereby constraining the unification process; on the other hand, there are also the reflex of linguistic, non-semantic (e.g. syntactic or prosodic) information. A full grammar implementation would make this connection more precise.\\n\\nAcknowledgements\\n\\nThe work reported in this paper was funded by the Deutsche Forschungsgemeinschaft (DFG) in Sonderforschungsbereich SFB-378, Project C2 (LISA).\\n\\nBibliography\\n\\nChristine Bartels.\\n\\n1995.\\n\\nSecond occurrence test.\\n\\nMs.\\n\\nNoam Chomsky. 1976. Conditions on rules in grammar. Linguistic Analysis, 2(4):303-351.\\n\\nMary Dalrymple, Stuart Shieber, and Fernando Pereira. 1991. Ellipsis and higher-order-unification. Linguistics and Philosophy, 14:399-452.\\n\\nDaniel Dougherty. 1993. Higher-order unification using combinators. Theoretical Computer Science B, 114(2):273-298.\\n\\nGilles Dowek. 1992. Third order matching is decidable. In Proc. LICS92, pages 2-10. IEEE Computer Society Press.\\n\\nClaire Gardent and Michael Kohlhase. 1996. Focus and higher-order unification. In Proc. COLING96 forthcoming.\\n\\nClaire Gardent, Michael Kohlhase, and Noor van Leusen. 1996. Corrections and higher-order unification. CLAUS report 77, University of Saarland.\\n\\nClaire Gardent. 1996. Anaphores parallles et techniques de rsolution. Langages.\\n\\nUlrich Hustadt. 1991. A complete transformation system for polymorphic higher-order unification. Technical Report MPI-I-91-228, MPI Informatik, Saarbrcken, Germany.\\n\\nRay S. Jackendoff. 1972. Semantic Interpretation in Generative Grammar. The MIT Press.\\n\\nAngelika Kratzer. 1991. The representation of focus. In Arnim van Stechow and Dieter Wunderlich, editors, Semantik: Ein internationales Handbuch der zeitgenoessischen Forschung. Berlin: Walter de Gruyter.\\n\\nManfred Pinkal. 1995. Radical underspecification. In The 10th Amsterdam Colloquium.\\n\\nChristian Prehofer. 1994. Decidable higher-order unification problems. In Alan Bundy, editor, Proc. CADE94, LNAI, pages 635-649, Nancy, France.\\n\\nSteve G. Pulman. 1995. Higher-order unification and the interpretation of focus. Paper submitted for publication.\\n\\nKai von Fintel. 1995. A minimal theory of adverbial quantification. Unpublished draft Ms. MIT, Cambridge, March.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9605004.xml'}),\n",
       " Document(page_content=\"Automatic Extraction of\\n\\nTagset Mappings from Parallel\\n\\n\\n\\nAnnotated Corpora\\n\\nSeveral research projects around the world are building grammatically analysed corpora; that is, collections of text annotated with part-of-speech wordtags and syntax trees. However, projects have used quite different wordtagging and parsing schemes. Developers of corpora adhere to a variety of competing models or theories of grammar and parsing, with the effect of restricting the accessibility of their respective corpora, and the potential for collation into a single fully parsed corpus. In view of this heterogeneity, we have begun to investigate and develop methods of automatically mapping between the annotation schemes of the most widely known corpora, thus assessing their differences and improving their reusability. Annotating a single corpus with the different schemes allows for comparisons and will provide a rich test-bed for automatic parsers. Collation of all the included corpora into a single large annotated corpus will provide a more detailed language model to be developed for tasks such as speech and handwriting recognition. This paper focuses on methods of developing mappings between tagsets and, in particular, the method of automatic extraction of mappings from corpora tagged with more than one annotation scheme.\\n\\nIntroduction\\n\\nWe are developing a multi-tagged corpus and a multi-treebank, a single text-set annotated with all the tagging and parsing schemes we include in the mappings. The text-set is the Spoken English Corpus (SEC); which is already annotated with two syntax schemes. However, the main deliverable to the computational linguistics research community is not the SEC-based multi-treebank, but its associated suite of mappings - this can be used to combine currently-incompatible syntactic training sets into a large unified corpus. Our development of the mapping algorithms aims to distinguish notational from substantive differences in the annotation schemes, and we will be able to evaluate tagging schemes in terms of how well they fit standard statistical language models such as n-pos (Markov) models.\\n\\nRelated Research\\n\\nObtaining Resources\\n\\nThe following table summarises the resources we have for the six main corpora we have included in the project so far. The first column reveals if we have the corpus itself: we have all but the International Corpus of English. The next column indicates if we have the software that was used in the automated part of annotating of the corpus. The next column shows for which corpora we have documentation giving formal descriptions of the annotation guidelines. The last column marks the London-Lund and Brown corpus with a `1' to indicate that we have a small sample of corpus annotated using both these schemes. The `2' marker in this column indicates the Parallel Annotated Corpus that we are building at the moment by adding the International Ccorpus English (GB) annotation to the Spoken English Corpus.\\n\\nDeriving Tagset Mappings\\n\\nWhen we began the AMALGAM project we anticipated that the following process would be the normal way that an annotation scheme was included in our `mapping suite':\\n\\nThe uneven spread of resources means that alternative mapping strategies must be adopted when including each annotation scheme (see table 1). As we have the software used to tag and parse the International Corpus of English we can incorporate that into the mapping. Good formal descriptions of the annotation scheme (such as for LOB) can be used to craft some rules by hand. Where the documentation is sparse rules can be extracted from the corpus itself.\\n\\nWe require a method to evaluate the alternative mapping strategies: A simple evaluation can be accomplished by tagging the untagged SEC using one annotation scheme (the evaluation scheme) by the tried and tested method of automatic annotation followed by hand correction. To test a mapping strategy one would apply the mapping from the evaluation scheme tags to produce those of the SEC. The success of the mapping would be determined by measuring the difference between this annotation and the original SEC (CLAWS tagged) annotation produced by Lancaster.\\n\\nThe Parallel Annotated Corpus (PAC) created when a (non-CLAWS) evaluation scheme is used to tag the Spoken English Corpus in this way itself provides further possibilities for developing mapping strategies. The PAC may intrinsically encode mapping information that would not be uncovered from other mapping strategies. Extracting a mapping from a PAC is computationally trivial; the difficulty is annotating an existing corpus with a new scheme. However, PACs already exist for pairs of annotation scheme and this provides an easy way to extract mapping information. This is particularly true when the annotation scheme of one corpus is replaced by another. Initially this would be done using the automatic annotator of the new scheme followed by hand-correction by linguistic experts. However, the addition of the new scheme to part of the corpus creates a PAC from which a mapping can be derived. The mapping could be used to update the performance of the automatic annotator. A process of refinement of the automatic annotator by feedback derived from the mapping would be established.\\n\\nThis paper focuses on deriving tagset mappings from PACs as we are currently in the phase of our project where we are concentrating on parts-of-speech annotation. However, we anticipate that the method will be even more useful when dealing with mapping between parse trees.\\n\\nExtraction of Correspondences from Parallel Annotated Corpora\\n\\nTo use the method of deriving mappings from PACs it is inevitable that some traditional tagging is required to build the parallel corpus. As an example of the process of extracting correspondences from PACs we shall use the example of the SEC-ICE corpus. As a PAC does not exist for this pair of tagsets we had to build our own. As we aimed to produce the multitagged corpus out of the texts of the Spoken English Corpus it made sense to annotate the Spoken English Corpus with ICE tags.\\n\\nWe employed an experienced annotator of corpora, Tim Willis, to learn the ICE annotation scheme and apply it to the Spoken English Corpus by editing the automatic output of the Nijmegen parser which was designed to annotate ICE-GB material. For the moment we are concentrating on deriving mappings between tagged annotation but it was felt more cost effective to parse and tag the Spoken English Corpus now as our project will eventually include parse mappings.\\n\\nTo align texts annotated by two schemes we used a method we term island driven alignment. The `islands' are the singletons found to be present in the output of both schemes. The position of these items can easily be aligned. The words next to the islands can be examined in turn. Often they will match and so can be aligned immediately, but occasionally the next pair of items will not match. Attempting to split enclitics, recombine split compounds or altering initial letter case may match some pairs but others such as the semicolon problem mentioned earlier will require pattern matching of the surrounding text. Occasionally an item in one of the annotations will match with no item in the other; the extra end of sentence markers in ICE texts being a good example. When this happens it can only be discovered after aligning the items on either side of it with neighbouring items in the other annotated output. The first few lines of the Spoken English Corpus when aligned with the ICE tags of the same text are shown figure 1, above. The first two columns are the words and CLAWS tags from the tagged SEC and the remaining column contains the corresponding ICE tags.\\n\\nThe Spoken English Corpus contains the short header: (In Perspective)(Rosemary Hill). The process by which ICE was annotated excluded headers such as this (they will be tagged by hand). As the header is not included in the ICE annotation of the text there is nothing to align it to.\\n\\nEach pairing of tags can now be counted and a list of correspondences made for each individual tag to show the probabilities of each pair. For instance the London-Lund/Brown PAC produced the list of London-Lund correspondences for the interrogative wh-determiner tag, WDT, in Brown shown in figure 2.\\n\\nLessons for the EAGLES Initiative\\n\\nAs many participants at the workshop will know, EAGLES is a European initiative to devise a set of common standards for Natural Language Processing technology across the range of European Union working languages. Of particular relevance to our research are the standards proposals for morphosyntactic wordclasses; a lengthy draft proposal (over 200 pages) has recently been made available to ELSNET nodes and a number of other centres of expertise for comment. The proposals aim to standardise a set of wordclasses to be applied to Danish, Dutch, English, French, German, Greek, Italian, Portuguese, and Spanish; once (or if) agreed, the standards may later be extended to cover other languages (e.g. Swedish, Finnish, Norwegian, Gaelic, Welsh, Basque, ...)  Even among the current EU main languages, there is considerable diversity in morphosyntax, so the EAGLES group are to be congratulated for achieving a compromise which on the face of it is largely uncontentious. EAGLES recommends several levels of refinement or delicacy in wordclasses, so that specific applications and/or language models are free to select an appropriate level of tagset granularity. For example, NOUN is a broad (level 1) category, a general class which all language models must recognise; within this, there is a level 2 subdivision into proper nouns and common nouns, which will apply to many but not all applications etc. Many other possible wordclass distinctions are captured by features, e.g.\\n\\nOur earlier example of parallel CLAWS/ICE tagging of the Spoken English Corpus illustrates the fuzziness in the distinction between proper noun and common noun. In general, a singular proper noun is NP in LOB and CLAWS, but N(prop,sing) in ICE. However, notice that Perspective, the second word in the corpus, is tagged NP. This may have been because the word begins with a capital, and the tagging system uses this as a deciding criterion (however, note that the previous word, In, escapes this default NP tagging because English text requires the first word of every sentence to start with a capital, so the tagging system by default converts this to lower case and tags according to dictionary-lookup). To a linguist, this analysis of Perspective may intuitively be an `error; however there are no definitions within the EAGLES guidelines which rule out such counter-intuitive computationally-motivated criteria.\\n\\nA second example of disagreement over the proper and common noun boundary is the analysis of Reverend Sun Myung Moon - in ICE this is tagged as a proper-noun sequence (or rather, a compound proper-noun single lexical item), but in LOB/CLAWS, one fuzzy boundary between common and proper nouns is recognised - the area of titular nouns tagged NPT (for example, Reverend can start with upper or lower case in much the same context, so NPT avoids conflicting taggings depending on the case of the initial letter). Further examples abound in the parallel corpus; generally the problem arises from differences in the handling of upper-case initial letter.\\n\\nOur conclusion for the EAGLES Initiative is that the morphosyntactic category proposals must be followed up with detailed definitions, preferably including computable criteria. In the specific example of nouns, there must be clear standards on handling of word-initial case. (This is relevant not only to English). Otherwise the `standards' will be interpreted differently (and incompatibly) in different tagged corpora. We had hoped that the EAGLES tagset might constitute an `interlingua' for translating between existing tagsets. However, we have already had to conclude that our task of automatic tagset-mapping extraction can never achieve perfect accuracy, as both source and target training data are noisy; using a fuzzy-edged tagset as an interlingua could only worsen matters.\\n\\nBibliography\\n\\nEric Atwell. 1982. LOB corpus tagging project: Manual post-edit handbook. Departments of Computer Studies and Linguistics, Lancaster University.\\n\\nEric Atwell. 1983. Constituent likelihood grammar. ICAME Journal, 7. 34-67.\\n\\nEric Atwell. 1987a. Constituent likelihood grammar. In Roger Garside, Geoffrey Sampson and Geoffrey Leech (eds. ), The computational analysis of English: A corpus-based approach. 57-65.\\n\\nEric Atwell. 1987b. A parsing expert system which learns from corpus analysis. In Willem Meijs (ed. ), Corpus Linguistics and Beyond: Proceedings of the ICAME 7[th] International Conference. Amsterdam: Rodopi. 227-235.\\n\\nEric Atwell. 1988a. Grammatical analysis of English by statistical pattern recognition. In Josef Kittler (ed. ), Pattern recognition: Proceedings of the 4[th] International Conference, Cambridge. Berlin: Springer-Verlag. 626-635.\\n\\nEric Atwell. 1988b. Transforming a parsed corpus into a corpus parser. In Merja Kyto, Ossi Ihalainen, and Matti Risanen (eds. ), Corpus Linguistics, hard and soft: Proceedings of the ICAME 8th International Conference. Amsterdam: Rodopi. 61-70.\\n\\nEric Atwell. 1990. Measuring grammaticality of machine-readable text. In Werner Bahner, Joachim Schildt, and Dieter Viehweger (eds. ), Proceedings of the Fourteenth International Congress of Linguists, III. Berlin: Akademie-Verlag. 2275-2277.\\n\\nEric Atwell. 1992. Overview of grammar acquisition research. In Henry Thompson (ed. ), Workshop on sublanguage grammar and lexicon acquisition for speech and language: Proceedings. 65-70.\\n\\nEric Atwell. 1993. Corpus-based statistical modelling of English grammar. In Souter and Atwell (eds. ), Corpus-based computational linguistics. Amsterdam: Rodopi. 195-215.\\n\\nEric Atwell, Geoffrey Leech, and Roger Garside. 1984. Analysis of the LOB corpus: Progress and prospects. In Jan Aarts and Willem Meijs (eds. ), Corpus linguistics: Proceedings of the ICAME 4[th] International Conference on the Use of Computer Corpora in English Language Research. Amsterdam: Rodopi.\\n\\nEric Atwell and Nikos Drakos. 1987. Pattern recognition applied to the acquisition of a grammatical classification system from unrestricted English text. In Bente Maegaard (ed. ), Proceedings of the Third Conference of the European Chapter of the Association for Computational Linguistics. New Jersey, USA. 56-63\\n\\nEric Atwell, Clive Souter and, Tim O'Donoghue. 1991. Training Parsers with Parsed Corpora:  Report 91.2. School of Computer Studies, University of Leeds, UK.\\n\\nEric Atwell, John Hughes, and Clive Souter. 1994a. A unified multicorpus for training syntactic constraint models. In Lindsay Evett and Tony Rose (eds. ), Proceedings of AISB Workshop on Computational Linguistics for Speech and Handwriting Recognition. Leeds University, UK.\\n\\nEric Atwell, John Hughes, and Clive Souter. 1994b. AMALGAM: Automatic mapping among lexico-grammatical annotation models. In Judith Klavans and Philip Resnik (eds. ), Proceedings of the balancing act - combining symbolic and statistical approaches to language, Workshop in Conjunction with the 32nd Annual Meeting of the Association for Computational Linguistics. New Mexico State University, Las Cruces, New Mexico, USA.\\n\\nHenk Barkema. 1993. The TOSCA Analysis Environment for ICE. TOSCA, University of Nijmegen, The Netherlands.\\n\\nEzra Black, Roger Garside, and Geoffrey Leech (eds.). 1991. Statistically driven computer grammars of English: The IBM-Lancaster approach. Rodopi.\\n\\nEric Brill. 1991. A simple rule-based part of speech tagger. Technical report: Department of Computer Science, University of Pennsylvania.\\n\\nEric Brill, David Magerman, Mitchell Marcus, and Beatrice Santorini. 1992. Deducing linguistic structure from the statistics of large corpora. In Proceedings of the AAAI-92 Workshop on Statistically-Based NLP Techniques. San Jose, California, USA.\\n\\nEric Brill and Mitchel Marcus. 1992. Tagging an unfamiliar text with minimal human supervision. In Robert Goldman (ed. ), Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, AAAI Press.\\n\\nTed Briscoe and Nick Waegner. 1992. Robust stochastic parsing using the Inside-Outside Algorithm. In Proceedings of the AAAI-92 Workshop on Statistically-Based NLP Techniques. San Jose, California, USA.\\n\\nPeter Brown, John Cocke, Stephen DellaPietra, Vincent DellaPietra, Frederik Jelinek, John Laffety, Robert Mercer, Paul Roosin. 1990. A statistical approach to machine translation. Computational Linguistics, 16. 29-85.\\n\\nPeter Brown, Stephen DellaPietra, Vincent DellaPietra, John Laffety, Robert Mercer. 1992. Analysis, statistical transfer, and synthesis in machine translation. in Fourth International Conference on Theoretical and Methodological Issues in Machine Translation. Montreal. 83-100.\\n\\nLou Burnard. 1991. What is the TEI? In D. Greenstein (ed. ), Modelling historical data. Goettingen: St. Katharinen.\\n\\nGlenn Carroll and Eugene Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. In Proceedings of the AAAI-92 Workshop on Statistically-Based NLP Techniques. San Jose, California, USA. 1-13.\\n\\nS.-C. Chen, J.-S. Chang, J.-N. Wang, and K.-Y. Su. 1991. ArchTran: A corpus-based statistics-oriented English-Chinese machine translation system. In Proceedings of Machine Translation Summit III. Washington, D.C.  33-40.\\n\\nGeorge Demetriou and Eric Atwell. 1993. Machine-learnable, non-compositional semantics for domain independent speech or text recognition. In Proceedings of 2[nd] Hellenic-European Conference on Mathematics and Informatics (HERMIS). Athens University of Economics and Business,  Greece.\\n\\nMats Eeg\\n\\n\\n\\nOlofsson.\\n\\n1991.\\n\\nWord\\n\\n\\n\\nclass tagging\\n\\n\\n\\nsome computational tools.\\n\\nGteborgs Universitet Institutionen fr\\n\\nSprkvetenskaplig Databehandling.\\n\\nRobin Fawcett and Michael Perkins. 1980. Child language transcripts 6-12. (With a preface, in 4 volumes). Department of Behavioural and Communication Studies, Polytechnic of Wales.\\n\\nW.N. Francis and H. Kucera. 1979. Manual of information to accompany a standard corpus of present-day edited American English, for use with digital computers (corrected and revised edition). Department of Linguistics, Brown University, Providence, Rhode Island.\\n\\nWilliam Gale, Kennethe Church, and David Yarowsky. 1992. Using bilingual materials to develop word sense disambiguation methods. In Fourth International Conference on Theoretical and Methodological Issues in Machine Translation. Montreal. 101-112.\\n\\nSidney Greenbaum. 1993. The tagset for the International Corpus of English. In Clive Souter and Eric Atwell (eds. ), Corpus-based Computational Linguistics. Amsterdam: Rodopi.\\n\\nHans van Halteren and Theo van den Heuvel. 1990. Linguistic Exploitation of Syntactic Databases. Amsterdam: Rodopi.\\n\\nJohn Hughes and Eric Atwell. 1993. Acquiring and evaluating a classification of words. In Simon Lucas (ed. ), IEE Grammatical Inference Colloquium. University of Essex, Colchester, UK.\\n\\nJohn Hughes and Eric Atwell. 1994 The automated evaluation of inferred word classifications. In Tony Cohn (ed. ), The 11[th] European Conference on Artificial Intelligence. RAI Congress Centre, Amsterdam, The Netherlands.\\n\\nStig Johansson, Eric Atwell,  Roger Garside, and  Geoffrey Leech. 1986. The tagged LOB corpus: Users' manual. The Norwegian Centre for the Humanities, Bergen.\\n\\nUwe Jost and Eric Atwell. 1994. Capturing long-distance syntactic constraints in a bigram model. In  Lindsay Evett and Tony Rose (eds. ), Proceedings of AISB Workshop on Computational Linguistics for Speech and Handwriting Recognition. Leeds University, UK.\\n\\nFranoise Keulen. 1986. The Dutch Computer Corpus Pilot Project. In Jan Aarts and Willem Meijs (eds. ), Corpus Linguistics II, Amsterdam: Rodopi. 127-163.\\n\\nGerry Knowles. 1993. From text to waveform: Converting the Lancaster/IBM Spoken English Corpus into a speech database. In Clive Souter and Eric Atwell (eds. ), Corpus-Based Computational Linguistics. Amsterdam: Rodopi. 47-58.\\n\\nK. Lari and S. J. Young. 1990. The estimation of stochastic context-free grammars using the Inside-Outside Algorithm. Computer Speech and Language, 4. 35-56.\\n\\nGeoffrey Leech, Roger Garside, and Eric Atwell. 1983. The automatic grammatical tagging of the LOB Corpus. ICAME Journal, 7:13-33.\\n\\nGeoffrey Leech and Roger Garside. 1991. Running a grammar factory: The production of syntactically-annotated corpora or `treebanks'. In Stig Johannsson and Anna-Brita Strenstrm (eds. ), English Computer Corpora. Berlin: Mouton de Gruyter. 15-32.\\n\\nD. Magerman and M. Marcus. 1991. Pearl: A probabilistic chart parser. In Proceedings of the 2[nd] International Workshop on Parsing Technologies. Cancun, Mexico. 193-199.\\n\\nMitchel Marcus and B. Santorini. 1992. Building very large natural language corpora:  The Penn treebank. In N. Ostler (ed. ), Proceedings of the 1992 Pisa Symposium on European Textual Corpora.\\n\\nGeoffrey Sampson, Robin Haigh, and Eric Atwell. 1989. Natural language analysis by stochastic optimisation: A progress report on project April. Journal of Experimental and Theoretical Artificial Intelligence, 1. 271-287.\\n\\nClive Souter. 1989. A short handbook to the Polytechnic of Wales corpus. Bergen: Norwegian Computing Centre for the Humanities, Bergen University.\\n\\nClive Souter. 1993. Towards a standard format for parsed corpora. In J. Aarts, P. de Haan and N. Oostdijk (eds. ), English Language Corpora: Design, Analysis and Exploitation. Amsterdam: Rodopi. 197-214.\\n\\nClive Souter and Tim O'Donoghue. 1991. Probabilistic parsing in the communal project. In Stig Johansson and Anna-Brita Stenstrom (eds. ), English Computer Corpora, Selected Papers and Research Guide. Berlin: Mouton de Gruyter. 33-48.\\n\\nClive Souter and Eric Atwell. 1992. A richly annotated corpus for probabilistic parsing. In In Proceeding of the AAAI-92 Workshop on Statistically-Based NLP Techniques. San Jose, California, USA.\\n\\nJan Svartvik (ed.). 1990. The London-Lund corpus of spoken English: Description and Research. Lund University Press, Lund, Sweden.\\n\\nL.J. Taylor and G. Knowles. 1988. Manual of information to accompany the SEC corpus. Technical report, Unit for Computer Research on the English Language, University of Lancaster, UK.\\n\\nDekai Wu and Xuanyin Xia. 1994. Learning an English-Chinese lexicon from a parallel corpus. In AMTA-94, Association for Machine Translation in the Americas. Columbia, Maryland, USA.\", metadata={'source': '../data/raw/cmplg-xml/9506006.xml'}),\n",
       " Document(page_content=\"TYPED FEATURE STRUCTURES AS DESCRIPTIONS\\n\\nA description is an entity that can be interpreted as true or false of an object, and using feature structures as descriptions accrues several computational benefits. In this paper, I create an explicit interpretation of a typed feature structure used as a description, define the notion of a satisfiable feature structure, and create a simple and effective algorithm to decide if a feature structure is satisfiable.\\n\\nINTRODUCTION\\n\\nA FEATURE STRUCTURE SEMANTICS\\n\\nA signature provides the symbols from which to construct typed feature structures, and an interpretation gives those symbols meaning. [ Definition\\n\\nis a signature iff\\n\\n[\\n\\nis a sextuple\\n\\n,\\n\\nis a set,\\n\\nis a partial order,\\n\\nis a set,\\n\\nis a partial function from the Cartesian product of\\n\\nand\\n\\nto\\n\\n,\\n\\nand\\n\\nfor\\n\\neach\\n\\n,\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nis defined and\\n\\nthen\\n\\nis defined, and\\n\\n. ] ] ] ] Henceforth, I tacitly work with a signature\\n\\n.\\n\\nI call members of\\n\\nstates,\\n\\nmembers of\\n\\ntypes,\\n\\nsubsumption, members of\\n\\nspecies,\\n\\nmembers of\\n\\nattributes, and\\n\\nappropriateness.\\n\\n[\\n\\nDefinition\\n\\nI is an interpretation iff [ I is a triple\\n\\n,\\n\\nU is a set,\\n\\nS is a total function from U to\\n\\nA is a total function from\\n\\nto the set of partial functions from U to U,\\n\\nfor\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nis defined\\n\\nthen\\n\\nis defined, and\\n\\n,\\n\\nand\\n\\n]\\n\\n]\\n\\nfor\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nis defined\\n\\nthen\\n\\nis defined. ] ] ] ] Suppose that I is an interpretation\\n\\n. I call each member of U an object in I.\\n\\nEach type denotes a set of objects in I. The denotations of the species partition U, and S assigns each object in I the unique species whose denotation contains the object: object u is in the denotation of species\\n\\niff\\n\\n. Subsumption encodes a relationship between the denotations of species and types: object u is in the denotation of type\\n\\niff\\n\\n.\\n\\nSo,\\n\\nif\\n\\nthen the denotation of type\\n\\ncontains\\n\\nthe denotation of type\\n\\n.\\n\\nEach attribute denotes a partial function from the objects in I to the objects in I, and A assigns each attribute the partial function it denotes. Appropriateness encodes a relationship between the denotations of species and attributes: if\\n\\nis defined then the denotation of attribute\\n\\nacts upon each object in the denotation of species\\n\\nto yield an object in the denotation of type\\n\\n,\\n\\nbut if\\n\\nis undefined then the denotation of attribute\\n\\nacts upon no object in the denotation of species\\n\\n.\\n\\nSo, if\\n\\nis defined then the denotation of attribute\\n\\nacts upon each object in the denotation of type\\n\\nto yield an object in the denotation of type\\n\\n.\\n\\nI call a finite sequence of attributes a path, and write\\n\\nfor the\\n\\nset of paths.\\n\\n[\\n\\nDefinition\\n\\nP is the path interpretation function under I iff [ I is an interpretation\\n\\n,\\n\\nP is a total function from\\n\\nto the set of partial functions from U to U, and\\n\\nfor\\n\\neach\\n\\n,\\n\\n[\\n\\nis the functional composition\\n\\nof\\n\\n. ] ] ] I write PI for the path interpretation function under I.\\n\\n[\\n\\nDefinition\\n\\nF is a feature structure iff [ F is a quadruple\\n\\n,\\n\\nQ is a finite subset of\\n\\n,\\n\\n,\\n\\nis a finite partial function from the Cartesian product of Q and\\n\\nto Q,\\n\\nis a total function from Q to\\n\\n,\\n\\nand\\n\\nfor\\n\\neach\\n\\n,\\n\\n[\\n\\nfor\\n\\nsome\\n\\n,\\n\\nruns to q' in F, ] ] where\\n\\nruns to q' in F iff [\\n\\n,\\n\\n,\\n\\nand\\n\\nfor\\n\\nsome\\n\\n,\\n\\n[\\n\\nq=q0,\\n\\nfor\\n\\neach i[n,\\n\\n[\\n\\nis defined, and\\n\\n,\\n\\nand\\n\\n]\\n\\n,\\n\\nand output\\n\\nalphabet\\n\\n.\\n\\n[\\n\\nDefinition\\n\\nF is true of u under I iff [ F is a feature structure\\n\\n,\\n\\nI is an interpretation\\n\\n,\\n\\nu is an object in I, and\\n\\nfor\\n\\neach\\n\\n,\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nruns to q' in F, and\\n\\nruns to q' in Fthen\\n\\nis defined,\\n\\nis defined,\\n\\n,\\n\\nand\\n\\n.\\n\\n]\\n\\n]\\n\\n]\\n\\n]\\n\\n[\\n\\nDefinition\\n\\nF is a satisfiable feature structure iff [ F is a feature structure, and\\n\\nfor some interpretation I and some object u in I, F is true of u under I. ] ]\\n\\nMORPHS\\n\\nThe abundance of interpretations seems to preclude an effective algorithm to decide if a feature structure is satisfiable. However, I insert morphs between feature structures and objects to yield an interpretation free characterisation of a satisfiable feature structure.\\n\\n[\\n\\nDefinition\\n\\nM is a semi\\n\\n\\n\\nmorph iff\\n\\n[\\n\\nM is a triple\\n\\n,\\n\\nis a nonempty subset of\\n\\n,\\n\\nis an equivalence relation over\\n\\n,\\n\\nfor\\n\\neach\\n\\n,\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nand\\n\\nthen\\n\\n,\\n\\n]\\n\\n]\\n\\nis a total function from\\n\\nto\\n\\n,\\n\\nfor\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\nif\\n\\nthen\\n\\n,\\n\\nand\\n\\n]\\n\\nfor\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nthen\\n\\n,\\n\\nis defined, and\\n\\n.\\n\\n]\\n\\n]\\n\\n]\\n\\n]\\n\\n[\\n\\nDefinition\\n\\nM is a morph iff [ M is a semi-morph\\n\\n,\\n\\nand\\n\\nfor\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nand\\n\\nis defined\\n\\nthen\\n\\n,\\n\\nand\\n\\noutput alphabet\\n\\n.\\n\\n[\\n\\nDefinition\\n\\nM abstracts u under I iff [ M is a morph\\n\\n,\\n\\nI is an interpretation\\n\\n,\\n\\nu is an object in I,\\n\\nfor\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\niff\\n\\nis defined,\\n\\nis defined, and\\n\\n,\\n\\nand\\n\\n]\\n\\n]\\n\\nfor\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\niff\\n\\nis defined, and\\n\\n.\\n\\n]\\n\\n]\\n\\n]\\n\\n]\\n\\n[\\n\\nProposition\\n\\nFor each interpretation I and each object u in I, [ some unique morph abstracts u under I. ] ] I thus write of the abstraction of u under I.\\n\\n[\\n\\nDefinition\\n\\nu is a standard object iff [ u is a quadruple\\n\\n,\\n\\nis a morph, and\\n\\nis an equivalence class under\\n\\n.\\n\\n]\\n\\n]\\n\\nI write\\n\\nfor the set of standard objects, write\\n\\nfor the\\n\\ntotal function from\\n\\nto\\n\\n,\\n\\nwhere\\n\\n[\\n\\nfor\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\niff\\n\\nfor\\n\\nsome\\n\\n,\\n\\n,\\n\\n]\\n\\n]\\n\\n]\\n\\nand write\\n\\nfor the total function from\\n\\nto the set of partial functions from\\n\\nto\\n\\n,\\n\\nwhere\\n\\n[\\n\\nfor\\n\\neach\\n\\n,\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nis defined, and\\n\\n,\\n\\nand\\n\\nfor\\n\\nsome\\n\\n,\\n\\n.\\n\\n]\\n\\n]\\n\\n]\\n\\n[\\n\\nLemma\\n\\nis an interpretation.\\n\\n]\\n\\nI write\\n\\nfor\\n\\n.\\n\\n[\\n\\nLemma\\n\\nFor\\n\\neach\\n\\n,\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nis defined, and\\n\\n,\\n\\nand\\n\\nfor\\n\\nsome\\n\\n,\\n\\n. ] ] ] [ By induction on the length of\\n\\n.\\n\\n]\\n\\n[\\n\\nLemma\\n\\nFor\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nis the equivalence class of the empty path under\\n\\nthen\\n\\nthe abstraction of\\n\\nunder\\n\\nis\\n\\n.\\n\\n]\\n\\n]\\n\\n]\\n\\n[\\n\\nProposition\\n\\nFor each morph M, [ for some interpretation I and some object u in I, [ M is the abstraction of u under I. ] ] ]\\n\\n[\\n\\nDefinition\\n\\nF approximates M iff [ F is a feature structure\\n\\n,\\n\\nM is a morph\\n\\n,\\n\\nand\\n\\nfor\\n\\neach\\n\\n,\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nruns to q' in F, and\\n\\nruns to q' in Fthen\\n\\n,\\n\\nand\\n\\nFor each interpretation I, each object u in I and each feature structure F, [ [ F is true of u under Iiff F approximates the abstraction of u under I. ] ] ]\\n\\n[\\n\\nTheorem\\n\\nRESOLVED FEATURE STRUCTURES\\n\\n[\\n\\nDefinition\\n\\nR is a resolved feature structure iff [ R is a feature structure\\n\\n,\\n\\nis a total function from Q to\\n\\n,\\n\\nand\\n\\nfor\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nis defined\\n\\nthen\\n\\nis defined, and\\n\\n.\\n\\n[\\n\\nDefinition\\n\\nR is a resolvant of F iff [ R is a resolved feature structure\\n\\n,\\n\\nF is a feature structure\\n\\n,\\n\\nand\\n\\nfor\\n\\neach\\n\\n,\\n\\n.\\n\\n]\\n\\n]\\n\\n[\\n\\nProposition\\n\\nFor each interpretation I, each object u in I and each feature structure F, [ [ F is true of u under Iiff some resolvant of F is true of u under I. ] ] ]\\n\\n[\\n\\nDefinition\\n\\nis rational iff\\n\\nfor\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nis defined\\n\\nthen\\n\\nfor\\n\\nsome\\n\\n,\\n\\n.\\n\\n]\\n\\n]\\n\\n]\\n\\n[\\n\\nProposition\\n\\nIf\\n\\nis rational then for each resolved feature structure R, R is satisfiable. ] [ Suppose that\\n\\nand\\n\\nis a bijection\\n\\nfrom ordinal\\n\\nto\\n\\n.\\n\\nLet\\n\\n[\\n\\n,\\n\\nlet\\n\\n[\\n\\n(_n(),)\\n\\n(_n(),)()\\n\\n,\\n\\nis a\\n\\nsemi\\n\\n\\n\\nmorph. Let\\n\\n[\\n\\n,\\n\\n,\\n\\nand\\n\\n.\\n\\n]\\n\\n[\\n\\nTheorem\\n\\nIf\\n\\nA SATISFIABILITY ALGORITHM\\n\\n[\\n\\nDefinition\\n\\nis computable iff\\n\\n[\\n\\n,\\n\\nand\\n\\nare countable,\\n\\nis finite,\\n\\nfor some effective function\\n\\n,\\n\\n[\\n\\nfor\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nthen\\n\\n`true'\\n\\notherwise\\n\\n`false', and\\n\\n]\\n\\n]\\n\\n]\\n\\nfor some effective function\\n\\n,\\n\\n[\\n\\nfor\\n\\neach\\n\\nand\\n\\neach\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nis defined\\n\\nthen\\n\\notherwise\\n\\n`undefined'.\\n\\n]\\n\\n]\\n\\n]\\n\\n]\\n\\n]\\n\\n[\\n\\nProposition\\n\\nIf\\n\\nis computable\\n\\nthen\\n\\nfor\\n\\nsome effective function\\n\\n,\\n\\n[\\n\\nfor\\n\\neach feature structure F,\\n\\n[\\n\\na list of the resolvants of F. ] ] ] [ Since\\n\\nis computable, for some\\n\\neffective function\\n\\n,\\n\\n[\\n\\nfor\\n\\neach finite\\n\\n,\\n\\n[\\n\\na list of the total functions from Q to\\n\\n,\\n\\n]\\n\\n]\\n\\nfor some effective function\\n\\n,\\n\\n[\\n\\nfor\\n\\neach finite set Q,\\n\\neach finite partial function\\n\\nfrom the Cartesian product of Q and\\n\\nto Q, and\\n\\neach total function\\n\\nfrom Q to\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nfor\\n\\neach\\n\\nin the domain of\\n\\n,\\n\\n[\\n\\nis defined, and\\n\\n]\\n\\nthen\\n\\n`true'\\n\\notherwise\\n\\n`false', ] ] ] and for some effective function\\n\\n,\\n\\n[\\n\\nfor\\n\\neach finite set Q,\\n\\neach total function\\n\\nfrom Q to\\n\\nand\\n\\neach total function\\n\\nfrom Q to\\n\\n,\\n\\n[\\n\\n[\\n\\nif\\n\\nfor\\n\\neach\\n\\n,\\n\\nthen\\n\\n`true'\\n\\notherwise\\n\\n`false'.\\n\\n]\\n\\n]\\n\\n]\\n\\nConstruct\\n\\nas follows:\\n\\n[\\n\\nfor\\n\\neach feature structure\\n\\n,\\n\\n[\\n\\nset\\n\\nand\\n\\nwhile\\n\\nis not empty\\n\\n[do\\n\\n=\\n\\nset\\n\\n[\\n\\nif\\n\\n`true',\\n\\n`true', and\\n\\nthen\\n\\nset\\n\\n]\\n\\n]\\n\\n[\\n\\nif\\n\\nthen\\n\\noutput\\n\\nis an effective algorithm, and [ for each feature structure F, [\\n\\na list of the resolvants of F. ] ] ]\\n\\n[\\n\\nTheorem\\n\\nIf\\n\\nis rational and computable\\n\\nthen\\n\\nfor\\n\\nsome effective function\\n\\n, [ for each feature structure F, [ [ if F is satisfiable then\\n\\n`true\\n\\notherwise\\n\\nto test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output function. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output functions. The Troll unifier is closed on these representations. Thus, though\\n\\nis computationally expensive,\\n\\nTroll uses\\n\\nonly during compilation, never during run time.\\n\\nReferences\\n\\n[ Robert Carpenter The logic of typed feature structures. Cambridge tracts in theoretical computer science 32. Cambridge University Press, Cambridge, England. 1992.\\n\\nDale Gerdemann. Troll: type resolution system, user's guide. Sonderforschungsbereich 340 technical report. Eberhard-Karls-Universitt, Tbingen, Germany. Forthcoming.\\n\\nDale Gerdemann and Paul John King. The correct and efficient implementation of appropriateness specifications for typed feature structures. In these proceedings.\\n\\nThilo Gtz. A normal form for typed feature structures. Master's thesis. Eberhard-Karls-Universitt, Tbingen, Germany. 1993.\\n\\nPaul John King. A logical formalism for head-driven phrase structure grammar. Doctoral thesis. The University of Manchester, Manchester, England. 1989.\\n\\nE. F. Moore. `Gedanken experiments on sequential machines'. In Automata Studies. Princeton University Press, Princeton, New Jersey, USA. 1956.\\n\\nMichael Andrew Moshier. Extensions to unification grammar for the description of programming languages. Doctoral thesis. The University of Michigan, Ann Arbor, Michigan, USA. 1988. ]\", metadata={'source': '../data/raw/cmplg-xml/9408003.xml'}),\n",
       " Document(page_content=\"A Study of the Context(s) in a Specific Type of Texts: Car Accident Reports\\n\\nThis paper addresses the issue of defining context, and more specifically the different contexts needed for understanding a particular type of texts. The corpus chosen is homogeneous and allows us to determine characteristic properties of the texts from which certain inferences can be drawn by the reader. These characteristic properties come from the real world domain (K-context), the type of events the texts describe (F-context) and the genre of the texts (E-context). Together, these three contexts provide elements for the resolution of anaphoric expressions and for several types of disambiguation. We show in particular that the argumentation aspect of these texts is an essential part of the context and explains some of the inferences that can be drawn.\\n\\nIntroduction\\n\\nWe must first emphasize that our objectives in this paper are not general considerations about context or the theory of context, but that they are guided by the particular goals of a specific project. The work reported here is the result of a study done within a larger project on the ``Semantics of Natural Languages'', viewed from the fields of Artificial Intelligence and Computational Linguistics, in which we are treating a corpus of real texts. The corpus consists of a number of insurance claim reports for car accidents. This corpus offers a number of advantages, the main one being its unity, with respect to a) the domain involved (which is relatively circumscribed) and b) the conditions of enunciation (which are almost always the same). Indeed, the texts are written after an accident and their quasi-institutional nature imposes a number of constraints on their production and, in a symmetric way, on their interpretation. This allows us to focus on the characteristic properties of the type of the texts, and to draw inferences from them.\\n\\nWe can then better define the notion of context and answer the following questions, which are those we address more specifically in this paper:\\n\\nThe Contexts of the Corpus\\n\\nAll the texts are written in similar circumstances and belong to a culturally well-defined genre which both writer and reader are perfectly aware of when writing or reading one of them. An insurance claim report for a car accident is not a newspaper story, nor a letter to a friend narrating the accident, but an almost institutionalized document which obeys certains constraints concerning its content as well as its form. As emphasized by Rastier in [], genre constraints play a major role in the interpretation process. In order to describe these constraints, we distinguish three types of contexts, which we refer to as:\\n\\nK-context (knowledge context): This is what is usually meant by ``context'' and it refers to the extra-linguistic knowledge for the particular domain of the texts. In this case, our K-context is that of the world of road traffic, and it concerns vehicles, vehicle motions, traffic rules, the usual behavior of drivers and pedestrians, as well as their expectations, and also some elements of ``naive'' geometry.\\n\\nF-context (factual context): For this type of texts, there are two factual constraints bearing on the content of the text: -  the text is a narration in which an accident takes place; -  the text involves at least two participants, generally two vehicles, one of which is the author's.\\n\\nE-context (context of the enunciation): The conditions of enunciation (the discourse constraints) for those texts are: -  an imposed format: Before writing their text, the authors must check some boxes on the insurance claim form. These boxes are labelled with ready-made expressions and phrases which influence the vocabulary that is then used in the running text (terms such as vehicle, or the use of the labels ``A'' and ``B'' for each of the protagonists). On the insurance claim form, the space in which the author can write this text is pre-defined. It is rather small and thus the text must be rather short, at most one paragraph. -  known addressee: The recipient of the text is known, it is the insurance company. Thus the argumentative aspect of the text is also known in advance: the authors of the texts will try to lessen their responsibility.\\n\\nWe will show how these three context types are used by the authors to write their texts and symmetrically by the readers to interpret them. These two symmetrical tasks are both composed of a factual and an argumentative part, which coincide with the two goals we can define for an NLP approach to both understanding and processing these texts. One of these goals is the factual analysis which is necessary to recreate the event:  ``What happened? What real world events concerning the motions of these vehicles or the scene geometry actually occurred?''. The other is an argumentative analysis which takes into account the nature and intent of the text in order to uncover the argumentative devices used by the writer.\\n\\nAlthough we can thus clearly delineate these two goals, they may not be so neatly separated in practice and we find that, in real text processing, they are intertwined in such a way that solving one level of analysis requires elements from the other. Similarly, although the division of context into three context types is extremely useful and revealing, we sometimes have to invoke more than one of them to treat some aspects of our texts. Nevertheless, we organize the remainder of this paper along the way these three context types can be used to describe the processes necessary for both the production and interpretation of our texts.\\n\\nThe three types of Contexts\\n\\nWhereas E-context and F-context are particular to this type of texts, K-context is independent of the type of texts:  the knowledge involved will be the same whether the text is an accident report or a newspaper article and any text dealing with the road domain will invoke the same K-context.\\n\\nK\\n\\n\\n\\ncontext\\n\\nK-context can be taken as the domain ontology for these texts. It is already well-established that domain ontology is necessary for natural language understanding and that purely linguistic knowledge is not sufficient. In our texts, K-context includes knowledge about the Rules of the Road, about driving, and the typical knowledge about the objects evoked and their relations with each other, hierarchical or otherwise. This knowledge is shared by the writer (W) and the reader (R) and it is used by the reader in a number of specific tasks, in particular to solve anaphors and to make inferences.\\n\\nRules of the Road\\n\\n(1) I was at a stop-sign with two cars in front of me turning to the right towards Mours. While the first car was going through this stop-sign I performed my check to the left and started but I hit the second car which hadn't yet gone through the stop-sign  (A1)\\n\\nFirst, the knowledge that ``Drivers must stop at a stop-sign.'' is useful to infer that the interpretation for I was at a stop-sign here is clearly I was stopped at a stop-sign, although the verb to be in the past tense (an imperfect in the original text) followed by a locative adverbial does not necessarily entail that its subject is stopped; indeed, one can say I was on the highway without implying that one was not moving.\\n\\nSecond, the reader uses the rule ``If X is at a stop-sign and X has switched his right blinker on, X will turn right'' to interpret with two cars in front of me turning to the right as ``The two cars were stopped and had switched their right blinker on'' rather than ``The two cars were turning right'', an interpretation which the present participle would allow. This interpretation requires some reasoning which is very difficult to automatize in a computer program. Indeed, since W says that the two vehicles were turning right while in fact they were stopped, the expression turning right proves to be only an intention:  the cars were stopped but they were ``going to turn right''. If this intention had remained in the mind of the driver, it would have been opaque to W, therefore an element expressing it must have been visible and perceived by W:  the right blinker.\\n\\nThird, the sentence I performed my check to the left is correctly understood by R because W and R both know what actions are expected at a stop-sign through the stereotyped knowledge or script [] for ``X being at a stop-sign'':  it implies both that X will not stay at this stop-sign and that, in order to go through it, X must check the road for safety, i.e. check that no vehicle is coming.\\n\\n(2) Heavy traffic on Bd Sebastopol. I was driving between two lanes of stopped cars when one of the cars on my left opened its right front door. To avoid it, I swerved, which made me touch vehicle B with the rear of my motorcycle, which made me fall. Because of the heavy traffic that day, we only exchanged our insurance companies and names which explains why the report is only signed by me. (A3)\\n\\nThe second relative pronoun which refers not to the fact of having exchanged insurance information and names, but to the fact that this is all that happened, while much more would have been expected from the accident script:  i.e. get the report form, check the appropriate boxes, make the drawing, write the report, and sign it. The word only (juste in the French text) signals that something is missing, that there is a deviation with respect to the situation expected from the K-context. The relative pronoun thus refers to what is actually missing from the reported scene, but which would have been implied otherwise.\\n\\nTypicality\\n\\nTypicality can first be considered at the lexical level. Indeed, any domain ontology induces certain preferences for the interpretation of lexical items which may have several meanings, and there are examples in our texts where lexical typicality helps resolve polysemy (which may not even be noticed by a human reader, but would cause problems in machine processing).\\n\\n(3) Je roulais sur la partie droite de la chausse quand un vhicule arrivant en face dans le virage a t compltement dport. Serrant  droite au maximum, je n'ai pu viter la voiture qui arrivait  grande vitesse.\\n\\nI was driving on the right hand side of the road when a vehicle arriving in front of me in the curve was completely thrown off course. Keeping as close as possible to the right, I wasn't able to avoid the car which was coming with great speed. (A8)\\n\\n(4) b. I was entering (vehicle A) the lane into a gas station. The pump being out of order, I was backing up to leave when I hit vehicle B which had also entered the same lane to get gas. (A17)\\n\\nb. Having left my car to call a mechanic, I came back to find it with the right back door bashed in with no note left by the guilty party. (A2)\\n\\n(5) b. Vehicle B seemd to want to let vehicle A go through, (B42)\\n\\nb. Being momentarily stopped in the right lane on Boulevard des Italiens, I had switched my blinker on; I was at a stop and getting ready to change lanes. Vehicle B coming from my left squeezed too close to me and damaged the whole left front side. (A7)\\n\\n(6)\\n\\nb.\\n\\nmy bumper (A11)\\n\\nb. Je roulais (I was driving, literally I was rolling)\\n\\nSpatial and physical knowledge\\n\\nF\\n\\n\\n\\ncontext\\n\\nParameter ``Accident''\\n\\n(8) We were in Saint-Ouen, I was surprised by the person who braked in front of me, not being able to change lanes, and the road being wet, I couldn't stop completely in time (A15)\\n\\nParameter ``Participants''\\n\\nThe fact that car accidents usually involve two participants, most often two vehicles, is used to infer the identity of some entities in the texts or to establish coreference between two entities.\\n\\nA specific naming convention in French insurance claim reports for the vehicles involved in an accident is that claimants must refer to their own vehicle and to their opponent's as A or B. This convention arises from the pre-defined format of the claim report, on which each of the two drivers must first answer a set of questions by checking boxes in one of two columns A or B, thus choosing for themselves one of the roles. They usually then continue to use these labels for themselves and their opponent in the free-running text, but not necessarily in the whole text. Indeed the authors often mix first person expressions with these neutral third person labels.\\n\\nFrom R's point of view, resolving the problem of reference, i.e. identifying the different vehicles involved in the accident and their drivers, often requires knowledge of F-context, particularly of the ``Participants'' parameter. We now look at several examples where knowing that there are two vehicles involved in the scene of the accident helps resolve anaphors.\\n\\n(9) I was going down towards Bellefontaine. The road is a narrow, windy road, lined with trees. In a curve with not much visibility, we collided. (B33)\\n\\n(10) Vehicle A waiting and stopped at the Pont de Levallois lights. Vehicle B arrived and hit my left side mirror with its right side mirror. (C10)\\n\\n(11) Coming back home, the driver of vehicle B in front of me lost control of his vehicle because of sudden icing. In turn I couldn't control my vehicle which after 20 meters crashed into Mrs. Louvet's vehicle. I want to stress that there was no ice anywhere else and we were many vehicles skidding on this street. Nothing could allow foreseeing such icing conditions. (B28)The relative clause in the second sentence refers to an accident between Mrs. Louvet's vehicle and W's. Nothing, except F-context, warrants linking vehicle B and Mrs. Louvet's, and at first glance, there could appear to be three vehicles in this scene. However, because of the F-context ``Participants'' parameter, and because the text says that the accident takes place between W's vehicle and vehicle B, the reader can deduce that Mrs. Louvet is the driver of vehicle B and that there are only two vehicles involved.\\n\\nE\\n\\n\\n\\ncontext\\n\\nWhen setting to the task of writing such a report, W knows the ``Short'' parameter, the constraint that only about a paragraph (in a pre-defined area on the form) may be used to relate the accident. At the same time, W must not forget any important information whose absence would prevent R from reconstructing the correct factual content P, and he must thus be both exhaustive and concise.\\n\\nOn the other hand, the authors know that these few lines, meant for their insurance company, may contribute to the final decision about their share of legal and financial liability. They know that the intended readers, the insurance agents, must pass a judgement on their behavior and will determine their share of responsibility in the accident. Necessarily then, the authors of those reports attempt to present their case in the best possible light in order to minimize their responsability.\\n\\nIn short, W is faced with what we call ``W's selection problem'', namely the constraint on the choice of information to give in order to satisfy the three goals:  to be exhaustive, to be concise, and to lessen their responsability.\\n\\nThese goals are not contradictory and actually become intermingled. While describing the scene, W is trying to argue for his innocence. Thus, the choices of which elements are mentionned in the report can thus reveal an argumentative strategy while helping reconstruct the factual content of the text. The authors can choose to adopt a ``legal'' framework for describing the setting of the accident. They then try to speak the same language as the insurance agent and give exactly the information that the latter expects. They choose precise words to refer accurately to the objects which are present in this space seen from a legal point of view and which are directly relevant to traffic, e.g. road signs, markings, or referring to events happening in this domain also from a legal point of view, such as turn his blinker on, coming from the left, etc.\\n\\n(12) I was driving in my vehicle A in the right lane reserved for vehicles going straight ahead. Vehicle B was driving in the left lane reserved for vehicles going left (ground markings with arrows). It cut back in on my vehicle. (A12-markings)\\n\\nThe presence of ``ground markings with arrows'' implies a particular behavior on the part of vehicles driving in that lane:  they must go left. The last sentence exactly contradicts this expected behavior and is intended to prove that vehicle B made a mistake in ``cutting back in'' on W's vehicle since it was not respecting the ground markings. Of course, W has taken care to mention that his own vehicle was in the correct lane.\\n\\n(13) Vehicle B coming from my left, I find myself at the intersection,  at moderate speed, about 40 km/h, when vehicle B hits my vehicle, and denies me the right-of-way from the right. (A4)\\n\\n(14) The driver of vehicle B passing me on the right caught my right front bumper and dragged me towards the movable wall on the Genenevilliers Bridge, which I violently smashed into. According to the witness who was following me, the driver of vehicle B was doing a slalom between the cars. After hitting me, he ran away and couldn't be caught up with by the above-mentioned witness. (A11)\\n\\n(15) I was surprised by the person who braked in front of me, not being able to change lanes, and the road being wet (A15)\\n\\n(16) I didn't expect that a driver would wish to pass me for there weren't two lanes marked on the portion of the road where I was stopped. (A5)\\n\\n(17) b. on impact, and because of the slippery pavement, my vehicle skids, and hits the metal railing around a tree, whence a second front impact. (A4)\\n\\nb. and the road being wet, I wasn't able to stop completely in time. (A15)\\n\\n(18) I wasn't able to avoid the car which was coming with great speed. (A8)\\n\\nTo summarize, the examination of the terms used and of the elements which the authors choose to mention in their texts reveals that there are two strategies they can follow to argue their case in the most persuasive way:\\n\\n(19) I was driving at about 45 km/h in a small one-way street where cars were parked on both sides. Popping suddenly on my right coming out of a private building garage, Mrs. Glorieux's vehicle was at a very short distance from my vehicle; passage being impossible: surprised, I immediately put the brakes on but the impact was unavoidable. (A14)\\n\\nInferences\\n\\nIn this section, we give evidence for the role which the knowledge of the argumentative function of such texts plays in the process of interpretation, particularly in the reconstruction of its factual content.\\n\\nLexical Ambiguity\\n\\n(20) Je roulais sur la partie droite de la chausse (A8)\\n\\nI was driving on the right-hand side / straight portion of the road\\n\\nHere, even though the whole text can also be interpreted with the straight meaning, the right interpretation is more plausible. However, only an argumentative type of reasoning can lead R to prefer the latter. Since it is well-known to both R and W that in France one drives on the right, by specifying that he was driving on the right side of the road, W violates the Maxim of Quantity (i.e. not to say anything superfluous) and therefore must be taken as intending to convey some other information. In this case, it must be to assert that his behavior was conforming to the Code de la route (the ``Rules of the Road''), which is indeed a pertinent fact to mention. Here, informational redundancy by itself carries some information which allows inference. We can thus formulate the rule that:  ``In case of ambiguity, R should prefer the interpretation from which correct behavior on W's part can be inferred''.\\n\\nTime Reference Ambiguity\\n\\n(21) Being momentarily stopped in the right lane on Boulevard des Italiens, I had switched my blinker on;  I was at a stop and getting ready to change lanes. (A7)\\n\\nThe pluperfect implies that the process being talked about is perceived with another past event as a point of reference, which may not yet have been mentioned. Here, two different referential situations can be envisaged, with two different consequences:\\n\\nArguments of the ``Maxims'' type must then be used. R cannot assume that too much information is present in the text. For the blinker to be switched on before stopping would not be relevant since the accident occurred after that of stopping, when W started again. On the other hand, the fact that W did switch his blinker on before starting again is very relevant from an argumentative point of view, since it means ``W behaved in the right way and did what was required''. Therefore, by the rule proposed above, the first interpretation is chosen and R may conclude that W had his left blinker on.\\n\\nAction or Intention?\\n\\nSometimes, the problem for R is to determine whether an action presented as an intended future event has remained at a purely intentional level or whether actions have already been taken to reach it. For instance, when the intended action belongs to a script with sequential steps, the question arises whether some of the preparatory actions belonging to the script have already been accomplished.\\n\\nIn the inchoative to be about to interpretation, the action of ``switching the blinker on'' is an event independent of ``changing lanes''; in the agentive to prepare for interpretation, that same action corresponds to one of the preparatory acts. But more crucially, in the agentive interpretation, W may already have started changing lanes and then probably would be at fault, while in the inchoative reading, W would still be stopped and would be innocent.\\n\\n(22) Wanting to pass a hauler with its right blinker on, the latter turned left, forcing me to steer left to avoid it. The car skidded on the wet pavement and struck a sidewalk then a fence straight ahead. The truck driver had indeed switched on his left blinker, but the trailer was inverting the signal to the right. Not having touched me, the driver declared himself unconcerned by the situation and refused to draw a report. Having left my car to call a mechanic, I came back to find it with the right back door bashed in with no note left by the guilty party. (A2)\\n\\n(23) I was stopped at the intersection wishing to take the road on which the intense traffic is going one-way in two lanes; as the last vehicle of the flow was coming, I wanted to enter the second lane, leaving the first one free for it. The moment I started, I heard the shock in the back; I wasn't expecting a driver would wish to pass me for there weren't two lanes marked on the portion of the road where I was stopped. (A5)\\n\\nArgumentation\\n\\n\\n\\nBased Inference\\n\\nSince W does not mention checking to the right but only checking to the left, it means that W did not intend to turn left. If W was going to turn left, to mention checking to the right would be pertinent information for the insurance company since it would show that W had done everything required in such circumstances. In fact, it is the non-homogeneity of this discourse which suggests that W did turn left:  either no check should be mentioned or if only one is mentioned, then R may infer that the other one was not required. In another line of argument, checking to the left is also mentioned by W in order to explain that, since he was looking to the left (and not straight ahead), he could not have seen that the other car had not turned right.\\n\\nConclusion\\n\\nIn this paper, we address the general issue of ``how to define context'' and we have chosen an experimental rather than a theoretical approach to this question. By selecting real occurring texts, instead of texts written to illustrate particular phenomena, and an homogeneous corpus of texts written in similar circumstances, we were able to focus on the characteristic properties of this text type and thus to better define the notion of context.\\n\\nWe have tried to show the importance of situational, cultural and textual presuppositions from the point of view of both the writer and the reader. As this work constitutes a first step in the study of natural language semantics in the context of an NLP project, the approach adopted here is an attempt to automate the process of understanding these texts and deriving inferences from them. Some of the crucial issues in NLP are precisely how to define and describe the different types of knowledge involved in the processes of writing and reading texts, and how to establish rules which mimic the reasoning involved in these activities.\\n\\nHere, we take advantage of the specificity of the texts- the authors narrate events leading to a car accident while trying to lessen their responsability- to circumscribe the type of knowledge required and to give some rules of interpretation, valid for this type of text, in this type of context.\\n\\nWe have determined three types of context:  K-context, the non-linguistic knowledge required for this domain, F-context, the more specific context of the events being narrated, and E-context, the discourse context for this text type. The interest of the corpus we have chosen lies in the fact that it contains texts which involve the same three types of contexts:  K-context because they all deal with road traffic, F-context because they all deal with car accidents and E-context because they are all insurance claim reports.\\n\\nWe have shown the importance of E-context, in particular the crucial role played by the argumentation which the writer is known to be pursuing and which allows the reader to make a number of inferences. These inferences then help him clarify the text and choose between competing interpretations.\\n\\nIt would be interesting to analyze the two corresponding texts, by two opponents reporting the same accident, in order to establish which part of the information is objectively factual and shared by both texts, and which part of the information is argumentatively biased, thus better distinguishing the subjective part of the two discourses. The omission of information, which we mentioned as one of the argumentative devices on the part of W and as a basis for inference on the part of R, would then become an even more important factor in the analysis. Very few such pairs of texts are available, but in the continuation of this project, we may try to do some further work based on those we have.\\n\\nBibliography\\n\\nJon Barwise. On the circonstancial relation between meaning and content. The situation in logic. CSLI n.17, pp.59-76. 1988.\\n\\nOswald Ducrot. Dire et ne pas dire. Principes de smantique linguistique. Hermann, Paris. 1972.\\n\\nEstival, Dominique    Franoise Gayral. Contexte et Infrence. T.A.L. vol.35. p.19-36. 1994.\\n\\nFranoise Gayral. Smantique du langage naturel et profondeur variable : une premire approche. Thse de l'universit Paris-Nord, LIPN, Villetaneuse. 1992.\\n\\nFranoise Gayral, Philippe Grandemange, Daniel Kayser  Franois Levy. Interprtation des constats d'accidents: reprsenter le rel et le potentiel. T.A.L. vol.35. p.65-82. 1994.\\n\\nH.P. Grice. Logic and conversation Syntax and Semantics. Academic Press, New York. pp.41-58. 1975.\\n\\nRapport d'activit du groupe de travail inter-PRC (IA et CHM). Universit de Nancy. 1990.\\n\\nCatherine Kerbrat-Orecchioni. L'nonciation de la subjectivit dans le langage. A. Colin,  Paris. 1986.\\n\\nJames Pustejovsky. Type Coercion and Selection. Proceedings of WCCFL VIII, Vancouver. 1989.\\n\\nFranois Rastier, Marc Cavazza  Anne Abeill. Smantique pour l'analyse: De la linguistique l'informatique. Paris: Masson. Collection Sciences Cognitives. 1994.\\n\\nR.C. Schank  R.P. Abelson. Scripts, Plans, Goals and Understanding. Lawrence Erlbaum Associates, Hillsdale, New Jersey. 1977.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9506013.xml'}),\n",
       " Document(page_content='Focus and Higher\\n\\n\\n\\nOrder Unification\\n\\nPulman has shown that Higher-Order Unification (HOU) can be used to model the interpretation of focus. In this paper, we extend the unification-based approach to cases which are often seen as a test-bed for focus theory: utterances with multiple focus operators and second occurrence expressions. We then show that the resulting analysis favourably compares with two prominent theories of focus (namely, Rooth\\'s Alternative Semantics and Krifka\\'s Structured Meanings theory) in that it correctly generates interpretations which these alternative theories cannot yield. Finally, we discuss the formal properties of the approach and argue that even though HOU need not terminate, for the class of unification-problems dealt with in this paper, HOU avoids this shortcoming and is in fact computationally tractable.\\n\\nIntroduction\\n\\nFocus theory\\n\\nFocus is a much debated notion. In this paper, we assume a simplified version of Jackendoff\\'s definition: a focus is the semantic value of a prosodically prominent element. We take the identification of prosodically prominent elements as given.\\n\\nIt is also usually agreed that certain linguistic elements   associate with focus in that the meaning of the utterance containing these elements varies depending on the choice of focus. For instance in (2a-b), the focus operator only associates with focus so that the difference in focus between (2a) and (2b) induces a difference in meaning between the two utterances: in a world where Jon introduced Paul to Mary and Sarah, and no other introduction takes place, (2a) is necessarily false whilst (2b) is true.\\n\\nTo model this ``association-with-focus\" phenomenon, the semantics of associating-elements (e.g. focus operators, quantificational adverbs) is made contingent on the FSV which itself, varies with the choice of focus. The following example illustrates this. Suppose that the meaning of only is determined by the following rule:\\n\\nwhere NP\\', VP\\' represent the meaning of NP and VP respectively, and FSV stands for the focus semantic value of the VP. As we have seen above, the FSV of (1a) is (1b), hence by the above semantic for   only, the semantics of (1a) is:\\n\\nIntuitively, the only property of the form like-ing y that holds of Jon is the property of like-ing Mary.\\n\\nThe basic analysis\\n\\nAs mentioned before, this yields a focus semantic value which is in essence Rooth\\'s Alternative Set.\\n\\nTo illustrate the workings of our approach, we now run through a simple example. Consider (1a). To determine the meaning of only likes MARY, the FSV of the VP must be known. Hence the following equation must be solved:\\n\\nBy HOU, the value of Gd is then:\\n\\nAnd by definition (3.1), the FSV is:\\n\\nAssuming the semantic of only given above, the semantic representation of (1a) is then:\\n\\nIn short, we obtain a reading similar to that of Rooth, the difference being in the way the FSV is determined: by HOU in our approach, by means of a semantic definition in Rooth\\'s.\\n\\nLinguistic applications\\n\\nIn this section, we show that the HOU approach favourably compares with Rooth\\'s and Krifka\\'s analysis in that it correctly generates interpretations which these two theories fail to yield. As we shall see, the main reason for this is that the HOU approach makes minimal assumptions about the role syntax plays in determining the FSV. In particular, it relies neither on the use of Quantifier Raising, nor on the assumption of a rule-to-rule definition of the FSV. In this way, it avoids some of the pitfalls these theories encounter.\\n\\nWe begin by a brief summary of Rooth\\'s and Krifka\\'s theories and stress the properties relevant for the present discussion. We then confront the three theories with the data.\\n\\nTwo alternative theories of focus\\n\\nRooth\\'s Alternative Semantics\\n\\nIn Rooth\\'s approach, the FSV is defined by recursion on the truth-conditional structure which is itself derived from LF (i.e. Logical Form, the Government and Binding level of semantic representation). Focus is then seen as introducing a free variable whose value is determined by the current context and is furthermore constrained to be an element or a subset of the FSV. For our purpose, the following characteristics are particularly important:\\n\\nGiven Rooth\\'s definition of the Alternative Set, a focus operator associates with any focus occurring in its scope. Any NP may be subject to Quantifier Raising. Importantly, this includes focused NPs. Quantifier Raising may not apply to quantifiers occurring in a scope-island.\\n\\nNote that Rooth\\'s approach critically relies on quantifier raising as a means of moving a focused NP out of the scope of a focus operator. However this only applies if the focus NP is not embedded in a scope island.\\n\\nKrifka\\'s Structured Meanings\\n\\nMultiple Focus Operators\\n\\nUtterances with multiple focus operators are known pathological cases of focus theory:\\n\\nIn the given context, the preferred reading of (3b) can be glossed as follows: it is also the case for SUE2, that Jon only1 read the letters she sent to PAUL1 - i.e. Jon didn\\'t read the letters she2  sent to e.g. Peter. In other words, the preferred reading is that also2 associates with SUE2 and only1 with   PAUL1.\\n\\nThe HOU analysis\\n\\nAnalysis then proceeds further and the ground equation\\n\\nmust be solved to determine the meaning of also2 only1 read the letters that SUE2 sent to PAUL1. A possible solution for G[2] is\\n\\nwe obtain the desired reading\\n\\nComparison with Rooth and Krifka Second Occurrence Expressions\\n\\nWe call second occurrence expressions (SOE) utterances which partially or completely repeat a previous utterance. Typical cases of SOEs are: corrections (5a), echo-sentences (5b) and variants (5c).\\n\\nThe HOU analysis\\n\\nOur proposal is to analyse SOEs as involving a deaccented anaphor which consists of the repeated material, and is subject to the condition that its semantic representation must unify with the semantic representation of its antecedent.\\n\\nIntuitively, these two equations require that target and source clause share a common semantics An, the semantics of the deaccented anaphor.\\n\\nGiven this proposal, the analysis of (5a) involves three equations:\\n\\nBy applying An to p, the left-hand side of the second equation is then determined so that the second equation becomes\\n\\nand the value of Gd is identified as being\\n\\nComparison with Rooth and Krifka\\n\\nUnder the Alternative Semantics approach, SOEs are captured as follows. It is assumed that the quantification domain of focus operators is a variable whose value is contextually determined. In the standard case (i.e. the case where the focus is prosodically marked), this quantification domain of focus operators is usually identified with the FSV of the VP. However, in the SOE cases, the assumption is that the quantification domain of focus operators is identified with the FSV of the source clause. Thus in (5a), the quantification of only in the second clause is identified with the FSV of the preceding utterance i.e. the set of properties of the form   like-ing somebody.\\n\\nBut now, consider the following example:\\n\\nThe Structured Meanings proposal distinguishes between proper- and quasi-SOEs. Proper-SOEs involve an exact repetition of some previous linguistic material, and are analysed as involving an anaphor which is constrained by the restriction that it be a segmental copy of its antecedent. For instance, the semantics of only likes Mary in (5b) is not determined by the semantics of its parts but is instead identified with the semantic value of its antecedent only likes MARY in (5a). In contrast, quasi-SOEs only involve semantic equivalence between repeating and repeated material (for instance, in a quasi-SOE a repeated element may be pronominalised). Krifka claims that quasi-SOEs have prosodically marked foci and thus do not raise any specific difficulty.\\n\\nFormal properties of the HOU approach\\n\\nConclusion\\n\\nIn this paper, we have argued that Higher-Order Unification provides an adequate tool for computing Focus Semantic Values. To this end, we have considered data which is viewed as a test-bed for focus theory and shown that, whilst existing theories either under-generate, over-generate or are methodologically unsatisfactory, the HOU approach yields a simple and transparent analysis. There appear to be two main reasons for this.\\n\\nFirst, the HOU analysis makes minimal assumptions about the role syntax is called to play in determining the FSV. It is defined on a purely semantic level in the sense that unification operates on semantic representations, and relies neither on quantifier raising, nor on a rule-to-rule definition of the FSV. As we have seen, this type of approach is a plausible way to avoid under-generation.\\n\\nSecond, the HOU approach permits an equational analysis which can naturally be further constrained by additional equations. The interest of such an approach was illustrated in our treatment of SOEs which we characterise as involving two phenomena: the computation of an FSV, and the resolution of a deaccented anaphor. Not only did we show that this analysis is methodologically and empirically sound, we also showed that it finds a natural realisation in the equational framework of HOU: each linguistic phenomena is characterised by some equation(s) and the equations may mutually constrain each other. For instance, in the case of SOEs, we saw that the equations characterising the deaccented anaphor help determine the unidentified FSV of the utterance containing the unmarked focus.\\n\\nOf course, there are still many open issues. First, how does the proposed analysis interact with quantification? Second, how does it extend to a dynamic semantics (e.g. Discourse Representation Theory)?\\n\\nAcknowledgments\\n\\nThe work reported in this paper was funded by the Deutsche Forschungsgemeinschaft (DFG) in Sonderforschungsbereich SFB-378, Project C2 (LISA).\\n\\nBibliography\\n\\nChristine Bartels.\\n\\n1995.\\n\\nSecond occurrence test.\\n\\nMs.\\n\\nMary Dalrymple, Stuart Shieber, and Fernando Pereira. 1991. Ellipsis and higher-order-unification. Linguistics and Philosophy, 14:399-452.\\n\\nDaniel Dougherty. 1993. Higher-order unification using combinators. Theoretical Computer Science B, 114(2):273-298.\\n\\nGilles Dowek. 1992. Third order matching is decidable. In Proc. LICS-7, pages 2-10, IEEE.\\n\\nClaire Gardent and Michael Kohlhase. 1996. Higher-order coloured unification and natural language semantics. In Proc. ACL96, Santa Cruz, USA. forthcoming.\\n\\nClaire Gardent, Michael Kohlhase and Noor van Leusen. 1996. Corrections and higher-order unification. CLAUS report 77, University of Saarland.\\n\\nUlrich Hustadt. 1991. A complete transformation system for polymorphic higher-order unification. Technical Report MPI-I-91-228, MPI Informatik, Saarbrcken, Germany.\\n\\nRay S. Jackendoff. 1972. Semantic Interpretation in Generative Grammar. The MIT Press.\\n\\nAngelika Kratzer. 1991. The representation of focus. In Arnim van Stechow and Dieter Wunderlich, editors, Semantik: Ein internationales Handbuch der zeitgenoessischen Forschung. Berlin: Walter de Gruyter.\\n\\nManfred Krifka. 1992. A compositional semantics for multiple focus constructions. In Joachim Jacobs, editor, Informationsstruktur and Grammatik. Sonderheft 4.\\n\\nChristian Prehofer. 1994. Decidable higher-order unification problems. In Alan Bundy, editor, Proc. CADE94, LNAI, pages 635-649, Nancy, France.\\n\\nSteve G. Pulman. 1995. Higher-order unification and the interpretation of focus. Paper submitted for publication.\\n\\nMats Rooth. 1992. A theory of focus interpretation. Natural Language Semantics, pages 75-116.\\n\\nKai von Fintel. 1995. A minimal theory of adverbial quantification. Unpublished draft Ms. MIT, Cambridge, March.\\n\\nFootnotes', metadata={'source': '../data/raw/cmplg-xml/9605005.xml'}),\n",
       " Document(page_content='ABSTRACT GENERATION BASED ON RHETORICAL STRUCTURE EXTRACTION\\n\\nKenji Ono, Kazuo Sumita, Seiji Miike Research, Development Center, Toshiba Corporation Komukai-Toshiba-cho 1, Saiwai-ku, Kawasaki, 210, Japan\\n\\nAbstract:\\n\\nWe have developed an automatic abstract generation system for Japanese expository writings based on rhetorical structure extraction. The system first extracts the rhetorical structure, the compound of the rhetorical relations between sentences, and then cuts out less important parts in the extracted structure to generate an abstract of the desired length.\\n\\nEvaluation of the generated abstract showed that it contains at maximum 74% of the most important sentences of the original text. The system is now utilized as a text browser for a prototypical interactive document retrieval system.\\n\\nINTRODUCTION\\n\\nRHETORICAL STRUCTURE\\n\\nRhetorical structure represents relations between various chunks of sentences in the body of each section. In this paper, the rhetorical structure is represented by two layers: intra-paragraph and inter-paragraph structures. An intra-paragraph structure is a structure whose representation units are sentences, and an inter-paragraph structure is a structure whose representation units are paragraphs.\\n\\nThe rhetorical structure is represented by a binary tree on the analogy of a syntactic tree of a natural language sentence. Each sub tree of the rhetorical structure forms an argumentative constituent, just as each sub-tree of the syntactic tree forms a grammatical constituent. Also, a sub-tree of the rhetorical structure is sub-categorized by a relation of its parent node as well as a syntactic tree.\\n\\nRHETORICAL STRUCTURE EXTRACTION\\n\\nThe rhetorical structure represents logical relations between sentences or blocks of sentences of each section of the document. A rhetorical structure analysis determines logical relations between sentences based on linguistic clues, such as connectives, anaphoric expressions, and idiomatic expressions in the input text, and then recognizes an argumentative chunk of sentences.\\n\\nRhetorical structure extraction consists of five major sub-processes:\\n\\n(1) Sentence analysis accomplishes morphological and syntactic analysis for each sentence. (2) Rhetorical relation extraction detects rhetorical relations and constructs the sequence of sentence identifiers and relations. (3) Segmentation detects rhetorical expressions between distant sentences which define rhetorical structure. They are added onto the sequence produced in step 2, and form restrictions for generating structures in step 4. For example, expressions like ``...3 reasons. First, ...  Second,  ...  Third, ...\\'\\', and ``...  Of course, ... ...But, ...\\'\\' are extracted and the structural constraint is added onto the sequence so as to form a chunk between the expressions.\\n\\nThe system analyzes inter-paragraph structures after the analysis of intra-paragraph structures. While the system uses the rhetorical relations of the first sentence of each paragraph for this analysis, it executes the same steps as it does for the intra-paragraph analysis.\\n\\nABSTRACT GENERATION\\n\\nThe Actual sentence evaluation is carried out in a demerit marking way. In order to determine important text segments, the system imposes penalties on both nodes for each rhetorical relation according to its relative importance. The system imposes a penalty on the left node for the RightNucleus relation, and also on the right node for the LeftNucleus relation. It adds penalties from the root node to the terminal nodes in turn, to calculate the penalties of all nodes.\\n\\nThen, in the structure reduction stage, the system recursively cuts out the nodes, from the terminal nodes, which are imposed the highest penalty. The list of terminal nodes of the final structure becomes an abstract for the original document. Suppose that the abstract is longer than the expected length. In that case the system cuts out terminal nodes from the last sentences, which are given the same penalty score.\\n\\nIf the text is written loosely, the rhetorical structure generally contains many BothNucleus relations (e.g., parallel(mata(and, also)), and the system cannot gradate the penalties and cannot reduce sentences smoothly.\\n\\nAfter sentences of each paragraph are reduced, inter-paragraph structure reduction is carried out in the same way based on the relative importance judgement on the inter-paragraph rhetorical structure.\\n\\nAfter the sentences to be included in the abstract are determined, the system alternately arranges the sentences and the connectives from which the relations were extracted, and realizes the text of the abstract.\\n\\nThe important feature of the generated abstracts is that since they are composed of the rhetoricaly consistent units which consist of several sentences and form a rhetorical substructure, the abstract does not contain fragmentary sentences which cannot be understood alone. For example, in the abstract generation mentioned above, sentence two does not appear solely in the abstract, but appears always with sentence one. If sentence two appeared alone in the abstract without sentence one, it would be difficult to understand the text.\\n\\nEVALUATION\\n\\nThe generated abstracts were evaluated from the point of view of key sentence coverage. 30 editorial articles of \"Asahi Shinbun\", a Japanese newspaper, and 42 technical papers of \"Toshiba Review\", a journal of Toshiba Corp. which publishes short expository papers of three or four pages, were selected and three subjects judged the key sentences and the most important key sentence of each text. As for the editorial articles, The average correspondence rates of the key sentence and the most important key sentence among the subjects were 60% and 60% respectively. As for the technical papers, they were 60% and 80 % respectively.\\n\\nThe reason why the compression rate and the key sentence coverage of the technical papers were higher than that of the editorials is  considered as follows. The technical papers contains so many rhetorical expressions in general as to be expository. That is, they provide many linguistic clues and the system can extract the rhetorical structure exactly. Accordingly, the structure can be reduced further and the length of the abstract gets shorter, without omitting key sentences. On the other hand, in the editorials most of the relations between sentences are supposed to be understood semantically, and are not expressed rhetorically. Therefore, they lack linguistic clues and the system cannot extract the rhetorical structure exactly.\\n\\nCONCLUSION\\n\\nWe have developed an automatic abstract generation system for Japanese expository writings based on rhetorical structure extraction.\\n\\nThe rhetorical structure provides a natural order of importance among sentences in the text, and can be used to determine which sentence should be extracted in the abstract, according to the desired length of the abstract. The rhetorical structure also provides the rhetorical relation between the extracted sentences, and can be used to generate appropriate connectives between them.\\n\\nThe system is now utilized as a text browser for a prototypical interactive document retrieval system.\\n\\nBibliography\\n\\nCohen, R. : ``Analyzing the Structure of Argumentative Discourse\\'\\', Computational Linguistics, Vol.13, pp.11-24, 1987.\\n\\nFum, D. : ``Tailoring Importance Evaluation to Reader\\'s Goals: A Contribution to Descriptive Text Summarization\\'\\', Proc. of Coling, pp.252-259, 1986.\\n\\nGrosz, B.J. and Sidner, C.L. : ``Attention, Intentions and the Structure of Discourse\\'\\', Computational Linguistics, Vol.12, pp.175-204, 1986.\\n\\nHobbs, J.R.:\\n\\n``Coherence and Coreference\\'\\',\\n\\nCognitive Science, Vol.3,\\n\\n1979, pp.67\\n\\n\\n\\n90.\\n\\nKuhn, H.P. : ``The Automatic Creation of Literature Abstracts\\'\\', IBM Journal, Apr. 1958, pp.159-165.\\n\\nLehnert, W. :\\n\\n``Narrative Text Summarization\\'\\',\\n\\nProc. of AAAI, pp.337\\n\\n\\n\\n339, 1980.\\n\\nLitman, D.J. and Allen, J.F. : ``A Plan Recognition Model for Subdialogues in Conversations\\'\\', Cognitive Science, Vol.11, 1987, pp.163-200.\\n\\nMann, W.C. and Thompson, S.A. : ``Rhetorical Structure Theory: A Framework for the Analysis of Texts\\'\\', USC/Information Science Institute Research Report RR-87-190, 1987.\\n\\nSumita, K., et al. : ``A Discourse Structure Analyzer for Japanese Text\\'\\', Proc. Int. Conf. Fifth Generation Computer Systems 1992 (FGCS\\'92), pp.1133-1140, 1992.', metadata={'source': '../data/raw/cmplg-xml/9411023.xml'}),\n",
       " Document(page_content=\"Robust Parsing Based on Discourse Information: Completing partial parses of ill-formed sentences on the basis of discourse information\\n\\nIn a consistent text, many words and phrases are repeatedly used in more than one sentence. When an identical phrase (a set of consecutive words) is repeated in different sentences, the constituent words of those sentences tend to be associated in identical modification patterns with identical parts of speech and identical modifiee-modifier relationships. Thus, when a syntactic parser cannot parse a sentence as a unified structure, parts of speech and modifiee-modifier relationships among morphologically identical words in complete parses of other sentences within the same text provide useful information for obtaining partial parses of the sentence. In this paper, we describe a method for completing partial parses by maintaining consistency among morphologically identical words within the same text as regards their part of speech and their modifiee-modifier relationship. The experimental results obtained by using this method with technical documents offer good prospects for improving the accuracy of sentence analysis in a broad-coverage natural language processing system such as a machine translation system.\\n\\nIntroduction\\n\\nIn order to develop a practical natural language processing (NLP) system, it is essential to deal with ill-formed sentences that cannot be parsed correctly according to the grammar rules in the system. In this paper, an ``ill-formed sentence'' means one that cannot be parsed as a unified structure. A syntactic parser with general grammar rules is often unable to analyze not only sentences with grammatical errors and ellipses, but also long sentences, owing to their complexity. Thus, ill-formed sentences include not only ungrammatical sentences, but also some grammatical sentences that cannot be parsed as unified structures owing to the presence of unknown words or to a lack of completeness in the syntactic parser. In texts from a restricted domain, such as computer manuals, most sentences are grammatically correct. However, even a well-established syntactic parser usually fails to generate a unified parsed structure for about 10 to 20 percent of all the sentences in such texts, and the failure to generate a unified parsed structure in syntactic analysis leads to a failure in the output of a NLP system. Thus, it is indispensable to establish a correct analysis for such a sentence.\\n\\nRelaxing constraints in the condition part of a grammatical rule, such as number and gender constraints\\n\\nJoining partial parses by using meta rules.\\n\\nStarting from the viewpoint that an interpretation of a sentence must be consistent in its discourse, we worked on completing incomplete parses by using information extracted from complete parses in the discourse. The results were encouraging. Since most words in a sentence are repeatedly used in other sentences in the discourse, the complete parses of well-formed sentences usually provided some useful information for completing incomplete parses in the same discourse. Thus, rather than trying to enhance a syntactic parser's grammar rules in order to support ill-formed sentences, which seems to be an endless task after the parser has obtained enough coverage to parse general grammatical sentences, we treat the syntactic parser as a black box and complete incomplete parses, in the form of partially parsed chunks that a bottom-up parser outputs for ill-formed sentences, by using information extracted from the discourse.\\n\\nIn the next section, the effectiveness of using information extracted from the discourse to complete syntactic analysis of ill-formed sentences. After that, we propose an algorithm for completing incomplete parses by using discourse information, and give the results of an experiment on completing incomplete parses in technical documents.\\n\\nDiscourse information for completing incomplete parses Implementation Algorithm\\n\\nAs we showed in the previous section, information that is very useful for obtaining correct parses of ill-formed sentences is provided by complete parses of other sentences in the same discourse in cases where a parser cannot construct a parse tree by using its grammar rules. In this section, we describe an algorithm for completing incomplete parses by using this information.\\n\\nThe first step of the procedure is to extract from an input text discourse information that the system can refer to in the next step in order to complete incomplete parses. The procedure for extracting discourse information is as follows:\\n\\n2. When all the sentences have been parsed, the discourse information is used to select the most preferable candidate for sentences with multiple possible parses, and the data of the selected parse are added to the discourse information.\\n\\nAfter all the sentences except the ill-formed sentences that caused incomplete parses have provided data for use as discourse information, the parse completion procedure begins.\\n\\nThe completion procedure consists of two steps:\\n\\nStep 1: Inspecting each partial parse and restructuring it on the basis of the discourse information\\n\\nFor each word in a partial parse, the part of speech and the modifiee-modifier relationships with other words are inspected. If they are different from those in the discourse information, the partial parse is restructured according to the discourse information.\\n\\nStep 2: Joining partial parses on the basis of the discourse information\\n\\nIf the partial parses are not unified into a single structure in the previous step, they are joined together on the basis of the discourse information until a unified parse is obtained.\\n\\nPartial parses are joined as follows:\\n\\nFirst, the possibility of joining the first two partial parses is examined, then, either the unification of the first two parses or the second parse is examined to determine whether it can be joined to the third parse, then the examination moves to the next parse, and so on.\\n\\nTwo partial parses are joined if the root (head node) of either parse tree can modify a node in the other parse without crossing the modification of other nodes.\\n\\nSince the discourse information consists of modification patterns extracted from complete parses, it reflects the grammar rules of the parser, and a matching pattern with a part of speech rather than an actual word on one side can be regarded as a relaxation rule, in the sense that syntactic and semantic constraints are less restrictive than the corresponding grammar rule in the parser.\\n\\nThese matching conditions at different levels are applied in such a manner that partial parses are joined through the most preferable nodes.\\n\\nResults\\n\\nConclusion\\n\\nIn this paper, the term ``discourse'' is used as a set of words in a text together with the usage of each of those words in that text - namely, a part of speech and modifiee-modifier relationships with other words. The basic idea of our method is to improve the accuracy of sentence analysis simply by maintaining consistency in the usage of morphologically identical words within the same text. Thus, the effectiveness of this method is highly dependent on the source text, since it presupposes that morphologically identical words are likely to be repeated in the same text. However, the results have been encouraging at least with technical documents such as computer manuals, where words with the same lemma are frequently repeated in a small area of text. Moreover, our method improves the translation accuracy, especially for frequently repeated phrases, which are usually considered to be important, and leads to an improvement in the overall accuracy of the natural language processing system.\\n\\nAcknowledgements\\n\\nI would like to thank Michael McDonald for invaluable help in proofreading this paper. I would also like to thank Taijiro Tsutsumi, Masayuki Morohashi, Koichi Takeda, Hiroshi Maruyama, Hiroshi Nomiyama, Hideo Watanabe, Shiho Ogino, Naohiko Uramoto, and the anonymous reviewers for their comments and suggestions.\\n\\nBibliography\\n\\nDouglas, S. and Dale, R. 1992. Towards Robust PATR. In Proceedings of COLING-92.\\n\\nGale, W.A., Church, K.W., and Yarowsky, D. 1992. One Sense per Discourse. In Proceedings of the 4th DARPA Speech and Natural Language Workshop.\\n\\nJensen, K., Heidorn, G.E., Miller, L.A. and Ravin, Y. 1983. Parse Fitting and Prose Fixing: Getting a Hold on Ill-Formedness. Computational Linguistics, Vol. 9, Nos. 3-4.\\n\\nJensen, K. 1992. PEG: The PLNLP English Grammar. Natural Language Processing: The PLNLP Approach, K. Jensen, G. Heidorn, and S. Richardson, eds., Boston, Mass. : Kluwer Academic Publishers.\\n\\nMcCord, M.\\n\\n1991.\\n\\nThe Slot Grammar System.\\n\\nIBM Research Report, RC17313.\\n\\nNasukawa, T. 1993. Discourse Constraint in Computer Manuals. In Proceedings of TMI-93.\\n\\nNasukawa, T. 1995. Shallow and Robust Context Processing for a Practical MT System. To appear in Proceedings of IJCAI-95 Workshop on ``Context in Natural Language Processing.''\\n\\nRichardson, S.D. and Braden-Harder, L.C. 1988. The Experience of Developing a Large-Scale Natural Language Text Processing System: CRITIQUE. In Proceedings of ANLP-88.\\n\\nTakeda, K., Uramoto, N., Nasukawa, T., and Tsutsumi, T. 1992. Shalt2 - A Symmetric Machine Translation System with Conceptual Transfer. In Proceedings of COLING-92.\\n\\nIBM 1992. IBM Application System/400 New User's Guide Version 2. IBM Corp.\\n\\nCOLLINS\\n\\n1984.\\n\\nThe New Collins Thesaurus.\\n\\nCollins Publishers, Glasgow.\\n\\nFootnotes\\n\\nFor example, in the sentence You can use the folder on the desktop, the ambiguous phrase, on the desktop, forms two candidate collocation patterns: ``use -(on)- desktop'' and ``folder -(on)- desktop.'' Thick arrows indicate dependencies extracted from the discourse information. This structure resulting from an incomplete parse does not indicate that the grammar of the parser lacks a rule for handling a possessive case indicated by an apostrophe and an s.  When the parser fails to generate a unified parse, it outputs partial parses in such a manner that fewer partial parses cover every word in the input sentence.\", metadata={'source': '../data/raw/cmplg-xml/9505042.xml'}),\n",
       " Document(page_content='A fast partial parse of natural language sentences using a connectionist method\\n\\nThe pattern matching capabilities of neural networks can be used to locate syntactic constituents of  natural language. This paper describes a fully automated hybrid system, using neural nets operating within a grammatic framework. It addresses the representation of  language for connectionist processing, and describes methods of constraining the problem size. The function of the network is briefly explained, and results are given.\\n\\nIntroduction\\n\\nThe well known complexity of parsing  is  addressed by decomposing the problem, and then locating one syntactic constituent at a time. The sentence is first decomposed into the broad syntactic categories pre-subject  -  subject  -  predicate by locating the subject. Then these constituents can be processed further. The underlying principle employed at each step is to take a sentence, or part of a sentence, and generate strings with the  boundary markers of the syntactic constituent in question placed in all possible positions. Then a neural net selects the string with the correct placement.\\n\\nThis paper gives an overview of  how natural language is converted to a representation that the neural nets can handle, and how the problem is reduced to a manageable size. It then outlines the neural net selection process. A comprehensive account is given in lyon1; descriptions of the neural net process are also in lyon2 and lyon3. This is a hybrid system. The core process is data driven, as the parameters of the neural networks are derived from training text. The neural net is trained in supervised mode on examples that have been manually marked ``correct\" and ``incorrect\". It will then be able to classify unseen examples. However, the initial processing stages, in which the problem size is constrained, operate within a skeletal grammatic framework. Computational tractability is further addressed by reducing data through the application of prohibitive rules as local constraints. The pruning process is remarkably effective.\\n\\nThe corpus of  sentences from technical manuals Language representation (I)\\n\\nIn order to reconcile computational feasibility to empirical realism an appropriate form of language representation is critical. The first step in constraining the problem size is to partition an unlimited vocabulary into a restricted number of part-of-speech tags. Different stages of processing place different requirements on the classification system, so customised tagsets have been developed. For the first processing stage we need to place the subject markers, and, as a further task, disambiguate tags. It was not found necessary to use number information at this stage. For example, Still waters run deep. (2) The word ``waters\" could be a 3rd person, singular, present verb or a plural noun. However, in order to disambiguate the tag and place the subject markers it is only necessary to know that it is a noun or else a verb. [ Still waters ]  run deep. (2.1) The tagset used at this stage, mode 1, has 21 classes, not distinguished for number. However, the head of the subject is then found and number agreement with the verb can be assessed. At this stage the tagset, mode 2, includes number information and has 28 classes. Devising optimal tagsets for given tasks is a field in which further work is planned. We need larger tagsets to capture more linguistic information, but smaller ones to constrain the computational load. Information theoretic tools can be used to find the entropy of different tag sequence languages, and support decisions on representation.\\n\\nA functional approach is taken to tagging: words are allocated to classes depending on their syntactic role. For instance, superlative adjectives can act as nouns, so they are initially given the 2 tags: noun or adjective. This approach can be  extended by taking adjacent words which act jointly as single lexical items as a unit. Thus the pair ``most [adjective]\" is taken as a single superlative adjective.\\n\\nRepresenting syntactic boundary markers Constraining the generation of candidate strings\\n\\nThis system generates sets of tag strings for each sentence, with the hypertags placed in all possible positions. Thus, for the subject detection task:\\n\\nThe skeletal grammatic framework\\n\\nA minimal grammar, set out in lyon1 in EBNF form, is composed of 9 rules. For instance, the subject must contain a noun-type word. Applying this particular rule to sentence (3) above would eliminate candidate strings (3.1) and (3.2). We also  have the 2 arbitrary limits on length of pre-subject and subject. There is  a small set of 4 extensions to the grammar, or semi-local constraints. For instance, if a relative pronoun occurs, then a verb must follow in that constituent. On the technical manuals the constraints of the grammatic framework put  up to 6% of declarative sentences outside our system, most commonly because the pre-subject is too long. A small number are excluded because the system cannot handle a co-ordinated head. With the  length of pre-subject extended to 15 words, and subject to 12 words, an average of 2% are excluded (7 out of 351).\\n\\nProhibition tables\\n\\nLanguage representation (II)\\n\\nThough the form in which the vector is written may give an illusion of representing order, no sequential order is maintained. A method of representing a sequence must be chosen. The sequential order of the input is captured here, partially, by taking adjacent tags, pairs and triples, as the feature elements. The individual tags are converted to a bipos and tripos representation. Using this method each tag is in 3 tripos and 2 bipos elements. This highly redundant code will aid the processing of sparse data typical of natural language.\\n\\nFor most of the work described here the sentence was dynamically truncated 2 words beyond the hypertag marking the close of the subject. This process has now been improved by going further along the sentence.\\n\\nThe function of the net\\n\\nThe training process\\n\\nThe net is presented with training strings whose desired classification has been manually marked. The weights on the connections between input and output nodes are adjusted until a required level of performance is reached. Then the weights are fixed and the trained net is ready to classify unseen sentences. The prototype accessible via the Internet has been trained on sentences from the technical manuals, slightly augmented.\\n\\nThe input layer potentially has a node for each possible tuple. With 28 tags, 2 hypertags and a start symbol the upper bound  on the number of input nodes is\\n\\n31[3] + 31[2]. In practice the maximum activated is currently about 1000. In testing mode, if a previously unseen tuple appears it makes zero contribution to the result. The activations at the input layer are fed forward through the weighted connections to the output nodes, where they are summed. The highest  output marks the winning node. If the desired node wins, then no action is taken. If the desired node does not win, then the weight on connections to the desired node are incremented, while the weights on connections to the unwanted node are decremented.\\n\\nRecall that weights are initialised to 1.0. After training we find that the weight range is bounded by\\n\\n10[-3] [ w [ 5.0\\n\\nThe testing process\\n\\nFor the results given below, the networks were trained on part of the corpus and tested on another part of the corpus. For the prototype in which users can process their own text, the net was trained on the whole corpus, slightly augmented.\\n\\nResults\\n\\nUsing negative information\\n\\nWhen parses are postulated for a sentence negative as well as positive examples are likely to occur. Now, in natural language   negative correlations  are an important source of information: the occurrence of some words or groups of words inhibit others from following. We wish to exploit these constraints. brill recognised this, and introduced  the idea of distituents. These are elements of a sentence that should be separated, as opposed to elements of constituents that cling together. Brill addresses the problem of finding a valid metric for distituency by using a generalized mutual information statistic. Distituency is marked by a mutual information minima. His method is supported by a small 4 rule grammar.\\n\\nHowever, this approach does not fully capture the sense in which inhibitory factors play a negative and not just a neutral role. We want to distinguish between items that are unlikely to occur ever, and those that have just not happened to turn up in the training data. For example, in sentence (3) above strings 3.1, 3.2 and 3.n  can never be correct. These should be distinguished from possibly correct parses that are not in the training data. In order that  ``improbabilities\" can be modelled by inhibitory connections niles show how a  Hidden Markov Model can be implemented by a neural network.\\n\\nRelationship between the neural net and prohibition table Conclusion\\n\\nThe supporting role of the grammatic framework and the prohibition filters should not be underestimated. Whenever the scope of the system is extended it has been found necessary to enhance these elements.\\n\\nThe most laborious part of this work is preparing the training data. Each time the representation is modified a new set of strings is generated that need marking up. An autodidactic check is now included which speeds up this task. We run marked up training data through an early version of the network trained on the same data, so the results should be almost all correct. If an ``incorrect\\'\\' parse occurs we can then check whether that sentence was properly marked up.\\n\\nSome of the features of the system described here could be used in a stochastic process. However, connectionist methods have low computational loads at runtime. Moreover, they can utilise more of the implicit information in the training data by modelling negative relationships. This is a powerful concept that can be exploited in the effort to squeeze out every available piece of useful information for natural language processing.\\n\\nFuture work is planned to extend this very limited partial parser, and decompose sentences further into their hierarchical constituent parts. In order to do this a number of subsidiary tasks will be addressed. The system is being improved by identifying groups of words that act as single lexical items. The decomposition of the problem can be investigated further: for instance, should the tag disambiguation task precede the placement of the subject boundary markers in a separate step? More detailed investigation of language representation issues will be undertaken. And the critical issues of investigating the most appropriate network architectures will be carried on.\\n\\nBibliography\\n\\nD Angluin. 1980. Inductive inference of formal languages from positive data. Information and Control, 45.\\n\\nE Atwell. 1987. Constituent-likelihood grammar. In R Garside, G Leech, and G Sampson, editors, The Computational Analysis of English: a corpus-based approach. Longman.\\n\\nG E Barton, R C Berwick, and E S Ristad. 1987. Computational Complexity and Natural Language. MIT Press.\\n\\nE Black, R Garside, and G Leech. 1993. Statistically driven computer grammars of English: the IBM/Lancaster approach. Rodopi.\\n\\nE Brill, D Magerman, M Marcus and B Santorini. 1990. Deducing linguistic structure from the statistics of large corpora. In DARPA Speech and Natural Language Workshop.\\n\\nK W Church, Bell Laboratories. 1989. A stochastic parts program and noun phrase parser for unrestricted text. In IEEE conference record  of ICASSP.\\n\\nP Garcia and E Vidal. 1990. Inference of k-testable languages in the strict sense and application to syntactic pattern recognition. IEEE Trans. on Pattern Analysis and Machine Intelligence, 12.\\n\\nR Garside. 1987. The CLAWS word-tagging system. In R Garside, G Leech, and G Sampson, editors, The Computational Analysis of English: a corpus based approach. Longman.\\n\\nE M Gold. 1967. Language identification in the limit. Information and Control, 10.\\n\\nC Lyon. 1994. The representation of natural language to enable neural networks to detect syntactic features. PhD thesis.\\n\\nC Lyon 1993. Using neural networks to infer grammatical structures in natural language. In Proc. of IEE Colloquium on Grammatical Inference.\\n\\nC Lyon and R Frank 1992. Detecting structures in natural language using a neural net with rules. In Proc. of International Conference on Artificial Neural Networks (ICANN).\\n\\nL Niles and H Silverman. 1990. Combining Hidden Markov Models and Neural Network Classifiers. In IEEE conference record of ICASSP.\\n\\nYoh-Han Pao. 1989. Adaptive Pattern Recognition and Neural Networks. Addison Wesley.\\n\\nR Pocock and E Atwell. 1994. Treebank trained probabilistic parsing of lattices. School of Computer Studies, Leeds University. In The Speech-Oriented Probabilistic Parser Project: Final Report to MoD.\\n\\nP Pym. 1993. Perkins Engines and Publications. In Proceedings of Technology and Language in Europe 2000. DGXIII-E of the European Commission.\\n\\nD Rumelhart and J McClelland. 1986. Parallel Distributed Processing MIT.\\n\\nC E Shannon 1951. Prediction and Entropy of Printed English. In Bell System Technical Journal.\\n\\nB Widrow and M Lehr. 1992. 30 years of adaptive neural networks. In Neural networks: theoretical foundations and analysis edited by C Lau. IEEE press.\\n\\nP Wyard and C Nightingale. 1990. A  Single Layer Higher Order Neural Net and its Application to Context Free Grammar Recognition In Connection Science, 4.', metadata={'source': '../data/raw/cmplg-xml/9503023.xml'}),\n",
       " Document(page_content=\"Tagset Design and Inflected Languages\\n\\nAn experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described, using corpora in English, French and Swedish. In particular, the question of internal versus external criteria for tagset design is considered, with the general conclusion that external (linguistic) criteria should be followed. Some problems associated with tagging unknown words in inflected languages are briefly considered.\\n\\nTagset Design\\n\\nTagging by means of a Hidden Markov Model (HMM) is  widely recognised as an effective technique for assigning parts of speech to a corpus in a robust and efficient manner. An attractive feature of the technique is that the algorithm itself is independent of the (natural) language to which it is applied. All of the ``knowledge engineering'' is localised in the choice of tagset and the method of training. Typically, training makes use of a manually tagged corpus, or an untagged corpus with some initial bootstrapping probabilities. Some attention has been given to how to make such techniques effective; for example Cutting et al. (1992)  suggest ways of training trigram taggers, and Merialdo (1994) and Elworthy (1994)  consider the amount and quality of the seeding data needed to construct an accurate tagger.\\n\\nThe problem of tagset design becomes particularly important for highly inflected languages, such as Greek or Hungarian. If all of the syntactic variations which are realised in the inflectional system were represented in the tagset, there would be a huge number of tags, and it would be practically impossible to implement or train a simple tagger. Note in passing that this may not as serious a problem as it first appears. If the language is very highly inflected, it may be be possible to do all (or a large part) of the work of a tagger with a word-by-word morphological analysis instead. Nevertheless, there are many languages which have enough ambiguity that tagging is useful, but a rich enough tagset that the criteria on which it is designed must be given careful consideration.\\n\\nIn this paper, I report two experiments which address the internal design criterion, by looking at how tagging accuracy varies as the tagset is modified, in English, French and Swedish. Although the choice of language was dictated by the corpora which were available, they represent three different degrees of complexity in their inflectional systems. English has a very limited system, marking little more than plurality on nouns and a restricted range of verb properties. French has a little more complexity, with gender, number and person marked, while Swedish has more detailed marking for gender, number, definiteness and case. As a subsidiary issue, we will also look at how the tagger performs on unknown words, i.e. ones not seen in the training data. The usual approach here is to hypothesise all tags in the tagset for an unknown word, other than ones where all the words that may have the tag can be enumerated in advance (closed class tags). HMM taggers often perform poorly on unknown words.\\n\\nAlternative tagsets were derived by taking the initial tagset for each corpus (from manual tagging of the corpus) and condensing sets of tags which represent a grammatical distinction such as gender into single tags. The changes were then applied to the training corpus. This allows us to effectively produce a corpus tagged according to a different scheme without having to manually re-tag the corpus. The changes in the tagsets were motivated purely by grammatical considerations, and did not take the errors actually observed into account. In general what we will look at in the results is how the tagging accuracy changes as the size of the tagset changes. This is a deliberately naive approach, and it is adopted with the goal of continuing in the relatively ``knowledge-free'' tradition of work in HMM tagging. The aim of the experiment is to determine, crudely, whether a bigger tagset is better than a smaller one, or whether external criteria requiring human intervention should be used to choose the best tagset. The results for the three languages turn out to be quite different, and the general conclusion (which is the overall contribution of the paper) will be that the external criterion should be the one to dominate tagset design: there is a limit to how knowledge-free we can be.\\n\\nAs a preliminary to this work, note that it is hard to reason about the effect of changing the tagset. It can be argued that a smaller tagset should improve tagging accuracy, since it puts less of a burden on the tagger to make fine distinctions. In information-theoretic terms, the number of decisions required is smaller, and hence the tagger need contribute less information to make the decisions. A smaller tagset may also mean that more words have only one possible tag and so can be handled trivially. Conversely, more detail in the tagset may help the tagger when the properties of two adjacent words give support to the choice of tag for both of them; that is, the transitions between tags contribute the information the tagger needs. For example, if determiners and nouns are marked for number, then the tagger can effectively model agreement in simple noun phrases, by having a higher probability for a singular determiner followed by a singular noun that it does for a singular determiner followed by a plural noun. Theory on its own does not help much in deciding which point of view should dominate.\\n\\nThe experiments\\n\\nDesign of the experiments\\n\\nTwo experiments were conducted on three corpora: 300k words of Swedish text from the ECI Multilingual CD-ROM, and 100k words each of English and French from a corpus of International Telecommunications Union text. In the first experiment the whole of each corpus was used to train the model, and a small sample from the same text was used as test data. For the second experiment, 95% of the corpus was used in training and the remainder in testing. The importance of the second test is that it includes unknown words, which are difficult to tag. The tagsets were progressively modified, by textually substituting simplified tags for the original ones and e e-running the training and test procedures using the modified corpora. The changes to the tagset are listed below. In the results that follow, we will identify tagset that include a given distinction with an uppercase letter and ones that do not with a lowercase letter; for example G for a tagset that marks gender, and g for one that does not. Swedish The changes made were entirely based on inflections. G Gender: masculine, neuter, common gender (``UTR'' in the tagset). N Number: singular, plural. D Definiteness: definite, indefinite. C Case: nominative, genitive. French The changes other than V were based on inflections. G Gender: masculine, feminine. N Number: singular, plural. P Person: identified as 1st to 6th in the tagset. V Verbs: treat avoir and etre as being the same as any other verb.\\n\\nEnglish The changes here are more varied than for the other languages, and generally consisted of removing some of the finer subdivisions of the major classes. The grouping of some of these changes is admittedly a little ad hoc, and was intended to give a good distribution of tagset sizes; not all combinations were tried. C Reduce specific conjunction classes to a common class, and simplify one adjective class. A Simplify noun and adverb classes. P Simplify pronoun classes. N Number: all singular/plural distinctions removed. V Use the same class for have, do and be as for other verbs. The sizes of the resulting tagsets and the degree of ambiguity in the corpora which resulted appear below. Accuracy figures quoted here are for ambiguous and unknown words only, and therefore factor out effects due to the varying degree of ambiguity as the tagset changes. In fact, this is a rather approximate way of accounting for ambiguity, since it does not take the length of ambiguous sequences into account, and the accuracy is likely to deteriorate more on long sequences of ambiguous words than on short ones.\\n\\nThe tests were run using Good-Turing correction to the probability estimates; that is, rather than estimating the probability of the transition from a tag i to a tag j as the count of transition from i to j in the training corpus divided by the total frequency of tag i, one was added to the count of all transitions, and the total tag frequencies adjusted correspondingly. The purpose in using this correction is to correct for corpora which might not provide enough training data. On the largest tagsets, the correction was found to give a very slight reduction in the accuracy for Swedish, and to improve the French and English accuracies by about 1.5%, suggesting that it is indeed needed.\\n\\nResults\\n\\nWhat seems to come out from these results is that there is not a consistent relationship between the size of the tagsets and the tagging accuracy. The most common pattern was for a larger tagset to give higher accuracy, but there were notable exceptions in French (where gender marking was the key factor), in Swedish unknown words (which show the reverse trend) and in English unknown words (which show no very clear trend at all). This seems to fit quite well with the difficulties that were suggested above in reasoning about the effect of tagset size. The main conclusion of this paper is therefore that the knowledge engineering component of setting up a tagger should concentrate on optimising the tagset for external criteria, and that the internal criterion of tagset size does not show sufficient generality to be taken into account without prior knowledge of properties of the language. Perhaps this is not too surprising, but it is useful to have an experimental confirmation that the linguistics matters rather than the engineering.\\n\\nUnknown words\\n\\nOne final observation about the experiments: the accuracy on unknown words was very low in all of the tests, and was particularly bad in Swedish. The tagger used in the experiments took a very simple-minded approach to unknown words. An alternative that is often used is to limit the possible tags using a simple morphological analysis or some other examination of the surface form of the word. For example, in a variant of the English tagger which was not used in these experiments, a module which reduces the range of possible tags based on testing for only seven surface characteristics such as capitalisation and word endings improved the unknown word accuracy by 15-20%.\\n\\nThe results above show that if it were not for unknown words, there might be some argument for favouring larger tagsets, since they have some tendency to give a higher accuracy. A tentative experiment on the contribution of using morphological or surface analysis in French and Swedish was therefore carried out. Firstly, in both languages, the unknown words from the second experiment were looked up in the lexicon trained from the full corpus to see what tags they might have. For Swedish, 96% of the unknown words came from inflected classes, and had a single tag; for French the figure was about 60%. In both cases, very few of the unknown words (less than 1%) had more than one tag. This provides some hope that an inflectional analysis might should help considerably with unknown words. For confirmation, the list of French unknown words was given to a French grammarian, who predicted that it would be possible to make a good guess at the correct tag from the morphology for around 70% of the words, and could narrow down the possible tags to two or three for about a further 25%. However, further research is needed to determine how realistic these estimates turn out to be.\\n\\nConclusion\\n\\nWe have shown how a simple experiment in changing the tagset shows that the relationship between tagset size and accuracy is a weak one and is not consistent against languages. This seems to go against the ``folklore'' of the tagging community, where smaller tagsets are often held to be better for obtaining good accuracy. I have suggested that what is important is to choose the tagset required for the application, rather than to optimise it for the tagger. A follow-up to this work might be to apply similar tests in other languages to provide a further confirmation of the results, and to see if language families which similar characteristics can be identified. A further conclusion might be that when a corpus is being tagged by hand, a large tagset should be used, since it can always be reduced to a smaller one if the application demands it. Perhaps the major factor we have to set against this is the danger of introducing more human errors into the manual tagging process, by increasing the cognitive load on the human annotators.\\n\\nBibliography\\n\\nDoug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun (1992). A Practical Part-of-Speech Tagger. In Third Conference on Applied Natural Language Processing. Proceedings of the Conference. Trento, Italy, pages 133-140, Association for Computational Linguistics.\\n\\nDavid Elworthy (1994). Does Baum-Welch Re-estimation Help Taggers? In Fourth Conference on Applied Natural Language Processing. Proceedings of the Conference. Stuttgart, Germany, pages 53-58, Association for Computational Linguistics.\\n\\nW. N. Francis and F. Kucera (1992). Frequency Analysis of English Usage. Houghton Mifflin.\\n\\nRoger Garside, Geoffrey Leech, and Geoffrey Sampson (1987). The Computational Analysis of English: A Corpus-based Approach. Longman, London.\\n\\nElliott Macklovitch (1992). Where the Tagger Falters. In Proceedings of the Fourth Conference on Theoretical and Methodological Issues in Machine Translation, pages 113-126.\\n\\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz (1993). Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330.\\n\\nA. M. McEnery, M. P. Oakes, R. Garside, J. Hutchinson, and G. N. Leech (1994). The Exploitation of Parallel Corpora in Projects ET10/63 and CRATER. In International Conference on New Methods in Language Processing. Proceedings of the Conference, pages 108-115, Centre for Computational Linguistics, UMIST.\\n\\nBernard Merialdo (1994). Tagging English Text with a Probabilistic Model. Computational Linguistics, 20(2):155-171.\\n\\nFootnotes\\n\\nThe English and French corpora were kindly supplied to us by Tony McEnery, and are translation-equivalent. See McEnery et al. (1994) for details. Although the figures here are likely to represent a best case, given how little of the corpora was held out.\", metadata={'source': '../data/raw/cmplg-xml/9504002.xml'}),\n",
       " Document(page_content=\"Cues and control in Expert-Client Dialogues\\n\\nWe conducted an empirical analysis into the relation between control and discourse structure. We applied control criteria to four dialogues and identified 3 levels of discourse structure. We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control. Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not.\\n\\nIntroduction\\n\\nA number of researchers have shown that there is organisation in discourse above the level of the individual utterance (5, 8, 9, 10), The current exploratory study uses control as a parameter for identifying these higher level structures. We then go on to address how conversational participants co-ordinate moves between these higher level units, in particular looking at the ways they use to signal the beginning and end of such high level units.\\n\\nPrevious research has identified three means by which speakers signal information about discourse structure to listeners: Cue words and phrases (5, 10); Intonation (7); Pronominalisation (6, 2). In the cue words approach, Reichman (10) has claimed that phrases like ``because'', ``so'', and ``but'' offer explicit information to listeners about how the speaker's current contribution to the discourse relates to what has gone previously. For example a speaker might use the expression ``so'' to signal that s/he is about to conclude what s/he has just said. Grosz and Sidner (5) relate the use of such phrases to changes in attentional state. An example would be that ``and'' or ``but'' signal to the listener that a new topic and set of referents is being introduced whereas ``anyway'' and ``in any case'' indicate a return to a previous topic and referent set. A second indirect way of signalling discourse structure is intonation. Hirschberg and Pierrehumbert (7) showed that intonational contour is closely related to discourse segmentation with new topics being signalled by changes in intonational contour. A final more indirect cue to discourse structure is the speaker's choice of referring expressions and grammatical structure. A number of researchers (4, 2, 6, 10) have given accounts of how these relate to the continuing, retaining or shifting of focus.\\n\\nThe above approaches have concentrated on particular surface linguistic phenomena and then investigated what a putative cue serves to signal in a number of dialogues. The problem with this approach is that the cue may only be an infrequent indicator of a particular type of shift. If we want to construct a general theory of discourse than we want to know about the whole range of cues serving this function. This study therefore takes a different approach. We begin by identifying all shifts of control in the dialogue and then look at how each shift was signalled by the speakers. A second problem with previous research is that the criteria for identifying discourse structure are not always made explicit. In this study explicit criteria are given: we then go on to analyse the relation between cues and this structure.\\n\\nThe data\\n\\nThe data were recordings of telephone conversations between clients and an expert concerning problems with software. The tape recordings from four dialogues were then transcribed and the analysis conducted on the typewritten transcripts rather than the raw recordings. There was a total of 450 turns in the dialogues.\\n\\n2.1 Criteria for classifying utterance types. Each utterance in the dialogue was classified into one of four categories: (a) Assertions - declarative utterances which were used to state facts. Yes or no answers to questions were also classified as assertions on the grounds that they were supplying the listener with factual information; (b) Commands - utterances which were intended to instigate action in their audience. These included various utterances which did not have imperative form, (e.g. ``What I would do if I were you is to relink X'') but were intended to induce some action; (c) Questions - utterances which were intended to elicit information from the audience. These included utterances which did not have interrogative form. e.g. ``So my question is....'' They also included paraphrases, in which the speaker reformulated or repeated part or all of what had just been said. Paraphrases were classified as questions on the grounds that the effect was to induce the listener to confirm or deny what had just been stated; (d) Prompts - These were utterances which did not express propositional content. Examples of prompts were things like ``Yes'' and ``Uhu''.\\n\\n2.2 Allocation of control in the dialogues. We devised several rules to determine the location of control in the dialogues. Each of these rules related control to utterance type: (a) For questions, the speaker was defined as being in control unless the question directly followed a question or command by the other conversant. The reason for this is that questions uttered following questions or commands are normally attempts to clarify the preceding utterance and as such are elicited by the previous speaker's utterance rather than directing the conversation in their own right. (b) For assertions, the speaker was defined as being in control unless the assertion was made in response to a question, for the same reasons as those given for questions; an assertion which is a response to a question could not be said to be controlling the discourse; (c) For commands, the speaker was defined as controlling the conversation. Indirect commands (i.e. utterances which did not have imperative form but served to elicit some actions) were also classified in this way; (d) For prompts, the listener was defined as controlling the conversation, as the speaker was clearly abdicating his/her turn. In cases where a turn consisted of several utterances, the control rules were only applied to the final utterance.\\n\\nWe applied the control rules and found that control did not alternate from speaker to speaker on a turn by turn basis, but that there were long sequences of turns in which control remained with one speaker. This seemed to suggest that the dialogues were organised above the level of individual turns into phases where control was located with one speaker. The mean number of turns in each phase was 8.03.\\n\\nMechanisms for switching control\\n\\nWe then went on to analyse how control was exchanged between participants at the boundaries of these phases. We first examined the last utterance of each phase on the grounds that one mechanism for indicating the end of a phase would be for the speaker controlling the phase to give some cue that he (both participants in the dialogues were always male) no longer wished to control the discourse. There was a total of 56 shifts of control over the 4 dialogues and we identified 3 main classes of cues used to signal control shifts These were prompts, repetitions and summaries. We also looked at when no signal was given (interruptions).\\n\\n3.1 Prompts. On 21 of the 56 shifts (38%), the utterance immediately prior to the control shift was a prompt. We might therefore explain these shifts as resulting from the person in control explicitly indicating that he had nothing more to say.\\n\\n(In the following examples a line indicates a control shift)\\n\\nExample 1 - Prompt  Dialogue C -\\n\\n3.2 Repetitions and summaries. On a further 15 occasions (27%), we found that the person in control of the dialogue signalled that they had no new information to offer. They did this either by repeating what had just been said (6 occasions), or by giving a summary of what they had said in the preceding utterances of the phase (9 occasions). We defined a repetition as an assertion which expresses part or all of the propositional content of a previous assertion but which contains no new information. A summary consisted of concise reference to the entire set of information given about the client's problem or the solution plan.\\n\\nExample 2\\n\\n\\n\\nRepetition. Dialogue C -\\n\\nHalf the repetitions were accompanied by cue words. These were ``and'', ``well'' and ``so'', which prefixed the assertion.\\n\\nExample 3\\n\\n\\n\\nSummary Dialogue B -\\n\\nWhat are the linguistic characteristics of summaries? Reichman (10) suggests that ``so'' might be a summary cue on the part of the speaker but we found only one example of this, although there were 3 instances of ``and'', one ``now'' one ``but'' and one ``so''. In our dialogues the summaries seemed to be characterised by the concise reference to objects or entities which had earlier been described in detail, e.g. (a) ``Now, I'm wondering how the two are related'' in which ``the two'' refers to the two error messages which it had taken several utterances to describe previously. The other characteristic of summaries is that they contrast strongly with the extremely concrete descriptions elsewhere in the dialogues, e.g. ``err the system program standard call file doesn't complete this means that the file does not have a tail record'' followed by ``And I've no clue at all how to get out of the situation''. Example 3 also illustrates this change from specific (1, 3, 5) to general (7). How then do repetitions and summaries operate as cues? In summarising, the speaker is indicating a natural breakpoint in the dialogue and they also indicate that they have nothing more to add at that stage. Repetitions seem to work in a similar way: the fact that a speaker reiterates indicates that he has nothing more to say on a topic.\\n\\n3.3 Interruptions. In the previous cases, the person controlling the dialogue gave a signal that control might be exchanged. There were 20 further occasions (36% of shifts) on which no such indication is given. We therefore went on to analyse the conditions in which such interruptions occurred. These seem to fall into 3 categories: (a) vital facts; (b) responses to vital facts; (c) clarifications.\\n\\n3.3.1 Vital facts. On a total of 6 occasions (11% of shifts) the client interrupted to contradict the speaker or to supply what seemed to be relevant information that he believed the expert did not know.\\n\\nExample 4  Dialogue C -\\n\\nTwo of these 6 interjections were to supply extra information and one was marked with the cue ``as well''. The other four were to contradict what had just been said and two had explicit markers ``though'' and ``well actually'': the remaining two being direct denials.\\n\\n3.3.2 Reversions of control following vital facts. The next class of interruptions occur after the client has made some interjection to supply a missing fact or when the client has blocked a plan or rejected an explanation that the expert has produced. There were 8 such occasions (14% of shifts).\\n\\nThe interruption in the previous example illustrates the reversion of control to the expert after the client has supplied information which he (the client) believes to be highly relevant to the expert. In the following example, the client is already in control.\\n\\nExample 5  Dialogue B -\\n\\n3.3.3 Clarifications. Participants can also interrupt to clarify what has just been said. This happened on 6 occasions (11%) of shifts.\\n\\nExample 6  Dialogue C -\\n\\nOn two occasions clarifications were prefixed by ``now'' and twice by ``so''. On the final two occasions there was no such marker, and a direct question was used.\\n\\n3.3.4 An explanation of interruptions. We have just described the circumstances in which interruptions occur, but can we now explain why they occur? We suggest the following two principles might account for interruptions: these principles concern: (a) the information upon which the participants are basing their plans, and (b) the plans themselves. (A). Information quality: Both expert and client must believe that the information that the expert has about the problem is true and that this information is sufficient to solve the problem. This can be expressed by the following two rules which concern the truth of the information and the ambiguity of the information: (A1) if the listener believes a fact P and believes that fact to be relevant and either believes that the speaker believes not P or that the speaker does not know P then interrupt; (A2) If the listener believes that the speaker's assertion is relevant but ambiguous then interrupt. (B). Plan quality: Both expert and client must believe that the plan that the expert has generated is adequate to solve the problem and it must be comprehensible to the client.\\n\\nIn this framework, interruptions can be seen as strategies produced by either conversational participant when they perceive that a either principle is not being adhered to.\\n\\n3.4 Cue reliability. We also investigated whether there were occasions when prompts, repetitions and summaries failed to elicit the control shifts we predicted. We considered two possible types of failure: either the speaker could give a cue and continue or the speaker could give a cue and the listener fail to respond. We found no instances of the first case; although speakers did produce phrases like ``OK'' and then continue, the ``OK'' was always part of the same intonational contour as that further information and there was no break between the two, suggesting the phrase was a prefix and not a cue. We did, however, find instances of the second case: twice following prompts and once following a summary, there was a long pause, indicating that the speaker was not ready to respond. We conducted a similar analysis for those cue words that have been identified in the literature. Only 21 of the 35 repetitions, summaries and interruptions had cue words associated with them and there were also 19 instances of the cue words ``now'', ``and'', ``so'', ``but'' and ``well'' occurring without a control shift.\\n\\nControl cues and global control\\n\\nThe analysis so far has been concerned with control shifts where shifts were identified from a series of rules which related utterance type and control. Examination of the dialogues indicated that there seemed to be different types of control shifts: after some shifts there seemed to be a change of topic, whereas for others the topic remained the same. We next went on to examine the relationship between topic shift and the different types of cues and interruptions described earlier. To do this it was necessary first to classify control shifts according to whether they resulted in shifts of topic.\\n\\n4.1 Identifying topic shifts. We identified topic shifts in the following way: Five judges were presented with the four dialogues and in each of the dialogues we had marked where control shifts occurred. The judges were asked to state for each control shift whether it was accompanied by a topic shift. All five judges agreed on 24 of the 56 shifts, and 4 agreed for another 22 of the shifts. Where there was disagreement, the majority judgment was taken.\\n\\n4.2 Topic shift and type of control shift. Analysing each type of control shift, it is clear that there are differences between the cues used for the topic shift and the no shift cases. For interruptions, 90% occur within topic, i.e. they do not result in topic shifts. The pattern is not as obvious for prompts and repetitions/summaries, with 57% of prompts occurring within topic and 67% of repetitions/summaries occurring within topic. This suggests that change of topic is a carefully negotiated process. The controlling participant signals that he is ready to close the topic by producing either a prompt or a repetition/summary and this may or may not be accepted by the other participant. What is apparent is that it is highly unusual for a participant to seize control and change topic by interruption. It seems that on the majority of occasions (63%) participants wait for the strongest possible cue (the prompt) before changing topic.\\n\\n4.3 Other relations between topic and control. We also looked at more general aspects of control within and between topics. We investigated the number of utterances for which each participant was in control and found that there seemed to be organisation in the dialogues above the level of topic. We found that each dialogue could be divided into two parts separated by a topic shift which we labelled the central shift. The two parts of the dialogue were very different in terms of who controlled and initiated each topic. Before the central shift, the client had control for more turns per topic and after it, the expert had control for more turns per topic. The respective numbers of turns client and expert are in control before and after the central shift are :Before 11-7,22-8,12-6,21-6; After 12-33,16-23,2-11,0-5 for the four dialogues. With the exception of the first topic in Dialogues 1 and 4, the client has control of more turns in every topic before the central shift, whereas after it, the expert has control for more turns in every topic. In addition we looked at who initiated each topic, i.e. who produced the first utterance of each topic. We found that in each dialogue, the client initiates all the topics before the central shift, whereas the expert initiates the later ones. We also discovered a close relationship between topic initiation and topic dominance. In 19 of the 21 topics, the person who initiated the topic also had control of more turns.\\n\\nConclusions\\n\\nThe main result of this exploratory study is the finding that control is a useful parameter for identifying discourse structure. Using this parameter we identified three levels of structure in the dialogues: (a) control phases; (b) topic; and (c) global organisation. For the control phases, we found that three types of utterances (prompts, repetitions and summaries) were consistently used to signal control shifts. For the low level structures we identified, (i.e. control phases), cue words and phrases were not as reliable in predicting shifts. This result challenges the claims of recent discourse theories (5, 10) which argue for a the close relation between cue words and discourse structure. We also examined how utterance type related to topic shift and found that few interruptions introduced a new topic. Finally there was evidence for high level structures in these dialogues as evidenced by topic initiation and control, with early topics being initiated and dominated by the client and the opposite being true for the later parts.\\n\\nAnother focus of current research has been the modelling of speaker and listener goals (1, 3) but there has been little research on real dialogues investigating how goals are communicated and inferred. This study identifies surface linguistic phenomena which reflect the fact that participants are continuously monitoring their goals. When plans are perceived as succeeding, participants use explicit cues such as prompts, repetitions and summaries to signal their readiness to move to the next stage of the plan. In other cases, where participants perceive obstacles to their goals being achieved, they resort to interruptions and we have tried to make explicit the rules by which they do this.\\n\\nIn addition our methodology is different from other studies because we have attempted to provide an explanation for whole dialogues rather than fragments of dialogues, and used explicit criteria in a bottom-up manner to identify discourse structures. The number of dialogues was small and taken from a single problem domain. It seems likely therefore that some of our findings (e.g the central shift) will be specific to the diagnostic dialogues we studied. Further research applying the same techniques to a broader set of data should establish the generality of the control rules suggested here.\\n\\nBibliography\\n\\n`=1000\\n\\nAllen, J.F. and Perrault, C.R. (1980). Analyzing intentions in utterances. Artificial Intelligence, 15, 143-178.\\n\\nBrennan, S. E., Friedman, M. W., and Pollard, C. (1987) A centering approach to pronouns. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics.\\n\\nCohen, P. R. and Levesque, H. J. (1985) Speech acts and rationality. In Proceedings of the 23th Annual Meeting of the Association for Computational Linguistics.\\n\\nGrosz, B. J., Joshi, A. K., Weinstein, S. (1986) Towards a computational theory of discourse interpretation. Draft.\\n\\nGrosz, B. J., and Sidner, C. L. (1986) Attentions, intentions and the structure of discourse. Computational Linguistics, 12, 175 - 204.\\n\\nGuindon, R., Sladky, P., Brunner, H., and Conner, J. (1986). The structure of user-adviser dialogues: Is there method in their madness? In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics.\\n\\nHirschberg, J. and Pierrehumbert, J. B. (1986) The intonational structuring of discourse. In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics.\\n\\nLevin, J. A. and Moore, J. A. (1977) Dialogue games: metacommunication structures for natural language interaction. Cognitive Science, 4, 395 - 421.\\n\\nPolanyi, L. and Scha, R. (1983). Connectedness in Sentence, Discourse and Text. Tilburg University, Tilburg, 141-178.\\n\\nReichman, R. (1985) Getting computers to talk like you and me. Cambridge, M.A. : MIT Press.\", metadata={'source': '../data/raw/cmplg-xml/9504006.xml'}),\n",
       " Document(page_content=\"Adapting the Core Language Engine to French and Spanish\\n\\nWe describe how substantial domain-independent language-processing systems for French and Spanish were quickly developed by manually adapting an existing English-language system, the SRI Core Language Engine. We explain the adaptation process in detail, and argue that it provides a fairly general recipe for converting a grammar-based system for English into a corresponding one for a Romance language.\\n\\nIntroduction\\n\\nThe syntactic rule-set for French covers nearly all the basic constructions of the language, including the following: declarative, interrogative and imperative clauses; formation of YN and WH-questions using inversion, complex inversion and ``est-ce que''; clitic pronouns; adverbial modification; negation; nominal and verbal PPs; complements to `tre'' and ``il y a''; relative clauses, including those with ``dont''; partitives, including use of ``en''; passives; pre- and post-nominal adjectival modification, including comparative and superlative; code expressions; sentential complements and embedded questions; complex determiners; numerical expressions; date and time expressions; conjunction of most major constituents; and a wide variety of verb types, including modals and reflexives. There is a good treatment of inflectional morphology which includes all major paradigms. The coverage of the Spanish grammar is comparable in scope, though slightly less extensive. The French and Spanish versions of the CLE are both ``reversible'', and can be used for either analysis or generation.\\n\\nWe will describe the adaptation process in detail, and argue that it provides a fairly general recipe for converting a grammar-based system for English into a corresponding one for a Romance language. Due to space limitations, and since it is rather the better of the two, we will concentrate on the French version. Examples will be taken from the Air Travel Planning (ATIS) domain used in the current SLT prototype.\\n\\nOverview of the Core Language Engine\\n\\nThe following two sections describe in detail the issues pertaining to the morphology and syntax rule-sets respectively.\\n\\nMorphology and spelling\\n\\nThe total number of rules required to describe inflectional morphology was around 75 for French and 50 for Spanish (inter-word rules being responsible for much of the difference). We concentrate here on the French phenomena, which are more complex.\\n\\nIntra\\n\\n\\n\\nword spelling changes\\n\\nLess trivial problems, however, arise from the fact that spelling changes in French generally cannot be predicted from the surface form of the word alone. This means the application of the rules must be controlled; we do this by specifying feature constraints, which must match between the rule and all morphemes it applies to. The following extended example describes our treatment of one of the most challenging cases.\\n\\nInter\\n\\n\\n\\nword spelling changes\\n\\nFrench syntax\\n\\nWhen comparing the French and English grammars, there are two types of objects of immediate interest: syntax rules and features. Looking first at the rules themselves, about 80% of the French syntax rules are either identical with or very similar to the English counterparts from which they have been adapted. Of the remainder, some rules (e.g those for date, time and number expressions) are different, but essentially too trivial to be worth describing in detail. Similar considerations apply to features.\\n\\nQuestion\\n\\n\\n\\nformation\\n\\nWe start this section by briefly reviewing the way in which question-formation is handled in the English CLE grammar. There are two main dimensions of classification: questions can be either WH- or Y-N; and they can use either the inverted or the uninverted word-order. Y-N questions must use the inverted word-order, but both word-orders are permissible for WH-questions. The phrase-structure rules analyse an inverted WH-question as constituting a fronted WH+ element followed by an inverted clause containing a gap element. The feature inv distinguishes inverted from uninverted clauses. The following examples illustrate the top-level structure of Y-N, unmoved WH- and moved WH-questions respectively. [Does he love Mary]\\n\\nS:[inv=y]\\n\\n[Who loves Mary]\\n\\nS:[inv=n] [[Whom]NP [does he love []NP]\\n\\nS:[inv=y]]\\n\\nThe French rules for question formation are structurally fairly similar to the English ones. However, there are several crucial differences which mean that the constructions in the two languages often differ widely at the level of surface form. Two phenomena in particular stand out. Firstly, English only permits subject-verb inversion when the verb is an auxiliary, or a form of ``have'' or ``be''; in contrast, French potentially allows subject-verb inversion with any verb. For this reason, English question-formation using auxiliary ``do'' lacks a corresponding construction in French.\\n\\nSecondly, French permits two other common question-formation constructions in addition to subject-verb inversion: prefacing the declarative version of the clause with the question particle ``est-ce que'', and ``complex inversion'', i.e. fronting the subject and inserting a dummy pronoun after the inverted verb. In certain circumstances, primarily if the subject is the pronoun `a'', it is also possible to form a non-subject WH-question out of a fronted WH+ phrase followed by an uninverted clause containing an appropriate gap. We refer to this last possibility as ``pseudo-inversion''.\\n\\nModification of the English syntax rules to capture the basic requirements so far is quite simple. In our grammar, we added three extra rules to cover the ``est-ce que'', complex-inversion and pseudo-inversion constructions: the second of these rules combines the complex-inverted verb with the following dummy pronoun to form a verb, in essence treating the dummy pronoun as a kind of verbal affix. A further rule deals with the hyphen linking an inverted verb with a following subject.\\n\\nWith regard to the feature-set, the critical change involves the inv feature. In English, as we saw, this feature had two possible values, y and n. In French, the corresponding feature has five values: inverted, uninverted, est_ce_que, complex and pseudo, distinguishing clauses formed using the different question-formation constructions. (It is important to note, though, that the semantic representation of the clause is the same irrespective of its inversion-type). To enforce the restrictions concerning combinations of inversion-type and subject form, we also added a new clausal feature which distinguished clauses in which the subject is a pronoun.\\n\\nThe attractive aspect of this treatment is that the remaining English rules used for question-formation can be retained more or less unchanged. In particular, the English semantic rules can still be used, and produce QLF representations with similar form.\\n\\nIt would almost be true to claim that the above constituted our entire treatment of French question-formation. In practice, we have found it desirable to add a few more features to the grammar in order to block infelicitous combinations of the inversion rules with certain commonly occurring lexical items. It is possible that the effect of these features could be achieved equally well by statistical modelling or other means, but we describe them here for completeness: Restrictions on use of ``est-ce que'': Question-formation with ``est-ce que'' is strongly dispreferred when the main verb is a clause-final occurrence of `tre'', or existential ``avoir'' (as in ``il y a''). For example: ?Quand est-ce que le prochain vol est? ?Combien de vols est-ce qu'il y a? We enforce this by adding a suitable feature to the verb category. Fronting of ``heavy'' NPs: Most languages prefer not to front ``heavy'' NPs, and this dispreference is particularly strong in French. We have consequently added an NP feature called heavy, which has the value y on NPs containing PP and VP post-modifiers. Thus for example generation of Quels vols en partance de Dallas y a-t-il? is blocked, but the preferable Quels vols y a-t-il en partance de Dallas? is permitted. Inverted subject NPs: Occurrence of some pronouns (in particular ``cela'', and `a'') is strongly dispreferred in inverted subject position. A binary feature enforces this as a rule, for example blocking Combien cote a pour aller  Boston?\\n\\nClitics\\n\\nAlthough the underlying framework is very different from the HPSG formalism used by Miller and Sag, our basic idea is the same: to treat ``clitic movement'' by a mechanism similar to the one used to handle WH movement. More specifically, we introduce two sets of new rules. The first set handles the ``surface'' clitics. They define the structure of the verb/clitic complex, which we, like Estival, regard as a constituent of category V composed of a main verb and a ``clitic-list''. A second set of ``gap'' rules defines empty constituents of category NP or PP, occurring at the notional ``deep'' positions occupied by the clitics. Thus, for example, on our account the constituent structure of ``Est-ce que vous le voulez?'' will be [Est-ce que [vousNP [le voulez]V []NP]S]Swhere the ``gap'' NP category represents the notional direct object of ``voulez'', realised at surface level by the pre-verbal clitic ``le''.\\n\\nTo make this work, we add an extra feature, clitics, to all categories which can participate in clitic movement: in our grammar, these are V, VP, S, NP and PP. The clitics feature is used to link the cliticised V constituent and its associated clitic gap or gaps. We have found it convenient to define the value of the clitics feature to be a bundle of five separate sub-features, one for each of the five possible clitic-positions in French. Thus for instance the second-position clitics ``le'', ``la'' and ``les'' are related to object-position clitic gaps through the second sub-feature of clitics; the fourth-position ``y'' clitic is related to its matching PP gap through the fourth sub-feature; and so on. The linking relation between a clitic-gap and its associated clitic is formally exactly the same as that obtaining between a WH-gap and its associated antecedent, and can if desired be conceptualized as a type of coindexing.\\n\\nThe clitics feature-bundle is threaded through the grammar rule which defines the structure of the list of clitics associated with a cliticised verb, and enforces the constraints on ordering of surface clitics. These constraints are encoded in the lexical entry for each clitic.\\n\\nThis basic framework is fairly straight-forward, though a number of additional features need to be added in order to capture the syntactic facts. We summarize the main points: Position of surface clitics: Clitics occur post-verbally in positive imperative clauses, otherwise pre-verbally. The clitic-list constituent consequently needs to share suitable features with the verb it combines with. Surface form of clitics: The first- and second-person singular clitics are realised differently depending on whether they occur pre- or post-verbally: for example ``Vous me rservez un vol'' versus ``Rservez-moi un vol''. Moreover, ``me'' and ``te'' are first-position clitics (e.g. ``Vous me les donnez''), while ``moi'' and ``toi'' are third-position (``Donnez-les-moi''). This alternation is achieved simply by having separate lexical entries for each form. The entries have different syntactic features, but a common semantic representation. Special problems with the ``en'' clitic: The most abstruse problems occur in connection with the ``en'' clitic, and are motivated by sentences like Combien en avez-vous? Here, our framework seems to dictate a constituent structure including three gaps, viz: [Combien [[en avez]V [vousNP [[]V [[]NP []PP]NP]]S]S]Sin which the V gap links to ``avez'', the NP gap to ``combien'', and the PP gap to ``en''. The specific difficulty here is that the ``en'' PP gap ends up as an NP modifier (it modifies the NP gap).\\n\\nAgreement\\n\\nAlthough grammatical agreement is a linguistic phenomenon that plays a considerably larger role in French than in English, the adjustments needed to the lexicon and syntax rules are usually obvious. For instance, a feature has to be added to the both daughters of the rule for pre-nominal adjectival modification, to enforce agreement in number and gender. In nearly all cases, this same procedure is used. A feature called agr is added to the relevant categories, whose value is a bundle representing the category's person, number and gender, and the agr feature is shared between the categories which are required to agree.\\n\\nThere are however some instances where agreement is less trivial. For example, the subject and nominal predicate complement of `tre'' may occasionally fail to agree in gender, e.g. La gare est le plus grand btiment de la ville. However, if the predicate complement is a pronoun (``lequel'', ``celui-ci'', ``quel''...)  agreement in both gender and number is obligatory: thus for instance Quel/*quelle/*quels est le premier vol. It would be most unpleasant to duplicate the syntax rules, with separate versions for the pronominal and non-pronominal cases. Instead, we add a second agreement feature (compagr) to the NP category, which is constrained to have the same value as agr on pronominal NPs; subject/predicate agreement can then use the compagr feature on the predicate, getting the desired behaviour.\\n\\nSpanish syntax\\n\\nConclusions\\n\\nThe preceding sections describe in essence all the changes we needed to make in order to adapt a substantial English language processing system to French and Spanish. Due to space limitations, we have been obliged to present some of the details in a more compressed form than we would ideally have wished, but nothing important has been omitted. Creation of a good initial French version required about five person-months of effort; after this, the Spanish version took only about two person-months. We do not believe that we were greatly aided by any special features of the Core Language Engine, other than the fact that it is a well-engineered piece of software based on sound linguistic ideas. Our overall conclusion is that an English-language system conforming to these basic design principles should in general be fairly easy to port to Romance languages.\\n\\nAcknowledgements\\n\\nThe work described here was supported by SRI International, Suissetra, and Telia Research AB, Sweden. We would like to acknowledge the assistance provided by Gabriela Fernandez of the University of Seville in developing the Spanish version of the system, and thank Sabine Lehmann, David Milward and Steve Pulman for helpful comments.\\n\\nBibliography\\n\\nAgns, M-S., Alshawi, H., Bretan, I., Carter, D.M.,  Ceder, K., Collins, M., Crouch, R., Digalakis, V., Ekholm, B., Gambck, B., Kaja, J., Karlgren, J., Lyberg, B., Price, P., Pulman, S., Rayner, M., Samuelsson, C. and Svensson, T. 1994. Spoken Language Translator: First Year Report. SRI technical report CRC-043\\n\\nAlshawi, H. (ed.)\\n\\n1992.\\n\\nThe Core Language Engine.\\n\\nMIT Press.\\n\\nAlshawi, H. Carter, D., Crouch, R., Pulman, S., Rayner, M. and Smith, A. 1992. CLARE: A Contextual Reasoning and Cooperative Response Framework for the Core Language Engine SRI technical report CRC-028.\\n\\nAlshawi, H., and Carter, D. 1994. Training and Scaling Preference Functions for Disambiguation. Computational Linguistics, 20:4.\\n\\nAlshawi, H. and Crouch, R. 1992. ``Monotonic Semantic Interpretation''. Proceedings of 30th ACL.\\n\\nBs, G. and Gardent, C. 1989. French Order without Order. Proceedings of 4th European ACL.\\n\\nCarter, D. 1995. Rapid Development of Morphological Descriptions for Full Language Processing Systems. Proceedings of 7th European ACL. Also SRI Technical Report CRC-047\\n\\nEstival, D. Generating French with a Reversible Unification Grammar. 1990. Proceedings of 13th COLING.\\n\\nGrimshaw, J. 1982. On the Lexical Representation of Romance Reflexives. In J. Bresnan (ed. ), The Mental Representation of Grammatical Relations. MIT Press.\\n\\nKaplan, R., and Kay, M. 1994. Regular Models of Phonological Rule Systems. Computational Linguistics, 20:3, 331-378.\\n\\nMiller, P. and Sag, I. 1995. French Clitic Movement Without Clitics or Movement. CSLI Technical Report.\\n\\nRayner, M. 1993. Abductive Equivalential Translation and its Application to Natural Language Database Interfacing. PhD thesis, Royal Institute of Technology/Stockholm University. Also SRI Technical Report CRC-052\\n\\nRayner, M., Alshawi, H., Bretan, I., Carter, D.M., Digalakis, V., Gambck, B., Kaja, J., Karlgren, J., Lyberg, B., Price, P., Pulman, S. and Samuelsson, C. 1993. A Speech to Speech Translation System Built From Standard Components. Proc. 1st ARPA workshop on Human Language Technology. Also SRI Technical Report CRC-031.\\n\\nRayner, M. and Bouillon, P. 1995. Hybrid Transfer in an English-French Spoken Language Translator. Proceedings of IA '95, Montpellier, France. Also SRI Technical Report CRC-056.\\n\\nFootnotes\\n\\nMost French grammars regard ``quel'' as an adjective, but for semantic reasons we have found it more convenient to treat it as a pronoun in this type of construction and as a determiner in expression like ``quel vol''. All SRI Cambridge technical reports are available through WWW from http://www.cam.sri.com\", metadata={'source': '../data/raw/cmplg-xml/9605015.xml'}),\n",
       " Document(page_content=\"LHIP: Extended DCGs for Configurable Robust Parsing\\n\\nWe present LHIP, a system for incremental grammar development using an extended DCG formalism. The system uses a robust island-based parsing method controlled by user-defined performance thresholds. Keywords: DCG, head, island parsing, robust parsing, Prolog\\n\\nLHIP Overview\\n\\nThis paper describes LHIP (Left-Head corner Island Parser), a parser designed for broad-coverage handling of unrestricted text. The system interprets an extended DCG formalism to produce a robust analyser that finds parses of the input made from `islands' of terminals (corresponding to terminals consumed by successful grammar rules). It is currently in use for processing dialogue transcripts from the HCRC Map Task Corpus (Anderson et al., 1991), although we expect its eventual applications to be much wider. Transcribed natural speech contains a number of frequent characteristic `ungrammatical' phenomena:  filled pauses, repetitions, restarts, etc. (as in e.g. Right I'll have ...you know, like I'll have to ...so I'm going between the picket fence and the mill, right.). While a full analysis of a conversation might well take these into account, for many purposes they represent a significant obstacle to analysis. LHIP provides a processing method which allows selected portions of the input to be ignored or handled differently.\\n\\nThe chief modifications to the standard Prolog `grammar rule' format are of two types:  one or more right-hand side (RHS) items may be marked as `heads', and one or more RHS items may be marked as `ignorable'. We expand on these points and introduce other differences below.\\n\\nThe behaviour of LHIP can best be understood in terms of the notions of island, span, cover and threshold:\\n\\nIsland: Within an input string consisting of the terminals\\n\\n, an island is a subsequence\\n\\n, whose length is m + 1.\\n\\nSpan:\\n\\nThe span of a grammar rule R is the length of the longest island\\n\\nsuch that terminals ti and tj are both consumed (directly or indirectly) by R.\\n\\nCover: A rule R is said to cover m items if m terminals are consumed within the island described by R.  The coverage of R is then m.\\n\\nThreshold: The threshold of a rule is the minimum value for the ratio of its coverage c to its span s which must hold in order for the rule to succeed. Note that\\n\\n, and that if c = s the rule has completely covered the span, consuming all terminals.\\n\\nAs implied here, rules need not cover all of the input in order to succeed. More specifically, the constraints applied in creating islands are such that islands do not have to be adjacent, but may be separated by non-covered input. Moreover, an island may itself contain input which is unaccounted for by the grammar. Islands do not overlap, although when multiple analyses exist they will in general involve different segmentations of the input into islands.\\n\\nThere are two notions of non-coverage of the input:  sanctioned and unsanctioned non-coverage. The latter case arises when the grammar simply does not account for some terminal. Sanctioned non-coverage means that some number of special `ignore' rules have been applied which simulate coverage of input material lying between the islands, thus in effect making the islands contiguous. Those parts of the input that have been `ignored' are considered to have been consumed. These ignore rules can be invoked individually or as a class. It is this latter capability which distinguishes ignore rules from regular rules, as they are functionally equivalent otherwise, mainly serving as a notational aid for the grammar writer.\\n\\nStrict adjacency between RHS clauses can be specified in the grammar. It is possible to define global and local thresholds for the proportion of the spanned input that must be covered by rules; in this way, the user of an LHIP grammar can exercise quite fine control over the required accuracy and completeness of the analysis.\\n\\nA chart is kept of successes and failures of rules, both to improve efficiency and to provide a means of identifying unattached constituents. In addition, feedback is given to the grammar writer on the degree to which the grammar is able to cope with the given input; in a context of grammar development, this may serve as notification of areas to which the coverage of the grammar might next be extended.\\n\\nThe notion of `head' employed here is connected more closely with processing control than linguistics. In particular, nothing requires that a head of a rule should share any information with the LHS item, although in practice it often will. Heads serve as anchor-points in the input string around which islands may be formed, and are accordingly treated before non-head items (RHS items are re-ordered during compilation-see below). In the central role of heads, LHIP resembles parsers devised by Kay (1989) and van Noord (1991); in other respects, including the use which is made of heads, the approaches are rather different, however.\\n\\nThe LHIP System\\n\\nIn this section we describe the LHIP system. First, we define what constitutes an acceptable LHIP grammar, second, we describe the process of converting such a grammar into Prolog code, and third, we describe the analysis of input with such a grammar.\\n\\nLHIP grammars are an extended form of Prolog DCG grammars. The extensions can be summarized as follows: 1. one or more RHS clauses may be nominated as heads; 2. one or more RHS clauses may be marked as optional; 3. `ignore' rules may be invoked; 4. adjacency constraints may be imposed between RHS clauses; 5. a global threshold level may be set to determine the minimum fraction of spanned input that may be covered in a parse, and 6. a local threshold level may be set in a rule to override the global threshold within that rule. We provide a syntactic definition (below) of a LHIP grammar rule, using a notation with syntactic rules of the form\\n\\nwhich indicates that the category C may take any of the forms F1 to Fn. An optional item in a form is denoted by surrounding it with square brackets `[...]'. Syntactic categories are italicised, while terminals are underlined: `\\n\\n'.\\n\\nA LHIP grammar rule has the form:\\n\\nlhiprule\\n\\n[\\n\\n] term [\\n\\n]\\n\\nlhipbody\\n\\nwhere T is a value between zero and one. If present, this value defines the local threshold fraction for that rule.\\n\\nThis local threshold value overrules the global threshold. The symbol `\\n\\n' before the name of a rule marks it as being an `ignore' rule. Only a rule defined this way can be invoked as an ignore rule in an RHS clause.\\n\\nlhipbody\\n\\nlhipclause\\n\\nThe connectives `,' and `;' have the same precedence as in Prolog, while `:' has the same precedence as `,'. Parentheses may be used to resolve ambiguities. The connective `,' is used to indicate that strings subsumed by two RHS clauses are ordered but not necessarily adjacent in the input. Thus `A , B' indicates that A precedes B in the input, perhaps with some intervening material. The stronger constraint of immediate precedence is marked by `:'; `A : B' indicates that the span of A precedes that of B, and that there is no uncovered input between the two. Disjunction is expressed by `;', and optional RHS clauses are surrounded by `(?...? )'.\\n\\nlhipclause\\n\\nterm\\n\\nThe symbol `*' is used to indicate a head clause. A rule name is a Prolog term, and only rules and terminal items may act as heads within a rule body. The symbol `@' introduces a terminal string. As previously said, the purpose of ignore rules is simply to consume input terminals, and their intended use is in facilitating repairs in analysing input that contains the false starts, restarts, filled pauses, etc. mentioned above. These rules are referred to individually by preceding their name by the `\\n\\n' symbol. They can also be referred to as a class in a rule body by the special RHS clause `\\n\\n'. If used in a rule body, they indicate that input is potentially ignored-the problems that ignore rules are intended to repair will not always occur, in which case the rules succeed without consuming any input. There is a semantic restriction on the body of a rule which is that it must contain at least one clause which necessarily covers input (optional clauses and ignore rules do not necessarily cover input).\\n\\nThe following is an example of a LHIP rule. Here, the sub-rule `conjunction(Conj)' is marked as a head and is therefore evaluated before either of `s(Sl)' or `s(Sr)':\\n\\ns(conjunct(Conj, Sl, Sr))  ]\\n\\ns(Sl),\\n\\n\\n\\nconjunction(Conj),\\n\\ns(Sr).\\n\\nHow is such a rule converted into Prolog code by the LHIP system? First, the rule is read and the RHS clauses are partitioned into those marked as heads, and those not. A record is kept of their original ordering, and this record allows each clause to be constrained with respect to the clause that precedes it, as well as with respect to the next head clause which follows it. Additional code is added to maintain a chart of known successes and failures of each rule. Each rule name is turned into the name of a Prolog clause, and additional arguments are added to it. These arguments are used for the input, the start and end points of the area of the input in which the rule may succeed, the start and end points of the actual part of the input over which it in fact succeeds, the number of terminal items covered within that island, a reference to the point in the chart where the result is stored, and a list of pointers to sub-results. The converted form of the above rule is given below (minus the code for chart maintenance):\\n\\nThe important points to note about this converted form are the following:\\n\\n1. the conjunction clause is searched for before either of the two s clauses;\\n\\n2. the region of the input to be searched for the conjunction clause is the same as that for the rule's LHS (B-C): its island extends from O to P and covers Q items;\\n\\n3. the search region for the first s clause is B-O (i.e. from the start of the LHS search region to the start of the conjunction island), its island starts at D and covers T items;\\n\\n4. the search region for the second s clause is P-C (i.e. from the end of the conjunction island to the end of the LHS search region), its island ends at E and covers U items;\\n\\n5. the island associated with the rule as a whole extends from D to E and covers F items, where F is U + Q + T;\\n\\n6. lhip_threshold_value/1 unifies its argument M with the current global threshold value.\\n\\nIn the current implementation of LHIP, compiled rules are interpreted depth-first and left-to-right by the standard Prolog theorem-prover, giving an analyser that proceeds in a top-down, `left-head-corner' fashion. Because of the reordering carried out during compilation, the situation regarding left-recursion is slightly more subtle than in a conventional DCG. The `s(conjunct(...))' rule shown above is a case in point. While at first sight it appears left-recursive, inspection of its converted form shows its true leftmost subrule to be `conjunction'. Naturally, compilation may induce left-recursion as well as eliminating it, in which case LHIP will suffer from the same termination problems as an ordinary DCG formalism interpreted in this way. And as with an ordinary DCG formalism, it is possible to apply different parsing methods to LHIP in order to circumvent these problems (see e.g. Pereira and Shieber, 1987). A related issue concerns the interpretation of embedded Prolog code. Reordering of RHS clauses will result in code which precedes a head within a LHIP rule being evaluated after it; judicious freezing of goals and avoidance of unsafe cuts are therefore required.\\n\\nLHIP provides a number of ways of applying a grammar to input. The simplest allows one to enumerate the possible analyses of the input with the grammar. The order in which the results are produced will reflect the lexical ordering of the rules as they are converted by LHIP. With the threshold level set to 0, all analyses possible with the grammar by deletion of input terminals can be generated. Thus, supposing a suitable grammar, for the sentence John saw Mary and Mark saw them there would be analyses corresponding to the sentence itself, as well as John saw Mary, John saw Mark, John saw them, Mary saw them, Mary and Mark saw them, etc.\\n\\nBy setting the threshold to 1, only those partial analyses that have no unaccounted for terminals within their spans can succeed. Hence, Mark saw them would receive a valid analysis, as would Mary and Mark saw them, provided that the grammar contains a rule for conjoined NPs; John saw them, on the other hand, would not. As this example illustrates, a partial analysis of this kind may not in fact correspond to a true sub-parse of the input (since Mary and Mark was not a conjoined subject in the original). Some care must therefore be taken in interpreting results.\\n\\nA number of built-in predicates are provided which allow the user to constrain the behaviour of the parser in various ways, based on the notions of coverage, span and threshold:\\n\\n=1=2em lhip_phrase(+C,+S) Succeeds if the input S can be parsed as an instance of category C.\\n\\n=1=2em lhip_cv_phrase(+C,+S) As for lhip_phrase/2, except that all of the input must be covered.\\n\\n=1=2em lhip_phrase(+C,+S,-B,-E,-Cov) As for lhip_phrase/2, except that B binds to the beginning of the island described by this application of C, E binds to the position immediately following the end, and Cov binds to the number of terminals covered.\\n\\n=1=2em lhip_mc_phrases(+C,+S,-Cov,-Ps) The maximal coverage of S by C is Cov. Ps is the set of parses of S by C with coverage Cov.\\n\\n=1=2em lhip_minmax_phrases(+C,+S,-Cov,-Ps) As for lhip_mc_phrases/4, except that Ps is additionally the set of parses with the least span.\\n\\n=1=2em lhip_seq_phrase(+C,+S,-Seq) Succeeds if Seq is a sequence of one or more parses of S by C such that they are non-overlapping and each consumes input that precedes that consumed by the next.\\n\\n=1=2em lhip_maxT_phrases(+C,+S,-MaxT) MaxT is the set of parses of S by C that have the highest threshold value. On backtracking it returns the set with the next highest threshold value.\\n\\nIn addition, other predicates can be used to search the chart for constituents that have been identified but have not been attached to the parse tree. These include:\\n\\n=1=2em lhip_success Lists successful rules, indicating island position and coverage.\\n\\n=1=2em lhip_ms_success As for lhip_success, but lists only the most specific successful rules (i.e. those which have themselves succeeded but whose results have not been used elsewhere).\\n\\n=1=2em lhip_ms_success(N) As for lhip_ms_success, but lists only successful instances of rule N.\\n\\nThe conversion of the grammar into Prolog code means that the user of the system can easily develop analysis tools that apply different constraints, using the tools provided as building blocks.\\n\\nUsing LHIP\\n\\nAs previously mentioned, LHIP facilitates a cyclic approach to grammar development. Suppose one is writing an English grammar for the Map Task Corpus, and that the following is the first attempt at a rule for noun phrases (with appropriate rules for determiners and nouns):\\n\\nnp(N, D, A) # 0.5\\n\\ndeterminer(D),\\n\\n\\n\\nnoun(N).\\n\\nWhile this rule will adequately analyse simple NPs such as your map, or a missionary camp, on a NP such as the bottom right-hand corner it will give analyses for the bottom, the right-hand and the corner. Worse still, in a long sentence it will join determiners from the start of the sentence to nouns that occur in the latter half of the sentence. The number of superfluous analyses can be reduced by imposing a local threshold level, of say 0.5. By looking at the various analyses of sentences in the corpus, one can see that this rule gives the skeleton for noun phrases, but from the fraction of coverage of these parses one can also see that it leaves out an important feature, adjectives, which are optionally found in noun phrases.\\n\\nnp(N, D, A) # 0.5\\n\\ndeterminer(D), \\t\\t(? adjectives(A) ? ),\\n\\n\\n\\nnoun(N).\\n\\nWith this rule, one can now handle such phrases as the left-hand bottom corner, and a banana tree. Suppose further that this rule is now applied to the corpus, and then the rule is applied again but with a local threshold level of 1. By looking at items parsed in the first case but not in the second, one can identify features of noun phrases found in the corpus that are not covered by the current rules. This might include, for instance, phrases of the form a slightly dipping line. One can then go back to the grammar and see that the noun phrase rule needs to be changed to account for certain types of modifier including adjectives and adverbial modifiers.\\n\\nIt is also possible to set local thresholds dynamically, by making use of the `{prolog code}' facility:\\n\\nnp(N, D, A) # T\\n\\ndeterminer(D), \\t\\t(? adjectives(A) ? ),\\n\\n\\n\\nnoun(N),\\n\\n{set_dynamic_threshold(A,T)}.\\n\\nIn this way, the strictness of a rule may be varied according to information originating either within the particular run-time invocation of the rule, or elsewhere in the current parse. For example, it would be possible, by providing a suitable definition for set_dynamic_threshold/2, to set T to 0.5 when more than one optional adjective has been found, and 0.9 otherwise.\\n\\nOnce a given rule or set of rules is stable, and the writer is satisfied with the performance of that part of the grammar, a local threshold value of 1 may be assigned so that superfluous parses will not interfere with work elsewhere.\\n\\nThe use of the chart to store known results and failures allows the user to develop hybrid parsing techniques, rather than relying on the default depth-first top-down strategy given by analysing with respect to the top-most category. For instance, it is possible to analyse the input in `layers' of linguistic categories, perhaps starting by analysing noun-phrases, then prepositions, verbs, relative clauses, clauses, conjuncts, and finally complete sentences. Such a strategy allows the user to perform processing of results between these layers, which can be useful in trying to find the `best' analyses first.\\n\\nPartial results\\n\\nThe discussion of built-in predicates mentioned facilities for recovering partial parses. Here we illustrate this process, and indicate what further use might be made of the information thus obtained.\\n\\nIn the following example, the chart is inspected to reveal what constituents have been built during a failed parse of the truncated sentence Have you the tree by the brook that...:\\n\\n] lhip_phrase(s(S),\\n\\n[have,you,the,tree,by,the,brook,that]).\\n\\nno\\n\\n] lhip_success.\\n\\n(\\n\\n\\n\\n1) [7-\\n\\n\\n\\n8) /1 ~~] @brook\\n\\n(\\n\\n\\n\\n1) [5-\\n\\n\\n\\n6) /1 ~~] @by\\n\\n(\\n\\n\\n\\n1) [1-\\n\\n\\n\\n2) /1 ~~] @have\\n\\n(\\n\\n\\n\\n1) [8-\\n\\n\\n\\n9) /1 ~~] @that\\n\\n(\\n\\n\\n\\n1) [3-\\n\\n\\n\\n4) /1 ~~] @the\\n\\n(\\n\\n\\n\\n1) [6-\\n\\n\\n\\n7) /1 ~~] @the\\n\\n(\\n\\n\\n\\n1) [4-\\n\\n\\n\\n5) /1 ~~] @tree\\n\\n(\\n\\n\\n\\n1) [2-\\n\\n\\n\\n3) /1 ~~] @you\\n\\n(4) [2-\\n\\n\\n\\n8) /4 ~~] np(nppp(you,pp(by,np(the,brook,B))))\\n\\n(4) [3-\\n\\n\\n\\n8) /5 ~~] np(nppp(np(the,tree,C),pp(by,np(the,brook,D))))\\n\\n(5) [3-\\n\\n\\n\\n8) /2 ~~] np(np(the,brook,A))\\n\\n(5) [6-\\n\\n\\n\\n8) /2 ~~] np(np(the,brook,G))\\n\\n(5) [3-\\n\\n\\n\\n5) /2 ~~] np(np(the,tree,E))\\n\\n(7) [4-\\n\\n\\n\\n5) /1 ~~] noun(tree)\\n\\n(8) [7-\\n\\n\\n\\n8) /1 ~~] noun(brook)\\n\\n(9) [2-\\n\\n\\n\\n3) /1 ~~] np(you)\\n\\n(10) [5-\\n\\n\\n\\n8) /3 ~~] pp(pp(by,np(the,brook,F)))\\n\\n(11) [3-\\n\\n\\n\\n4) /1 ~~] det(the)\\n\\n(11) [6-\\n\\n\\n\\n7) /1 ~~] det(the)\\n\\nyes\\n\\nEach rule is listed with its identifier (`-1' for lexical rules), the island which it has analysed with beginning and ending positions, its coverage, and the representation that was constructed for it. From this output it can be seen that the grammar manages reasonably well with noun phrases, but is unable to deal with questions (the initial auxiliary have remains unattached).\\n\\nUsers will often be more interested in the successful application of rules which represent maximal constituents. These are displayed by lhip_ms_success:\\n\\n] lhip_ms_success.\\n\\n(\\n\\n\\n\\n1) [1-\\n\\n\\n\\n2) /1 ~~] @have\\n\\n(\\n\\n\\n\\n1) [8-\\n\\n\\n\\n9) /1 ~~] @that\\n\\n(4) [2-\\n\\n\\n\\n8) /4 ~~] np(nppp(you,pp(by,np(the,brook,J))))\\n\\n(4) [3-\\n\\n\\n\\n8) /5 ~~] np(nppp(np(the,tree,H),pp(by,np(the,brook,I))))\\n\\n(5) [3-\\n\\n\\n\\n8) /2 ~~] np(np(the,brook,K))\\n\\nyes\\n\\nHere, two unattached lexical items have been identified, together with two instances of rule 4, which combines a NP with a postmodifying PP. The first of these has analysed the island you the tree by the brook, ignoring the tree, while the second has analysed the tree by the brook, consuming all terminals. There is a second analysis for the tree by the brook, due to rule 5, which has been obtained by ignoring the sequence tree by the. From this information, a user might wish to rank the three results according to their respective span:coverage ratios, probably preferring the second.\\n\\nDiscussion\\n\\nThe ability to deal with large amounts of possibly ill-formed text is one of the principal objectives of current NLP research. Recent proposals include the use of probabilistic methods (see e.g. Briscoe and Carroll, 1993) and large robust deterministic systems like Hindle's Fidditch (Hindle, 1989). Experience so far suggests that systems like LHIP may in the right circumstances provide an alternative to these approaches. It combines the advantages of Prolog-interpreted DCGs (ease of modification, parser output suitable for direct use by other programs, etc.) with the ability to relax the adjacency constraints of that formalism in a flexible and dynamic manner.\\n\\nLHIP is based on the assumption that partial results can be useful (often much more useful than no result at all), and that an approximation to complete coverage is more useful when it comes with indications of how approximate it is. This latter point is especially important in cases where a grammar must be usable to some degree at a relatively early stage in its development, as is, for example, the case with the development of a grammar for the Map Task Corpus. In the near future, we expect to apply LHIP to a different problem, that of defining a restricted language for specialized parsing.\\n\\nThe rationale for the distinction between sanctioned and unsanctioned non-coverage of input is twofold. First, the `ignore' facility permits different categories of unidentified input to be distinguished. For example, it may be interesting to separate material which occurs at the start of the input from that appearing elsewhere. Ignore rules have a similar functionality to that of normal rules. In particular, they can have arguments, and may therefore be used to assign a structure to unidentified input so that it may be flagged as such within an overall parse. Secondly, by setting a threshold value of 1, LHIP can be made to perform like a standardly interpreted Prolog DCG, though somewhat more efficiently due to the use of the chart.\\n\\nA number of possible extensions to the system can be envisaged. Whereas at present each rule is compiled individually, it would be preferable to enhance preprocessing in order to compute certain kinds of global information from the grammar. One improvement would be to determine possible linking of `root-to-head' sequences of rules, and index these to terminal items for use as an oracle during analysis. A second would be to identify those items whose early analysis would most strongly reduce the search space for subsequent processing and scan the input to begin parsing at those points rather than proceeding strictly from left to right. This further suggests the possibility of a parallel approach to parsing. We expect that these measures would increase the efficiency of LHIP.\\n\\nCurrently, also, results are returned in an order determined by the order of rules in the grammar. It would be preferable to arrange matters in a more cooperative fashion so that the best (those with the highest coverage to span ratio) are displayed first. Support for bidirectional parsing (see Satta and Stock, to appear) is another candidate for inclusion in a later version. These appear to be longer-term research goals, however.\\n\\nAcknowledgments:\\n\\nThe authors would like to thank Louis des Tombe and Dominique Estival for comments on earlier versions of this paper.\\n\\nReferences\\n\\n=1=2em Anderson, A.H., M. Bader, E.G. Bard, E. Boyle, G. Doherty, S. Garrod, S. Isard, J. Kowtko, J. McAllister, J. Miller, C. Sotillo, H. Thompson and R. Weinert (1991) ``The HCRC Map Task Corpus'', Language and Speech 34(4), 351-366.\\n\\n=1=2em Briscoe, T. and J. Carroll (1993) ``Generalized Probabilistic LR Parsing of Natural Language (Corpora) with Unification-Based Grammars'', Computational Linguistics 19(1), 25-59.\\n\\n=1=2em Hindle, D. (1989) ``Acquiring Disambiguation Rules from Text'', Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, 118-125.\\n\\n=1=2em Kay, M. (1989) ``Head-Driven Parsing'', Proceedings of the Workshop on Parsing Technologies, 52-62.\\n\\n=1=2em Pereira, F.C.N. and S.M. Shieber (1987) Prolog and Natural Language Analysis, CSLI Lecture Notes No.10, Stanford University.\\n\\n=1=2em Satta, G. and O. Stock (to appear) ``Bidirectional Context-Free Grammar Parsing for Natural Language Processing'', Artificial Intelligence.\\n\\n=1=2em van Noord, G. (1991) ``Head Corner Parsing for Discontinuous Constituency'', Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, 114-121.\\n\\nFootnotes\\n\\nThis work was carried out under grants nos. 20-33903.92 and 12-36505.92 from the Swiss National Fund. Note that the input consists of written texts within the Map Task Corpus; LHIP is not intended for use in speech processing. This example is taken from the Map Task Corpus. A version of LHIP exists which permits a form of negation on RHS clauses. That version is not described here. Indeed, the ability of Fidditch to return a sequence of parsed but unattached phrases when a global analysis fails has clearly influenced the design of LHIP. In large grammars there is a significant time gain. The chart's main advantage, however, is in identifying unattached constituents and allowing a `layered' approach to analysis of input. Source code for the LHIP system has been made publicly available. For information, contact the authors.\", metadata={'source': '../data/raw/cmplg-xml/9408006.xml'}),\n",
       " Document(page_content=\"Bi-Lexical Rules for Multi-Lexeme Translation in Lexicalist MT Introduction\\n\\nTransfer based approaches to machine translation (MT) involve three main phases: analysis, transfer and generation. During analysis, the syntactic and semantic structure of a sentence is made explicit through a source language (SL) grammar and semantic processing modules. The result of analysis is one or more syntactic and semantic representations which are used to construct a syntactic and/or semantic representation in the target language (TL) through a series of transfer rules and a bilingual lexicon. From this representation a TL sentence is generated based on some form of mapping procedure, usually exploiting the TL grammar .\\n\\nWe describe the main characteristics of the LMT approach. This is followed by a description of the problems posed by certain multi-lexeme translations, and of how bi-lexical rules, in conjunction with lexical semantic information provide a framework for overcoming these problems. We then point out some limitations in our approach and give some idea as to the status of our implementation.\\n\\nLexicalist Machine Translation\\n\\nNotation\\n\\nOther Properties of LMT\\n\\nReversibility is an important property in bi-directional systems as it reduces development costs. In LMT, grammars are fully reversible since they are used in similar ways for analysis and generation: the difference is that during lexicalist generation, ordering information is disregarded. However, the process is complete because the generator is guaranteed to generate all the strings accepted by the TL grammar which satisfy the constraints imposed by the TL bag. Lexicalist generation is also sound because only strings which satisfy the constraints of the TL grammar are constructed. In addition, termination is guaranteed if it is guaranteed for parsing since one can at worst construct a generation algorithm which simply attempts all permutations of the TL bag and then parses them in order to test whether they are appropriate TL sentences.\\n\\nMulti\\n\\n\\n\\nLexical Translations\\n\\nWe therefore argue that a one-to-one translation for such phrases is not adequate but instead consider the highlighted phrases above as the correct equivalences between the two languages. The task then, is to find a mechanism for efficiently capturing regularities of this sort in the present framework. There are a number of alternatives for achieving this. We will consider three.\\n\\nLexical Neutralization\\n\\nLexical Variables\\n\\nThe third mechanism uses bilingual lexical rules to map bilexical entries into new bilexical entries. We have adopted this mechanism for certain multi-lexeme translations because it allows the exploitation of monolingual lexical rules in a motivated manner which integrates naturally with the LMT architecture, and because it provides a framework in which to study differences between lexical processes in different languages.\\n\\nLexical and Bi\\n\\n\\n\\nLexical Rules\\n\\nSimple Bi\\n\\n\\n\\nlexical Rule\\n\\nWe give a simple example of a bi-lexical rule before addressing the multi-lexeme translations introduced earlier. Consider the relationship that exists in English-Spanish translations between the translation of fruits and\\n\\nThe relevant relationship may be described by the following bi-lexical rule:\\n\\nThis bi-lexical rule says that if there is a bilexical entry translating English fruit nouns into Spanish fruit nouns, then there is a bilexical entry translating `noun tree' in English into a morphologically derived tree-denoting noun in Spanish.\\n\\nIn the abbreviated notation introduced earlier, the above bi-lexical rule will be represented as:\\n\\ntranslations are achieved for other fruits.\\n\\nClearly this rule should only apply to fruits which grow on trees and not to fruits such as strawberries which are found on low growing plants. Such restrictions need to be incorporated in the monolingual lexical signs and rules.\\n\\nImplementationally, bilexical rules may be applied off-line in order to expand the bilexicon before processing, or they may be applied during transfer to extend the bilexicon just sufficiently to enable transfer. We have opted for the latter approach.\\n\\nSupport Verbs\\n\\nThe intuitive description of the above process is that we consider `is thirsty' not to be translatable compositionally, but instead to require a multi-lexeme translation. The purpose of bi-lexical rules then is to minimize the repetition of information in the bi-lexicon while allowing the exploitation of monolingual lexical processes.\\n\\nLexicalization Patterns\\n\\nApplication of this rule to `swim - nadar' may be depicted as follows:\\n\\ntranslating `swims across' with the output of this rule and the remaining elements of the input via other bilexical entries.\\n\\nHead Switching\\n\\nThe phenomenon of head switching in translation can be exemplified by the following pair of sentences: just arrived. Juan acaba de llegar.The problem with such translations is that the syntactic head in the SL sentence is not the syntactic head in its translation. This is a major obstacle for syntactic and even some semantic based translation systems because of the recursive nature of their transfer representations.\\n\\nIt may be noticed that head selection by the TL grammar is possible because the event semantic constants in acabar_de and llegar are the same. The consequence of this is that modifiers which apply to `just arrived' and `arrived' separately will be indistinguishable during TL generation. Avoiding this problem entails transferring scoping domains for modifiers in order to constraint generation. However, we have no readily implementable mechanism for achieving this in LMT as yet.\\n\\nThis concludes our overview of the different translation mismatches that may be handled through bi-lexical rules. We now consider some unresolved issues arising from their use.\\n\\nBi\\n\\n\\n\\nlexical Rule Interaction\\n\\nOne difficulty we have found with  bilexical rules has been their composition. For example, consider the following translation: 1) John marched the soldiers across the valley. 1') Juan le hizo cruzar el valle a los soldados marchando.In our framework, two bi-lexical rules should be applied in such cases: one to\\n\\nacross - cruzar marchando'. The problem is that in isolation neither of these rules could perform the above translation. Ideally one should be able to use the output of one as input to the other to derive `march across - hacer cruzar marchando', but this is not possible because both bi-lexical rules expect a mono-lexeme bilexical entry.\\n\\nOne possible solution is to manually add further bi-lexical rules which incorporate the\\n\\nbi\\n\\n\\n\\nlexical rules.\\n\\nan entry which in the context of a causative verb introduces hacer in the\\n\\nOur main problem is that of resolving conflicts between the syntactic constraints imposed by each bi-lexical rule. The causative rule requires the Spanish side to include an infinitive verb, while the lexicalization pattern rule requires a gerundive verb. Clearly both constraints cannot be satisfied for the same lexical sign marchar1. The problem reflects itself in our proposal in that the rule which includes the contextual pattern must be chosen carefully. If the lexicalization pattern rule rather than the causative rule had included the contextual verb lexical sign, the gerundive marchando could not have been generated. Instead, a sentence analogous to `John made the soldiers march crossing the valley' would result, which is perhaps not desirable. In other words, the conflict between gerundive and infinitive morphology for `march' is decided manually in advance. The interaction of such decisions with other bi-lexical rules therefore might be unpredictable, and hence is left for further investigation.\\n\\nImplementation\\n\\nThe implemented prototype system contains approximately 250 bilexical entries; this figure includes 20 proper names, 20 multi-lexeme translations and 6 contextual rules. The following translations were done on a SUN Sparc workstation using Allegro Common Lisp. The time taken to find all possible TL sentences is given in seconds; total times are for CPU + typical garbage\\n\\nimplementation, rather than reflect the performance of an optimized system.\\n\\nConclusion\\n\\nWe have introduced the mechanism of bi-lexical rules for incorporating lexical rules in MT. These rules establish correspondences between bilexical entries such that given the presence of one entry, the existence of another bilexical entry can be inferred. We presented various phenomena that can be described using such rules: noun sense extensions, support verbs, lexicalization patterns and head switching. The rules provide a useful and motivated extension to the LMT paradigm by providing it with a uniform approach to the description of a number of translation phenomena.\\n\\nThe problems arising from conflicting constraints imposed by different translation relations are described, and a partial solution to these was offered involving the combined use of bi-lexical rules and contextual variables.\\n\\nAcknowledgements\\n\\nThanks to two anonymous reviewers for their valuable comments. The LKB was implemented by Ann Copestake as part of the ESPRIT ACQUILEX project. Remaining errors are mine.\\n\\nBibliography\\n\\nAlshawi, H., ed. (1992).\\n\\nThe Core Language Engine.\\n\\nMIT Press, Cambridge, MA.\\n\\nArnold, D. and Sadler, L. (1992). Rationalism and the treatment of referential dependencies. In Proceedings of the Fourth TMI, pp. 195-204, Montreal, Canada.\\n\\nBeaven, J. L. (1992). Shake-and-Bake machine translation. In Proceedings of COLING '92, pp. 602-09, Nantes, France.\\n\\nBech, A., Maegaard, B., and Nygaard, A. (1991). The Eurotra MT formalism. Machine Translation, 6(2) pp. 83-101.\\n\\nBrew, C. (1992). Letting the cat out of the bag: Generation for Shake-and-Bake MT. In Proceedings of COLING '92, pp. 610-16, Nantes, France.\\n\\nBriscoe, E., Copestake, A., and de Paiva, V., eds. (1993). Inheritance, Defaults and the Lexicon. Cambridge University Press, Cambridge, UK.\\n\\nCalder, J., Reape, M., and Zeevat, H. (1989). An algorithm for generation in unification categorial grammar. In Proceedings of the Fourth European Conference of the ACL, pp. 233-40, Manchester, England.\\n\\nCopestake, A., Flickinger, D., Malouf, R., Riehemann, S., and Sag, I. (1995). Translation using minimal recursion semantics. In Proceedings of the 6th International Conference on Theoretical and Methodological Issues in Machine Translation, Leuven, Belgium.\\n\\nDanlos, L. and Samvelian, P. (1992). Translation of the predicative element of a sentence: category switching, aspect and diathesis. In Proceedings of the Fourth TMI, pp. 21-34, Montreal, Canada.\\n\\nDevlin, K. (1991). Logic and Information. Cambridge University Press, Cambridge, UK.\\n\\nDorr, B. J. (1992). The use of lexical semantics in interlingual machine translation. Machine Translation, 7(3) pp. 135-93.\\n\\nFarkas, D., Jacobsen, W. M., and Todrys, K. W., eds. (1978). Papers from the Parasession on the Lexicon. Chicago Linguistic Society, University of Chicago, IL.\\n\\nFlickinger, D. (1987). Lexical Rules in the Hierarchical Lexicon. PhD thesis, Stanford University, Stanford, CA.\\n\\nFlickinger, D. and Nerbonne, J. (1992). Inheritance and complementation: A case study of easy adjectives and related nouns. Computational Linguistics, 18(3) pp. 269-310.\\n\\nFrank, A. and Reyle, U. (1995). Principle based semantics for HPSG. In Proceedings of Seventh Conference of the European Chapter of the ACL, pp. 9-16, Dublin, Ireland.\\n\\nGarey, M. R. and Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory of NP-Completeness. W. H. Freeman, New York.\\n\\nHeylen, D., Maxwell, K. G., and Verhagen, M. (1994). Lexical functions and machine translation. In Proceedings of the 15th COLING, pp. 1240-44, Kyoto, Japan.\\n\\nHinrichs, E. W., Meurers, W. D., and Nakazawa, T. (1994). Partial-VP and Split-NP Topicalization in German - An HPSG Analysis and its Implementation. Technical Report Arbeitspapiere des SFB 340 Nr. 58, Universitt Tbingen, Germany.\\n\\nHutchins, W. J. and Somers, H. L. (1992). An Introduction to Machine Translation. Academic Press, London.\\n\\nJohnston, M., Boguraev, B., and Pustejovsky, J. (1994). The acquisition and interpretation of complex nominals. Paper presented at the Workshop on the Future of the Dictionary, Uriage-les-bains, France.\\n\\nKaplan, R. M. and Kay, M. (1994). Regular models of phonological rule systems. Computational Linguistics, 20(3) pp. 331-78. Special Issue on Computational Phonology.\\n\\nKaplan, R. M., Netter, K., Wedekind, J., and Zaenen, A. (1989). Translation by structural correspondences. In Proceedings of the Fourth Conference of the European Chapter of the ACL, pp. 272-81, Manchester, UK.\\n\\nKaplan, R. M. and Wedekind, J. (1993). Restriction and correspondence-based translation. In Proceedings of the Sixth Conference of the European Chapter of the ACL, pp. 193-202, The Netherlands. OTS, Utrecht University.\\n\\nKay, M., Gawron, J. M., and Norvig, P. (1994). Verbmobil: A Translation System for Face-to-Face Dialog. Number 33 in Lecture Notes. Centre for the Study of Language and Information, Stanford, CA.\\n\\nLascarides, A., Briscoe, T., Asher, N., and Copestake, A. (in press). Order independent and persistent typed default unification. Linguistics and Philosophy.\\n\\nLevin, B. and Rappaport, M. (1995). Unaccusativity: At the syntax-lexical semantics interface. MIT Press, Cambridge, MA.\\n\\nOehrle, R., Bach, E., and Wheeler, D., eds. (1988). Categorial Structures and Natural Language Structures. D. Reidel, Dordrecht, The Netherlands.\\n\\nPollard, C. and Sag, I. (1987). Information Based Syntax and Semantics: Vol. 1. Lecture Notes. CSLI, Stanford, CA.\\n\\nPollard, C. and Sag, I. (1994). Head Driven Phrase Structure Grammar. Chicago University Press, IL.\\n\\nPopowich, F. (1995). Improving the efficiency of a generation algorithm for Shake and Bake machine translation using Head-Driven Phrase Structure Grammar. In Proceedings of Natural Language Understanding and Logic Programming V, Lisbon, Portugal.\\n\\nPoznanski, V., Beaven, J. L., and Whitelock, P. (1995). An efficient generation algorithm for lexicalist MT. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, Boston, MA.\\n\\nPustejovsky, J. (1991). The generative lexicon. Computational Linguistics, 17(4) pp. 409-441.\\n\\nPustejovsky, J. and Boguraev, B. (1993). Lexical knowledge representation and natural language processing. Artificial Intelligence, 63(1-2) pp. 193-224. Special Volume: Natural Language Processing.\\n\\nReyle, U. (1995). On reasoning with ambiguities. In Proceedings of the Seventh Conference of the European Chapter of the Association for Computational Linguistics, pp. 1-15, Dublin, Ireland.\\n\\nRussell, G., Ballim, A., Estival, D., and Warwick, S. (1991). A language for the statement of binary relations over feature structures. In Proceedings of the Fifth Conference of the European Chapter of the ACL, Bonn, Germany.\\n\\nSadler, L., Crookston, I., and Way, A. (1990). LFG and translation. In Proceedings of the Third TMI, University of Texas at Austin. LRC.\\n\\nSadler, L. and Thompson, H. (1991). Structural non-correspondence in translation. In Proceedings of the Fifth Conference of the European Chapter of the ACL, pp. 293-98, Bonn, Germany.\\n\\nSchenk, A. (1986). Idioms in the Rosetta machine translation system. In Proceedings of COLING '86, Bonn, Germany.\\n\\nShopen, T., ed. (1985). Language Typology and Syntactic Description Vol. III: Grammatical Categories and the Lexicon. Cambridge University Press, Cambridge, UK.\\n\\nSoler, C. and Marti, M. A. (1993). Dealing with lexical mismatches. Technical Report 4, ESPRIT BRA-7315 ACQUILEX II Working Paper, Publishing Division, Cambridge University Press, UK.\\n\\nTrujillo, A. (1992). Spatial lexicalization in the translation of prepositional phrases. In Proceedings of the 30th Annual Conference of the ACL, Student Session, pp. 306-08, Newark, Delaware.\\n\\nTrujillo, A. (forthcoming). Towards a cross-linguistically valid classification of spatial prepositions. Machine Translation.\\n\\nWhitelock, P. (1992). Shake-and-Bake translation. In Proceedings of COLING '92, pp. 784-91, Nantes, France.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9508006.xml'}),\n",
       " Document(page_content=\"The Effect of Pitch Accenting on Pronoun Referent Resolution\\n\\nBy strictest interpretation, theories of both centering and intonational meaning fail to predict the existence of pitch accented pronominals. Yet they occur felicitously in spoken discourse. To explain this, I emphasize the dual functions served by pitch accents, as markers of both propositional (semantic/pragmatic) and attentional salience. This distinction underlies my proposals about the attentional consequences of pitch accents when applied to pronominals, in particular, that while most pitch accents may weaken or reinforce a cospecifier's status as the center of attention, a contrastively stressed pronominal may force a shift, even when contraindicated by textual features.\\n\\nIntroduction\\n\\nTo predict and track the center of attention in discourse, theories of centering (Grosz et al., 1983; Brennan et al., 1987; Grosz et al., 1989) and immediate focus (Sidner, 1986) rely on syntactic and grammatical features of the text such as pronominalization and surface sentence position. This may be sufficient for written discourse. For oral discourse, however, we must also consider the way intonation affects the interpretation of a sentence, especially the cases in which it alters the predictions of centering theories. I investigate this via a phenomenon that, by the strictest interpretation of either centering or intonation theories, should not occur -- the case of pitch accented pronominals.\\n\\nCentering theories would be hard pressed to predict pitch accents on pronominals, on grounds of redundancy. To bestow an intonational marker of salience (the pitch accent) on a textual marker of salience (the pronominal) is unnecessarily redundant and especially when textual features correctly predict the focus of attention.\\n\\nIntonational theories would be similarly hard pressed, but on grounds of information quality and efficient use of limited resources. Given the serial and ephemeral nature of speech and the limits of working memory, it is most expedient to mark as salient the information-rich nonpronominals, rather than their semantically impoverished pronominal stand-ins. To do otherwise is an injudicious use of an attentional cue.\\n\\nHowever, when uttered with  contrastive stress on the pronouns, (1) John introduced Bill as a psycholinguist and then HE insulted HIM. (after Lakoff, 1971) is felicitously understood to mean that after a slanderous introduction, Bill retaliated in kind against John.\\n\\nPitch accent semantics H+L evokes an inference path. H*+L commits to the existence of inference path that would support the proposition as mutually believed, indicates that it can be found or derived from the set of mutual beliefs; H+L* conveys uncertainty about the existence of such a path.\\n\\nL+H evokes a scale or ordered set to which the accented constituent belongs: L+H* commits to the salience of the scale, and is typically used to convey contrastive stress; L*+H also evokes a scale but fails to commit to its salience, e.g., conveying uncertainty about the salience of the scale with regard to the accented constituent.\\n\\nCentering structures and operations\\n\\nAfter each utterance, one of three operations are possible:\\n\\nThe Cb retains both its position at the head of Cf and its status as the Cb; therefore it continues as the center in the next utterance.\\n\\nThe Cb retains its centered status for the current utterance but its rank is lowered -- it no longer resides at the head of Cf and therefore ceases to be the center in the next utterance.\\n\\nThe Cb loses both its centered status and ranking in the current utterance as attention shifts to a new center.\\n\\nWhen intonation and centering collide\\n\\nThe corollaries for pitch accented pronominals are: (1) when a pitch accent is applied to a pronominal, its main effect is attentional, on the order of items in Cf;  (2)  the obligation to accent a pronominal for attentional reasons depends on the variance between what the text  predicts and what the speaker would like to assert about the order of items in Cf.\\n\\nThese hypotheses arise from the following chain of assumptions:\\n\\n(1) To analyze the effects of pitch accents on pronominals, it is necessary to distinguish between attentional and propositional salience. Attentional salience  measures the degree to which an item is salient, expressible as a partial ordering,  e.g., its ranking in Cf. It is a quantitative feature. In contrast, propositional salience, addressing an item's status in relation to mutual beliefs, is qualitative. It is  calculated through inference chains that link semantic and pragmatic propositions.\\n\\nBoth attentional (Cf) and propositional (mutual beliefs) structures are updated throughout. However, unlike attentional structures which are ephemeral in various time scales and empty at the end of the discourse (Grosz and Sidner, 1986), mutual beliefs persist throughout the conversation, preserving at the end the semantic and pragmatic outcome of the discourse.\\n\\nIn addition, while propositions can be excluded from the mutual beliefs because they fail to meet some inclusion criterion, no lexical denotation is excluded from Cf regardless of its propositional value. This is because the  salience most relevant to the attentional state is the proximity of a discourse entity to the head of Cf -- the closer it is, the more it is centered and therefore, attentionally salient.\\n\\n(2) Pitch accents on pronominals are primarily interpreted for what they say about attentional salience. One determiner of whether attentional or propositional effects are dominant is the type of information provided by the accented constituent. Because nonpronominals contribute discourse content, pitch accented nonpronominals are mainly interpreted with respect to the mutual beliefs, that is, for their propositional content. However, pronominals, with little intrinsic semantics, perform primarily an attentional function. Therefore pitch accented pronominals are mainly interpreted with respect to Cf, for their attentional content.\\n\\nH* indicates instantiation of the pronominal's cospecifier as the Cb, while L* fails to instantiate it as the Cb;\\n\\nThe partially ordered set (salient scale) invoked by L+H is Cf;\\n\\nThe inference path evoked by H+L is, for attentional purposes, a traversal of Cf.\\n\\n(4) And therefore, the attentional effect of pitch accents can be formally expressed as an effect on the order of items in Cf.\\n\\nFrom these assumptions, I derive the following attentional consequences for pitch accented pronominals:\\n\\nOnly one pitch accent, L+H*, selects a Cb other than that predicted by centering theory and thereby reorders Cf.\\n\\nL*+H appears to support an impending reordering but does not compel it.\\n\\nBy analogy, the remaining pitch accents, seem to either weaken or strengthen the current center's Cb status, but do not force a reordering.\\n\\nAvailability of cospecifiers\\n\\nThe attentional interpretations are constrained by what has been mutually established in the prior discourse, or is situationally evident. Therefore, while contrastive stress may be mandated when grammatical features select the wrong cospecifier, the accenting is only felicitous when there is an alternate referent available.\\n\\nFor example, in (2) John introduced Bill as a psycholinguist and then heL+H* insulted him. L+H* indicates that he no longer cospecifies with John. If the hearer is hasty, she might select Bill as the new Cb. However, this is not borne out by the unaccented him, which continues to cospecify with  Bill. Since he and him cannot select the same referent, he requires a cospecifier that is neither John nor Bill. Because, the utterance itself does not provide a any other alternatives, heL+H* is only felicitous (and coherent) if an alternate cospecifier has been placed in Cf by prior discourse,  or by the speaker's concurrent deictic gesture towards a discourteous male.\\n\\nConclusion and Future Work\\n\\nBy combining Pierrehumbert and Hirschberg's (1990) analysis of intonational meaning with Grosz, Joshi and Weinstein's (1989) theory of centering in discourse,  the attentional affect of pitch accents becomes evident, and the paradox of pitch accented pronominals unravels. My goal here is  to develop an analysis and a line of inquiry and to suggest that my derivative claims are plausible, and even extensible to an attentional analysis of pitch accents on nonpronominals. The proof, of course, will come from investigation by multiple means -- constructed examples (e.g., Cahn, 1990), computer simulation, empirical analysis of speech data (e.g., Nakatani, 1993), and psycholinguistic experiments.\\n\\nReferences\\n\\nDwight Bolinger. A Theory of Pitch Accent in English. Word, 14(2-3):109-149, 1958.\\n\\nSusan E. Brennan, Marilyn W. Friedman, and Carl J. Pollard. A Centering Approach to Pronouns. Proceedings of the 25th Conference of the Association for Computational Linguistics, 1987.\\n\\nJanet Cahn. The Effect of Intonation on Pronoun Referent Resolution. Draft, 1990. Available as: Learning and Common Sense TR 94-06, M.I.T. Media Laboratory.\\n\\nHerbert H. Clark and Catherine R. Marshall. Definite Reference and Mutual Knowledge. In Webber, Joshi and Sag, editors, Elements of Discourse Understanding. Cambridge University Press, 1981.\\n\\nBarbara Grosz, Aravind K. Joshi, and Scott Weinstein. Providing a unified account of definite noun phrases in discourse. Proceedings of the 21st Conference of the Association for Computational Linguistics, 1983.\\n\\nBarbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. Towards a Computational Theory of Discourse Interpretation. Draft, 1989.\\n\\nBarbara J. Grosz and Candace L. Sidner. Attention, Intentions, and the Structure of Discourse. Computational Linguistics, 12(3):175-204, 1986.\\n\\nGeorge Lakoff. Presupposition and relative well-formedness. In Danny D. Steinberg and Leon A. Jakobovits, editors, Semantics: An Interdisciplinary Reader in Philosophy, Linguistics and Psychology, Cambridge University Press, 1971.\\n\\nChristine Nakatani. Accenting on Pronouns and Proper Names in Spontaneous Narrative. Proceedings of the European Speech Communication Association Workshop on Prosody, 1993.\\n\\nJanet B. Pierrehumbert. The Phonology and Phonetics of English Intonation. Ph.D. thesis, Massachusetts Institute of Technology, 1980.\\n\\nJanet B. Pierrehumbert and Julia Hirschberg. The Meaning of Intonation Contours in the Interpretation of Discourse. In Philip R. Cohen, Jerry Morgan, and Martha E. Pollack, editors, Intentions in Communication, MIT Press, 1990.\\n\\nCandace L. Sidner. Focusing in the Comprehension of Definite Anaphora. In Barbara J. Grosz, Karen Sparck-Jones, and Bonnie Lynn Webber, editors, Readings in Natural Language Processing, Morgan Kaufman Publishers, Inc., 1986.\\n\\nFootnotes\\n\\nMutual beliefs: propositions expressed or implied by the discourse, and which all conversants believe each other to accept as true and relevant same (Clark and Marshall, 1981). For simplicity's sake, we assume the items in Cf to be words and phrases; in actuality, they may be nonlexical representations of concepts, or some hybrid of lexical, conceptual and sensory data.\", metadata={'source': '../data/raw/cmplg-xml/9506017.xml'}),\n",
       " Document(page_content=\"Indefeasible Semantics and Defeasible Pragmatics1\\n\\nAn account of utterance interpretation in discourse needs to face the issue of how the discourse context controls the space of interacting preferences. Assuming a discourse processing architecture that distinguishes the grammar and pragmatics subsystems in terms of monotonic and nonmonotonic inferences, I will discuss how independently motivated default preferences interact in the interpretation of intersentential pronominal anaphora. In the framework of a general discourse processing model that integrates both the grammar and pragmatics subsystems, I will propose a fine structure of the preferential interpretation in pragmatics in terms of defeasible rule interactions. The pronoun interpretation preferences that serve as the empirical ground draw from the survey data specifically obtained for the present purpose.\\n\\nDiscourse Processing Architecture\\n\\nI will assume in this paper that a discourse is a sequence of utterances produced (spoken or written) by one or more discourse participants. Utterances are tokens of sentences or sentence fragments with which the speakers communicate certain information, and it is done in a context. Utterance interpretation depends on the context, and utterance meaning updates the context.\\n\\nA specification of the complex interdependencies involved in utterance interpretation is greatly facilitated if it is couched in a discourse processing architecture that is both logically coherent and as closely as possible an approximation of the human cognitive architecture for discourse processing. What are the major modules of the architecture, and what types of inferences do they support? I claim that the most fundamental separation is between the spaces of possibilities and preferences.\\n\\nSeparating Combinatorics and Preferences\\n\\nThere is an assumption in computational linguistics that combinatorics should take precedence over preferences. The wisdom is to maximize the combinatoric space of utterance interpretation and to keep a firm line between this space and the other, preferential, space of interpretation. Preferences are affected by computationally expensive open-ended commonsense inferences. Combinatorics determine all and only possible interpretations, and preferences prioritize the possibilities. Seen from another point of view, combinatorics are indefeasible -- that is, never overridden by commonsense plausibility, whereas preferences are defeasible -- that is, can be overridden by commonsense plausibility. I will henceforth assume that the grammar subsystem consists only of indefeasible possibilities, hence  monotonic, whereas the pragmatics subsystem consists mostly (or possibly entirely) of defeasible preferences.\\n\\nAn example of indefeasible rules of grammar in English is the Subject-Verb-Object constituent order. The sentence Coffee drinks Sally uttered in a normal intonation cannot mean ``Sally drinks coffee'' despite the commonsense support. An example of defeasible preferences is the interpretation of the pronoun he in discourse ``John hit Bill. He was severely injured.'' The combinatoric rule of pronoun interpretation would say that both John and Bill are possible referents of he, while the preferential rule would say that Bill is preferred here because it is more plausible that the one who is hit gets injured rather than vice versa. Crucially, this preference is overridden in certain contexts. For instance, if Bill is an indestructible cyborg, the preferred semantic value of he would shift to John.\\n\\nThe inferential properties of the grammar subsystem as a space of possibilities are well-illustrated in the so-called unification-based grammatical formalisms (UBG). A UBG system consists of context-free phrase structure constraints and unification constraints. Maxwell and Kaplan (1993) describe how the constraint interactions can be made efficient by exploiting the following properties of a UBG system: (1) monotonicity -- no deduction is ever retracted when new constraints are added, (2) independence -- no new constraints can be deduced when two systems are conjoined, (3) conciseness -- the size of the system is a polynomial function of the input that it was derived from, and (4) order invariance -- sets of constraints can be processed in any order without changing the final result.\\n\\nThe inferential properties of the pragmatics subsystem are much less understood. Its general features can be characterized as those of preferential reasoning, a topic more studied in AI than in linguistics. The pragmatics subsystem contains sets of preference rules that, in certain combinations, could lead to conflicting preferences. This fundamental indeterminacy leads to the properties opposite from those of the grammar subsystem: (1) nonmonotonicity -- preferences can be canceled when overriding preferences are added, (2) dependence -- new preferences may result when two pragmatic subsystems are conjoined, (3) explosion -- the system size is possibly an exponential (or worse) function of the input that it was derived from, and (4) order variance -- changing the order in which sets of preferences are processed may also change the final result. The key to a discourse processing architecture is to preserve the above computational properties of the grammar subsystem while striving for a maximal control of the preference interactions in the pragmatics subsystem.\\n\\nExisting logical semantic theories employing dynamic interpretation rules (e.g., Kamp, 1981; Heim, 1982; Groenendijk and Stokhof, 1991; Kamp and Reyle, 1993) formalize the basic context dependence of indefeasible semantics. While these theories predict the possible dynamic interpretations of utterances, they are not concerned with how to compute the relative preferences among them. Lascarides and Asher (1993) extend the Discourse Representation Theory (DRT) (Kamp, 1981) with the interaction of defeasible rules for integrating a new utterance content into the discourse information state. The input to their defeasible reasoning is a fully interpreted DR Structure (DRS), with all the NPs already interpreted. The pragmatics subsystem I am concerned with here also includes the defeasible rules for NP interpretation and constituent attachments needed for DRS construction. The input to pragmatics in the present proposal is a much less specified logical form, and pragmatics kicks in during DRS construction.\\n\\nThe Processing Architecture\\n\\nThe discourse processing architecture that I will assume in the background of the remainder of  this paper is this.\\n\\nLet discourse be a sequence of utterances,\\n\\nLet grammar G consist of rules of syntax and semantics that assign each utterance utti the initial logical form\\n\\n.\\n\\nrepresents a disjunctive set of underspecified formulas containing unresolved references, unscoped quantifiers, and vague relations.\\n\\nLet pragmatics P consist of rules for specifying and disambiguating\\n\\nin context Ci-1. Ideally, P outputs the single preferred interpretation\\n\\n(\\n\\nis subsumed by\\n\\nand there is no\\n\\nthat is preferred over\\n\\nand also subsumed by\\n\\n), and integrating\\n\\ninto context Ci-1 produces the preferred output context Ci. In a less felicitous case, the rules of P do not converge, resulting in multiple interpretations and output contexts.\\n\\nContext\\n\\nMy aim here is to introduce the basic components of the context C in the above discourse processing architecture that I assume in the remainder of the paper.\\n\\nContext Ci is a 6-tuple\\n\\nconsisting of the fast\\n\\n\\n\\nchanging components,\\n\\n, significantly affected by the dynamic import of utterances and the slow-changing components,\\n\\nrelatively stable in a given stretch of discourse instance.\\n\\n. Di contains sets of situations, eventualities, entities, and relations among them, associated with the evolving event, temporal, and discourse structures. Ai is the attentional state -- a partial order of the entities and propositions in Di,  where the ordering is by salience. Ai is separated from Di because the same Di may correspond to different variants of Ai depending on the particular sequence of utterances in particular forms describing the same set of facts. Ii is the set of indexical anchors -- the indexically accessible objects in the current discourse situation -- for instance, the values of indexical expressions such as I, you, here, and now. The slow-changing components are the linguistic knowledge L and  world knowledge K used by the discourse participants. Although we know that discourse participants never share exactly the same mental state representing these components of the context, there must be a significant overlap in order for a discourse to be mutually intelligible. For the purpose of this paper, I will simply assume that context C is sufficiently shared by the participants.\\n\\nThe next section elaborates on the initial logical form\\n\\nthat plays a crucial role of defining the grammar-pragmatics boundary in the discourse processing architecture.\\n\\nIndefeasible Semantics\\n\\nThe initial logical form (ILF)\\n\\nrepresents the utterance's structure and meaning at the grammar-pragmatics boundary. This section discusses the general features of ILF with examples.\\n\\nGeneral Considerations\\n\\nThere are specific proposals for the ILF\\n\\nin the computational literature (e.g., Alshawi and van Eijck, 1989; Alshawi, 1992; Alshawi and Crouch, 1992; Hwang and Schubert, 1992a, 1992b; Pereira and Pollack, 1991). Details in these proposals vary, but there is a remarkable agreement on the general features.\\n\\nThe ILF\\n\\ncontains ``vague'' predicates and functions representing what the utterance communicates. Vague predicates and functions represent various expression and construction types whose interpretation depends on the discourse context. They include unresolved referring expressions such as the pronoun he, unscoped quantifiers such as each, vague relations such as the relation between the nouns in a noun-noun compound, unresolved operators such as the tense operator past and the mood operator imperative, and attachment ambiguities such as for PP-attachments. The idea can also be extended to underspecify lexical senses at the ILF level. These predicates and functions generate `assumptions' that need to be resolved or `discharged' in the union of the discourse and sentence contexts. The ILF is thus partial and indefeasible -- partial because it does not always have a truth value, and indefeasible because further contextual interpretations only prioritize possibilities and specify vagueness.\\n\\nThe ILF\\n\\nalso represents aspects of the utterance's surface structure relevant to how the utterance communicates the information content (e.g., the Topic-Focus Articulation of Sgall et al., 1986). Such a syntax-semantics corepresentation could be achieved in either of the two options: (1) the logical form is structured, representing aspects of phonological and surface syntactic structures such as the grammatical functions of nominal expressions, linear order, and topic-comment structure, or (2) the partial semantic representation and the phonological and syntactic structures are separately represented with mappings among corresponding parts. In this paper, the choice is arbitrary as long as certain syntactic information is available at the logical form.\\n\\nThere is a general question of how far and how soon the ILF gets specified and disambiguated by the pragmatics. The above existing proposals in the computational literature assume that each utterance is completely specified and disambiguated before the next utterance comes in. This includes the integration of the utterance content into the evolving discourse structure, event structure, and temporal structure in the context, as discussed by Lascarides and Asher (1993). An utterance's complete interpretation is not in general available on the spot, however, and it often has to wait till some more information is supplied in the subsequent discourse (Grosz et al., 1986). It is also possible that only the information concerning those entities that are significant or salient (or `in focus') in the current discourse need to be fully specified and disambiguated. The present discourse processing architecture allows such incremental and partial specification and disambiguation of the information state along discourse progression though this perspective is not explored in any technical detail here.\\n\\nIn sum, the ILF represents the indefeasible semantics of an utterance by leaving the following context-dependent interpretations underdetermined: reference of nominal expressions, modifier attachments, quantifier scoping, vague relations, and lexical senses. The ILF also leaves open how the given utterance is integrated into the temporal, event, and discourse structures in the context.\\n\\nOur Working Formalism\\n\\nI will use a simplified ILF in this paper. It is an underspecified predicate logic in a davidsonian style -- a version of QLF (Kameyama, 1995) without the aterm-qterm distinction. The ILF for the utterance ``He made a robot spider'' is as follows:\\n\\nIt contains the following vague predicates and  functions:\\n\\nunresolved unstressed pronoun ``he'' --\\n\\nunscoped quantificational determiner ``a'' --\\n\\na vague relation for a noun-noun compound ``robot spider'' --\\n\\n(a relation between a spider entity and a robot property)\\n\\nunresolved past tense --\\n\\nunresolved declarative mood --\\n\\nIf the preferred interpretation of the utterance is that ``John'' made a robot shaped like a spider, we have the following DRS-like logical form:\\n\\nThe interpretation is complete when the content is integrated into the discourse, event, and temporal structures in the context. These structures are assumed to be in the discourse model D. The pragmatics subsystem must make all of the preferential decisions including NP interpretation and operator interpretation as well as contextual integration.\\n\\nAmbiguity and Underspecification\\n\\nThe initial logical form mixes both ambiguity and underspecification. The choice is largely arbitrary when the number of possible interpretations is exhaustively enumerable. Whenever there are npossible interpretations for a linguistic item or construction type, we can have either (1) a disjunctive set of n interpretations\\n\\ni1,...,in, from which the pragmatics chooses the best, or (2) one underspecified interpretation that the pragmatics further specifies. Pragmatic disambiguation and specification involve exactly the same kind of an interplay of linguistic and commonsense preferences, and relative preferences in disambiguation and specification are often interdependent.\\n\\nConsider He made a robot spider with six legs. There is a preference for the interpretation ``a robot spider with six legs'' over the alternative ``a male person with six legs''. This preference is overridden in certain contexts -- for instance, if the person is a fictional figure who can freely change the number of legs to be two, four, or six, the alternative reading becomes equally plausible. Note that the attachment disambiguation and pronoun interpretation are interdependent here.\\n\\nWhen the number of possible interpretations cannot be exhaustively enumerated, however, ambiguity and underspecification are not interchangeable, and we must posit an underspecified relation as a semantic primitive. A sufficient but not necessary condition for positing an underspecified relation is this (Kameyama, 1995): An underspecified relation is posited when there is an open-ended set of possible specific relations associated with a construction type, and the interpretation is typically affected by ad hoc facts known in the discourse context.\\n\\nA canonical example is the interpretation of noun-noun compounds such as elephant pen. It could mean a pen shaped like an elephant, a pen with elephant pictures on the body, a pen with a small toy elephant glued on the top, or, depending on the context, a pen that the speaker found on the ground when she was pretending to be an elephant. All we can tell from the grammar of noun-noun compounds is that it is a pen that has some salient relation with elephants. It makes sense, then, to explicitly state in the grammar output the vague notion of ``some salient relation'' as a primitive. This is the basic motivation of the proposal for underspecified relations in the logical form in the computational literature (e.g., Alshawi, 1990; Hobbs et al., 1993). The same thing goes with scope ambiguities. The number of possible scopings is always bounded but possibly very large (on the order of hundreds), and speakers are often unable to select a single specific scoping, so the grammar should defer assigning specific scopings to a sentence and give it to pragmatics (Hobbs, 1983; Reyle, 1993; Poesio, 1993).\\n\\nIn sum, with the ILF sealing off the space of grammatical reasoning, the present discourse processing architecture magnifies the importance of pragmatics in utterance interpretation. Pragmatics achieves anaphora resolution, attachment disambiguation, quantifier scoping, vague relation specification, and contextual integration all in one module. Is there a system in the chaos? That is the question we turn to now.\\n\\nDefeasible Pragmatics\\n\\nThis section discusses the features and examples of the defeasible rules in the pragmatics subsystem.\\n\\nGeneral Considerations\\n\\nBy defeasible, I mean a conclusion that has to be retracted when some additional facts are introduced. This characterizes the preferential aspect of utterance interpretation with the nonmonotonicity property. Grammatical reasoning is governed by the Tarskian notion of valid inference in standard logic -- ``Each model of the premises is also a model for the conclusion.'' Pragmatic reasoning distinguishes among models as to their relevance or plausibility, and is governed by the notion of plausible inference (Shoham, 1988) -- ``Each most preferred model of the premises is a model for the conclusion.'' The preference can be stated in terms of default rules as well, so the general reasoning takes the form of ``as long as no exception is known, prefer the default.'' In utterance interpretation, this form of reasoning chooses the best interpretation from among the set of possible ones. The present focus is the interpretation preferences of intersentential pronominal anaphora.\\n\\nEarlier Computational Approaches to Pronoun Interpretation\\n\\nComputational research on pronoun interpretation has always recognized the existence of powerful grammatical preferences, but there are different views on their status in the overall processing architecture. Hobbs (1978) discussed the relative merit of purely grammar-based and purely commonsense-based strategies for pronoun interpretation. His grammar-based strategy that accounts for 98% of a large number of pronouns in naturally occurring texts simply could not be extended to account for the remaining cases that only commonsense reasoning can explain. He settled in a ``deeper'' method that seeks a global coherence arguing that coreference can be determined as a side-effect of coherence-seeking interpretation. The abduction-based approach (Hobbs et al., 1993) is an example of such a general inference system, where syntax-based preferences for coreference resolution are used as the last resort when other inferences do not converge.\\n\\nSidner's (1983) local focusing model used an attentional representation level to mediate the grammar's control of discourse inferences. For each pronoun, there is an ordered list of potential referents determined by local focusing rules, and the highest one that leads to a consistent commonsense interpretation of the utterance is chosen. Common sense has a veto power over grammar-based focusing in the ultimate interpretation, but common sense is the last resort, contrary to Hobbs's approach. Carter (1987) implemented Sidner's theory combined with Wilks's (1975) preferential semantics, and reported the success rate of 93% for resolving pronouns in a variety of stories -- of which only 12% relied on commonsense inferences.\\n\\nGrammar's role in the control of inferences was the original motivation of the centering model (Joshi and Kuhn, 1979; Joshi and Weinstein, 1981). The proposal was to use the monadic tendency of discourse (i.e., tendency to be centrally about one thing at a time) to control the amount of computation required in discourse interpretation. Grosz, Joshi, and Weinstein (1983) proposed a refinement of Sidner's model in terms of centering, and highlighted the crucial role of pronouns in linking an utterance to the discourse context. Subsequent work on centering converged on an equally significant role of the main clause SUBJECT (Kameyama, 1985, 1986; Grosz, Joshi, and Weinstein, 1986; Brennan, Friedman, and Pollard, 1987). Hudson D'Zurma (1988) experimentally verified that speakers had a difficulty in interpreting a discourse where a centering prediction was in conflict with commonsense plausibility, leading to a `garden path' effect. An example from her experiment is: ``Dick had a jam session with Brad. He played trumpet while Brad played bass. ? ?He plucked very quickly.'' Centering models the local attentional state management in an overall discourse model proposed by Grosz and Sidner (1986).\\n\\nThese computational approaches to discourse have recognized the non-truth-conditional effects on utterance interpretation coming from the utterance's surface structure (i.e., phonological, morphological, and syntactic structures). Although this aspect of interpretation cannot be neglected in a discourse processing model, its relevance to a logical model of discourse semantics and pragmatics has remained unclear. It is worth pointing out that discourse pragmatics in the above computational approaches as well as in philosophy (e.g., Lewis, 1979; Stalnaker, 1980) has generally assumed a dynamic architecture. Would there be a potential fit with the dynamic semantic theories in linguistics (e.g., Kamp, 1981; Heim, 1982; Groenendijk and Stokhof, 1991) in a way that forms a basis for an integrated logical model of discourse semantics and pragmatics? In this paper, I propose a pronoun interpretation model taking ideas from both computational and linguistic traditions, and present it in such a way that it becomes tractable for logical implementation.\\n\\nPronoun Interpretation Preferences: Facts\\n\\nPronoun interpretation must be carried out in an often vast space of possibilities, somehow controlling the inferences with default preferences coming from different aspects of the current context. Pronouns such as he, she, it and they can refer to entities talked about in the current discourse, present in the current indexical context, or simply salient in the model of the world implicitly shared by the discourse participants. Since the problem space is vast and complex, we need to narrow it down to come to grips with interesting generalizations. I will now limit our discussion to the interpretation of the anaphoric use of unstressed male singular third person pronoun he or him in English.\\n\\nSurvey and the Results\\n\\nOne set of examples, A-H, involves pronouns that occur in the second of two-sentence discourses. They were presented to competent (some nonnative) speakers of English in the A-F-C-H-E-D-B-G order, avoiding sequential effects of two adjancent similar examples. The speakers were instructed to read them with no special stresses on words, and to answer the who-did-what questions about pronouns in italics. The answer ``unclear'' was also allowed, in which case, the speaker was encouraged to state the reason. The total number of the speakers was 47, of which 10 were nonlinguist natural language researchers and 4 were nonnative but fluent English speakers. The second set of examples, I-L, are longer discourses. They were given to disjoint sets of native English speakers, none of whom are linguists.\\n\\nThe examples fall under two general categories, as indicated in Table 1. One group isolates the grammatical effects by minimizing commonsense biases. In these examples, it is conjectured that there is no relevant commonsense knowledge that affects the pronoun interpretation in question. The other group examines the commonsense effects of a specific causal knowledge of hitting and injuring in relation to the grammatical effects observed in the first group.\\n\\nTable 2 shows the survey results. The\\n\\nsignificance for each example was computed by adding an evenly divided number of the ``unclear'' answers to each explicitly selected answer, reflecting the assumption that an ``unclear'' answer shows a genuine ambiguity. Preference is considered significant if p[.05, weakly significant if .05[p[.10, and insignificant if .10[p. Insignificant preference is interpreted to mean ambiguity or incoherence. It follows from the Gricean Maxim that ambiguity must be avoided in order for an utterance to be pragmatically felicitous. An example with an insignificant preference is thus infelicitous, and should not be generated.\\n\\nIt must be noted that the present survey results exhibit only one aspect of preferential interpretation -- namely, the final preference reached after an unlimited time to think. They do not represent the process of interepretation -- for instance, a number of speakers commented that they had to retract the first obvious choice in example I. This garden-path effect verified in Hudson D'Zurma's (1988) experiments does not show in the present survey results.\\n\\nDiscussion of the Results\\n\\nSUBJECT Antecedent Preference. A hierarchy of the preferred intersentential antecedent of a pronoun has been proposed in the centering framework, which basically says that the main clause SUBJECT is preferred over the OBJECT (Kameyama, 1985,1986; Grosz et al., 1986). This preference is confirmed in examples A and B.\\n\\nThe consistency of this preference across examples A and B demonstrates that grammatical functions rather than thematic roles are the adequate level of generalization. In both A and B, the thematic roles of Bill and John in the first sentence are agent and theme (or patient), respectively, but the switch in grammatical functions by passivization causes the preferred interpretation to switch accordingly.\\n\\nPronominal Chain Preference. This is the preference for a chain of pronouns across utterances to corefer. Examples K and L are a minimal pair of structural effects without a commonsense bias. Their contrast shows the effect of grammatical positions. The SUBJECT-SUBJECT chain of pronouns (example K) supports a significant coreference preference (p[.001), whereas the OBJECT-SUBJECT chain (example L) supports a weakly significant noncoreference preference (.05[p[.10) indicating a parallelism effect below.\\n\\nExample I shows that the causal knowledge also in the end overrides a stretch of SUBJECT pronominal chain, but as noted above, this example causes the speakers to first interpret the SUBJECT pronouns to corefer, then retract the choice due to the inconsistency with a causal knowledge. This processing tendency indicates that the grammatical preference is processed faster than the commonsense preference. We will come back to this issue later.\\n\\nIn example J, the strong preference for a SUBJECT pronominal chain is undermined by the indefiniteness of the referent (one of the boys) that the generic causal knowledge supports and by the additional inference -- when one hits one of a group of boys, he would be revenged by the group. The grammar-based preference and common sense are in a tie here, showing a genuine ambiguity (.50[p[.70).\\n\\nGrammatical Parallelism Preference. There is a general preference for two adjacent utterances to be grammatically parallel. The parallelism requires, roughly, that the SUBJECTs of two adjacent utterances corefer and that the OBJECTs, if applicable, also corefer. This preference is demonstrated in example D that involves two pronouns. In example L, the parallelism preference overrides the pronominal chain preference.\\n\\nExample E shows the defeasibility of the parallelism preference in the face of the presupposition triggered by adverb back. An ``x hit y back'' event conventionally presupposes that a ``y hit x'' event has previously occurred, leading to the near-unanimous interpretation ``Bill hit John back.''\\n\\nCommonsense Preference. Examples F-H illustrate the effect of a simple causal knowledge that dictates the final interpretation over and above the grammatical preferences. In example F, the SUBJECT Antecedent Preference is defeated by an inference derived from the generic causal knowledge -- ``when X hits Y, Y is normally hurt,'' and ``being injured is being hurt.'' Since the example involves some ``normal'' fellows called John and Bill, it applies with full force (46/47).\\n\\nExamples G and H show what happens to this baseline default when the described event involves some special individuals (fictitious or nonfictitious) that the speakers have some knowledge about. In example H, the preferred interpretation (34/47) swings to the one where the normal fellow, John, is injured as a result of attempting to assault the indestructible cyborg. The cyborg also could have been injured (6/47) (because the movie showed that it can be destroyed after all). In example G, John attempts to assault a warm-blooded real person, Arnold, who seems a little stronger than normal fellows. Here, more speakers thought that John was injured (24/47) than Arnold was (13/47), but this preference is insignificant (.10[p[.20). It reflects the indeterminacy of whether Arnold is a normal fellow or not, which affects the applicability of the generic causal knowledge.\\n\\nDescriptive Generalizations\\n\\nTable 3 summarizes the preference predicted by each of the four sources discussed above and the final outcome verified in the survey. We see the following general patterns of conflict resolution: 1. Conventional Presuppositions (triggered by adverbs in examples C and E) and Commonsense Preferences (examples F, G, and H) dictate the final preference. 2. Grammatical Preferences take charge in the absence of relevant Commonsense Preferences (examples A-E, K, and L). 3. The SUBJECT Antecedent Preference overrides the Grammatical Parallelism Preference when in conflict (see examples A and B), and both are in turn stronger than the Pronominal Chain Preference (example L).\\n\\n-- due to the conventional presupposition triggered by adverb too.\\n\\n-- due to the conventional presupposition triggered by adverb back.\\n\\n-- Tommy is the first choice, which is later retracted.\\n\\nThe cases of indeterminate final preference in examples G and J are worth noting. This kind of an indeterminate preference is infelicitous and uncooperative, which should be avoided in discourse generation. The indeterminacy in example G is due to the indeterminacy of Arnold being a normal person subject to injury or an abnormally strong person who would not let himself be injured. The indeterminacy in example J is due to the conflict between the general causal knowledge about an injury caused by hitting and the insalience of an indefinite referent as a possible pronominal referent.\\n\\nPronoun Interpretation Preferences: Account\\n\\n, and the discourse model D.\\n\\nThe main thrust of the present account is the general interaction of preferences that apply on different context components. It explains the basic fact that preferences may or may not be determinate. The present perspective of preference interactions also extends and explains the role of the attentional state in Grosz and Sidner's (1986) discourse theory.\\n\\nThe Role of the Attentional State\\n\\nA discourse describes situations, eventualities, and entities, together with the relations among them. The attentional state A represents a dynamically updated  snapshot of their salience. We thus assume the property salient to be a primitive representing the partial order among a set of entities in A. The property salient is gradient and relative. A certain absolute degree of salience may not be achieved by any entities in a given A, but there is always a set of maximally salient entities, which is often, but not necessarily, a singleton set. Thus it is crucial that a rule about the single maximally salient entity in a given A is only sometimes determinate.\\n\\nWe will now recast some elements of the centering model in the present discourse processing architecture. In the input context Ci-1 for utterance utti, the form and content (\\n\\n) of the immediately preceding utterance utti-1 occupy an especially salient status. The entities realized in utti-1 are among the most salient subpart of Ai-1. I assume that this is achieved by a general A-updating mechanism. One of the entities in Ai-1 may be the\\n\\nCenteri-1, what the current discourse is centrally about, hence the high salience: CENTER The Center is normally more salient than other entities in the same attentional state.\\n\\nAt least two default linguistic hierarchies are relevant to the dynamics of salience One is the grammatical function hierarchy (GF ORDER), and the other is the nominal expression type hierarchy (EXP ORDER). The GF ORDER in utti predicts the relative salience of entities in the output attentional state Ai whereas the EXP ORDER in uttipredicts the relative salience of entities assumed in the input attentional state Ai-1. EXP ORDER is also crucial to the management of the Center (EXP CENTER): GF ORDER: Given a hierarchy, [ SUBJECT ] OBJECT ] OBJECT2 ]OTHERS], an entity realized by a higher ranked phrase is normally more salient in the output attentional state. EXP ORDER: Given a hierarchy, [ ZERO PRONOMINAL ] PRONOUN ] DEFINITE NP ] INDEFINITE NP],  an entity realized by a higher-ranked expression type is normally more salient in the input attentional state. EXP CENTER: An expression of the highest ranked type normally realizes the Center in the output attentional state. EXP CENTER can be interpreted in two ways. One computes the ``highest-ranked type'' per utterance, sometimes allowing a nonpronominal expression type to output the Center. The other takes it to be fixed, namely, only the pronominals. The choice is empirical. In this paper, I will take the second interpretation.\\n\\nSince matrix subjects and objects cannot be omitted in English,  the highest-ranked expression type is the (unstressed) pronoun (see Kameyama, 1985:Ch.1). From EXP ORDER, it follows that a pronoun normally realizes a maximally salient entity in the input attentional state. A pronoun can also realize a submaximally salient entity if this choice is supported by another overriding preference. The grammatical features of pronouns also constrain the range of possible referents -- for instance, a he-type entity is a male agent. The maximal salience thus applies on the suitably restricted subset of the domain for each type of pronoun.\\n\\nThe center transition types of ``establishing'' and ``chaining'' (Kameyama, 1985,1986) result from the interactions of CENTER, EXP ORDER, and EXP CENTER. The Center is ``established'' when a pronoun picks a salient non-Center in the input context and makes it the Center in the output context. It is ``chained'' when a pronoun picks the Center in the input context and makes it the Center in the output context. Examples A-H are thus concerned with Center-establishing pronouns, whereas examples I-L are concerned with Center-chaining pronouns. These transition types are not the primitives that directly drive preferences, however.\\n\\nThe Role of the LF Register\\n\\nThe grammatical parallelism of two adjacent utterances in discourse affects the preferred interpretation of pronouns (Kameyama, 1986), tense (Kameyama, Passonneau, and Poesio, 1993), and ellipsis (Pruest, 1992; Kehler, 1993). This general tendency warrants a separate statement. Parallelism is achieved, in the present account, by a computation on the pair of logical forms, one in the LF register in the context, and the other being interpreted. PARA: The LF register in the input context and the ILF being interpreted seek maximal parallelism. The present perspective of rule interaction explains the ``property-sharing'' constraint on Center-chaining (Kameyama, 1986) as follows. GF ORDER, EXP ORDER, and PARA join forces to create a strong grammatical preference for SUBJECT-SUBJECT coreference (examples D,K). When they are in conflict, that is, when the maximally salient entity is not in a parallel position, PARA is defeated (examples A,B). When maximal salience is indeterminate, the parallelism preference affects the choice (example L), leading to a noncoreference preference for an OBJECT-SUBJECT pronominal chain.\\n\\nThe Role of the Discourse Model\\n\\nThe discourse model contains a set of information states about situations, eventualities, entities, and the relations among them. It also contains the evolving discourse structure, temporal structure, and event structure. Both linguistic semantics and commonsense preferences apply on the same discourse model.\\n\\nLexically Triggered Presuppositions. Adverbs too and back trigger conventional presuppositionsabout the input discourse model. These presuppositions are part of lexical semantics, thus indefeasible.\\n\\nAdverb too triggers a presupposition that appears to seek parallelism between an utterance in the context and the utterance being interpreted. This is actually due to a general similarity presupposition associated with too. Consider each of the following utterances immediately preceding ``John hit Bill too'': ``Mary hit Bill'', ``John hit Mary'', ``Mary kicked Bill'', ``John kicked Mary'', ``Mary hit Jane'', and `John called Bill''. What's construed as `similar' in each case is a function of the particular utterance pair, and intuitively, preferred pairs support more similarities. Thus similarity comes in degrees, and a parallel interpretation is due to the preference for a maximal similarity.\\n\\nAdverb back triggers a presupposition for a reverse parallelism. That is, the utterance ``Bill hit John back'' presupposes that it occurred after ``John hit Bill''.\\n\\nCommonsense Knowledge. In contrast to the above rules that belong to the linguistic knowledge, the commonsense knowledge consists of all that an ordinary speaker knows about the world and life. Formalizing common sense is a major research goal of AI, where nonmonotonic reasoning has been intensively studied. My goal here is not to propose a new approach to commonsense reasoning but simply to highlight its interaction with linguistic pragmatics in the overall pragmatics subsystem. We know one thing for sure -- there will be a relatively small number of linguistic pragmatic rules that systematically interact with an open-ended mass of commonsense rules. Since the linguistic rules can be seen to control commonsense inferences, our aim is to describe the former as fully as possible, and specify how the ``control mechanism'' works. The commonsense rules posited in connection to the examples in this paper are thus meant to be exemplary. There will be different rules for each new example and domain to be treated. The linguistic rules, however, should be stable across examples and domains.\\n\\nThe single powerful causal knowledge at work in our examples is that hitting may cause injury on the hittee but less likely on the hitter: HIT: When an agent x hits an agent y, y is normally hurt. The effects of the Terminator and Arnold indicate that the applicability of the HIT rule depends on the normality of the agents involved. Relevant knowledge includes things like: An agent is normally vulnerable, Arnold is a normal agent or an abnormally strong agent, and Terminator is an abnormally strong agent.\\n\\nAccount of the Rule Interactions\\n\\nWe now state the preference interaction patterns observed in Table 3 above. The SUBJECT Antecedent Preference and Pronominal Chain Preference result from CENTER, GF ORDER, EXP ORDER, and EXP CENTER. These are the defeasible Attentional Rules (ATT) stating the preferred attentional state transitions. The Grammatical Parallelism Preference is PARA. This is an example of the defeasible LF Rules (LF) stating the preferred LF transitions. Conventional presuppositions triggered by too and back are examples of the indefeasible Semantic Rules (SEM) in the grammar constraining the interpretation in the discourse model. The causal knowledge of hitting is HIT, with associated knowledge ETC about agents, Terminator, and Arnold. These are examples of the defeasible Commonsense Rules (WK) stating the preferred discourse model. Table 4 identifies the rules that dominate the final interpretation in examples A-L.\\n\\nRules: ATT={CENTER, GF ORDER, EXP ORDER, EXP CENTER}, LF={PARA}, WK={HIT, ETC}, SEM={TOO, BACK}.\\n\\nGeneral Features. The first distinction among these rules is defeasibility. The SEM rules are indefeasible whereas all other rules are defeasible. It is predicted that indefeasible rules override all defeasible rules, as verified in examples C and E.\\n\\nWhat factor determines the interaction pattern among the defeasible rules? The three context components -- discourse model D, attentional state A, and LF register\\n\\n-- all have their preferred transitions. The D preference results from proposition-level (or ``sentence-level'') inferences directly determining the preferred model whereas the A and LF preferences result from entity-level (or ``term-level'') inferences only indirectly determining the preferred model. We have seen that proposition-level preferences, if applicable, generally override entity-level preferences, albeit with a varying degree of difficulty.\\n\\nTake two examples: (1) ``John met Bill. He was injured.'' and (2) ``John hit Bill. He was injured.'' In (1), the ATT and LF preference that the pronoun refers to John indirectly leads to the preference that John was injured, which becomes the overall preference in the absence of relevant WK rules. In (2), relevant WK rules directly support a proposition-level preference, Bill was injured, which wins out (with a varying degree of difficulty). These ``flows of preference'' during an utterance interpretation are illustrated below: (1)\\n\\n[S [NP he]:{John]Bill}\\n\\n]\\n\\nJohn was injured\\n\\n(2)\\n\\n[S [NP he]:{John]Bill}\\n\\n]:{Bill was injured ] John was injured}\\n\\nBill was injured.\\n\\nConflict Resolution Patterns. We see a straightforward overriding pattern in examples A-H involving ``Center-establishing'' pronouns: ATT overrides LF, and WK overrides ATT and LF. Such an overriding relation can be seen as a dynamic updating operation (;) (van Benthem et al., 1993) -- preferences are evaluated in turn, the later ones overriding the earlier ones: LF;ATT;WK. It may be the general pattern of ``changing preferences'' during utterance interpretation.\\n\\nExamples I-L involving ``Center-chaining'' pronouns show more or less the same pattern except that the overriding gets more difficult in some cases. It is more difficult when a SUBJECT pronoun chain supports a single maximally salient entity as in example I. This shows that the LF and ATT preferences in fact join forces to interact with the WK preferences. This intuition is expressed with brackets:\\n\\n[LF;ATT];WK. The ``retraction'' observed in example I still fits this pattern, but the increased difficulty in overriding is only implicit.\\n\\nLascarides and Asher (1993) illustrate patterns of defeasible rule interactions and the Penguin Principle defined below (\\n\\nmeans ``if\\n\\n,\\n\\nthen indefeasibly\\n\\n,'' and\\n\\nmeans ``if\\n\\n,\\n\\nthen\\n\\nnormally\\n\\n. ''): Nixon Diamond A conflict is unresolved resulting in an ambiguity or incoherence:\\n\\n.\\n\\nPenguin Principle A conflict is resolved by the more specific principle defeating the more general one:\\n\\nOn their account, any resolution of a conflict between two defeasible rules should be a case of the Penguin Principle. Does it explain all the conflict resolution patterns observed in pronoun interpretation?\\n\\nThe Penguin Principle explains some of the conflict resolution patterns -- for instance, the knowledge about specific agents, Terminator and Arnold, override the generic causal knowledge about hitting (examples G and H). There may also be a remote conceptual connection between the Penguin Principle and the pattern\\n\\n[LF;ATT];WKin the following line -- grammatical preferences (ATT and LF) tend to be more abstract than commonsense preferences (WK) about particular types of eventualities, so the more specific support wins (Kameyama et al., 1993). However, the LF, ATT, and WK rules apply on different data structures, and cannot always be reduced to an indefeasible implication (\\n\\n) as required in the Penguin Principle. For instance, hittee(x) can be\\n\\nsubject(x) or\\n\\ndepending on the sentence structure, so we cannot say that hittee(x) implies\\n\\nto derive the overriding pattern in example F. What additional kinds of conflict resolution inferences do we have then?\\n\\nThere are two additional conflict resolution patterns observed in the present examples, which I will call the Indefeasible Override and the Defeasible  Override, defined below: Indefeasible Override An indefeasible principle overrides a defeasible one:\\n\\nDefeasible Override Given an explicit overriding relation, one defeasible principle defeats another (even when\\n\\n):\\n\\n.\\n\\nThe Indefeasible Override follows from the monotonicity of classical implication (\\n\\n), and is an inherent principle in any nonmonotonic logic. It predicts the fact that the SEM rules override all the defeasible rules (examples C and E). The Defeasible Override captures a certain a priori given ``ranks'' or ``priorities'' among different sources of information, using the dynamic override (;) operator, where\\n\\nmeans ``\\n\\noverrides\\n\\n.'' It is motivated by the view that preferences come from different sources, and are associated with different ``degrees of defeasibility'' not necessarily in terms of the Penguin Principle. It enables us to state the override pattern\\n\\n[LF;ATT];WK while allowing a varying degree of difficulty for WK's overriding. I hope to define a logical system that axiomatizes these conflict resolution inferences.\\n\\nFurther Questions\\n\\nA number of questions related to the present topic have not been discussed. The first are logical questions. What are the connections with update logics (e.g., Veltman, 1993)? We can see that the grammar subsystem supports straight updating, whereas the pragmatics subsystem supports preferential updating or upgrading (van Benthem et al., 1993). The preference interaction patterns discussed here can perhaps be formulated as fine-grained upgrading inferences during utterance interpretation within the proposed utterance interpretation architecture. Can my proposal be couched in a system of preferential dynamic logic that combines elements of dynamic semantic theories and preferential models (e.g., McCarthy, 1980; Shoham, 1988)? Does the context as a multicomponent data structure proposed here also support the general contextual inferences such as lifting in the context logic (e.g., McCarthy, 1993; Buvac and Mason, 1993)?\\n\\nThere are also computational questions. Does the proposed discourse processing architecture with explicit contextual control of inferences actually help manage the computational complexity of the nonmonotonic reasoning in the pragmatic rule interactions?\\n\\nFinally, a cognitive question -- Does the proposed discourse processing architecture naturally extend to a more elaborate many-person discourse model that addresses the issue of coordinating different private contexts (e.g., Perrault, 1990; Thomason, 1990; Jaspars, 1994)?\\n\\nConclusions\\n\\nA discourse processing architecture with desirable computational properties consists of a grammar subsystem representing the space of possibilities and a pragmatics subsystem representing the space of preferences. Underspecified logical forms proposed in the computational literature define the grammar-pragmatics boundary. Utterance interpretation induces a complex interaction of defeasible rules in the pragmatics subsystem. Upon scrutiny of a set of examples involving intersentential pronominal anaphora, I have identified different groups of defeasible rules that determine the preferred transitions of different components of the dynamic context. There are grammatical preferences inducing fast entity-level inferences only indirectly suggesting the preferred discourse model, and commonsense preferences inducing slow proposition-level inferences directly determining the preferred discourse model. The attentional state in the context supports the formulation of attentional rules that significantly affect pronoun interpretation preferences. The observed patterns of conflict resolution among interacting preferences are predicted by a small set of inference patterns including the one that assumes an explicitly given overriding relation between rules or rule groups.\\n\\nReferences\\n\\nby2em\\n\\nAlshawi, Hiyan. 1990. Resolving Quasi Logical Forms. Computational Linguistics, 16(3), 133-144.\\n\\nAlshawi, Hiyan. ed. 1992. The Core Language Engine, The MIT Press, Cambridge, MA.\\n\\nAlshawi, Hiyan, and Richard Crouch. 1992. Monotonic Semantic Interpretation. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, Newark, DE, 32-39.\\n\\nAlshawi, Hiyan, and Jan van Eijck. 1989. Logical Forms in the Core Language Engine. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, Vancouver, Canada, 25-32.\\n\\nAsher, Nicholas and Michael Morreau. 1993. Commonsense Entailment: A Modal Theory of Nonmonotonic Reasoning. In Proceedings of the International Joint Conference on Artificial Intelligence, Chambray, France, 387-392.\\n\\nvan Benthem, Johan, Jan van Eijck, and Alla Frolova. 1993. Changing Preferences. Report CS-R9310. Institute for Logic, Language and Computation, University of Amsterdam.\\n\\nBrennan, Susan, Lyn Friedman, and Carl Pollard. 1987. A Centering Approach to Pronouns. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, 155-162.\\n\\nBuvac, Sasa and Ian Mason. 1993. Propositional Logic of Context. In Proceedings of the 11th National Conference on Artificial Intelligence, 412-419.\\n\\nCarter, David. 1987. Interpreting Anaphors in Natural Language Texts. Ellis Horwood, Chichester, Sussex, UK.\\n\\nGrdenfors, Peter and David Makinson. 1994. Nonmonotonic Inference Based on Expectations. Artificial Intelligence, 65, 197-245.\\n\\nGroenendijk, Jeroen and Martin Stokhof. 1991. Dynamic Predicate Logic. Linguistics and Philosophy, 14, 39-100.\\n\\nGrosz, Barbara, Aravind Joshi, and Scott Weinstein. 1983. Providing a Unified Account of Definite Noun Phrases in Discourse. In Proceedings of the 21st Meeting of the Association of Computational Linguistics, Cambridge, MA, 44-50.\\n\\nGrosz, Barbara, Aravind Joshi, and Scott Weinstein. 1986. Towards a computational theory of discourse interpretation. Unpublished manuscript. [The final version to appear in Computational Linguistics under the title ``Centering: A Framework for Modelling the Local Coherence of Discourse'']\\n\\nGrosz, Barbara and Candy Sidner. 1986. Attention, Intention, and the Structure of Discourse. Computational Linguistics, 12(3), 175-204.\\n\\nHeim, Irene. 1982. The Semantics of Definite and Indefinite Noun Phrases, Ph.D. Thesis, University of Massachusetts, Amherst.\\n\\nHobbs, Jerry. 1978. Resolving Pronoun References. Lingua, 44, 311-338. Also in B. Grosz, K. Sparck-Jones, and B. Webber, eds., Readings in Natural Language Processing, Morgan Kaufmann, Los Altos, CA, 1986, 339-352.\\n\\nHobbs, Jerry. 1983. An Improper Treatment of Quantification in Ordinary English. In Proceedings of the 21st Meeting of the Association of Computational Linguistics, Cambridge, MA, 57-63.\\n\\nHobbs, Jerry, Mark Stickel, Doug Appelt, and Paul Martin. 1993. Interpretation as Abduction. Artificial Intelligence, 63, 69-142.\\n\\nHudson D'Zmura, Susan. 1988. The Structure of Discourse and Anaphor Resolution: The Discourse Center and the Roles of Nouns and Pronouns. Ph.D. Thesis, University of Rochester.\\n\\nHwang, Chung Hee, and Lenhart K. Schubert. 1992a. Episodic Logic: A Comprehensive Semantic Representation and Knowledge Representation for Language Understanding. Technical Report, Dept. of Computer Science, University of Rochester, Rochester, NY.\\n\\nHwang, Chung Hee, and Lenhart K. Schubert. 1992b. Episodic Logic: A Situational Logic for Natural Language Processing. In P. Aczel, D. Israel, Y. Katagiri, and S. Peters, eds., Situation Theory and its Applications, Volume 3, CSLI, Stanford, CA.\\n\\nJaspars, Jan. 1994. Calculi for Constructive Communication, Ph.D. Thesis, University of Tilberg.\\n\\nJoshi, Aravind, and Steve Kuhn. 1979. Centered Logic: The Role of Entity Centered Sentence Representation in Natural Language Inferencing. In Proceedings of International Joint Conference on Artificial Intelligence, Tokyo, Japan, 435-439.\\n\\nJoshi, Aravind, and Scott Weinstein. 1981. Control of Inference: Role of Some Aspects of Discoruse Structure -- Centering. In Proceedings of International Joint Conference on Artificial Intelligence, Vancouver, Canada, 385-387.\\n\\nKameyama, Megumi. 1985. Zero Anaphora: The Case of Japanese. Ph.D. Thesis, Stanford University.\\n\\nKameyama, Megumi. 1986. A Property-sharing Constraints in Centering. In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, New York, NY, 200-206.\\n\\nKameyama, Megumi. 1994a. Indefeasible Semantics and Defeasible Pragmatics. CWI Report CS-R9441 and SRI Technical Note 544.\\n\\nKameyama, Megumi. 1994b. Stressed and Unstressed Pronouns: Complementary Preferences. In P. Bosch and R. van der Sandt, eds., Focus and Natural Language Processing, Institute for Logic and Linguistics, IBM, Heidelberg, 475-484.\\n\\nKameyama, Megumi. 1995. The Syntax and Semantics of the Japanese Language Engine. In R. Mazuka and N. Nagai, eds., Japanese Sentence Processing, Lawrence Erlbaum Associates, Hillsdale, NJ, 153-176.\\n\\nKameyama, Megumi, Rebecca Passonneau, and Massimo Poesio. Temporal Centering. 1993. In Proceedings of the 31st Meeting of the Association of Computational Linguistics, Columbus, OH, 70-77.\\n\\nKamp, Hans. 1981. A Theory of Truth and Semantic Representation. In J. Groenendijk, T. Janssen, and M. Stokhof, eds., Formal Methods in the Study of Language, Mathematical Center, Amsterdam, 277-322.\\n\\nKamp, Hans, and Uwe Reyle. 1993. From Discourse to Logic, Kluwer Academic Publishers, Dordrecht.\\n\\nKehler, Andrew. 1993. The Effect of Establishing Coherence in Ellipsis and Anaphora Resolution. In Proceedings of the 31st Meeting of the Association of Computational Linguistics, Columbus, OH, 62-69.\\n\\nLascarides, Alex, and Nicholas Asher. 1993. Temporal Interpretation, Discourse Relations, and Commonsense Entailment. Linguistics and Philosophy, 16, 437-493.\\n\\nLewis, David. 1979. Scorekeeping in a Language Game. Journal of Philosophical Logic, 8, 339-359.\\n\\nLifschitz, Vladimir. 1988. Circumscriptive Theories: A Logic-Based Framework for Knowledge Representation. Journal of Philosophical Logic, 17(4), 391-442.\\n\\nMaxwell, John, and Ronald Kaplan. 1993. The Interface between Phrasal and Functional Constraints. Computational Linguistics, 19(4), 571-590.\\n\\nMcCarthy, John. 1980. Circumscription-A Form of Non-monotonic Reasoning. Artificial Intelligence, 13(1,2), 27-39.\\n\\nMcCarthy, John. 1986. Applications of Circumscription to Formalizing Commonsense Knowledge. Artificial Intelligence, 28, 89-116.\\n\\nMcCarthy, John. 1993. Notes on Formalizing Context. In Proceedings of the International Joint Conference on Artificial Intelligence, Chambray, France, 555-560.\\n\\nPereira, Fernando and Martha Pollack. 1991. Incremental Interpretation. Artificial Intelligence, 50, 37-82.\\n\\nPereira, Fernando, and David Warren. 1980. Definite Clause Grammars for Language Analysis. Artificial Intelligence, 13, 231-278.\\n\\nPerrault, C. Ray. 1990. An Application of Default Logic to Speech Act Theory. In P. Cohen, J. Morgan, and M. Pollack, eds., Intentions in Communications, MIT Press, Cambridge, MA, 161-185.\\n\\nPoesio, Massimo. 1993. Discourse Interpretation and the Scope of Operators. Ph.D. Thesis, University of Rochester.\\n\\nPruest, Hub. 1992. On Discourse Structuring, VP Anaphora and Gapping. Ph.D. Thesis, University of Amsterdam.\\n\\nReyle, Uwe. 1993. Dealing with Ambiguities by Underspecification: Construction, Representation, and Deduction. Journal of Semantics, 10(2).\\n\\nSag, Ivan, and Jorge Hankamer. 1984. Toward a Theory of Anaphoric Processing. Linguistics and Philosophy, 7, 325-345.\\n\\nSgall, Petr, Eva Hajicov, and Jarmila Panevov.. 1986. The Meaning of the Sentence in its Semantic and Pragmatics Aspects, Reidel, Dordrecht and Academia, Prague.\\n\\nShoham, Yoav. 1988. Reasoning about Change: Time and Causality from the Standpoint of Artificial Intelligence, MIT Press, Cambridge, MA.\\n\\nSidner, Candy. 1983. Focusing in the comprehension of definite anaphora. In M. Brady and R. C. Berwick, eds., Computational Models of Discourse, The MIT Press, Cambridge, MA, 267-330.\\n\\nStalnaker, Robert, C. 1972. Pragmatics. In Davidson and Harman, eds., Semantics of Natural Language, Reidel, Dordrecht, 380-397.\\n\\nStalnaker, Robert, C. 1980. Assertion. In P. Cole, ed., Syntax and Semantics Vol.9: Pragmatics, Academic Press, New York, 315-332.\\n\\nThomason, Richmond. 1990. Propagating Epistemic Coordination through Mutual Defaults I. In R. Parikh, ed., Theoretical Aspects of Reasoning about Knowledge, Proceedings of the Third Conference (TARK 1990), Morgan Kaufmann, Palo Alto, CA, 29-38.\\n\\nVeltman, Frank. 1993. Defaults in Update Semantics. Manuscript, Department of Philosophy, University of Amsterdam. To appear in the Journal of Philosophical Logic.\\n\\nWilks, Yorick. 1975. A Preferential, Pattern-seeking Semantics for Natural Language Inference. Artificial Intelligence, 6, 53-74.\\n\\nFootnotes\\n\\nI would like to thank David Beaver, Johan van Benthem, Paul Dekker, Jan van Eijck, Jan Jaspars, Aravind Joshi, Alex Lascarides, Daniel Marcu, Becky Passonneau, Henritte de Swart, and Frank Veltman for helpful discussions and comments on earlier versions of the paper. The thoughtful comments by an anonymous reviewer helped reshape the focus of the paper. I also profited from the comments from the seminar participants at the University of Bielefeld and the University of Amsterdam. I would also like to thank those who responded to the pronoun interpretation questionnaire whose results are discussed herein. Part of the work was sponsored by project NF 102/62-356 (`Structural and Semantic Parallels in Natural Languages and Programming Languages'), funded by the Netherlands Organization for the Advancement of Research (N.W.O.). This separation of rule types does not imply a sequential ordering between the two processing modules. Different rule types can be interleaved for interpreting or generating a subsentential constituent. The same formal system can be viewed from different viewpoints -- as a system of rules, constraints, or inferences. Rules produce and transform structures in a system, constraints reduce possible structures, and inferences are used to reason about structures (e.g., manipulating assertions or drawing conclusions) as the ``logic'' in the standard sense.\\n\\nTo take a prominent example, in the ``parsing as deduction'' paradigm (Pereira and Warren, 1980), context-free rules are also seen as deductive inference rules. The rule S\\n\\nNP VP is translated into the inference rule NP(i,j)\\n\\nVP(j,k)\\n\\nS(i,k). I will not adhere to one particular viewpoint in this paper, and rather take advantage of the flexibility. Grammar rules can be seen from two viewpoints -- they eliminate as well as create possibilities. The former applies when communication is seen as incremental elimination of possible information states. The latter applies when it is seen as incremental increase of information content. I leave the choice open here. In contrast, the abduction-based system (Hobbs et al., 1993) does not separate grammar and pragmatics. All the rules are defeasible and directly interact in one big module. (The defeasibility of grammar rules is motivated by the fact of disfluencies in language use.) The result is an increased computational complexity. This architecture is in line with Stalnaker's (1972:385) conception: The syntactical and semantical rules for a language determine an interpreted sentence or clause; this, together with some features of the context of use of the sentence or clause, determines a truth value. An interpreted sentence, then, corresponds to a function from contexts into propositions, and a proposition is a function from possible worlds into truth values. A comment by Paul Dekker. I assume that various preferential decisions are interleaved rather than sequentially ordered within pragmatics. We have here an operational criterion for separating out grammar and pragmatics. It leads to a discovery of cross-linguistic variation in the grammar-pragmatics boundary.\\n\\nLong-distance dependency is a case in point (Kameyama, 1995). Grammatical functions will be in uppercase in order to avoid the ambiguity of these words. Some speakers indicated that they had to assume additional facts in order to make a plausible scenario -- for instance, in example A, ``Mary is a teacher, and she sent John home as a punishment''. The speakers seem to want some more information to make the judgment more conclusive. What are the relationships among these three people mentioned out of the blue? I realize that impoverished examples of this sort rarely occur in our real-life discourses. To sort out some rather delicate interplay of preferences, however, we need to start out with simplified examples. This is analogous to the use of the ``blocks world'' (i.e., the world of blocks) in AI. I will use the simple terminology of ``referent'' and ``coreference'' without committing to their realist connotation because this does not affect the points I wish to make in this paper. Another possible source of preference is the causal link between the two described eventualities, John's hitting Bill (e1) and someone disliking someone (e2). The preferred interpretation supports the causal link ``e1 because e2'', while the alternative interpretation, which nobody took, supports ``e1 therefore e2''. These could be stated in terms of discourse relations of Explanation and Cause (e.g., Lascarides and Asher, 1993).\\n\\nI'm not aware of any empirical studies of this kind of preference effects. I suspect that the two speakers who took the opposite interpretation used the sense of back close to ``again''. The Terminator is a cyborg played by Arnold Schwarzenegger in a popular science-fiction movie. Of interest here is the fact that the three speakers who knew nothing about what a ``Terminator'' is all interpreted that John was injured in example H. They clearly sensed ``something nasty and abnormal'' from this name alone. I will not discuss the partial order of propositions. Those entities that are ``inaccessible'' in the DRT sense do not participate in the salience ordering, or even if they do, they are below a certain minimal threshold of salience. In the centering model, the entities realized in\\n\\nare the ``forward\\n\\n\\n\\nlooking centers''\\n\\n(Cf), and\\n\\nCenteri-1 is the ``backward-looking center'' (Cb). Consituents' linear ordering and animacy are also relevant. This order also approximates the relative salience of entities in the output attentional state, as demonstrated in part in example J. There is a pragmatic difference between stressed and unstressed pronouns, which should be accounted for by an independent treatment of stress -- for example, in terms of a preference reversal function (Kameyama, 1994b). This paper concerns only unstressed pronouns. Except in a telegraphic register. This notion of the single maximally salient entity corresponds to the ``preferred center'' Cp (Grosz et al., 1986) that is determined solely by the GF ORDER. The difference here is that it is determined by both the Center and GF ORDER, predicting an indeterminacy in certain cases. What I have previously called retain is now called chain. It covers both CONTINUE and RETAIN technically distinguished by Grosz et al. (1986) and Brennan et al. (1987). This statement is intentionally left vague. See Pruest's (1992) MSCD operation for a general definition of parallelism preference, and my property-sharing constraint (Kameyama, 1986) for a subcase relevant to pronoun interpretation.\\n\\nmeans\\n\\nwhere p[X] means\\n\\n(update state X with p). In these definitions, I use the notations from Asher and Morreau's (1993) Commonsense Entailment (CE) logic as a theoretical meta-formalism without strictly adhering to the CE ontology. It follows from Cautious Monotonicity [A\\n\\nB, A\\n\\nC /\\n\\nA,B\\n\\nC]:\\n\\nbecause\\n\\n. Grdenfors and Makinson's (1994) use of expectation ordering in preferential reasonning achieves essentially the same effect. In the longer version of this paper (Kameyama, 1994a), a logical implementation of the preferential rule interactions is proposed using prioritized circumscription (McCarthy, 1980, 1986; Lifschitz, 1988), a nonmonotonic reasoning formalism in AI.\", metadata={'source': '../data/raw/cmplg-xml/9506016.xml'}),\n",
       " Document(page_content=\"Weak Subsumption Constraints for Type Diagnosis: An Incremental Algorithm\\n\\nWe introduce constraints necessary for type checking a higher-order concurrent constraint language, and solve them with an incremental algorithm. Our constraint system extends rational unification by constraints\\n\\nIntroduction\\n\\nWe give an algorithm which is at the heart of a type diagnosis system for a higher-order concurrent constraint language, viz. the\\n\\nand\\n\\n,\\n\\nand constraints\\n\\n.\\n\\nFor example, we allow\\n\\nas an instance\\n\\nof\\n\\neven if\\n\\nType Diagnosis.\\n\\nAs an illustrating example for the form of type diagnosis we have in mind, consider the following\\n\\nprogram:\\n\\nThis program declares four variables x,y,z, and p. It defines a relational abstraction p, which states that its two arguments u and v are related through the equation\\n\\n. Furthermore, it states the equality\\n\\nand\\n\\napplies p to yy.\\n\\nThis application\\n\\nreduces to a copy of the abstraction p with the actual arguments yy replaced for the formal ones uv:\\n\\nObserve how the abstraction p is  defined by reference to the global variable x, while the value of x is defined through an application of p:\\n\\n. Such a cycle is specific to the\\n\\nThe types of the variables involved are described by the following constraint. For ease of reading, we slightly abuse notation and pick the type variables identical to the corresponding object variables:\\n\\nis the relational type of p, and the application gives rise to the constraint\\n\\n, which says that y is constrained by both formal arguments of the procedure p. The subconstraint\\n\\nreflects the cyclic dependency between x and p. It says that y be in the set of instances of v which depends through\\n\\non x, and at the same time that x should be exactly\\n\\n.\\n\\nRelated Work.\\n\\nPlan of the Paper.\\n\\nConstraints and Semantics\\n\\nWe assume a signature\\n\\nof function symbols with at least two elements ranged over by\\n\\nf,g,h,a,b,c and an infinite\\n\\nset of base variables\\n\\nranged over by\\n\\n. If V is a further set of variables then\\n\\nstands for the set of all finite or infinite trees over signature\\n\\nand variables V. Trees of\\n\\nare always ranged over by s and t. The set of variables occurring in a tree t is denoted by\\n\\n. Sequences of variables are written as\\n\\n,\\n\\nor\\n\\n.\\n\\nWe build constraints over a set of constraint variables ranged over by x, y, z, u, v, w. Constraint variables must contain at least base variables. The syntax of our constraints\\n\\n,\\n\\nis as follows:\\n\\nAs atomic constraints we consider equations\\n\\nor\\n\\nand\\n\\nconstraints\\n\\n. Constraints are atomic constraints closed under conjunction. First-order formulae build over constraints\\n\\nare denoted by\\n\\n.\\n\\nWe define\\n\\nto be the least binary relation on\\n\\nsuch that\\n\\nis associative and commutative. For convenience, we shall use the following notation:\\n\\nAs semantic structures we pick tree-structures which we also call\\n\\nfor some set V. The domain of a tree-structure\\n\\nis the set of trees\\n\\n. Its interpretation is defined by\\n\\n.\\n\\nWe define the application\\n\\nof f to a sequences of sets of trees\\n\\nelementwise,\\n\\n.\\n\\nGiven a tree\\n\\n,\\n\\nthe set\\n\\nof weak instances of s is defined as the greatest fixed point of:\\n\\nNotice that this definition implies\\n\\n,\\n\\neven if\\n\\n. Let V1, V2 be two sets whose elements we call variables. A V1-V2-substitution\\n\\nis a\\n\\nmapping from V1 to\\n\\n. By homomorphic extension, every substitution can be extended to a mapping from\\n\\nto\\n\\n. The set of strong instances of s is defined by\\n\\n. Note that\\n\\n,\\n\\nand that\\n\\nif\\n\\n.\\n\\nUsing\\n\\ninstead of\\n\\nLet\\n\\nbe a V1\\n\\n\\n\\nV2\\n\\n\\n\\nsubstitution,\\n\\n,\\n\\nand\\n\\nconstraints such that\\n\\n,\\n\\n.\\n\\nThen we define:\\n\\nA V1\\n\\n\\n\\nV2\\n\\n\\n\\nsolution of\\n\\nis a V1\\n\\n\\n\\nV2\\n\\n\\n\\nsubstitution\\n\\nsatisfying\\n\\n.\\n\\nA constraint\\n\\nis called satisfiable, if there exists a V1-V2-solution for\\n\\n.\\n\\nThe notion of\\n\\nextends to arbitrary first\\n\\n\\n\\norder\\n\\nformulae\\n\\nin the usual way. We say that a formula\\n\\nis valid, if\\n\\nholds for all V1\\n\\n\\n\\nV2\\n\\n\\n\\nsubstitutions\\n\\nwith\\n\\n.\\n\\nIn symbols,\\n\\n.\\n\\nOur setting is a conservative extension of the usual rational unification problem. This means that free variables in the semantic domain do not affect equality constraints. A constraint\\n\\nis satisfiable in the\\n\\ntree\\n\\n\\n\\nmodel\\n\\n,\\n\\nif there\\n\\nexists a\\n\\nV-solution of\\n\\n.\\n\\nThe trees of\\n\\nare called ground trees.\\n\\nThe statement would be wrong for\\n\\n's containing weak subsumption constraints. For instance, consider the following\\n\\nwith\\n\\n:\\n\\nThis\\n\\nis not satisfiable in the model of ground trees, since the set\\n\\nis a singleton for all ground trees t, whereas any V1-V2-solution\\n\\nof\\n\\nhas to satisfy\\n\\n. However, there exists a\\n\\n\\n\\nsolution\\n\\nof\\n\\n,\\n\\nwhere\\n\\nis an singleton:\\n\\nWeak Subsumption vs. Sets of Weak Instances.\\n\\ncan be considered\\n\\nequivalent to:\\n\\nLet us write\\n\\nto say that the tree s has some direct subtree at f. A simulation between\\n\\nand\\n\\nis a relation\\n\\nsatisfying:\\n\\nIf\\n\\nthen\\n\\nNow, the weak subsumption preorder\\n\\nis defined by:\\n\\nWe have the following lemma:\\n\\nand Drre's notion of weak subsumption is that he does not require Arity Simulation, while we naturally do since we  start from constructor trees. For type checking, constructor trees seem more natural: For illustration note that the arity of a procedure is essential type information.\\n\\nA Non\\n\\n\\n\\nterminating Solution\\n\\nthere is a derivation\\n\\nfrom\\n\\nto\\n\\n. However, this intuitive algorithm loops due to the introduction of new variables.\\n\\nNote that some form of descending is necessary in order to derive the clash from the inconsistent constraint\\n\\nAlgorithm\\n\\nTo consider trees with free variables as set of instances means that we need to compute intersections of such sets and to decide their emptiness. When we simplify\\n\\nin a context\\n\\n, we have to compute the intersection of the sets of instances of y and z. In order to avoid the introduction of new variables we add a new class of variables to represent such intersections, and one new constraint. Intersection variables are defined as nonempty finite subsets of base variables. In order capture the intended semantics, we write\\n\\ninstead of\\n\\n.\\n\\nThe equality\\n\\non intersection variables is the equality on powersets, which  satisfies:\\n\\nWe call an x a component of y, if\\n\\nfor some z. The set of components of a variable x is denoted by\\n\\n.\\n\\nNote that\\n\\nAcknowledgements.\\n\\nWe would like to thank Ralf Treinen for pointing us to Drre's paper and the anonymous referees for useful remarks. The research reported in this paper has been supported by the Bundesminister fr Bildung, Wissenschaft, Forschung und Technologie (FTZ-ITW-9105), the Esprit Project ACCLAIM (PE 7195), the Esprit Working Group CCL (EP 6028), and a fellowship of the Graduiertenkolleg 'Kognition' at the Universitt des Saarlandes of the first author.\\n\\nOutlook\\n\\nBibliography\\n\\nA. Aiken and E. Wimmers. Type Inclusion Constraints and Type Inference. In ACM Conference on Functional Programming and Computer Architecture, pages 31-41, Copenhagen, Denmark, June 1993.\\n\\nRolf Backofen. Expressivity and Decidability of First-order Languages over Feature Trees. Doctoral Dissertation. Universitt des Saarlandes, Technische Fakultt, 66041 Saarbrcken, Germany, 1994.\\n\\nR. Cartwright and M. Fagan. Soft Typing. In ACM Conference on Programming Language Design and Implementation, pages 278-292, June 1991.\\n\\nJochen Drre and William C. Rounds. On Subsumption and Semiunification in Feature Algebras. In IEEE Symposium on Logic in Computer Science, pages 300-310, 1990.\\n\\nT. Frhwirth, E. Shapiro, M. Y. Vardi, and E. Yardeni. Logic Programs as Types for Logic Programs. In IEEE Symposium on Logic in Computer Science, pages 300-309, 1991.\\n\\nF. Henglein. Type Inference and Semi-Unification. In ACM Conference on LISP and Functional Programming, pages 184-197, January 1988.\\n\\nRobert Harper, Dave MacQueen, and Robin Milner. Standard ML. Technical Report ECS-LFCS-86-2, Department of Computer Science, University of Edinburgh, 1986.\\n\\nSverker Janson and Seif Haridi. Programming Paradigms of the Andorra Kernel Language. In International Logic Programming Symposium, pages 167-186, 1991.\\n\\nDexter Kozen, Jens Palsberg, and Michael I. Schwartzbach. Efficient Inference of Partial Types. Journal of Computer and System Sciences, 49(2):306-324, 1994. has also appared in Proc. FOCS, pp.363-371.\\n\\nA. Kfoury, J. Tiuryn, and P. Urzyczyn. The Undecidability of the Semi-Unification Problem. In ACM Symposium on Theory of Computation, pages 468-476, May 1990.\\n\\nA. J. Kfoury, J. Tiuryn, and Urzycyn. Type Recursion in the Presence of Polymorphic Recursion. ACM Transactions on Programming Languages and Systems, pages 290-311, 1993.\\n\\nMartin Mller and Joachim Niehren. A Type is a Type is a Type. Research report, DFKI, Stuhlsatzenhausweg 3, D-66123 Saarbrcken, Germany, 1995. In Preparation.\\n\\nMartin Mller. Type Diagnosis for a Higher-Order Concurrent Constraint Language. Doctoral Dissertation. Universitt des Saarlandes, Technische Fakultt, 66041 Saarbrcken, Germany, 1996. In Preparation.\\n\\nAlan Mycroft. Polymorphic Type Schemes and Recursive Definitions. In International Symposium on Programming, number 167 in Lecture Notes in Computer Science, 1984.\\n\\nJoachim Niehren and Andreas Podelski. Feature Automata and Recognizable Sets of Feature Trees. In Tapsoft, pages 356-375, April 1993.\\n\\nJoachim Niehren, Andreas Podelski, and Ralf Treinen. Equational and Membership Constraints for Infinite Trees. In Claude Kirchner, editor, Proceedings of the RTA '93, pages 106-120, 1993.\\n\\nBenjamin C. Pierce and David N. Turner. PICT User Manual. Technical Report to appear, LFCS, Edinburgh, 1995. Version 3.5g.\\n\\nC. J. Rupp, M. A. Rosner, and R. L. Johnson, editors. Constraints, Languages, and Computation. Academic Press, 1994.\\n\\nGert Smolka. A Foundation for Concurrent Constraint Programming. In Jean-Pierre Jouannaud, editor, Constraints in Computational Logics, volume 845 of Lecture Notes in Computer Science, pages 50-72, Mnchen, Germany, 7-9 September 1994.\\n\\nGert Smolka and Ralf Treinen. Records for Logic Programming. In International Conference and Symposium on Logic Programming, pages 240-254, November 1992. Has appeared in the Journal of Logic Programming, April 1994.\\n\\nGert Smolka and Ralf Treinen, editors. DFKI Oz Documentation Series. Stuhlsatzenhausweg 3, D-66123 Saarbrcken, Germany, 1994. Documentation and System available through anonymous ftp from ps-ftp.dfki.uni-sb.de or through WWW from http://ps-www.dfki.uni-sb.de.\\n\\nMitchell Wand. A Simple Algorithm and Proof for Type Inference. Fundamenta Informaticae, 10:115-122, 1987.\\n\\nAndrew K. Wright and Robert Cartwright. A Practical Soft Type System for Scheme. Technical Report 93-218, Rice University, December 1993.\\n\\nFootnotes\\n\\nNote that\\n\\nis different from a\\n\\nnamed\\n\\nabstraction\\n\\nbecause it is relational rather than functional, and also different to the Prolog program !-- MATH: $p(u,v)\\\\ \\\\mbox{:-}\\\\ v=cons(x\\\\: u).$ -->\", metadata={'source': '../data/raw/cmplg-xml/9506002.xml'}),\n",
       " Document(page_content=\"Training and Scaling Preference Functions for Disambiguation\\n\\nWe present an automatic method for weighting the contributions of preference functions used in disambiguation. Initial scaling factors are derived as the solution to a least-squares minimization problem, and improvements are then made by hill-climbing. The method is applied to disambiguating sentences in the ATIS (Air Travel Information System) corpus, and the performance of the resulting scaling factors is compared with hand-tuned factors. We then focus on one class of preference function, those based on semantic lexical collocations. Experimental results are presented showing that such functions vary considerably in selecting correct analyses. In particular we define a function that performs significantly better than ones based on mutual information and likelihood ratios of lexical associations.\\n\\nIntroduction\\n\\nThe importance of good preference functions for ranking competing analyses produced by language processing systems grows as the coverage of these systems improves. Increasing coverage usually also increases the number of analyses for sentences previously covered, bringing the danger of lower accuracy for these sentences. Large scale rule based analysis systems have therefore tended to employ a collection of functions to produce a score for sorting analyses in a preference order. In this paper we address two issues relating to the application of preference functions.\\n\\nCombining Multiple Preference Functions\\n\\nIn our case, the problem is formulated as follows. Each preference function is defined as a numerical (possibly real-valued) function on representations corresponding to the sentence analyses. A weighted sum of these functions is then used as the overall measure to rank the possible analyses of a particular sentence. We refer to the coefficients, or weights, used in this linear combination as the ``scaling factors'' for the functions. We determine these scaling factors automatically in order both to avoid the need for expert hand-tuning and to achieve performance that is at least locally optimal. We start with the solution to minimizing a squared-error cost function, a well known technique applied to many optimisation and classification problems. This solution is then enhanced by application of a hill-climbing technique.\\n\\nWord Sense Collocation Functions\\n\\nWe have experimented with a variety of specific functions which make use of collocations between word senses. The results we present show that these functions vary considerably in disambiguation accuracy, but that the best collocation functions are more effective than a function based on simple estimates of syntactic rule probabilities. In particular, the best collocation function performs significantly better than a related function that defines collocation strength in terms of mutual information, reducing the error rate in a disambiguation task from approximately 30% to approximately 10%.\\n\\nThe Experimental Setup\\n\\nDisambiguation Task\\n\\nThe SemColl function is the only one that prefers QH to QL. Because this function has a relatively large scaling factor, it is able to override the other four, which all prefer QL for syntactic reasons.\\n\\nTraining Data\\n\\nThe Penn Treebank contained around 650 ATIS trees, which we used during initial development of training and optimisation software. Some of the results in these initial trials were encouraging, but most appeared to be below reasonable thresholds of statistical significance. So, we concluded that it was worthwhile to produce more training data. For this purpose we developed a semi-automatic mechanism for producing skeletal constituent structure trees directly from QLF analyses proposed by our analyser. In order to make these trees compatible with the treebank and also to make them relatively insensitive to minor changes in semantic analysis, these QLF-induced trees simply consist of nested constituents with two categories, A (argument) and P (predication), corresponding to constituents induced by QLF term and form expressions respectively. The tree for the example sentence used above is: (P do (A I) get (A dinner) (P on (A this flight)))\\n\\nThe interactive software for producing the trees proposes constituents for confirmation by a user, and takes account of answers given, to minimize the number of interactive choices that need to be made. Of the 4615 sentences in our training set, the CLE produced an acceptable constituent structure for 4092 (about 89%). A skeletal tree for each of these 4092 sentences was created in this way and used in the various experiments whose results are described below. We do not directly address here the problems of applying preference functions to select the best analysis when none is completely correct; we assume, based on our experience with the spoken language translator, that functions and scaling factors trained on cases where a completely correct analysis exists will also perform fairly well on cases where one does not.\\n\\nTraining Score\\n\\nThe training score functions we considered for a QLF q with respect to a treebank tree t were functions of the form\\n\\nwhere Q is the set of string segments induced by the term and form expressions of q, T is the set of constituents in t, a1, a2, and a3 are positive constants, and the ``\\n\\n'' operator denotes set difference. The idea is to reward the QLF for constituents in common with the treebank to penalize it for differences. Trial and error led us to choose a1=1, a2=10, a3=0which penalizes hallucination of incorrect constituents (modeled by\\n\\n) more heavily than a shortfall in completeness (modeled by\\n\\n). These constants were fixed before we carried out the experiments whose results are presented below.\\n\\nThe explanation for setting a3 to 0 was that trees in the Penn Treebank contain many constituents that do not correspond to QLF form or term expressions; we had to avoid penalizing QLF analyses simply because the treebank uses a different kind of linguistic representation. For QLF-induced trees, where the correspondence is one-to-one, it is also reasonable to set a3 to 0 because when\\n\\ntends to be non-maximal. Among the 4092 sentences for which skeletal trees were derived, there were only five with alternative QLFs for which the training score value was the same with a3=0 but would be different if a3 were non-zero.\\n\\nComputing Scaling Factors\\n\\nWhen we first implemented a disambiguation mechanism of the kind described above, an initial set of scaling factors was chosen by hand according to knowledge of how the particular raw preference functions were computed and introspection about the `strength' of the functions as indicators of preference. These initial scaling factors were subsequently revised according to their observed behaviour in ranking analyses, eventually leading to reasonably well behaved rankings.\\n\\nHowever, as suggested earlier, there are a number of disadvantages to manual tuning of scaling factors. These include the effort spent in maintaining the parameters; this effort is greater for those with less knowledge of how the raw preference functions are computed, since this increases the effort for trial-and-error tuning. A point of diminishing returns is also reached after which further attempts at improvement through hand-tuning often turn out to be counter-productive. Another problem was that it became difficult to detect preference functions that were ineffective, or simply wrong, if they were given sufficiently low scaling factors. Probably a more serious problem is that the contributions of different preference functions to selecting the most plausible analyses seem to vary from one sublanguage to another. These disadvantages point to the need for automatic procedures to determine scaling factors that optimise preference function rankings for a particular sublanguage.\\n\\nIn our framework, a numerical `preference score' is computed for each of the alternative analyses, and the analyses are ranked according to this score. As mentioned earlier, the preference score is a weighted sum of a set of preference functions: Each preference function fj takes a complete QLF representation qi as input, returning a numerical score sij, the overall preference score being computed by summing over the product of function scores with their associated scaling factors cj:\\n\\nCollection Procedure\\n\\nThe training process begins by analysing the corpus sentences and computing, for each analysis of each sentence, the training score of the analysis with respect to the manually-approved skeletal tree and the (unscaled) values of the preference functions applied to that analysis.\\n\\nOne source of variation in the data that we want to ignore in order to derive scaling factors appropriate for selecting QLFs is the fact that preference function values for an analysis often reflect characteristics shared by all analyses of a sentence, as much as the differences between alternative analyses. For example, a function that counts the occurrences of certain constructs in a QLF will tend to give higher values for QLFs for longer sentences. In the limit, one can imagine a function\\n\\nthat, for an N-word sentence, returned a value of N+G for a QLF with training score G with respect to the skeletal tree. Such a function, if it existed, would be extremely useful, but (if sentence length were not also considered) would not be a particularly accurate predictor of QLF training score.\\n\\nIn order to discount irrelevant intersentence variability, both the training score with respect to the skeletal tree and all the preference function scores are therefore relativised by subtracting from them the corresponding values for the analysis of that sentence which best matches the skeletal tree. If the best match is shared by several analyses, the average for those analyses is subtracted. The relativised training score is the distance function with respect to which the first stage of scaling factor calculation takes place. It can be seen that the relativised results of our hypothetical preference function\\n\\nare a perfect predictor of relativised training score. Consider, for example, a six-word sentence with three QLFs, two of which, q1 and q2, have completely correct skeletal tree structures, and the third of which, q3, does not. Suppose also that the training scores and the scores assigned by preference functions,\\n\\n, f1 and f2 are as follows:\\n\\nLeast Squares Calculation\\n\\nAn initial set of scaling factors is calculated in a straightforward analytic way by approximating gi, the relativised training score of qi, by\\n\\n, where cj is the scaling factor for preference function fj and zij is the relativised score assigned to qi by fj. We vary the values of cj to minimize the sum, over all QLFs for all training sentences, of the squares of the errors in the approximation,\\n\\nDefining the error function as a sum of squares of differences in this way means that the minimum error is attained when the derivative with respect to each ck,\\n\\n, is zero. These linear simultaneous equations, one for each of\\n\\nIterative Scaling Factor Adjustment\\n\\nThe least-squares scaling factors are therefore adjusted iteratively by a hill-climbing procedure that directly examines the QLF choices they give rise to on the training corpus. Scaling factors are altered one at a time in an attempt to locally optimise the number of correct disambiguation decisions, i.e. the number of training sentences for which a QLF with a correct skeletal tree receives the highest score.\\n\\nA step in the iteration involves calculating the effect of an alteration to each factor in turn. If factors\\n\\nare held constant, it is easy to find a set (possibly empty) of real-valued intervals\\n\\n[uij,vij] such that a correct choice will be made on sentence i if\\n\\n. By collecting these intervals for all the functions and for all the sentences in the training corpus, one can determine the effect on the number of correct disambiguation decisions of any alteration to any single scaling factor. The alteration selected is the one that gives the biggest increase in the number of sentences for which a correct choice is made. When no increase is possible, the procedure terminates. We found that convergence tends to be fairly rapid, with the number of steps seldom exceeding the number of scaling factors involved (although the process does occasionally change a scaling factor it has previously altered, when intervening changes make this appropriate).\\n\\nOne of the functions we used shows the limitations of least-squares scaling factor optimisation, alluded to above, in quite a dramatic way. The function in question returns the number of temporal modifiers in a QLF. Its intended purpose is to favour readings of utterances like ``Atlanta to Boston Tuesday'' where ``Tuesday'' is a temporal modifier of the (elliptical) sentence rather than forming a compound noun with ``Boston''. Linear scaling always gives this function a negative weight, causing temporal modifications to be downgraded, and in fact the relativised training score of a QLF turns out to be negatively correlated with the number of temporal modifiers it contains. However, the intuitions that led to the introduction of the function do seem to hold for QLFs that are close to being correct, and therefore iterative adjustment makes the weight positive.\\n\\nComparing Scaling Factor Sets\\n\\n``Normalized'' factors: the magnitude of each factor is the inverse of the standard deviation of the preference function in question, making each function contribute equally. A factor is positive if it correlates positively with training scores, negative otherwise.\\n\\nFactors chosen and tuned by hand for ATIS sentences before the work described in this paper was done, or, for functions developed during the work described here, without reference to any automatically-derived values.\\n\\nThe performance of each set of factors was evaluated as follows. The set of 4092 sentences with skeletal trees was divided into five subsets of roughly equal size. Each subset was ``held out'' in turn: the functions and scaling factors were trained on the other four subsets, and the system was then evaluated on the held-out subset. The system was deemed to have correctly processed a sentence if the QLF to which it assigned the highest score agreed exactly with the corresponding skeletal tree.\\n\\nA possible objection to this analysis is that, because QLFs are much richer structures than constituent trees, it is possible for a QLF to match a tree perfectly but have some other characteristic that makes it incorrect. In general, the principle source of such discrepancies is a wrong choice of word sense, but pure sense ambiguity (i.e. different predicates for the same syntactic behaviour of the same word) turns out to be extremely rare in the ATIS corpus. An examination of the selected QLFs for the 20+36=56 sentences making up the + and -values for the comparison between the least squares and hill climbing factor sets showed that in no case did a QLF have a correct constituent structure but fail to be acceptable on other criteria. Thus while the absolute percentage correctness figures for a set of scaling factors may be very slightly (perhaps up to 1%) overoptimistic, this has no noticeable effect on the differences between factor sets.\\n\\nLexical Semantic Collocations\\n\\nIn this section we move from the problem of calculating scaling factors to the other main topic of this paper, showing how our experimental framework can be used diagnostically to compare the utility of competing suggestions for preference functions. We refer to the variant of collocations we used as lexical semantic collocations because (i) they are collocations between word senses rather than lexical items, and (ii) the relationships used are often deeper than syntactic relations (for example the relations between a verb and its subject are different for passive and active sentences).\\n\\nThe semantic collocations extracted from QLF expressions take the form of (H1,R,H2) triples where H1 and H2 are the head predicates of phrases in a sentence and R indicates the relationship (e.g. a preposition or an argument position) between the two phrases in the proposed analysis. For this purpose, the triple derivation software abstracted away from proper names and some noun and verb predicates when they appeared as heads of phrases, replacing them by hand-coded class predicates. For example, predicates for names of meals are mapped onto the class name cc_SpecificMeal on the grounds that their distributions in unseen sentences are likely to be very similar.\\n\\nSome of the triples for the high-attachment QLF for ``Do I get dinner on this flight?'' are: (get_Acquire,2,personal) (get_Acquire,3,cc_SpecificMeal) (get_Acquire,on,flight_AirplaneTrip) The first two triples correspond to the agent and theme positions in the predicate for get, while the third expresses the vital PP attachment. In the triple set for the other QLF, this triple is replaced by (cc_SpecificMeal,on,flight_AirplaneTrip).\\n\\nMutual information: this relates the probability\\n\\nP1(a) P2(b) P3(c) of the triple (a,b,c) assuming independence between its three fields, where Pp(x) is the probability of observing x in position p, with the probability Aestimated from actual observations of triples derived from analyses ranked highest (or joint highest) in training score. More specifically, we use\\n\\nln(A/(P1(a)P2(b)P3(c))).\\n\\nMean distance: the average of the relativised training score for all QLF analyses (not necessarily highest ranked ones) which include the semantic collocation corresponding to the triple. In other words, the mean distance value for a triple is the mean amount by which a QLF giving rise to that triple falls short of a perfect score.\\n\\nComputation of the mutual information and\\n\\nfunctions for triples involves the simple smoothing technique, suggested by Ken Church, of adding 0.5 to actual counts.\\n\\nFrom these five functions on triples we define five semantic collocation preference functions applied to QLFs, in each case by averaging over the result of applying the function to each triple derived from a QLF. We refer to these functions by the same names as their underlying functions on triples. The collocation functions are normalized by multiplying up by the number of words in the sentence to which the function is being applied. This normalization keeps scores for QLFs in the same sentence comparable, while at the same time ensuring the triple function scores tend to grow with sentence length in the same way that the non-collocation functions tend to do. Thus the optimality of a set of scaling factors is relatively insensitive to sentence length.\\n\\nOur use of the mean distance function was motivated by the desire to take into account additional information from the training material which is not exploited by the other collocation functions. Specifically, it takes into account all analyses proposed by the system, as well as the magnitude of the training score. In contrast, the other collocation functions only make use of the training score to select the best analysis of a sentence, discarding the rest. Another way of putting this is that the mean distance function is making use of negative examples and a measure of the degree of unacceptability of an analysis.\\n\\nComparing Semantic Collocation Functions\\n\\nTo arrive at the figures shown, where a function judged N QLFs equally plausible, of which 0 [ G [ N were correct, we assigned a fractional count\\n\\nto that sentence; a random choice among the N QLFs would pick a correct one with probability\\n\\n. For significance tests, which require binary data, we took a function as performing correctly only if all the QLFs it selected were correct. Such ties did not occur at all for the other experiments reported in this paper.\\n\\nA pairwise comparison of the results shows that all the differences between collocational functions are statistically highly significant. The syntactic rule cost function is significantly worse than all the collocational functions except the mutual information one, for which the difference is not significant either way. (There may, of course, exist better syntactic functions than the one we have tried.) The mean distance function is much superior to all the others when acting alone. Presumably, this function has an edge over the other functions because it exploits the additional information from negative examples and degree of correctness.\\n\\nand\\n\\nfunctions is no longer quite so clear cut, and the relative advantage of the mean distance function compared with the\\n\\nfunction is less. It may be that other preference functions make up for some shortfall of the\\n\\nfunction that is, at least in part, taken account of by the mean distance function.\\n\\nConclusion\\n\\nWe have presented a relatively simple analytic technique for automatically determining a set of scaling factors for preference functions used in semantic disambiguation. The initial scaling factors produced are optimal with respect to a score provided by a training procedure, and are further improved by comparison with instances of the task they are intended to perform. The experimental results presented indicate that, by using a fairly crude training score measure (comparing only phrase structure trees) with a few thousand training sentences, the method can yield a set of scaling factors that are significantly better than those derived by a labour intensive hand tuning effort.\\n\\nWe have also confirmed empirically that considerable differences exist between the effectiveness of differently formulated collocation functions for disambiguation. The experiments provide a basis for selecting among different collocational functions, and suggest that a collocation function must be evaluated in the context of other functions, rather than on its own, if the correct selection is to be made.\\n\\nAcknowledgements\\n\\nWe would like to thank Manny Rayner for many useful suggestions in carrying out this work, for making the selections necessary to create the database of skeletal trees, and for helping with inspection of experimental results. The paper has also benefited from discussions with Fernando Pereira, Ido Dagan, Michael Collins, Steve Pulman and Jaan Kaja, and from the comments of four anonymous CL referees.\\n\\nMost of the work reported here was carried out under a Spoken Language Translation project funded by Telia (formerly Televerket/Swedish Telecom). Other parts were done under the CLARE project (JFIT project IED4/1/1165), funded by the UK Department of Trade and Industry, SRI International, the Defence Research Agency, British Telecom, British Petroleum and British Aerospace.\\n\\nReferences\\n\\nAgns, M-S., et al. 1993. Spoken Language Translator: First Year Report. SRI International Cambridge Computer Science Research Centre, Report 043.\\n\\nAlshawi, H.  and R.  Crouch. 1992. ``Monotonic Semantic Interpretation''. Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, Newark, Delaware, 32-39.\\n\\nAlshawi, H., ed. 1992. The Core Language Engine. Cambridge, Massachusetts: The MIT Press.\\n\\nCalzolari, N. and R. Bindi. 1990. ``Acquisition of Lexical Information from a Large Textual Italian Corpus''. Proceedings of the 13th International Conference on Computational Linguistics, 3:54-59.\\n\\nChang, J., Y. Luo, and K. Su. 1992. ``GPSM: A Generalized Probabilistic Semantic Model for Ambiguity Resolution''. Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, 177-192.\\n\\nChurch, K.W. and P. Hanks. 1990. ``Word Association Norms, Mutual Information, and Lexicography''. Computational Linguistics 16:22-30.\\n\\nDagan, I., S. Marcus and S. Markovitch. 1993. ``Contextual Word Similarity and Estimation from Sparse Data''. Proceedings of the 31st meeting of the Association for Computational Linguistics, ACL, 164-171.\\n\\nDixon, W.J. and F.J. Massey. 1968. Introduction to Statistical Analysis, third edition. New York: McGraw-Hill.\\n\\nDunning, T. ``Accurate Methods for Statistics of Surprise and Coincidence''. Computational Linguistics 19:61-74.\\n\\nGale, W.A. and K.W. Church. 1991. ``Identifying Word Correspondences in Parallel Texts''. Proceedings of the DARPA Speech and Natural Language Workshop, Morgan Kaufmann, 152-157.\\n\\nHindle, D. and M. Rooth. 1993. ``Structural Ambiguity and Lexical Relations''. Computational Linguistics. 19:103-120.\\n\\nHobbs, J. R. and J. Bear. 1990. ``Two Principles of Parse Preference''. Vol. 3, Proceedings of the 13th International Conference on Computational Linguistics, Helsinki, 162-167.\\n\\nMarcus, M.P., B. Santorini, and M.A. Marcinkiewicz. 1993. ``Building a Large Annotated Corpus of English: the Penn Treebank''. Computational Linguistics 19:313-330.\\n\\nMcCord, M.C. 1989. ``A New Version of Slot Grammar''. IBM Research Report RC 14506, IBM Thomas J. Watson Research Center, Yorktown Heights, New York.\\n\\nMcCord, M.C. 1993. ``Heuristics for Broad-Coverage Natural Language Parsing''. Proceedings of the ARPA Human Language Technology Workshop. Los Altos: Morgan Kaufmann, 127-132.\\n\\nMoore, D. S. and G. P. McCabe. 1989. Introduction to the Practice of Statistics. New York/Oxford: Freeman.\\n\\nOstendorf, M., A. Kannan, S. Austin, O. Kimball, R. Schwartz, J.R. Rohlicek. 1991. ``Integration of Diverse Recognition Methodologies Through Reevaluation of N-Best Sentence Hypotheses''. Proceedings of the DARPA Speech and Natural Language Workshop, 83-87.\\n\\nPereira, F., N. Tishby and L. Lee. 1993. ``Distributional Clustering of English Words''. Proceedings of the 31st meeting of the Association for Computational Linguistics, ACL, 183-190.\\n\\nRayner, M., D. M. Carter, V. Digalakis and P. Price. 1994 (to appear). ``Combining Knowledge Sources to Reorder N-best Speech Hypothesis Lists'', Proceedings of the ARPA HLT Meeting.\\n\\nResnik, P. and M. A. Hearst. 1993. ``Structural Ambiguity and Conceptual Relations''. Proceedings of the Workshop on Very Large Corpora, ACL, 58-64.\\n\\nSekine, S., J. J. Carroll, S. Ananiadou, and J. Tsujii. 1992. ``Automatic Learning for Semantic Collocation''. Proceedings of the Third Conference on Applied Natural Language Processing, ACL, 104-110.\\n\\nFootnotes\\n\\nThe hand-parsed sub-corpus was that on the ACL DCI CD-ROM 1 of September 1991. The larger corpus, used for the bulk of the work reported here, consisted of 4615 class A and D sentences from the ATIS-2 training corpus. These were all such sentences of up to fifteen words that we had access to at the time, excluding a set of randomly selected sentences that were set aside for other testing purposes. Finding a global optimum would of course be desirable. However, inspection of the results, over various conditions, of the iterative scheme presented here did not suggest that the introduction of a technique such as simulated annealing, which in general can improve the prospect of finding a more global optimum, would have had much effect on performance. An algorithm based on gradient descent might appear preferable, on the grounds that it would alter all factors simultaneously and have a better chance of locating a global optimum. However, the objective function, the number of correct disambiguation decisions, varies discontinuously with the scaling factors, so no gradients can be calculated. We estimate the probability of occurrence of a syntactic rule R as the number of occurrences of R leading to QLFs with correct skeletal trees, divided by the number of occurrences of all rules leading to such QLFs.\", metadata={'source': '../data/raw/cmplg-xml/9408013.xml'}),\n",
       " Document(page_content=\"Clustering Words with the MDL Principle\\n\\nWe address the problem of automatically constructing a thesaurus by clustering words based on corpus data. We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs, and propose a learning algorithm based on the Minimum Description Length (MDL) Principle for such estimation. We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering, and found that the former outperforms the latter. We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus. Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation.\\n\\nIntroduction\\n\\nWe empirically evaluated the effectiveness of our method. In particular, we compared the performance of an MDL-based simulated annealing algorithm in hierarchical word clustering against that of one based on the Maximum Likelihood Estimator (MLE, for short). We found that the MDL-based method performs better than the MLE-based method. We also evaluated our method by conducting pp-attachment disambiguation experiments using a thesaurus automatically constructed by it and found that disambiguation results can be improved.\\n\\nSince some words never occur in a corpus, and thus cannot be reliably classified by a method solely based on corpus data, we propose to combine the use of an automatically constructed thesaurus and a hand made thesaurus in disambiguation. We conducted some experiments in order to test the effectiveness of this strategy. Our experimental results indicate that combining an automatically constructed thesaurus and a hand made thesaurus widens the   coverage of our disambiguation method, while maintaining  high accuracy.\\n\\nThe Problem Setting\\n\\nA method of constructing a thesaurus based on corpus data usually consists of the following three steps: (i) Extract co-occurrence data (e.g. case frame data, adjacency data) from a corpus, (ii) Starting from a single class (or each word composing its own class), divide (or merge) word classes based on the co-occurrence data using some similarity (distance) measure. (The former approach is called `divisive,' the latter `agglomerative.') (iii) Repeat step (ii) until some stopping condition is met, to construct a thesaurus (tree). The method we propose here consists of the same three steps.\\n\\nClustering with MDL\\n\\nIn what follows, we will describe in detail how the description length is to be calculated in our current context, as well as our simulated annealing algorithm based on MDL.\\n\\nCalculating Description Length\\n\\nGiven a model M and data S, its total description length L(M) is computed as the sum of the model description length\\n\\nLmod(M), the description length of its parameters\\n\\nLpar(M), and the data description length\\n\\nLdat(M). (We often refer to\\n\\nLmod(M) + Lpar(M) as the model description length). Namely,\\n\\nLpar(M) is calculated by\\n\\nwhere\\n\\nf(Cn,Cv) denotes the observed frequency of the noun verb pairs belonging to cluster (Cn,Cv).\\n\\nWith the description length of a model defined in the above manner, we wish to select a model having the minimum description length and output it as the result of clustering. Since the model description length Lmod is the same for each model, in practice we only need to calculate and compare\\n\\nL'(M) = Lpar(M) + Ldat(M).\\n\\nA Simulated Annealing\\n\\n\\n\\nbased Algorithm\\n\\nRelated Work\\n\\nAdvantages of Our Method\\n\\nIn this section, we elaborate on the merits of our method.\\n\\nExperimental Results\\n\\nWe describe our experimental results in this section.\\n\\nExperiment 1: MDL v.s. MLE Experiment 2: Qualitative Evaluation Experiment 3: Disambiguation\\n\\nWe also evaluated our method by using a constructed thesaurus in a pp-attachment disambiguation experiment.\\n\\nConcluding Remarks\\n\\nIn the future, hopefully with larger training data size, we plan to construct larger thesauri as well as to test other clustering algorithms.\\n\\nAcknowledgement\\n\\nWe thank Mr.K.Nakamura, Mr.T.Fujita, and Dr.K.Kobayashi of NEC CC Res. Labs. for their constant encouragement. We thank Dr.K.Yamanishi of CC Res. Labs. for his comments. We thank Ms.Y.Yamaguchi of NIS for her programming effort.\\n\\nBibliography\\n\\nAndrew R. Barron and Thomas M. Cover. 1991. Minimum complexity density estimation. IEEE Transaction on Information Theory, 37(4):1034-1054.\\n\\nPeter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):283-298.\\n\\nThomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley  Sons Inc.\\n\\nIdo Dagan, Shaul Marcus, and Shaul Makovitch. 1992. Contextual word similarity and estimation from sparse data. Proceedings of the 30th ACL, pages 164-171.\\n\\nWilliams A. Gale and Kenth W. Church. 1990. Poor estimates of context are worse than none. Proceedings of the DARPA Speech and Natural Language Workshop, pages 283-287.\\n\\nDonald Hindle and Mats Rooth. 1991. Structural ambiguity and lexical relations. Proceedings of the 29th ACL, pages 229-236.\\n\\nDonald Hindle. 1990. Noun classification from predicate-argument structures. Proceedings of the 28th ACL, pages 268-275.\\n\\nHang Li and Naoki Abe. 1995. Generalizing case frames using a thesaurus and the MDL principle. Proceedings of Recent Advances in Natural Language Processing, pages 239-248.\\n\\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The penn treebank. Computational Linguistics, 19(1):313-330.\\n\\nGeorge A. Miller, Richard Beckwith, Chirstiane Fellbaum, Derek Gross, and Katherine Miller. 1993. Introduction to WordNet: An on-line lexical database. Anonymous FTP: clarity.princeton.edu.\\n\\nFernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of english words. Proceedings of the 31st ACL, pages 183-190.\\n\\nJ. Ross Quinlan and Ronald L. Rivest. 1989. Inferring decision trees using the minimum description length principle. Information and Computation, 80:227-248.\\n\\nJorma Rissanen. 1978. Modeling by shortest data description. Automatic, 14:37-38.\\n\\nJorma Rissanen. 1983. A universal prior for integers and estimation by minimum description length. The Annals of Statistics, 11(2):416-431.\\n\\nJorma Rissanen. 1984. Universal coding, information, predication and estimation. IEEE Transaction on Information Theory, 30(4):629-636.\\n\\nJorma Rissanen. 1986. Stochastic complexity and modeling. The Annals of Statistics, 14(3):1080-1100.\\n\\nJorma Rissanen. 1989. Stochastic Complexity in Statistical Inquiry. World Scientific Publishing Co.\\n\\nAndreas Stolcke and Stephen Omohundro. 1994. Inducing probabilistic grammars by bayesian model merging. Proceedings of ICGI'94.\\n\\nTakenobu Tokunaga, Makoto Iwayama, and Hozumi Tanaka. 1995. Automatic thesaurus construction based-on grammatical relations. Proceedings of IJCAI'95.\\n\\nKenji Yamanishi. 1992. A learning criterion for stochastic rules. Machine Learning, 9:165-203.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9605014.xml'}),\n",
       " Document(page_content=\"NLG vs. Templates\\n\\nOne of the most important questions in applied NLG is what benefits (or `value-added', in business-speak) NLG technology offers over template-based approaches. Despite the importance of this question to the applied NLG community, however, it has not been discussed much in the research NLG community, which I think is a pity. In this paper, I try to summarize the issues involved and recap current thinking on this topic. My goal is not to answer this question (I don't think we know enough to be able to do so), but rather to increase the visibility of this issue in the research community, in the hope of getting some input and ideas on this very important question. I conclude with a list of specific research areas I would like to see more work in, because I think they would increase the `value-added' of NLG over templates.\\n\\nIntroduction\\n\\nThere are thousands, if not millions, of application programs in everyday use that automatically generate texts; but probably fewer than ten of these programs use the linguistic and knowledge-based techniques that have been studied by the Natural-Language Generation (NLG) community. The other 99.9% of systems use programs that simply manipulate character strings, in a way that uses little, if any, linguistic knowledge. For lack of a better name, I will call this the `template' approach.\\n\\nIn order for NLG technology to make it out of the lab and into everyday fielded application systems, the NLG community will need to prove that there are at least some niches where using linguistic/AI approaches in a text-generation system provides real commercial advantages, such as reducing the effort required to build (or maintain) the system, or improving the effectiveness of the generated texts. Determining under what conditions and in what aspects NLG techniques are `better' than character-string manipulation is of utmost importance to the applied NLG community, and should also be of interest to the research community; if nothing else, research funding for NLG is likely to increase if there are a large number of successful fielded systems that use NLG technology.\\n\\nIn this paper, I will use the term automatic text generation (ATG) to refer to any computer program that automatically produces texts from some input data, regardless of whether NLG or template technology is used internally. The topic of this paper is thus when is NLG `appropriate technology' for building ATG systems, and when should simpler approaches be used. My goal is not to provide a definitive answer to this question, because I don't think we (currently) know enough to be able to do this, but rather to present the issues, summarize comments made by other people, and present some opinions of my own. Hopefully this will help start a discussion within the community about this very important but (so far) somewhat ignored issue.\\n\\nTemplate systems\\n\\nAll ATG systems are, of course, simply computer programs that run on some input data and produce an output (the text) from this data. Non-linguistic (`template') text-generation is done via manipulating character strings; the user writes a program which includes statements such as `include XXXXX if condition Y is true, and YYYYY otherwise.' This program can be written directly in a programming language such as Lisp or C++, or it can be specified via a `mail-merge' system which allows conditional texts (eg, Microsoft Word). The key difference between this approach and NLG is that all manipulation is done at the character string level; there is no attempt to represent the text in any deeper way, at either the syntactic or `text-planning' level.\\n\\nTo the best of my knowledge, most programming languages and mail-merge environments provide very little, if any, support for manipulating texts in even the simplest `linguistic' manner. For programming languages, the most sophisticated feature that I am aware of is the ~P construct in the Lisp format function, which will do some simple pluralizations (eg, win vs. wins, or try vs. tries) depending on whether a numeric parameter is one or not.\\n\\nMail-merge systems can have slightly more sophisticated capabilities, such as automatically capitalizing an inserted word if it is the first word of a sentence. However, even something as simple as changing pronouns according to gender needs to be explicitly programmed. Some mail-merge systems are integrated with grammar checkers that might in theory be able to handle some low-level syntactic problems such as verb agreement, a vs. an, and elimination of multiple commas; however, current grammar checkers may not be robust enough to be able to do this in a reliable fashion.\\n\\nExample: Apple Balloon Help\\n\\nA simple example of template-based generation is the Apple Macintosh Balloon Help system. It can produce texts such as This is the kind of item displayed at left. This shows that test data is a(n) Microsoft Word document. and This is a folder -- a place to store related files. Folders can contain files and other folders.\\n\\nThe icon is dimmed because the folder is open. In the first text, test data and Microsoft Word were inserted into template slots for `filename' and `application program.' Note the use of a(n); even this simple type of agreement is not done in the Balloon Help system. In the second text, the last sentence (The icon is dimmed because the folder is open) only appears when the mouse is positioned over an open folder; just the first two sentences will appear if the mouse is positioned over a closed folder. This is an example of conditional text.\\n\\nExample: Employee Appraiser and Performance Now\\n\\nTwo more sophisticated `non-linguistic' automatic text-generation systems are Austin-Haynes Employee Appraiser and KnowledgePoint's Performance Now. Both of these systems help managers write appraisals of employees (eg, for justifying salary increases). Each system provides a set of general evaluation topics, such as Communication, which are broken up into more specific subtopics, such as Communicates ideas verbally. Managers give employees rankings on each of these subtopics, and the system then composes a complete appraisal, which the manager can edit in a word processor.\\n\\nIn Employee Appraiser, when the manager chooses a subtopic rating, the system retrieves an appropriate paragraph from a library and does some simple linguistic processing. In particular, the manager can specify whether the employee is male or female, and whether the report should be written in second or third person. This affects the pronouns used in the text (eg, he, she, or you), and also verb conjugation (eg, you do vs. he does). This is the most sophisticated syntactic processing that I am aware of in a `template' system.\\n\\nPerformance Now does less syntactic processing (it does not allow the manager to choose between second and third person; only third-person is possible), but it does do some simple sentence planning. In particular, Performance Now combines all the information about a particular high-level topic (such as Communication) into a single paragraph, and this requires the system to use conjunctions and pronouns, and to add initial conjuncts (eg, Furthermore) to sentences.\\n\\nAn example output of Performance Now is Bert does not display the verbal communication skills required, and his written communications fall short of the quality needed. Additionally, he does not exhibit the listening and comprehension skills necessary for satisfactory performance of his job. The system has composed this from three separate phrases retrieved from its library. The first two phrases are combined with and to produce the first sentence above. The third phrase is left as a separate sentence, but the conjunct Additionally is added to it, and the subject is pronominalized. The system also orders phrases by putting the most positive ones first, and most negative ones last (this is not shown in the above example).\\n\\nPerformance Now performs the most sophisticated sentence-planning of any `template' system that I am aware of; indeed, one might argue that Performance Now is doing enough linguistic processing that it really should be regarded as a (simple) NLG system. KnowledgePoint's marketing literature in fact stresses their `IntelliText[TM]' technology, which ``generates clear, logical sentences and modifies them to work together as if you wrote them yourself''. This is the only mass-market system I am aware of which advertises NLG-like abilities as part of its competitive advantage.\\n\\nAdvantages of NLG\\n\\nMany advantages of NLG over templates have been described in the literature. In this section, I try to summarize these arguments, paying special attention to those arguments that seem important to the success of current applied NLG systems.\\n\\nMaintainability\\n\\nImproved Text Quality\\n\\nContent Planning\\n\\nSentence Planning\\n\\nSyntactic Realization\\n\\nTexts that are comprehensible but ungrammatical can be annoying to readers, and it may be expensive (in terms of programming effort) to set up a template system to correctly handle agreement, morphology, punctuation reduction, and other `low-level' phenomena. It is straightforward, in contrast, for an NLG system to handle such phenomena.\\n\\nAlso, in many cases texts can be phrased in a manner which minimizes the need for syntactic adjustment. For example, problems will occur with the template N iterations were run  when N is 1; these problems can be avoided, however, by changing the text to Number of iterations run: N.\\n\\nA good syntactic module may of course be needed to support a sophisticated content determination or sentence planning module. For example, as mentioned in the previous section, proper syntactic representation is probably needed for many kinds of aggregation. By itself, however, good syntactic processing may not provide much `competitive advantage' to an NLG system.\\n\\nOther advantages\\n\\nTwo other advantages of NLG that may be important in some cases are multilingual output and guaranteed conformance to standards. Multilingual output can of course be achieved with templates; many error-message systems, for example, are localized to other languages simply by inserting a new set of format strings. The quality of texts generated by this approach is not high, but this may be acceptable in some circumstances.\\n\\nAt the other extreme, multilingual output could also be achieved by building several separate systems, one for each target language. Such a system would be expensive to construct and might prove difficult to maintain, however.\\n\\nAdvantages of Templates\\n\\nTemplates, of course, also have advantages over NLG. The most basic of these is probably that NLG systems cannot generate text unless they have a representation of the information that the text is supposed to communicate; and in the great majority of today's application systems, such representations do not exist.\\n\\nFor instance, suppose a scientific program wishes to inform the user that N iterations of an algorithm were performed. In principle, NLG techniques could be used to improve the handling of special cases such as N=0 and N=1, so that the system could produce No iterations were performed 1 iteration was performed 2 iterations were performed However, doing this with NLG techniques would require the system to have either a declarative representation of concepts such as algorithms and iterations; or a syntactic representation of the sentence N iterations were performed. Neither of these is likely to exist in a scientific program, and few scientific programmers would bother putting them in. Instead, such programmers would either accept low-level syntactic problems (eg, the output 1 iteration were performed); use an alternate formulation that did not suffer from this problem (eg, number of iterations performed: 1); or write special code to produce the appropriate output when N is 0 or 1.\\n\\nThis is perhaps an extreme case, but it illustrates the point that switching to NLG will be expensive if the application does not already have a declarative domain knowledge base and/or syntactic representations of output text, and no one is going to pay this cost if the resultant improvement in text quality (or system maintainability) is not perceived as significantly enhancing the usefulness of the application. Since knowledge-based application systems are still rare, and even the ones that do exist often do not have all the information that an NLG system would need, it may be the case that NLG is not the most appropriate technology for many current text-generation applications.\\n\\nBesides the above problem, NLG also suffers from generic problems that are common to all new technologies. There are very few people who can build NLG systems, compared to the millions of programmers who can build template systems; there is also very little awareness of what NLG can (and cannot) do among most developers of application systems. Additionally, there is very little in the way of reusable NLG resources (software, grammars, lexicons, etc), which means that most NLG developers still have to more or less start from scratch. Finally, the fact that NLG is an experimental technology means that conservative developers may want to avoid using it. As mentioned above, these problems are common to all new technologies, and will evaporate with time if NLG proves to be a truly useful and valuable technology.\\n\\nHybrid Systems\\n\\nReal-world decisions about where NLG should be used in a hybrid system are likely to be based on practical criteria as well as theoretical ones. NLG modules that are slow, error-prone, written in unusual programming languages, and/or difficult to maintain will of course not get used much in real applications. But also, even a well-engineered module is unlikely to get used if it does not fit into the way the developer wishes to build his or her system, or give the developer sufficient control over the system's output. For example, a system that reserves the right to reorder sentences based on some rhetorical model may be unacceptable to a developer who insists that the sentences must appear in a specific order that he or she thinks is best.\\n\\nAnother way of saying this is that NLG shouldn't `get in the way'. Developers will use NLG modules and techniques if NLG helps them produce the kind of texts they want to produce; if an NLG system is seen not as a helpful tool but as something that needs to be worked around, it will not be used. NLG should also only be used when it clearly increases maintainability, text readability, or some other important attribute of the target application system. If a certain portion of the output text never varies, for example, it would be silly to generate it with NLG, and much more sensible to simply use canned text for this portion of the document.\\n\\nI believe, by the way, that most current hybrid systems use `real NLG' in content-determination and perhaps sentence-planning, and use template techniques mainly in syntactic realization. This may simply be a coincidence, but it may also suggest that much of the real `value-added' of many NLG systems may be in the high-level processing, not in ensuring correct syntax.\\n\\nMaking NLG More Useful\\n\\nThere are several areas where I think more academic research could help improve the advantages of NLG-based text generators over template-based systems.\\n\\nConclusion\\n\\nAs NLG technology begins to move out of the lab and into real applications, the NLG community needs to begin thinking not just about how to generally improve our understanding of this research area, but also about questions such as (a) what advantages NLG offers over simpler approaches; (b) under what circumstances using NLG `adds value' to real-world systems; and (c) where further advances in NLG could really increase the usefulness of applied NLG systems. It will probably be many years before we can confidently provide answers to these questions, but an important step on this path would be to start more explicitly discussing and exploring these issues within the community; I can only hope that the presentation in this paper will at least in a small way encourage people to start thinking more about these issues.\\n\\nBibliography\\n\\nAECMA. A guide for the preparation of aircraft maintenance documentation in the international aerospace maintenance language, 1986. Available from BDC Publishing Services, Slack Lane, Derby, UK.\\n\\nB. Buchanan, J. Moore, D. Forsythe, G. Carenini, and S. Ohlsson. Using medical informatics for explanation in a clinical setting. Technical Report 93-16, Intelligent Systems Laboratory, University of Pittsburgh, 1994.\\n\\nJose Coch and Raphael David. Representing knowledge for planning multisentential text. In Proceedings of the Fourth Conference on Applied Natural Language Processing (ANLP-1994), pages 203-204, 1994.\\n\\nHercules Dalianis and Eduard Hovy. Aggregation in natural language generation. In Proceedings of the Fourth European Workshop on Natural Language Generation, pages 67-78, 1993.\\n\\nMilitary Standard DoD-Std-2167A: Defense System Software Development, 1988.\\n\\nEli Goldberg, Norbert Driedger, and Richard Kittredge. Using natural-language processing to produce weather forecasts. IEEE Expert, 9(2):45-53, 1994.\\n\\nGerard Kempen, Gert Anbeek, Peter Desain, Leo Konst, and Koenraad DeSmedt. Author environments: Fifth generation text processors. In Directorate General XIII, European Commission, editor,   ESPRIT'86 Results and Achievements, pages 365-372. Elsevier, 1986.\\n\\nRichard Kittredge, Eli Goldberg, Myunghee Kim, and Alain Polgure. Sublanguage engineering in the FOG system. In Proceedings of the Fourth Conference on Applied Natural Language Processing (ANLP-1994), pages 215-216, 1994.\\n\\nJohn McDermott. R1: A rule-based configurer of computer systems. Artificial Intelligence, 19:39-88, 1982.\\n\\nKathleen McKeown, Karen Kukich, and James Shaw. Practical issues in automatic document generation. In Proceedings of the Fourth Conference on Applied Natural-Language Processing (ANLP-1994), pages 7-14, 1994.\\n\\nVibhu Mittal. Generating natural language descriptions with integrated text and examples. Research Report ISI/RR-93-392, Information Sciences Institute, University of Southern California, Marina del Rey, California, 1993.\\n\\nEhud Reiter. Has a consensus NL Generation architecture appeared, and is it psycholinguistically plausible? In Proceedings of the Seventh International Workshop on Natural Language Generation (INLGW-1994), pages 163-170, 1994.\\n\\nEhud Reiter and Chris Mellish. Optimising the costs and benefits of natural language generation. In Proceedings of the 13th International Joint Conference on Artificial Intelligence (IJCAI-1993), volume 2, pages 1164-1169, 1993.\\n\\nEhud Reiter, Chris Mellish, and John Levine. Automatic generation of technical documentation. Applied Artificial Intelligence, 9, 1995. Forthcoming.\\n\\nElliot Soloway, Judy Bachant, and Keith Jensen. Assessing the maintainability of XCON-in-RIME: Coping with the problems of a very large rule-base. In Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI-1987), volume 2, pages 824-829, 1987.\\n\\nStephen Springer, Paul Buta, and Thomas Wolf. Automatic letter composition for customer service. In Reid Smith and Carlisle Scott, editors, Innovative Applications of Artificial Intelligence 3 (Proceedings of CAIA-1991). AAAI Press, 1991.\\n\\nFootnotes\\n\\nAfter 1 August 1995, Dr. Reiter's address will be Department of Computing Science, University of Aberdeen, King's College, Aberdeen AB9 2UE, BRITAIN. His email address will be ereiter@csd.abdn.ac.uk This assumes that the system uses a large number of templates. If only a small set of templates is needed to generate the system's texts, maintaining them is unlikely to be a problem. Also, a forecaster who uses FoG is not dependent on a third-party to translate his or her forecasts; this feeling of `more control' may be a significant plus to some users. This list has been heavily influenced by discussions with Chris Mellish.\", metadata={'source': '../data/raw/cmplg-xml/9504013.xml'}),\n",
       " Document(page_content=\"Mixed Initiative in Dialogue: An Investigation into Discourse Segmentation\\n\\nConversation between two people is usually of  MIXED-INITIATIVE, with  CONTROL over the conversation being transferred from one person to another. We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns. The application of the control rules lets us derive domain-independent discourse structures. The derived structures indicate that initiative plays a role in the structuring of discourse. In order to explore the relationship of control and initiative to discourse processes like centering, we analyze the distribution of four different classes of anaphora for two data sets. This distribution indicates that some control segments are hierarchically related to others. The analysis suggests that discourse participants often mutually agree to a change of topic. We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types. These differences can be explained in terms of collaborative planning principles.\\n\\nIntroduction\\n\\nThe application of the control rules to these dialogues lets us derive domain-independent discourse segments with each segment being controlled by one or other discourse participant. We propose that control segments correspond to different subgoals in the evolving discourse plan. In addition, we argue that various linguistic devices are necessary for conversational participants to coordinate their contributions to the dialogue and agree on their mutual beliefs with respect to a evolving plan, for example, to agree that a particular subgoal has been achieved. A final phenomenon concerns shifts of control and the devices used to achieve this. Control shifts occur because it is unusual for a single participant to be responsible for coordinating the achievement of the whole discourse plan. When a different participant assumes control of a discourse subgoal then a control shift occurs and the participants must have mechanisms for achieving this. The control framework distinguishes instances in which a control shift is negotiated by the participants and instances where one participant seizes control.\\n\\nRules for the Allocation and Transfer of Control ASSERTIONS: Declarative utterances used to state facts. Yes and No in response to a question were classified as assertions on the basis that they are supplying information.\\n\\nCOMMANDS: Utterances intended to instigate action. Generally imperative form, but could be indirect such as My suggestion would be that you do .....\\n\\nQUESTIONS: Utterances which are intended to elicit information, including indirect forms such as I was wondering whether I should ....\\n\\nPROMPTS: Utterances which did not express propositional content, such as Yeah, Okay, Uh-huh ....\\n\\nThe rules for the allocation of control are based on the utterance type classification and allow a dialogue to be divided into segments that correspond to which speaker is the controller of the segment.\\n\\nCONTROL RULES\\n\\nWhittaker and Stenton also performed a post-hoc analysis of the segment boundaries that are defined by the control rules. The boundaries fell into one of three types:\\n\\nABDICATION: Okay, go on.\\n\\nREPETITION/SUMMARY: That would be my recommendation and that will ensure that you get a logically integral set of files.\\n\\nINTERRUPTION: It is something new though um.\\n\\nThis classification suggests that the transfer of control is often a collaborative phenomenon. Since a noncontroller(OCP), has the option of seizing control at any juncture in discourse, it would seem that controllers(ICPs), are in control because the noncontroller allows it. These observations address problems raised by Grosz and Sidner, namely how ICPs signal and OCPs recognize segment boundaries. The claim is that shifts of control often do not occur until the controller indicates the end of a discourse segment by abdicating or producing a repetition/summary.\\n\\nControl Segmentation and Anaphora\\n\\nTo determine the relationship between the derived control segments and ATTENTIONAL STATE we looked at the distribution of anaphora with respect to the control segments in the ADs. All data were analysed statistically by\\n\\nand all differences cited are significant at the 0.05 level. We looked at all anaphors (excluding first and second person), and grouped them into 4 classes.\\n\\n3RD PERSON: it, they, them, their, she, he, her, him, his\\n\\nONE/SOME, one of them, one of those, a new one, that one, the other one, some\\n\\nDEICTIC: Noun phrases, e.g. this, that, this NP, that NP, those NP, these NP\\n\\nEVENT: Verb Phrases, Sentences, Segments, e.g. this, that, it\\n\\nThe class  DEICTIC refers to deictic references to material introduced by noun phrases, whereas the class  EVENT refers to material introduced clausally.\\n\\nHierarchical Relationships\\n\\nThe first phenomenon we noted was that the anaphora distribution indicated that some segments are hierarchically related to others. This was especially apparent in cases where one discourse participant interrupted briefly, then immediately passed control back to the other.\\n\\nThe following example illustrates the same point.\\n\\nThe control segments as defined would treat both of these cases as composed of 3 different segments. But this ignores the fact that utterances (1) and (5) have closely related propositional content in the first example, and that the plural pronoun straddles the central subsegment with the same referents being picked out by they and their in the second example. Thus we allowed for hierarchical segments by treating the interruptions of 2-4 as subsegments, and utterances 1 and 5 as related parts of the parent segments. All interruptions were treated as embeddings in this way. However the relationship of the segment after the interruption to the segment before must be determined on independent grounds such as topic or intentional structure.\\n\\nDistribution\\n\\nWe also looked at the distribution of anaphora in the Support ADs and found similar results.\\n\\nFor both dialogues, the distribution of anaphors varies according to which type of control shift occurred at the previous segment boundary. When we look at the different types of anaphora, we find that third person and one anaphors cross boundaries extremely rarely, but the event anaphors and the deictic pronouns demonstrate a different pattern. What does this mean?\\n\\nThe fact that anaphora is more likely to cross segment boundaries following interruptions than for summaries or abdications is consistent with the control principles. With both summaries and abdications the speaker gives an explicit signal that s/he wishes to relinquish control. In contrast, interruptions are the unprompted attempts of the listener to seize control, often having to do with some `problem' with the controller's utterance. Therefore, interruptions are much more likely to be within topic.\\n\\nWe also looked at the TODs for instances of anaphora being used to describe a future act in the way that we observed in the ADs. However, over the 938 turns in the TODs, there were only 18 instances of event anaphora, because in the main there were few occasions when it was necessary to talk about the plan. The financial ADs had 45 event anaphors in 474 utterances.\\n\\nControl and Collaborative Plans INFORMATION QUALITY: The listener must believe that the information that the speaker has provided is true, unambiguous and relevant to the mutual goal. This corresponds to the two rules: (A1)  TRUTH: If the listener believes a fact P and believes that fact to be relevant and either believes that the speaker believes not P or that the speaker does not know P then interrupt; (A2) AMBIGUITY: If the listener believes that the speaker's assertion is relevant but ambiguous then interrupt.\\n\\nPLAN QUALITY: The listener must believe that the action proposed by the speaker is a part of an adequate plan to achieve the mutual goal and the action must also be comprehensible to the listener. The two rules to express this are: (B1) EFFECTIVENESS: If the listener believes P and either believes that P presents an obstacle to the proposed plan or believes that P is part of the proposed plan that has already been satisfied, then interrupt; (B2)  AMBIGUITY: If the listener believes that an assertion about the proposed plan is ambiguous, then interrupt.\\n\\nThese principles indirectly provide a means to ensure mutual belief. Since a participant must interrupt if any condition for an interrupt holds, then lack of interruption signals that there is no discrepancy in mutual beliefs. If there is such a discrepancy, the interruption is a necessary contribution to a collaborative plan, not a distraction from the joint activity.\\n\\nThree things are striking about this data. As we predicted, the distribution of control between expert and client is completely different in the ADs and the TODs. The expert has control for around 90% of utterances in the TODs whereas control is shared almost equally in the ADs. Secondly, contrary to our expectations, we did find some instances of shifts in the TODs. Thirdly, the distribution of interruptions and summaries differs across dialogue types. How can the collaborative planning principles highlight the differences we observe?\\n\\nThere seem to be two reasons why shifts occur in the TODs. First, many interruptions in the TODs result from the apprentice seizing control just to indicate that there is a temporary problem and that plan execution should be delayed.\\n\\nSecond, control was exchanged when the execution of the task started to go awry.\\n\\nThe problem with the physical situation indicates to the apprentice that the relevant beliefs are no longer shared. The Instructor is not in possession of critical information such as the current state of the apprentice's pump. This necessitates an information exchange to resynchronize mutual beliefs, so that the rest of the plan may be successfully executed. However, since control is explicitly allocated to the instructor in TODs, there is no reason for that participant to believe that the other has any contribution to make. Thus there are fewer attempts by the instructor to coordinate activity, such as by using summaries to synchronize mutual beliefs. Therefore, if the apprentice needs to make a contribution, s/he must do so via interruption, explaining why there are many more interruptions in these dialogues. In addition, the majority of Interruptions (73%) are initiated by apprentices, in contrast to the ADs in which only 29% are produced by the Clients.\\n\\nSummaries are more frequent in ADs. In the ADs both participants believe that a plan cannot be constructed without contributions from both of them. Abdications and summaries are devices which allow these contributions to be coordinated and participants use these devices to explicitly set up opportunities for one another to make a contribution, and to ensure mutual beliefs. The increased frequency of summaries in the ADs may result from the fact that the participants start with discrepant mutual beliefs about the situation and that establishing and maintaining mutual beliefs is a key part of the ADs.\\n\\nDiscussion\\n\\nAcknowledgements\\n\\nWe would like to thank Aravind Joshi for his support, comments and criticisms. Discussions of joint action with Phil Cohen and the members of CSLI's DIA working group have influenced the first author. We are also indebted to Susan Brennan, Herb Clark, Julia Hirschberg, Jerry Hobbs, Libby Levison, Kathy McKeown, Ellen Prince, Penni Sibun, Candy Sidner, Martha Pollack, Phil Stenton, and Bonnie Webber for their insightful comments and criticisms on drafts of this paper.\\n\\nBibliography\\n\\nSusan E. Brennan, Marilyn Walker Friedman, and Carl J. Pollard. A centering approach to pronouns. In Proc. 25th Annual Meeting of the ACL, Stanford, pages 155-162, 1987.\\n\\nPhillip R. Cohen, Hector J. Levesque, Jose H. T. Nunes, and Sharon L. Oviatt. Task oriented dialogue as a consequence of joint activity. In Pacific Rim Conference on Artificial Intelligence, 1990.\\n\\nPhillip R. Cohen. The pragmatics of referring and the modality of communication. Computational Linguistics, 10:97-146, 1984.\\n\\nRobin Cohen. Analyzing the structure of argumentative discourse. Computational Linguistics, 13:11-24, 1987.\\n\\nPhillip R. Cohen, C. Raymond Perrault, and James F. Allen 1982. Beyond question answering. In Wendy Lehnert and Martin Ringle, editors, Strategies for Natural Language Processing, pages 245-274. Lawrence Erlbaum Ass. Inc, Hillsdale, N.J., 1982.\\n\\nPhilip R. Cohen and C. Raymond Perrault. Elements of a plan-based theory of speech acts. In Barbara J. Grosz, Karen Sparck Jones, and Bonnie Lynn Webber, editors, Readings in Natural Language Processing, pages 423-440. Morgan Kauffman, Los Altos, Ca., 1986.\\n\\nHerbert H. Clark and Deanna Wilkes-Gibbs. Referring as a collaborative process. Cognition, 22:1-39, 1986.\\n\\nDavid M. Frohlich and Paul Luff. Conversational resources for situated action. In Proc. Annual Meeting of the Computer Human Interaction of the ACM, 1989.\\n\\nBarbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. Towards a computational theory of discourse interpretation. Unpublished Manuscript, 1986.\\n\\nBarbara J. Grosz. The representation and use of focus in dialogue understanding. Technical Report 151, SRI International, 333 Ravenswood Ave, Menlo Park, Ca. 94025, 1977.\\n\\nBarbara J. Grosz and Candace L. Sidner. Attentions, intentions and the structure of discourse. Computational Linguistics, 12:175-204, 1986.\\n\\nBarbara J. Grosz and Candace L. Sidner. Plans for discourse. In Cohen, Morgan and Pollack, eds. Intentions in Communication, MIT Press, 1990.\\n\\nJerry R. Hobbs and Michael H. Agar. The coherence of incoherent discourse. Technical Report CSLI-85-38, Center for the Study of Language and Information, Ventura Hall, Stanford University, Stanford, CA 94305, 1985.\\n\\nJulia Hirschberg and Diane Litman. Now lets talk about now: Identifying cue phrases intonationally. In Proc. 25th Annual Meeting of the ACL, Stanford, pages 163-171, Stanford University, Stanford, Ca., 1987.\\n\\nJerry R. Hobbs.\\n\\nCoherence and coreference.\\n\\nCognitive Science, 3:67\\n\\n\\n\\n90, 1979.\\n\\nAravind K. Joshi. Mutual beliefs in question-answer systems. In Neil V. Smith, editor, Mutual Knowledge, pages 181-199. Academic Press, New York, New York, 1982.\\n\\nAlison Kidd. The consultative role of an expert system. In P. Johnson and S. Cook, editors, People and Computers: Designing the Interface. Cambridge University Press, Cambridge, U.K., 1985.\\n\\nDiane Litman and James Allen. Recognizing and relating discourse intentions and task-oriented plans. In Cohen, Morgan and Pollack, eds. Intentions in Communication, MIT Press, 1990.\\n\\nKathleen R. McKeown. Discourse strategies for generating natural language text. Artificial Intelligence, 27(1):1-42, September 1985.\\n\\nR.S. Nickerson. On converational interaction with computers. In SiegFried Treu, editor, User-Oriented Design of Interactive Graphics Systems, pages 101-65. Elsevier Science, 1976.\\n\\nSharon L. Oviatt and Philip R. Cohen. The effects of interaction on spoken discourse. In Proc. 27th Annual Meeting of the Association of Computational Linguistics, pages 126-134, 1989.\\n\\nJanet Pierrehumbert and Julia Hirschberg. The meaning of intonational contours in the interpretation of discourse. In Cohen, Morgan and Pollack, eds. Intentions in Communication, MIT Press, 1990.\\n\\nMartha Pollack, Julia Hirschberg, and Bonnie Webber. User participation in the reasoning process of expert systems. In AAAI82, 1982.\\n\\nMartha Pollack. Inferring domain plans in question answering. Technical Report 403, SRI International - Artificial Intelligence Center, 1986. University of Pennsylvania Dissertation.\\n\\nCraige Roberts. Modal Subordination and Anaphora. PhD thesis, Linguistics Dept, University of Massachusetts, Amherst, 1986. Published by Garland Press.\\n\\nEmanuel A. Schegloff. Discourse as an interactional achievement: Some uses of 'uh huh' and other things that come between sentences. In D. Tannen, editor, Analyzing Discourse: Text and Talk, pages 71-93. Georgetown University Press, 1982.\\n\\nCandace L. Sidner. Toward a computational theory of definite anaphora comprehension in English. Technical Report AI-TR-537, MIT, 1979.\\n\\nCandace Sidner. What the speaker means: the recognition of speakers plans in discourse. International Journal of Computers and Mathematics, 9:71-82, 1983.\\n\\nHarvey Sacks, Emmanuel Schegloff, and Gail Jefferson. A simplest systematics for the organization of turn-taking in conversation. Language, 50:325-345, 1974.\\n\\nMarilyn A. Walker. Evaluating discourse processing algorithms. In Proc. 27th Annual Meeting of the Association of Computational Linguistics, pages 251-261, 1989.\\n\\nBonnie Lynn Webber. Two steps closer to event reference. Technical Report MS-CIS-86-74, Linc Lab 42, Department of Computer and Information Science, University of Pennsylvania, 1986.\\n\\nBonnie Lynn Webber. Discourse deixis: Reference to discourse segments. In Proc. 26th Annual Meeting of the ACL, Association of Computational Linguistics, pages 113-123, 1988.\\n\\nSteve Whittaker and Phil Stenton. Cues and control in expert client dialogues. In Proc. 26th Annual Meeting of the ACL, Association of Computational Linguistics, pages 123-130, 1988.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9504007.xml'}),\n",
       " Document(page_content='From compositional to systematic semantics\\n\\nWe prove a theorem stating that any semantics can be encoded as a compositional semantics, which means that, essentially, the standard definition of compositionality is formally vacuous. We then show that when compositional semantics is required to be \"systematic\" (that is, the meaning function cannot be arbitrary, but must belong to some class), it is possible to distinguish between compositional and non-compositional semantics. As a result, we believe that the paper clarifies the concept of compositionality and opens a possibility of making systematic formal comparisons of different systems of grammars.\\n\\nIntroduction\\n\\nGiven a class of functions F, we say that the compositional semantics is systematic  if the meaning function belongs to the class F. We show that when compositional semantics is required to be systematic we can distinguish between grammars with compositional and non-compositional semantics; we present an example of a simple grammar for which there is no \"systematic\" compositional semantics (Section 3).\\n\\nThe answer to this question is somewhat disturbing. It turns out  that whatever we decide that some language expressions should mean, it is always possible to produce a function that would give compositional semantics to it (see below for a more precise formulation of this fact). The upshot is that compositionality, as commonly defined, is not a strong constraint on a semantic theory.\\n\\nProving the existence of compositional semantics\\n\\nLet S be  any collection of expressions (intuitively, sentences and their parts). Assume that elements of S (e.g. s.t) are composed from other elements of S (that is, s and t)  by concatenation (\".\"). We do not assume that concatenation is associative, that is\\n\\n(a. (b.c)) = ((a.b).c). Intuitively, this means that we assign semantics to parse trees,  and not to strings of words.\\n\\nLet M be a set  of meanings, and let for any\\n\\n,\\n\\nbe the meaning of s. We want to show that there is  a compositional semantics for S which agrees with the function massociating s with m(s).\\n\\nSince elements of Mcan be of any type, we do not automatically have\\n\\n,\\n\\nwhere\\n\\nis some operation on the meanings. To get that kind of homomorphism we have to perform a type raising operation that would map elements of Sinto functions  and  then the functions into the required meanings. Note that such a type raising operation is quite common both in mathematics  (e.g. 1 being a function equal to 1 for all values) and in mathematical linguistics. The  meaning function\\n\\nthat we want to define will provide compositional semantics for S by mapping it into a set of functions in such a way that\\n\\n, for all elements s.t of S.\\n\\nSecondly, we want that the original semantics be easily decoded from\\n\\n. There is more than one way of doing this. One can trivially extend the language S by adding to   it an \"end of expression\" character\\n\\n, which may appear only as the last element of any expression. The purpose of it is to encode the function m(x) in the following way:\\n\\n, for all s in S. Intuitively, the character\\n\\nis like the period at the end of a sentence, or the pause marking the end of an utterance. In effect, we will be treating all sentences as idioms, or garden path sentences, where the meanings are clear only once the sentence is completed (Theorem 2). But, as we are going to show now, the original semantics can be encoded in a different way, without extending the original language, e.g. by assuring\\n\\n, for all s in S (Theorem 1).\\n\\nTo make the notation simple, we have assumed that there is only one way of composing elements  of S, by concatenation, but  all our arguments work for languages with many operators as well. We show an example of how such operators can be handled in Section 4.\\n\\nTHEOREM 1. Let M be an arbitrary set. Let A be an arbitrary alphabet. Let \".\" be a binary operation, and let S be the set closure of A under \".\". Let\\n\\nbe an arbitrary function.\\n\\nThen there is a set of functions M[*] and a unique map\\n\\nsuch that for all s,\\n\\n, which may be infinite (countable or uncountable). We can form equations of the form\\n\\nwhere\\n\\nis a set expression involving the variables and elements of V, for instance, if\\n\\nand\\n\\n, we can write the following equations:\\n\\nWe say that such a set is well-formed if each variable appear only once on the left, and each left hand side is a variable. The solution lemma says that any set of such equations (finite or infinite) has a unique solution. That is, there is a unique collection of sets that satisfy them.\\n\\nPROOF OF THEOREM 1 AND COROLLARY 2\\n\\nProof of Theorem 1. It is enough to ensure that for all\\n\\nClearly,\\n\\nis a function, because it is a collection of pairs. The proof is complete once we check that for  s\\'s and   t\\'s in S we have (i)\\n\\n, and\\n\\n(ii)\\n\\n. Using the above equality we check (i): If\\n\\n,\\n\\nthen\\n\\n.\\n\\nSimilarly for (ii).\\n\\nIt remains to show that using the solution lemma we can make the above equation true for all\\n\\n.\\n\\nWe begin by\\n\\nintroducing a set variable\\n\\nXs for every\\n\\n,\\n\\nand observing that\\n\\nis a well-formed set equation for any\\n\\n. (The pair [ a , b ] is set theoretically defined as\\n\\n. Hence the solution lemma applies, and there are unique sets\\n\\nthat satisfy each equation. But each such\\n\\nis a collection of pairs, i.e. a function. Furthermore, since each\\n\\nis unique, and S is a set, the mapping\\n\\nassociating\\n\\nwith each\\n\\nis a function. This completes the proof of Theorem 1.\\n\\nProof of Corollary 2. It is enough to observe that we can add an extra condition in the main equation of Theorem 1, and the proof still works:\\n\\nNOTE. Notice that we can view using the solution lemma in the above proofs as an extreme example of defining a function by cases. To see it more clearly, one can make the main equation of the proof of Theorem 1 explicit. Let\\n\\nenumerate  S. We can create a big table specifying meaning values for all strings and  their combinations. Then the  conditions on the meaning functions\\n\\ncan be written as the set of equations below\\n\\n>\\n\\nIn ordinary mathematics, this would correspond to saying that if x is 1 then f(x)=32, if x is 2 then\\n\\nf(x)=14732, if x is 3 then f(x)=1, and so on. Clearly, such a process defines the function f, but, intuitively, it is not a definition we would care much for. Before showing that requiring a better description of an f than as a set of pairs makes sense, we want to observe that the encoding of the original meaning function can be uniform in the following sense:\\n\\nPROPOSITION 3. In addition to the assumptions of Theorem 1, let\\n\\n,\\n\\nand\\n\\nlet\\n\\nbe the language obtained by the mapping\\n\\n,\\n\\nfor all\\n\\n. Then there is a set of functions M[*] and a unique map\\n\\nsuch that for all s,\\n\\nProof. As in the proof of Corollary 2, we can change the set of equations to\\n\\nTo finish the construction of\\n\\n, we make sure that the equation\\n\\nholds. Formally, this requires adding the pair\\n\\ninto\\n\\nthe graph of\\n\\nthat was obtained from the solution lemma. Also, we have to extend the domain of function\\n\\nto include\\n\\n. This is easily done by adding to the already constructed part of\\n\\nthe set of pairs\\n\\n. The proof is complete once we check that for  s\\'s  in S we have\\n\\n,\\n\\nand that\\n\\n(because\\n\\n, and, according to the equation,\\n\\n).\\n\\nNote that, as in Corollary 2, if a certain string does not belong to the language, we can assume that the corresponding value in this table is undefined; thus\\n\\nis not necessarily defined for all possible concatenations of strings of S.\\n\\nIn view of the above theorems, any semantics is equivalent to a compositional semantics, and hence it would be meaningless to keep the definition of compositionality as the existence of a homomorphism from syntax to semantics without imposing some conditions on this homomorphism. Notice that requiring the computability of the meaning function will not do. In mathematics, where semantics obviously is compositional, we can talk about noncomputable functions, and it is usually clear what we postulate about them. Also, we have the following proposition.\\n\\nPROPOSITION 4. If the set of expressions S and the original meaning function m(x) are computable, then so is the meaning function\\n\\n.\\n\\nProof. One can easily check that the table defining the meaning functions\\n\\nin the note above is effectively computable from the functions m(x) and\\n\\n.\\n\\nHence\\n\\nso is the function\\n\\n.\\n\\nNOTE. What does it mean that the table is effectively computable from the functions m(x) and\\n\\n? It means that given a Turing machine, T1, that prints all elements of S, and another Turing machine, T2, that takes an element s on the output tape of T1 as input and produces as output m(s), we can construct a third Turing machine, T3, that produces the successive elements of the table, i.e. enumerates all the equations (e.g. for any pair [m, n] gives the nth value pair of the mth equation). But these equations define our function\\n\\n.\\n\\nHence\\n\\nSystematic semantics. I. Examples\\n\\nGrammar ND\\n\\nN\\n\\nN D\\n\\nN\\n\\nD\\n\\nD\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\nGrammar DN\\n\\nN\\n\\nD N\\n\\nN\\n\\nD\\n\\nD\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\nPROPOSITION 5. For the grammar ND, the meaning of any numeral can be expressed in the model\\n\\nas\\n\\nthat is, a polynomial in two variables with coefficients in natural numbers.\\n\\nOn the other hand, for the grammar DN, we can prove that no such a polynomial exists:\\n\\nTHEOREM 6. There is no polynomial p in two variables x, y such that\\n\\nand such that the value of\\n\\nis the number expressed by the string  D  N in base 10.\\n\\nProof.\\n\\nWe are looking for\\n\\nwhere the function pmust be a polynomial in these two variables. If such a polynomial exists, it would have to be equal to\\n\\nfor\\n\\nin the interval 0..9, and to\\n\\nfor\\n\\nin 10..99, and to\\n\\nfor\\n\\nin 100..999, and so on. Let the degree of this polynomial be less than n , for some n. Let us consider the interval\\n\\n10[n] .. 10[(n+1)]- 1. On this interval the polynomial would have to be equal identically to\\n\\n. Now, if two polynomials of degrees less than n agree on n different values, they must be identical. Hence,\\n\\n. But this would give wrong values for other intervals, e.g 10..99. Contradiction.\\n\\nBut notice that there is a compositional semantics  for the grammar DN that does not agree with intuitions:\\n\\n, which corresponds to reading the number backwards. And there are many other semantics corresponding to all possible polynomials in\\n\\nand\\n\\n. Also observe that (a) if we specify enough values of the meaning function we can exclude any particular polynomial; (b) if we do not restrict the degree of the polynomial, we can write one that would give any values we want on a finite number of words in the grammar. The moral is that not only it is natural to restrict meaning functions to, say, polynomials, but to further restrict them. E.g. if we restrict the meaning functions to polynomials of degree 1, then by specifying only three values  of the meaning function we can (a) have a unique compositional semantics for the first grammar; and (b) show that there is no compositional semantics for the second grammar (directly from the proof of the above theorem).\\n\\nSome linguistic examples\\n\\nIn this section we want to explain how the theorems we proved in Section 2 apply to typical linguistic examples. In the process we also explain how languages with operators can be handled by our method. We begin by discussing the simple case of idioms.\\n\\nIDIOMS. Intuitively, the meaning of \"high seas\" is not compositional, because \"high\" refers to length or distance, and not to open spaces; moreover one could even argue that although \"seas\" is plural, \"high seas\" is semantically singular, for it means an \"open sea\". (However, the precise semantics of the expression is not important at this point). We want to show how we can assign compositional semantics to such non-compositional examples.\\n\\nLet the language S consist of\\n\\nwall, seas,  high, high.wall, and high.seas. The equations we need to ensure the compositionality of semantics have the familiar form:\\n\\n>\\n\\nNotice, that we could add building and other words to the language and easily extend this set of equations. The intuition we associate with compositionality would be captured by the uniformity of the meanings of high.X as\\n\\nhigh(m(X)), where X ranges over\\n\\nwall, building, .... However the formal expression of this intuition as the principle of compositionality does not work, which can be seen by noticing that the meaning of high.seas is a composition of the meaning functions for high and seas. What is happening has to do with the fact that, by definition, functions defined by cases are as good as any others. And what we have done is to have defined the meaning of high by cases.\\n\\nCOORDINATION. We now turn our attention to a slightly more complicated example. Consider disjunction and conjunction, +and\\n\\n. We plan to prove the following results:\\n\\nPROPOSITION 7. Let + and\\n\\ndenote \"or\" and \"and\". Then: (A). It is possible to assign compositionally the \"natural\" semantics of\\n\\nto expressions of type\\n\\nand preserve the original\\n\\nmeanings of a+b and\\n\\n. (B). (A) is not possible if the meaning functions have to be Boolean polynomials. Proof. To keep things as simple as possible, consider language S consisting of\\n\\n,\\n\\n, b, c, and a. To apply directly the solution lemma we should represent the operators in their prefix form; e.g. a+b becomes +.a.b. Then our language S becomes\\n\\n,\\n\\n,\\n\\n+,\\n\\n, b, c, and a. (And for the sake of completeness we can add to it\\n\\n, and b.c). As before we write our equations (this time using the version with $):\\n\\n>\\n\\nIt can be easily checked that as before\\n\\nfor all\\n\\n. The meaning m(a) of a is arbitrary, but we would typically identify it with its logical value (true or false). Also, notice that without loss of generality those  a, b, c(and hence the language S) can be \"expanded\" to sets of variables\\n\\nai , bj , ck, resulting in a slight change in the equations, but not changing the content of the theorem. Then, for all pairs of variables disjunction and conjunction would behave as usual; however for a combination of a variable with a Boolean formula they could behave arbitrarily. This proves the first part.\\n\\nWe now prove the second part, i.e. that if the meaning functions are restricted to Boolean polynomials, it is impossible to assign compositional semantics given by the \"natural\" semantics of\\n\\nto expressions of the type\\n\\nand preserve the original meanings of the connectives. (The \"natural\" semantics is of course the conjunction of the Boolean value of a + b with the Boolean value of c).\\n\\nThe proof consists in observing that\\n\\ncannot be obtained as a Boolean polynomial (function)\\n\\n. To see it, it is enough to construct a truth table showing the values of\\n\\nand\\n\\nand observing that there cannot be a functional dependence of the former on the latter and the value of a(compare the values for the triples\\n\\n[ a=1 , b =0 , c=1 ] and\\n\\n[ a=1 , b =1 , c=0 ]).\\n\\nSystematic semantics. II. Discussion\\n\\nAbove, we have argued that the existence of a homomorphism from syntax to semantics does not restrict the grammar, but if we put some constraints on such a homomorphism, they actually might restrict grammars of languages. We have called such homomorphisms (F-)systematic. However the nature of systematicity seems to be very much an open problem. In this section we discuss some of the most obvious issues, and propose some research possibilities in this area.\\n\\nThe first natural question that arises is: What should be this class F? We have shown that for a grammar of numbers, and a grammar of two Boolean connectives the natural classes are polynomials. Clearly, this cannot always be the case. For instance, it seems natural to map verbs into predicates and nouns into their arguments. But we know that if we want to provide compositional semantics for more than the simplest case of subject-verb-object construction we need other mechanism, e.g. type raising. On the other hand, unrestricted type raising leads to the results we have just discussed. We arrive then at the following variant of the natural question: How should we restrict type raising? (So that we can account e.g. for ellipsis, but at the same time constrain the grammars).\\n\\nThe last point we want to make is that while it is true that the homomorphism condition is too weak (in general) to count as systematicity, the semanticists (e.g. Montague) have not been using arbitrary homomorphisms. Thus a careful examination of their work should lead to some characterization of \"good\" homomorphisms; and this seems to be another interesting avenue of research. (As suggested by one of the referees, a technique from universal algebra might also prove helpful, where one first gives a class of algebras and then specifies meanings as homomorphisms from the initial algebra of the class).\\n\\nConclusions\\n\\nIn this paper we have shown the formal vacuity of the compositionality principle. That is, we have shown that the property that the meaning of the whole is a function of the meanings of its parts does not put any material constraints on syntax or semantics. Theorem 1 (and its corollaries) explain formally why the postulate of a homomorphism between syntax and semantics is not restrictive enough: the syntactic operator of concatenation \".\" can be mapped into functional composition operating on functions that encode arbitrary semantics of an arbitrary language. As we have seen in the examples, the presence of other operators does not change the result, because they can be treated as yet another letter of the alphabet, and one can still produce a homomorphism between syntax and semantics.\\n\\nThe problem of the vacuity of compositional semantics arises, because in the formal definition of compositionality meaning functions can be completely arbitrary. Therefore we have proposed that the meaning functions should be systematic, i.e., non-arbitrary. We have shown that this notion makes sense formally; that is, we presented examples of semantic classes of functions, for which there are grammars with meaning functions in that class, as well as we have shown that there are grammars that cannot have a meaning function in that class. As we noted in the previous section, both the formal and the linguistic nature of systematicity remains an open problem, but with a few promising avenues of research.\\n\\nFinally, the reader should note that one of the more bizarre consequences of Theorem 1 is that we do not have to start building compositional semantics for natural language beginning with assigning of the meanings to words. We can do equally well by assigning meanings to phonems  or even LETTERS, assuring that, for any sentence, the intuitive meaning we associate with it would be a function of the meanings of the letters from which that sentence is composed. But then the cabalists had always known it.\\n\\nACKNOWLEDGEMENTS. I would like to thank Alexis Manaster Ramer for our many discussions of compositionality, and the referees for their insightful comments.\\n\\nBibliography\\n\\nP. Aczel. Lectures on Non-Well-Founded Sets. CSLI Lecture Notes, Stanford, CA, 1987.\\n\\nJon Barwise and John Etchemendy. The Liar. Oxford University Press, New York, NY, 1987.\\n\\nJon Barwise. Admissible Sets and Structures. Springer, New York, NY, 1975.\\n\\nL. Bloomfield. Language. The University of Chicago Press, Chicago, IL, 1933.\\n\\nCharles J. Fillmore, Paul Kay, and Mary Catherine O\\'Connor. Regularity and idiomaticity in grammatical constructions. Language, 64(3):501-538, 1988.\\n\\nC.A. Gunter. Semantics of Programming Languages. MIT Press, Cambridge, MA, 1993.\\n\\nGraeme Hirst. Semantic interpretation and the resolution of ambiguity. Cambridge University Press, Cambridge, Great Britain, 1987.\\n\\nD. Jurafsky. An On-line Computational Model of Sentence Interpretation. PhD thesis, University of California, Berkeley, 1992. Report No. UCB/CSD 92/676.\\n\\nEdward L. Keenan and Leonard M. Faltz. Boolean Semantics for Natural Language. D Reidel, Dordrecht, Holland, 1985.\\n\\nAlexis Manaster\\n\\n\\n\\nRamer and Wlodek Zadrozny.\\n\\nSystematic semantics.\\n\\nin preparation, 1994.\\n\\nBarbara H. Partee, Alice ter Meulen, and Robert E. Wall. Mathematical Methods in Lingusitics. Kluwer, Dordrecht, The Netherlands, 1990.\\n\\nBarbara H. Partee. The logic of semantics. In Fred Lanman and Frank Veltman, editors, Varieties of Formal Semantics, pages 281-312. Foris, Dordrecht, Holland, 1982.\\n\\nWalter J. Savitch. Why it might pay to assume that languages are infinite. Annals of Mathematics and Artificial Intelligence, 8(1,2):17-26, 1993.\\n\\nP. Sells. Lectures on Contemporary Syntactic Theories. CSLI Lecture Notes (3), Stanford, CA, 1985.\\n\\nJohan van Benthem. The logic of semantics. In Fred Lanman and Frank Veltman, editors, Varieties of Formal Semantics, pages 55-80. Foris, Dordrecht, Holland, 1982.\\n\\nWlodek Zadrozny and Alexis Manaster-Ramer. The significance of constructions. submitted to Computational Linguistics, 1994.\\n\\nWlodek Zadrozny.\\n\\nOn compositional semantics.\\n\\nProc. Coling\\'92, pages 260\\n\\n\\n\\n266, 1992.\\n\\nFootnotes\\n\\nLinguistics and Philosophy(17):329\\n\\n\\n\\n342, 1994', metadata={'source': '../data/raw/cmplg-xml/9503024.xml'}),\n",
       " Document(page_content=\"Discourse and Deliberation: Testing a Collaborative Strategy\\n\\nA discourse strategy is a strategy for communicating with another agent. Designing effective dialogue systems requires designing agents that can choose among discourse strategies. We claim that the design of effective strategies must take cognitive factors into account, propose a new method for testing the hypothesized factors, and present experimental results on an effective strategy for supporting deliberation. The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics.\\n\\nIntroduction\\n\\nA discourse strategy is a strategy for communicating with another agent. Agents make strategy choices via decisions about when to talk, when to let the other agent talk, what to say, and how to say it. One choice a conversational agent must make is whether an utterance should include some relevant, but optional, information in what is communicated. For example, consider 1:\\n\\nLet's walk along Walnut St. It's shorter.\\n\\nThe speaker made a strategic choice in 0 to include 0b since she could have simply said 0a. What determines the speaker's choice?\\n\\nDeliberation in Discourse\\n\\nConsider a situation in which an agent A wants an agent B to accept a proposal P.  If B is a `helpful' agent (nonautonomous), B will accept A's proposal without a warrant. Alternatively, if B deliberates whether to accept P, but B knows of no competing options, then P will be the best option whether or not A tells B the warrant for P. Since a warrant makes the dialogue longer, the Explicit-Warrant strategy might be inefficient whenever either of these situations hold.\\n\\nListen to Ramesh.\\n\\nHe's Indian.\\n\\nFinally, the task determines whether there are penalties for leaving a warrant implicit and relying on B to infer or retrieve it. Some tasks require that two agents agree on the reasons for adopting a proposal, e.g. in order to ensure robustness in situations of environmental change. Other tasks, such as a management/union negotiation, only require the agents to agree on the actions to be carried out and each agent can have its own reasons for wanting those actions to be done without affecting success in the task.\\n\\nDesign\\n\\n\\n\\nWorld\\n\\nDesign World Domain and Task\\n\\nTo negotiate an agreement, each agent carries out means-end reasoning about the furniture pieces that they have that can be used in the floor plan. Means-end reasoning generates  OPTIONS - these options are the content of  PROPOSALS to the other agent to PUT a piece of furniture into one of the rooms. Dialogue 1 illustrates agents' communication for part of designing room-1, including both the artificial language that the agents communicate with and a gloss generated from that language in italics:\\n\\n[1:] BILL: First, let's put the green rug in the study. (propose agent-bill agent-kim option-10: put-act (agent-bill green rug room-1))\\n\\n[2:] KIM: Next, let's put the green lamp there.\\n\\n2:(propose agent-kim agent-bill option-33: put-act (agent-kim green lamp room-1)) [3:] BILL: Then, let's put the green couch in the study. (propose agent-bill agent-kim option-45: put-act (agent-bill green couch room-1))\\n\\n[4:] KIM: No, instead let's put in the purple couch. (reject agent-kim agent-bill option-56: put-act (agent-kim purple couch room-1))\\n\\nVarying Discourse Strategies\\n\\nThe Design-World experiments reported here compare the All-Implicit strategy with the Explicit-Warrant strategy. Agents are parametrized for different discourse strategies by placing different expansions of discourse plans in their plan libraries. Discourse plans are plans for PROPOSAL,  REJECTION,  ACCEPTANCE, CLARIFICATION,  OPENING and  CLOSING. The only variations discussed here are variations in the expansions of  PROPOSALS.\\n\\nThe All-Implicit strategy is an expansion of a discourse plan to make a  PROPOSAL, in which a  PROPOSAL decomposes trivially to the communicative act of  PROPOSE. In dialogue 0, both Design-World agents communicate using the All-Implicit strategy, and the proposals are shown in utterances 1, 2, and 3. The All-Implicit strategy never includes warrants in proposals, leaving it up to the other agent to retrieve them from memory.\\n\\nThe Explicit-Warrant strategy expands the  PROPOSAL discourse act to be a  WARRANT followed by a  PROPOSE utterance. Since agents already know the point values for pieces of furniture, warrants are always IRUs in the experiments here. For example, 1-1 is a WARRANT for the proposal in 1-2: The names of agents who use the Explicit-Warrant strategy are a numbered version of the string ``IEI'' to help the experimenter keep track of the simulation data files; IEI stands for Implicit acceptance, Explicit warrant, Implicit opening and closing.\\n\\n[1:] IEI: Putting in the green rug is worth  56. (say agent-iei agent-iei2 bel-10: score  (option-10: put-act (agent-iei green rug room-1) 56))\\n\\n[2:] IEI: Then, let's put the green rug in the study. (propose agent-iei agent-iei2 option-10: put-act (agent-iei green rug room-1))\\n\\n[3:] IEI2: Putting in the green lamp is worth  55. (say agent-iei2 agent-iei bel-34: score  (option-33: put-act (agent-iei2 green lamp room-1) 55) )\\n\\n[4:] IEI2: Then, let's put the green lamp in the study. (propose agent-iei2 agent-iei option-33: put-act (agent-iei2 green lamp room-1))\\n\\nCognitive and Task Parameters\\n\\nWhen an agent retrieves items from memory, search starts from the current pointer location and spreads out in a spherical fashion. Search is restricted to a particular search radius: radius is defined in Hamming distance. For example if the current memory pointer loci is (0 0 0), the loci distance 1 away would be (0 1 0) (0 -1 0) (0 0 1) (0 0 -1) (-1 0 0) (1 0 0). The actual locations are calculated modulo the memory size. The limit on the search radius defines the capacity of attention/working memory and hence defines which stored beliefs and intentions are  SALIENT.\\n\\nThe radius of the search sphere in the AWM model is used as the parameter for Design-World agents' resource-bound on attentional capacity. In the experiments below, memory is 16x16x16 and the radius parameter varies between 1 and 16, where AWM of 1 gives severely attention limited agents and AWM of 16 means that everything an agent knows is accessible. This parameter lets us distinguish between an agent's ability to access all the information stored in its memory, and the effort involved in doing so.\\n\\nAnother hypothetical factor was the relative cost of retrieval and communication. AWM also gives us a way to measure the number of retrievals from memory in terms of the number of locations searched to find a proposition. The amount of effort required for each retrieval step is a parameter, as is the cost of each inference step and the cost of each communicated message. These cost parameters support modeling various cognitive architectures, e.g. varying the cost of retrieval models different assumptions about memory. For example, if retrieval is free then all items in working memory are instantly accessible, as they would be if they were stored in registers with fast parallel access. If AWM is set to 16, but retrieval isn't free, the model approximates slow spreading activation that is quite effortful, yet the agent still has the ability to access all of memory, given enough time. If AWM is set lower than 16 and retrieval isn't free, then we model slow spreading activation with a timeout when effort exceeds a certain amount, so that an agent does not have the ability to access all of memory.\\n\\nFinally, we hypothesized that the Explicit-Warrant strategy may be beneficial if the relationship between the warrant and the proposal must be mutually believed. Thus the definition of success for the task is a Design-World parameter: the Standard task does not require a shared warrant, whereas the Zero NonMatching Beliefs task gives a zero score to any negotiated plan without agreed-upon warrants.\\n\\nEvaluating Performance\\n\\nTo evaluate  PERFORMANCE, we  compare the Explicit-Warrant strategy with the All-Implicit strategy in situations where we vary the task requirements, agents' attentional capacity, and the cost of retrieval, inference and communication. Evaluation of the resulting  DESIGN-HOUSE plan is parametrized by (1) COMMCOST: cost of sending a message; (2)  INFCOST: cost of inference; and (3)  RETCOST: cost of retrieval from memory:\\n\\nPERFORMANCE\\n\\nRAW SCORE is task specific: in the Standard task we simply summarize the point values of the furniture pieces in each PUT-ACT in the final Design, while in the Zero NonMatching Beliefs task, agents get no points for a plan unless they agree on the reasons underlying each action that contributes to the plan.\\n\\nA strategy A is  BENEFICIAL as compared to a strategy B, for a set of fixed parameter settings, if the difference in distributions using the Kolmogorov-Smirnov two sample test is significant at p [.05, in the positive direction, for two or more AWM settings. A strategy is  DETRIMENTAL if the differences go in the negative direction. Strategies may be neither  BENEFICIAL or DETRIMENTAL, as there may be no difference between two strategies.\\n\\nResults: Explicit Warrant\\n\\nExplicit Warrant reduces Retrievals\\n\\nDialogues in which one or both agents use the Explicit-Warrant strategy are more efficient when retrieval has a cost.\\n\\nExplicit Warrant is detrimental if Communication is Expensive Explicit Warrant Achieves a High Level of Agreement\\n\\nIf we change the definition of success in the task, we change whether a strategy is beneficial. When the task is Zero-Nonmatching-Beliefs, the Explicit-Warrant strategy is beneficial even if retrieval is free (KS ] .23 for AWM from 2 to 11, p [ .01) The warrant information that is redundantly provided is exactly the information that is needed in order to achieve matching beliefs about the warrants for intended actions. The strategy virtually guarantees that the agents will agree on the reasons for carrying out a particular course of action. The fact that retrieval is indeterminate produces this effect; a similar result is obtained when warrants are required and retrieval costs something.\\n\\nThese result suggests that including warrants is highly effective when agents must agree on a specific warrant, if they are attention-limited to any extent.\\n\\nConclusion\\n\\nThis paper has discussed an instance of a general problem in the design of conversational agents: when to include optional information. We presented and tested a number of hypotheses about the factors that contribute to the decision of when to include a warrant in a proposal. We showed that warrants are useful when the task requires agreement on the warrant, when the warrant is not currently salient, when retrieval of the warrant is indeterminate, or when retrieval has some associated cost, and that warrants hinder performance if communication is costly and if the warrant can displace information that is needed to complete the task, e.g. when AWM is very limited and warrants are not required to be shared.\\n\\nTo my knowledge, no previous work on dialogue has ever argued that conversational agents' resource limits are a major factor in determining effective conversational strategies in collaboration. The results presented here suggest that cooperative strategies cannot be defined in the abstract, but cooperation arises from the interaction of two agents in dialogue. If one agent has limited working memory, then the other agent can make the dialogue go more smoothly by adopting a strategy that makes deliberative premises salient. In other words, strategies are cooperative for certain conversational partners, under particular task definitions, for particular communication situations.\\n\\nAlthough much work remains to be done, there is reason to believe that these results are domain independent. The simplicity of the Design-World task means that its structure is a subcomponent of many other tasks. The model of limited resources is cognitively based, but the cost parameters support modeling different agent architectures, and we explored the effects of different cost parameters. The Explicit-Warrant strategy is based on simple relationships between different facts which we would expect to occur in any domain, i.e. the fact that some belief can be used as a  WARRANT for accepting a proposal should occur in almost any task. Future work should extend these results, showing that a `cooperative strategy' need not always be `cooperative', and investigate additional factors that determine when strategies are effective.\\n\\nBibliography\\n\\nAlan Baddeley.\\n\\nWorking Memory.\\n\\nOxford University Press, 1986.\\n\\nSusan E. Brennan. Seeking and Providing Evidence for Mutual Understanding. PhD thesis, Stanford University Psychology Dept., 1990. Unpublished Manuscript.\\n\\nJean C. Carletta. Risk Taking and Recovery in Task-Oriented Dialogue. PhD thesis, Edinburgh University, 1992.\\n\\nHerbert H. Clark and Edward F. Schaefer. Contributing to discourse. Cognitive Science, 13:259-294, 1989.\\n\\nJon Doyle. Rationality and its roles in reasoning. Computational Intelligence, November 1992.\\n\\nJulia R. Galliers. Autonomous belief revision and communication. In P. Gardenfors, editor, Belief Revision, pages 220 - 246. Cambridge University Press, 1991.\\n\\nSteve Hanks, Martha E. Pollack, and Paul R. Cohen. Benchmarks, testbeds, controlled experimentation and the design of agent architectures. AI Magazine, December 1993.\\n\\nD. L. Hintzmann and R. A. Block. Repetition and memory: evidence for a multiple trace hypothesis. Journal of Experimental Psychology, 88:297-306, 1971.\\n\\nThomas K. Landauer. Memory without organization: Properties of a model with random storage and undirected retrieval. Cognitive Psychology, pages 495-531, 1975.\\n\\nW.C. Mann and S.A. Thompson. Rhetorical structure theory: Description and construction of text structures. In Gerard Kempen, editor, Natural Language Generation, pages 83-96. Martinus Nijhoff, 1987.\\n\\nJohanna D. Moore and Ccile L. Paris. Planning text for advisory dialogues: Capturing intentional and rhetorical information. Computational Linguistics, 19(4), 1993.\\n\\nDonald A. Norman and Daniel G. Bobrow. On data-limited and resource-limited processes. Cognitive Psychology, 7(1):44-6, 1975.\\n\\nMartha Pollack, Julia Hirschberg, and Bonnie Webber. User participation in the reasoning process of expert systems. In AAAI82, 1982.\\n\\nMartha E. Pollack and Marc Ringuette. Introducing the Tileworld: Experimentally Evaluating Agent Architectures. In AAAI90, pages 183-189, 1990.\\n\\nRichard Power. Mutual intention. Journal for the Theory of Social Behaviour, 14, 1984.\\n\\nEllen F. Prince. Toward a taxonomy of given-new information. In Radical Pragmatics, pages 223-255. Academic Press, 1981.\\n\\nOwen Rambow and Marilyn A. Walker. The role of cognitive modeling in communicative intentions. In The 7th International Conference on Natural Language Generation, 1994.\\n\\nCandace Sidner. Using discourse to negotiate in collaborative activity: An artificial language. AAAI Workshop on Cooperation among Heterogeneous Agents, 1992.\\n\\nSidney Siegel. Nonparametric Statistics for the Behavioral Sciences. McGraw Hill, 1956.\\n\\nMarilyn A. Walker. Redundancy in collaborative dialogue. In Fourteenth International Conference on Computational Linguistics, pages 345-351, 1992.\\n\\nMarilyn A. Walker. Informational Redundancy and Resource Bounds in Dialogue. PhD thesis, University of Pennsylvania, 1993.\\n\\nMarilyn A. Walker. Experimentally evaluating communicative strategies: The effect of the task. In AAAI94, 1994.\\n\\nMarilyn A. Walker. Rejection by implicature. In Proceedings of the 20th Meeting of the Berkeley Lingustics Society, 1994.\\n\\nMarilyn A. Walker and Steve Whittaker. Mixed initiative in dialogue: An investigation into discourse segmentation. In Proc. 28th Annual Meeting of the ACL, pages 70-79, 1990.\\n\\nBonnie Webber and Aravind Joshi. Taking the initiative in natural language database interaction: Justifying why. In COLING84: Proc. 9th International Conference on Computational Linguistics. Prague, 1982.\\n\\nSteve Whittaker, Erik Geelhoed, and Elizabeth Robinson. Shared workspaces: How do they work and when are they useful? IJMMS, 39:813-842, 1993.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9503018.xml'}),\n",
       " Document(page_content=\"Hybrid Transfer in an English-French Spoken Language Translator\\n\\nThe paper argues the importance of high-quality translation for spoken language translation systems. It describes an architecture suitable for rapid development of high-quality limited-domain translation systems, which has been implemented within an advanced prototype English to French spoken language translator. The focus of the paper is the hybrid transfer model which combines unification-based rules and a set of trainable statistical preferences; roughly, rules encode domain-independent grammatical information and preferences encode domain-dependent distributional information. The preferences are trained from sets of examples produced by the system, which have been annotated by human judges as correct or incorrect. An experiment is described in which the model was tested on a 2000 utterance sample of previously unseen data.\\n\\nIntroduction\\n\\nDuring the last five years, people have started to believe there is a serious possibility of building practically useful spoken language translators for limited domains. There are now a number of high-profile projects with large budgets, the most well-known being the German Verbmobil effort. At the moment, the best systems are at the level of advanced prototypes; making projections from current performance, it seems reasonable to hope that these could be developed into commercially interesting systems within a time-scale of five to ten more years.\\n\\nOne of the most important differences between spoken language translation and text translation is that there are much stronger demands on quality of output. If output is not good enough, people frequently have difficulty understanding what has been said. There is no possibility of the pre- or post-editing which nearly all text translation systems rely on. Quite apart from the problem of generating natural-sounding speech, it is also necessary to ensure that the translated text sent to the speech synthesizer is itself of sufficient quality. A high-quality translation must fulfill several criteria: in particular, it should preserve the meaning of the original utterance, be grammatical, and contain correct word-choices.\\n\\nThe basic design philosophy of the SLT project has been to build a framework which is theoretically clean, on the usual grounds that this makes for a system that is portable and easy to scale up. We have attempted to subsume as much of the system as possible under two standard paradigms:  unification-based language processing and the noisy-channel statistical model. The unification-based part of the system encodes domain-independent grammatical rules; for each source-language word or grammatical construction covered by the system, it describes the possible target-language translations. When the rules permit more than one potentially valid translation, the statistical component is used to rank them in order of relative plausibility. The next two paragraphs give some examples to motivate this division of knowledge sources.\\n\\nThe SLT system\\n\\nTrainable transfer preferences\\n\\nThe transfer rule score and the target language model score are computed using the same method; for clarity, we first describe this method with reference to transfer rules. The transfer rule score for the bag of transfer rules used to produce a given target QLF is a sum of the discriminant scores for the individual transfer rules. The discriminant score for a rule R is calculated from the training corpus, and summarizes the reliability of R as an indicator that the transfer is correct or incorrect. The intent is that transfer rules which tend to occur more frequently in correct transfers than incorrect ones will get positive scores; those which occur more frequently in incorrect transfers than correct ones will get negative scores.\\n\\nS is a source language utterance,\\n\\nT1 and T2 are possible transfers for S, exactly one of which is correct,\\n\\nThe transfer rule R is used in exactly one of T1 and T2.\\n\\nThis formula is a symmetric, logarithmic transform of the function\\n\\n(g + 1)/(g + b + 2), which is the expected a posteriori probability that a new\\n\\n(S, T1, T2) 3-tuple will be a good occurrence of R, assuming that, prior to the quantities g and b being known, this probability has a uniform a priori distribution on the interval [0,1].\\n\\nPre\\n\\n\\n\\nand post\\n\\n\\n\\ntransfer\\n\\nIdeally, we would like to say that unification-based rules and trainable transfer preferences constituted the whole transfer mechanism. In fact, we have found it necessary to bracket the unification-based transfer component between pre- and post-transfer phases. Each phase consists of a small set of rewriting rules, which are applied recursively to the QLF structure. It would in principle have been possible to express these as normal unification-based transfer rules, but efficiency considerations and lack of implementation time persuaded us to adopt the current solution.\\n\\nThe post-transfer phase reduces the transferred QLF to a canonical form; the only non-trivial aspect of this process concerns the treatment of nominal and verbal PP modifiers. In French, PP modifier sequences are subject to a strong ordering constraint: locative PPs should normally be first and temporal PPs last, with other PPs in between. In the limited context of the ATIS domain, this requirement can be implemented fairly robustly with a half-dozen simple rules, and leads to a marked improvement in the quality of the translation.\\n\\nTransfer packing\\n\\nAs already indicated, the basic philosophy of the transfer component is to make the transfer rules more or less context-independent, and let the results be filtered through the statistically trained transfer preferences. The positive side of this is that the transfer rules are robust and simple to understand and maintain. The negative side is that non-deterministic transfer choices multiply out, giving a combinatoric explosion in the number of possible transferred QLFs.\\n\\nFrench language description\\n\\nExamples of non\\n\\n\\n\\ntrivial translation problems\\n\\nThis section will give examples of non-trivial translation problems from the ATIS domain, and describe how SLT deals with them. We were interested to discover that even a domain as simple as ATIS actually contains many quite difficult transfer problems; also, that English/French is considerably more challenging than the English/Swedish transfer pair used in the original SLT system. We will begin by giving examples where it is fairly clear that the problem is essentially grammatical in nature, and thus primarily involves the rule-based part of the system; later, we give examples where the problem mainly involves the preference component, and examples where both types of knowledge are needed.\\n\\nRecall that the preference score for a given transfer candidate is a weighted sum of a channel contribution (discriminants on transfer rules) and a target language model score (discriminants from target language semantic triples). The transfer rule discriminants make transfer rules act more or less strongly as defaults. If a transfer rule R is correct more often than not when a choice arises, it will have a positive discriminant, and will thus be preferred if there is no reason to avoid it. If use of R produces a strong negative target-language discriminant, however, the default will be overridden.\\n\\nImplementation status and results\\n\\nSo far, the French version of SLT has consumed about eleven person-months of effort over and above the effort expended on the original English/Swedish SLT project. Of this, about seven person-months were spent on the French language description, two on transfer rules, and two on other tasks. The small quantity of effort required to develop a good French language description underlines the extent to which its structure overlaps with that of the original English grammar and lexicon.\\n\\nWe now describe preliminary experiments designed to test the performance of the system. A set of 2000 ATIS utterances was used, randomly selected from the subset of the ATIS corpus consisting of A or D class utterances of length up to 15 words, which had not previously been examined during the development of the French version of SLT. Utterances were supplied in text form, i.e. the speech recognition part of the system was not tested here.\\n\\nBibliography\\n\\nAgns, M-S., Alshawi, H., Bretan, I., Carter, D.M. Ceder, K., Collins, M., Crouch, R., Digalakis, V., Ekholm, B., Gambck, B., Kaja, J., Karlgren, J., Lyberg, B., Price, P., Pulman, S., Rayner, M., Samuelsson, C. and Svensson, T. 1994. Spoken Language Translator: First Year Report. SRI technical report CRC-043 (also SICS research report R94:03) Available through WWW from http://www.cam.sri.com\\n\\nAlshawi, H., Carter, D., Rayner, M. and Gambck, B. 1991. Transfer through Quasi Logical Form. Proc. 29th ACL, Berkeley, CA.\\n\\nAlshawi, H. (ed.)\\n\\n1992.\\n\\nThe Core Language Engine.\\n\\nMIT Press.\\n\\nAlshawi, Hiyan, and David Carter. 1994. Training and Scaling Preference Functions for Disambiguation. Computational Linguistics, 20:4.\\n\\nBouillon, P. and L. Tovena. 1990. L'analyse morphologique du francais et de l'italien avec le lexique ALVEY. ISSCO working paper 57, Geneva.\\n\\nBrown, P.F., J. Cook, S.A. della Pietra, V.G. della Pietra, F. Jelinek, J.D. Lafferty, R.L. Mercer and P.S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16:2.\\n\\nHemphill, C.T., J.J. Godfrey and G.R. Doddington. 1990. The ATIS Spoken Language Systems pilot corpus. Proc. DARPA Speech and Natural Language Workshop, Hidden Valley, Pa.\\n\\nMurveit, H., Butzberger, J., Digalakis, V. and Weintraub, M. 1993. Large Vocabulary Dictation using SRI's DECIPHER(TM) Speech Recognition System: Progressive Search Techniques. Proc. Inter. Conf. on Acoust., Speech and Signal, Minneapolis, Minnesota.\\n\\nRayner, M., Alshawi, H., Bretan, I., Carter, D.M., Digalakis, V., Gambck, B., Kaja, J., Karlgren, J., Lyberg, B., Price, P., Pulman, S. and Samuelsson, C. 1993. A Speech to Speech Translation System Built From Standard Components. Proc. 1st ARPA workshop on Human Language Technology.\\n\\nRayner, M., D. Carter, V. Digalakis and P. Price. 1994. Combining Knowledge Sources to Reorder N-Best Speech Hypothesis Lists. Proc. 2nd ARPA workshop on Human Language Technology.\\n\\nRayner, M., D. Carter, P. Price and B. Lyberg. 1994. Estimating the Performance of Pipelined Spoken Language Translation Systems. Proc. of ICSLP '94, Yokohama.\\n\\nShieber, S. M., van Noord, G., Pereira, F.C.N and Moore, R.C. 1990. Semantic-Head-Driven Generation. Computational Linguistics, 16:30-43.\\n\\nTMI. 1992. Proc. Fourth International Conference on Theoretical and Methodological Issues in Machien Translation. Montreal, Canada.\\n\\nTomita, M. 1986. Efficient Parsing for Natural Language. Kluwer Academic Publisher.\\n\\nFootnotes\\n\\nAll examples presented in this section are correctly processed by the current French version of SLT. This means roughly that the sentence represented a valid inquiry to the database, either alone or in the context in which it was uttered. There were a further 246 sets in which at least one candidate translation was produced; in most of these cases, the best translation was comprehensible and grammatically correct, but was rejected on stylistic grounds.\", metadata={'source': '../data/raw/cmplg-xml/9505045.xml'}),\n",
       " Document(page_content=\"Building Natural Language Generation Systems Content Determination and Text Planning Sentence Planning Conjunction and other aggregation. For example, transforming (1) into (2): 1) Sam has high blood pressure. Sam has low blood sugar. 2) Sam has high blood pressure and low blood sugar.\\n\\nPronominalization and other reference. For example, transforming (3) into (4): 3) I just saw Mrs. Black. Mrs Black has a high temperature. 4) I just saw Mrs. Black. She has a high temperature.\\n\\nIntroducing discourse markers. For example, transforming (5) into (6): 5) If Sam goes to the hospital, he should go to the store. 6) If Sam goes to the hospital, he should also go to the store.\\n\\nSentence planning is important if the text needs to read fluently and, in particular, if it should look like it was written by a human (which is usually the case for business letters, for example). If it doesn't matter if the text sounds stilted and was obviously produced by a computer, then it may be possible to de-emphasize sentence planning, and perform minimal aggregation, use no pronouns, etc.\\n\\nRealization Point absorption and other punctuation rules. For example, the sentence I saw Helen Jones, my sister-in-law should end in ``. '', not ``,.''\\n\\nMorphology. For example, the plural of box is boxes, not boxs.\\n\\nAgreement. For example, I am here instead of I is here.\\n\\nReflexives. For example, John saw himself, instead of John saw John.\\n\\nIn many cases, acceptable performance can be achieved without using complex linguistic modules. In particular, if only a few different types of sentences are being generated, then it may be simpler and cheaper to use fill-in-the-blank templates for realization, instead of `proper' syntactic processing.\\n\\nConclusion\\n\\nBibliography\\n\\nB. Buchanan, J. Moore, D. Forsythe, G. Carenini, and S. Ohlsson. Using medical informatics for explanation in a clinical setting. Technical Report 93-16, Intelligent Systems Laboratory, University of Pittsburgh, 1994.\\n\\nMichael Elhadad. Using Argumentation to Control Lexical Choice: A Functional Unification Implementation. PhD thesis, Columbia University, 1992.\\n\\nEli Goldberg, Norbert Driedger, and Richard Kittredge. Using natural-language processing to produce weather forecasts. IEEE Expert, 9(2):45-53, 1994.\\n\\nEduard Hovy. Planning coherent multisentential text. In Proceedings of 26th Annual Meeting of the Association for Computational Linguistics (ACL-88), pages 163-169, 1988.\\n\\nRichard Kittredge, Tanya Korelsky, and Owen Rambow. On the need for domain communication language. Computational Intelligence, 7(4):305-314, 1991.\\n\\nKathleen McKeown.\\n\\nText Generation.\\n\\nCambridge University Press, 1985.\\n\\nKathleen McKeown, Karen Kukich, and James Shaw. Practical issues in automatic document generation. In Proceedings of the Fourth Conference on Applied Natural-Language Processing (ANLP-1994), pages 7-14, 1994.\\n\\nPenman Natural Language Group. The Penman user guide. Technical report, Information Sciences Institute, Marina del Rey, CA 90292, 1989.\\n\\nOwen Rambow and Tanya Korelsky. Applied text generation. In Proceedings of the Third Conference on Applied Natural Language Processing (ANLP-1992), pages 40-47, 1992.\\n\\nEhud Reiter. Has a consensus NL Generation architecture appeared, and is it psycholinguistically plausible? In Proceedings of the Seventh International Workshop on Natural Language Generation (INLGW-1994), pages 163-170, 1994.\\n\\nEhud Reiter and Chris Mellish. Optimising the costs and benefits of natural language generation. In Proceedings of the 13th International Joint Conference on Artificial Intelligence (IJCAI-1993), volume 2, pages 1164-1169, 1993.\\n\\nEhud Reiter, Chris Mellish, and John Levine. Automatic generation of technical documentation. Applied Artificial Intelligence, 9(3):259-287, 1995.\\n\\nStephen Springer, Paul Buta, and Thomas Wolf. Automatic letter composition for customer service. In Reid Smith and Carlisle Scott, editors, Innovative Applications of Artificial Intelligence 3 (Proceedings of CAIA-1991). AAAI Press, 1991.\", metadata={'source': '../data/raw/cmplg-xml/9605002.xml'}),\n",
       " Document(page_content=\"Parsing for Semidirectional Lambek Grammar is NP-Complete\\n\\nWe study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call     semidirectional. In semidirectional Lambek calculus SDL there is an additional non-directional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent's left-hand side, thus permitting non-peripheral extraction. SDL grammars are able to generate each context-free language and more than that. We show that the parsing problem for semidirectional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem.\\n\\nIntroduction\\n\\nA drawback of the pure Lambek Calculus LMBK is that it only allows for so-called `peripheral extraction', i.e., in our example the trace should better be initial or final in the relative clause.\\n\\nNote that our purpose for studying SDL is not that it might be in any sense better suited for a theory of grammar (except perhaps, because of its simplicity), but rather, because it exhibits a core of logical behaviour that any richer system also needs to include, at least if it should allow for non-peripheral extraction. The sources of complexity uncovered here are thus a forteriori present in all these richer systems as well.\\n\\nSemidirectional Lambek Grammar\\n\\nLambek calculus\\n\\nFirst of all, since we don't need products to obtain our results and since they only complicate matters, we eliminate products from consideration in the sequel.\\n\\nIn Semidirectional Lambek Calculus we add as additional connective the LP implication llimp, but equip it only with a right rule.\\n\\nThe cut-free system enjoys, as usual for Lambek-like logics, the   Subformula Property: in any proof only subformulae of the goal sequent may appear.\\n\\nThe invariant now states that for any primitive b, the b-count of the RHS and the LHS of any derivable sequent are the same. By noticing that this invariant is true for Ax and is preserved by the rules, we immediately can state:\\n\\nLambek Grammar\\n\\nConcerning SDL, it is straightforward to show that all context-free languages can be generated by SDL-grammars.\\n\\nMoreover, some languages which are not context-free can also be generated.\\n\\nThe distinguished primitive type is x. To simplify the argumentation, we abbreviate types as indicated above.\\n\\nNP-Completeness of the Parsing Problem\\n\\nWe show that the Parsing Problem for SDL-grammars is NP-complete by a reduction of the 3-Partition Problem to it.\\n\\nwhich completes the proof.\\n\\nThe reduction above proves NP-hardness of the parsing problem. We need strong NP-completeness of 3-Partition here, since our reduction uses a unary encoding. Moreover, the parsing problem also lies within NP, since for a given grammar Gproofs are linearly bound by the length of the string and hence, we can simply guess a proof and check it in polynomial time. Therefore we can state the following:\\n\\nConclusion\\n\\nWe have defined a variant of Lambek's original calculus of types that allows  abstracted-over categories to freely permute. Grammars based on SDL can generate any context-free language and more than that. The parsing problem for SDL, however, we have shown to be NP-complete. This result indicates that efficient parsing for grammars that allow for large numbers of unbounded dependencies from within one node may be problematic, even in the categorial framework. Note that the fact, that this problematic case doesn't show up in the correct analysis of normal NL sentences, doesn't mean that a parser wouldn't have to try it, unless some arbitrary bound to that number is assumed. For practical grammar engineering one can devise the motto avoid accumulation of unbounded dependencies by whatever means.\\n\\nOn the theoretical side we think that this result for SDL is also of some importance, since SDL exhibits a core of logical behaviour that any (Lambek-based) logic must have which accounts for non-peripheral extraction by some form of permutation. And hence, this result increases our understanding of the necessary computational properties of such richer systems. To our knowledge the question, whether the Lambek calculus itself or its associated parsing problem are NP-hard, are still open.\\n\\nBibliography\\n\\nJ. van Benthem. The Lambek Calculus. In R. T. O. et al. (Ed. ), Categorial Grammars and Natural Language Structures, pp. 35-68. Reidel, 1988.\\n\\nM. R. Garey and D. S. Johnson. Computers and Intractability--A Guide to the Theory of NP-Completeness. Freeman, San Francisco, Cal., 1979.\\n\\nJ.\\n\\n\\n\\nY. Girard.\\n\\nLinear Logic.\\n\\nTheoretical Computer Science, 50(1):1\\n\\n\\n\\n102, 1987.\\n\\nE. Knig. LexGram - a practical categorial grammar formalism. In Proceedings of the Workshop on Computational Logic for Natural Language Processing. A Joint COMPULOGNET/ELSNET/EAGLES Workshop, Edinburgh, Scotland, April 1995.\\n\\nJ. Lambek. The Mathematics of Sentence Structure. American Mathematical Monthly, 65(3):154-170, 1958.\\n\\nP. Lincoln and T. Winkler. Constant-Only Multiplicative Linear Logic is NP-Complete. Theoretical Computer Science, 135(1):155-169, Dec. 1994.\\n\\nM. Moortgat. Residuation in Mixed Lambek Systems. In M. Moortgat (Ed. ), Lambek Calculus. Multimodal and Polymorphic Extensions, DYANA-2 deliverable R1.1.B. ESPRIT, Basic Research Project 6852, Sept. 1994.\\n\\nG. Morrill. Type Logical Grammar: Categorial Logic of Signs. Kluwer, 1994.\\n\\nM. Pentus. Lambek grammars are context free. In Proceedings of Logic in Computer Science, Montreal, 1993.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9605016.xml'}),\n",
       " Document(page_content=\"DISTRIBUTIONAL CLUSTERING OF ENGLISH WORDS\\n\\nWe describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts. Deterministic annealing is used to find lowest distortion sets of clusters. As the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ``soft'' clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.\\n\\nINTRODUCTION\\n\\nMethods for automatically classifying words according to their contexts of use have both scientific and practical interest. The scientific questions arise in connection to distributional views of linguistic (particularly lexical) structure and also in relation to the question of lexical acquisition both from psychological and computational learning perspectives. From the practical point of view, word classification addresses questions of data sparseness and generalization in statistical language models, particularly models for deciding among alternative analyses proposed by a grammar.\\n\\nIt is well known that a simple tabulation of frequencies of certain words participating in certain configurations, for example of frequencies of pairs of a transitive main verb and the head noun of its direct object, cannot be reliably used for comparing the likelihoods of different alternative configurations. The problem is that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities.\\n\\nProblem Setting\\n\\nWe will consider here only the problem of classifying nouns according to their distribution as direct objects of verbs; the converse problem is formally similar. More generally, the theoretical basis for our method supports the use of clustering to build models for any n-ary relation in terms of associations between elements in each coordinate and appropriate hidden units (cluster centroids) and associations between those hidden units.\\n\\nDistributional Similarity\\n\\nTo cluster nouns n according to their conditional verb distributions pn, we need a measure of similarity between distributions. We use for this purpose the relative entropy or Kullback-Leibler (KL) distance between two distributions\\n\\nThis is a natural choice for a variety of reasons, which we will just sketch here.\\n\\nFinally, relative entropy is a natural measure of similarity between distributions for clustering because its minimization leads to cluster centroids that are a simple weighted average of member distributions.\\n\\nTHEORETICAL BASIS\\n\\nIn general, we are interested on how to organize a set of linguistic objects such as words according to the contexts in which they occur, for instance grammatical constructions or n-grams. We will show elsewhere that the theoretical analysis outlined here applies to that more general problem, but for now we will only address the more specific problem in which the objects are nouns and the contexts are verbs that take the nouns as direct objects.\\n\\nFrom a learning perspective, this problem falls somewhere in between unsupervised and supervised learning. As in unsupervised learning, the goal is to learn the underlying distribution of the data. But in contrast to most unsupervised learning settings, the objects involved have no internal structure or attributes allowing them to be compared with each other. Instead, the only information about the objects is the statistics of their joint appearance. These statistics can thus be seem as a weak form of object labelling analogous to supervision.\\n\\nDistributional Clustering\\n\\nwhere p(c | n ) is the membership probability of n in c and\\n\\npc(v) = p(v|c) is v's conditional probability given by the centroid distribution for cluster c.\\n\\nGoodness of fit is determined by the model's likelihood of the observations. The maximum likelihood (ML) estimation principle is thus the natural tool to determine the centroid distributions pc(v).\\n\\nMaximum Likelihood Cluster Centroids\\n\\nwith p(n|c) and p(v|c) kept normalized. Using Bayes's formula, we have\\n\\nor\\n\\nAt this point we need to specify the clustering model in more detail. In the derivation so far we have treated p(n|c) and p(v|c)symmetrically, corresponding to clusters not of verbs or nouns but of verb-noun associations. In principle such a symmetric model may be more accurate, but in this paper we will concentrate on asymmetric models in which cluster memberships are associated to just one of the components of the joint distribution and the cluster centroids are specified only by the other component. In particular, the model we use in our experiments has noun clusters with cluster memberships determined by p(n|c) and centroid distributions determined by p(v|c).\\n\\nThe asymmetric model simplifies the estimation significantly by dealing with a single component, but it has the disadvantage that the joint distribution, p(n,v) has two different and not necessarily consistent expressions in terms of asymmetric models for the two coordinates.\\n\\nMaximum Entropy  Cluster Membership\\n\\nGiven any similarity measure d(n,c) between nouns and cluster centroids, the average cluster distortion is\\n\\nIf we maximize the cluster membership entropy\\n\\nwhere the variation of p(v|c) is now included in the variation of d(n,c).\\n\\nwhich, applying Bayes's rule, becomes\\n\\nor, sufficiently, if each of the inner sums vanish\\n\\nMinimizing the Average KL Distortion\\n\\nWe first show that the minimization of the relative entropy yields the natural expression for cluster centroids\\n\\nThe Free Energy Function\\n\\nThe combined minimum distortion and maximum entropy optimization is equivalent to the minimization of a single function, the free energy\\n\\nThe free energy determines both the distortion and the membership entropy through\\n\\nThe most important property of the free energy is that its minimum determines the balance between the ``disordering'' maximum entropy and ``ordering'' distortion minimization in which the system is most likely to be found. In fact the probability to find the system at a given configuration is exponential in F\\n\\nso a system is most likely to be found in its minimal free energy configuration.\\n\\nHierarchical Clustering\\n\\nCLUSTERING EXAMPLES\\n\\nAs a first experiment, we used our method to classify the 64 nouns appearing most frequently as heads of direct objects of the verb ``fire'' in one year (1988) of Associated Press newswire. In this corpus, the chosen nouns appear as direct object heads of a total of 2147 distinct verbs, so each noun is represented by a density over the 2147 verbs.\\n\\nMODEL EVALUATION\\n\\nThe preceding qualitative discussion provides some indication of what aspects of distributional relationships may be discovered by clustering. However, we also need to evaluate clustering more rigorously as a basis for models of distributional relationships. So, far, we have looked at two kinds of measurements of model quality: (i) relative entropy between held-out data and the asymmetric model, and (ii) performance on the task of deciding which of two verbs is more likely to take a given noun as direct object when the data relating one of the verbs to the noun has been witheld from the training data.\\n\\nThe evaluation described below was performed on the largest data set we have worked with so far, extracted from 44 million words of 1988 Associated Press newswire with the pattern matching techniques mentioned earlier. This collection process yielded 1112041 verb-object pairs. We selected then the subset involving the 1000 most frequent nouns in the corpus for clustering, and randomly divided it into a training set of 756721 pairs and a test set of 81240 pairs.\\n\\nRelative Entropy\\n\\nThe new noun test set is intended to test whether clusters based on the 1000 most frequent nouns are useful classifiers for the selectional properties of nouns in general. As the figure shows, the cluster model provides over one bit of information about the selectional properties of the new nouns, but the overtraining effect is even sharper than for the held-out data involving the 1000 clustered nouns.\\n\\nDecision Task\\n\\nWe also evaluated asymmetric cluster models on a verb decision task closer to possible applications to disambiguation in language analysis. The task consists judging which of two verbs v and v' is more likely to take a given noun n as object, when all occurrences of (v,n) in the training set were deliberately deleted. Thus this test evaluates how well the models reconstruct missing data in the verb distribution for n from the cluster centroids close to n.\\n\\nHere too we see some overtraining for the largest models considered, although not for the exceptional verbs.\\n\\nCONCLUSIONS\\n\\nWe have demonstrated that a general divisive clustering procedure for probability distributions can be used to group words according to their participation in particular grammatical relations with other words. The resulting clusters are intuitively informative, and can be used to construct class-based word coocurrence models with substantial predictive power.\\n\\nWhile the clusters derived by the proposed method seem in many cases semantically significant, this intuition needs to be grounded in a more rigorous assessment. In addition to predictive power evaluations of the kind we have already carried out, it might be worth comparing automatically-derived clusters with human judgements in a suitable experimental setting.\\n\\nACKNOWLEDGMENTS\\n\\nWe would like to thank Don Hindle for making available the 1988 Associated Press verb-object data set, the Fidditch parser and a verb-object structure filter, Mats Rooth for selecting the objects of ``fire'' data set and many discussions, David Yarowsky for help with his stemming and concordancing tools, and Ido Dagan for suggesting ways of testing cluster models.\\n\\nBibliography\\n\\nPeter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1990. Class-based n-gram models of natural language. In Proceedings of the IBM Natural Language ITL, pages 283-298, Paris, France, March.\\n\\nKenneth W. Church and William A. Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5:19-54.\\n\\nKenneth W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143, Austin, Texas. Association for Computational Linguistics, Morristown, New Jersey.\\n\\nThomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. Wiley-Interscience, New York, New York.\\n\\nIdo Dagan, Shaul Markus, and Shaul Markovitch. 1992. Contextual word similarity and the estimation of sparse lexical relations. Submitted for publication.\\n\\nA. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1-38.\\n\\nRichard O. Duda and Peter E. Hart. 1973. Pattern Classification and Scene Analysis. Wiley-Interscience, New York, New York.\\n\\nDonald Hindle. 1990. Noun classification from predicate-argument structures. In 28th Annual Meeting of the Association for Computational Linguistics, pages 268-275, Pittsburgh, Pennsylvania. Association for Computational Linguistics, Morristown, New Jersey.\\n\\nDonald Hindle. 1993. A parser for text corpora. In B.T.S. Atkins and A. Zampoli, editors, Computational Approaches to the Lexicon. Oxford University Press, Oxford, England. To appear.\\n\\nPhilip Resnik. 1992. WordNet and distributional analysis: A class-based approach to lexical discovery. In AAAI Workshop on Statistically-Based Natural-Language-Processing Techniques, San Jose, California, July.\\n\\nKenneth Rose, Eitan Gurewitz, and Geoffrey C. Fox. 1990. Statistical mechanics and phase transitions in clustering. Physical Review Letters, 65(8):945-948.\\n\\nYves Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Proceeedings of the 14th International Conference on Computational Linguistics, Nantes, France.\\n\\nDavid Yarowsky.\\n\\n1992.\\n\\nPersonal communication.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9408011.xml'}),\n",
       " Document(page_content=\"A MATCHING TECHNIQUE IN EXAMPLE-BASED MACHINE TRANSLATION\\n\\nThis paper addresses an important problem in Example-Based Machine Translation (EBMT), namely how to measure similarity between a sentence fragment and a set of stored examples. A new method is proposed that measures similarity according to both surface structure and content. A second contribution is the use of clustering to make retrieval of the best matching example from the database more efficient. Results on a large number of test cases from the CELEX database are presented.\\n\\nINTRODUCTION\\n\\nEBMT is based on the idea of performing translation by imitating translation examples of similar sentences [Nagao 84]. In this type of translation system, a large amount of bi/multi-lingual translation examples has been stored in a textual database and input expressions are rendered in the target language by retrieving from the database that example which is most similar to the input.\\n\\nThere are three key issues which pertain to example-based translation:\\n\\nestablishment of correspondence between units in a bi/multi-lingual text at sentence, phrase or word level\\n\\na mechanism for retrieving from the database the unit that best matches the input\\n\\nexploit the retrieved translation example to produce the actual translation of the input sentence\\n\\nThe third key issue of EBMT, that is exploiting the retrieved translation example, is usually dealt with by integrating into the system conventional MT techniques [Kaji 92], [Sumita 91]. Simple modifications of the translation proposal, such as word substitution, would also be possible, provided that alignment of the translation archive at word level was available.\\n\\nIn establishing a mechanism for the best match retrieval, which is the topic of this paper, the crucial tasks are: (i) determining whether the search is for matches at sentence or sub-sentence level, that is determining the ``text unit'', and (ii) the definition of the metric of similarity between two text units.\\n\\nAs far as (i) is concerned, the obvious choice is to use as text unit the sentence. This is because, not only are sentence boundaries unambiguous but also translation proposals at sentence level is what a translator is usually looking for. Sentences can, however, be quite long. And the longer they are, the less possible it is that they will have an exact match in the translation archive, and the less flexible the EBMT system will be.\\n\\nOn the other hand if the text unit is the sub-sentence we face one major problem, that is the possibility that the resulting translation of the whole sentence will be of low quality, due to boundary friction and incorrect chunking. In practice, EBMT systems that operate at sub-sentence level involve the dynamic derivation of the optimum length of segments of the input sentence by analysing the available parallel corpora. This requires a procedure for determining the best ``cover'' of an input text by segments of sentences contained in the database [Nirenburg 93]. It is assumed that the translation of the segments of the database that cover the input sentence is known. What is needed, therefore, is a procedure for aligning parallel texts at sub-sentence level [Kaji 92], [Sadler 90]. If sub-sentence alignment is available, the approach is fully automated but is quite vulnerable to the problem of low quality as mentioned above, as well as to ambiguity problems when the produced segments are rather small.\\n\\nDespite the fact that almost all running EBMT systems employ the sentence as the text unit, it is believed that the potential of EBMT lies on the exploitation of fragments of text smaller that sentences and the combination of such fragments to produce the translation of whole sentences [Sato 90]. Automatic sub-sentential alignment is, however, a problem yet to be solved.\\n\\nTurning to the definition of the metric of similarity, the requirement is usually twofold. The similarity metric applied to two sentences (by sentence from now on we will refer to both sentence and sub-sentence fragment) should indicate how similar the compared sentences are, and perhaps the parts of the two sentences that contributed to the similarity score. The latter could be just a useful indication to the translator using the EBMT system, or a crucial functional factor of the system as will be later explained.\\n\\nThe word-based metrics are the most popular, but other approaches include syntax-rule driven metrics [Sumita 88], character-based metrics [Sato 92] as well as some hybrids [Furuse 92]. The character-based metric has been applied to Japanese, taking advantage of certain characteristics of the Japanese. The syntax-rule driven metrics try to capture similarity of two sentences at the syntax level. This seems very promising, since similarity at the syntax level, perhaps coupled by lexical similarity in a hybrid configuration, would be the best the EBMT system could offer as a translation proposal. The real time feasibility of such a system is, however, questionable since it involves the complex task of syntactic analysis.\\n\\nIn section 2 a similarity metric is proposed and analysed. The statistical system presented consists of two phases, the Learning and the decision making or Recognition phase, which are described in section III. Finally, in section IV the experiment configuration is discussed and the results evaluated.\\n\\nTHE SIMILARITY METRIC\\n\\nTo encode a sentence into a vector, we exploit information about the functional words/phrases (fws) appearing in it, as well as about the lemmas and pos (part-of-speech) tags of the words appearing between fws/phrases. Based on the combination of fws/phrases data and pos tags, a simple view of the surface syntactic structure of each sentence is obtained.\\n\\nTo identify the fws/phrases in a given corpus the following criteria are applied:\\n\\nfws introduce a syntactically standard behaviour\\n\\nmost of the fws belong to closed classes\\n\\nthe semantic behaviour of fws is determined through their context\\n\\nmost of the fws determine phrase boundaries\\n\\nfws have a relatively high frequency in the corpus\\n\\nAccording to these criteria, prepositions, conjunctions, determiners, pronouns, certain adverbials etc. are regarded as fws. Having identified the fws of the corpus we distinguish groups of fws on the basis of their interchangeability in certain phrase structures. The grouping caters, also, for the multiplicity of usages of a certain word which has been identified as a fw, since a fw can be a part of many different groups. In this way, fws can serve the retrieval procedure with respect to the following two levels of contribution towards the similarity score of two sentences :\\n\\nIdentity of fws of retrieved example and input (I)\\n\\nfws of retrieved example and input not identical but belonging to the same group (G)\\n\\nTo obtain the lemmas and pos tags of the remaining words in a sentence, we use a part-of-speech Tagger with no disambiguation module, since this would be time consuming and not 100% accurate. Instead, we introduce the concept of ambiguity class (ac) and we represent each non-fw by its ac and the corresponding lemma(s) (for example, the unambiguous word ``eat'' would be represented by the ac which is the set verb and the lemma ``eat'') (in English, for an ambiguous word, the corresponding lemmas will usually be identical. But this is rarely true for Greek). Hence, the following two levels of contribution to the similarity score stem from non-fws:\\n\\noverlapping of the sets of possible lemmas of the two words (L)\\n\\noverlapping of the ambiguity classes of the two words (T)\\n\\nHence, each sentence of the source part of the translation archive is represented by a pattern, which is expressed as an ordered series of the above mentioned feature components.\\n\\nA similarity metric is defined between two such vectors, and is used in both the Learning and Recognition phases. Comparing a test vector against a reference vector is, however, not straightforward, since there are generally axis fluctuations between the vectors (not necessarily aligned vectors and of most probably different length). To overcome these problems we use a two-level Dynamic Programming (DP) technique [Sakoe 78], [Ney 84]. The first level treats the matches at fw level, while the second is reached only in case of a match in the first level, and is concerned with the lemmas and tags of the words within fw boundaries. Both levels utilise the same (DP) model which is next described.\\n\\nWe have already referred to the (I) and (G) contributions to the similarity score due to fws. But this is not enough. We should also take into account whether the fws appear in the same order in the two sentences, whether an extra (or a few) fws intervene in one of the two sentences, whether certain fws are missing ... To deal with these problems, we introduce a yet third contribution to the similarity score, which is negative and is called penalty score (P). So, as we are moving along a diagonal of the xy-plane (corresponding to matched fws), whenever a fw is mismatched, it produces a negative contribution to the score along a horizontal or vertical direction. In figure 1 the allowable transitions in the xy-plane are shown:\\n\\nWhenever a diagonal transition is investigated, the system calls the second level DP-algorithm which produces a local additional score due to the potential similarity of lemmas and tags of the words lying between the corresponding fws. This score is calculated using exactly the same DP-algorithm as the one treating fws (allowing additions, deletions,...), provided that we use (L), (T) and (PT) (a penalty score attributed to a mismatch at the tag-level) in place of (I), (G) and  (P) respectively.\\n\\nThe outcome of the DP-algorithm is the similarity score between two vectors which allows for different lengths of the two sentences, similarity of different parts of the two sentences (last part of one with the first part of the other) and finally variable number of additions and deletions. The score produced, corresponds to two coherent parts of the two sentences under comparison. Emphasis should be given to the variable number of additions and deletions. The innovation of the penalty score (which is in fact a negative score) provides the system with the flexibility to afford a different number of additions or deletions depending on the accumulated similarity score up to the point where these start. Moreover, the algorithm determines, through a backtracking procedure, the relevant parts of the two vectors that contributed to this score. This is essential for the sentence segmentation described in the next section.\\n\\nIt should also be noted that the similarity score produced is based mainly on the surface syntax of the two sentences (as this is indicated by the fws and pos tags) and in the second place on the actual words of the two sentences. This is quite reasonable, since the two sentences could have almost the same words in the source language but no similarity at all in the source or target language (due to different word order, as well as different word utilisation), while if they are similar in terms of fws as well as in terms of the pos tags of the words between fws, then the two sentences would almost certainly be similar (irrelevant of a few differences in the actual words) in the target language as well (which is the objective).\\n\\nThe DP-algorithm proposed seems to be tailored to the needs of the similarity metric but there is yet a crucial set of parameters to be set, that is A=I,G,P,L,T,PT. The DP-algorithm is just the framework for the utilisation of these parameters. The values of the parameters of A are set dynamically depending on the lengths of the sentences under comparison. I, G, L, T are set to values (I, G are normalised by the lengths of the sentences in fws, while L, T are normalised by the lengths of the blocks of words appearing between fws) which produce a 100% similarity score when the sentences are identical, while P, PT reflect the user's choise of penalising an addition or deletion of a word (functional or not).\\n\\nLEARNING AND RECOGNITION PHASES\\n\\nIn the Learning phase, the modified k-means clustering procedure [Wilpon 85] is applied to the source part of the translation archive, aiming to produce clusters of sentences, each represented by its centre only. The algorithm produces the optimum segmentation of the corpus into clusters (based on the similarity metric), and determines each cluster centre (which is just a sentence of the corpus) by using the minmax criterion. The number of clusters can be determined automatically by the process, subject to some cluster quality constraint (for example, minimum intra-cluster similarity), or alternatively can be determined externally based upon memory-space restrictions and speed requirements.\\n\\nOnce the clustering procedure is terminated, a search is made, among the sentences allocated to a cluster, to locate second best (but good enough) matches to the sentences allocated to the remaining clusters. If such matches are traced, the relevant sentences are segmented and then the updated corpus is reclustered. After a number of iterations, convergence is obtained (no new sentence segments are created) and the whole clustering procedure is terminated.\\n\\nAlthough the objective of a matching mechanism should be to identify in a database the longest piece of text that best matches the input, the rationale behind sentence segmentation is in this case self-evident. It is highly probable that a sentence is allocated to a cluster center because of a good match due to a part of it, while the remaining part has nothing to do with the cluster to which it will be allocated. Hence, this part will remain hidden to an input sentence applied to the system at the recognition phase. On the other hand, it is also highly probable that a given input sentence does not, as a whole, match a corpus sentence, but rather different parts of it match with segments belonging to different sentences in the corpus. Providing whole sentences as translation proposals, having a part that matched with part of the input sentence, would perhaps puzzle the translator instead of help him (her).\\n\\nBut sentence segmentation is not a straightforward matter. We can not just segment a sentence at the limits of the part that led to the allocation of the sentence to a specific cluster. This is because we need to know the translation of this part as well. Hence, we should expand the limits of the match to cover a ``translatable unit'' and then segment the sentence. Automatic sub-sentential alignment (which would produce the ``translatable units''), however, is not yet mature enough to produce high fidelity results. Hence, one resorts to the use of semi-automatic methods (in our application with the CELEX database, because of the certain format in which the texts appear, a rough segmentation of the sentences is straightforward and can therefore be automated).\\n\\nIf alignment at sub-sentential level is not available, the segmentation of the sentences of the corpus is not possible (it is absolutely pointless). Then, the degree of success of the Learning phase will depend on the length of the sentences contained in the corpus. The longer these sentences tend to be, the less successful the Learning phase. On the other hand, if alignment at sub-sentential level is available, we could just apply the clustering procedure to these segments. But then, we might end up with an unnecessary large number of clusters and ``sentences''. This is because, in a specific corpus quite a lot of these segments tend to appear together. Hence, by clustering whole sentences and then segmenting only in case of a good match with a part of  a sentence allocated to a different cluster, we can avoid the overgeneration of clusters and segments. When the iterative clustering procedure is finally terminated,  the sentences of the original corpus will have been segmented to ``translatable units'' in an optimum way, so that they are efficiently represented by a set of sentences which are the cluster centres.\\n\\nIn the Recognition phase, the vector of the input sentence is extracted and compared against the cluster centres. Once the favourite cluster(s) is specified, the search space is limited to the sentences allocated to that cluster only, and the same similarity metric is applied to produce the best match available in the corpus. If the sentences in the translation archive have been segmented, the problem is that, now, we do not know what the ``translatable units'' of the input sentence are (since we do not know its target language equivalent). We only have potential ``translatable unit'' markers. This is not really a restriction, however, since by setting a high enough threshold for the match with a segment (translatable piece of text) in the corpus, we can be sure that the part of the input sentence that contributed to this good match, will also be translatable and we can, therefore, segment this part. This process continues until the whole input sentence has been ``covered'' by segments of the corpus.\\n\\nAPPLICATION\\n\\n\\n\\nEVALUATION\\n\\nThe development of the matching method presented in this paper was part of the research work conducted under the LRE I project TRANSLEARN. The project will initially consider four languages: English, French, Greek and Portugese. The application on which we are developing and testing the method is implemented on the Greek-English language pair of records of the CELEX database, the computerised documentation system on Community Law, which is available in all Community languages. The matching mechanism is, so far, implemented on the Greek part, providing English translation proposals for Greek input sentences. The sentences contained in the CELEX database tend to be quite long, but due to the certain format in which they appear (corresponding to articles, regulations,...), we were able to provide the Learning phase with some potential segmentation points of these sentences in both languages of the pair (these segmentation points are in one-to-one correspondence across languages, yielding the ``sub-sentence'' alignment).\\n\\nIn tagging the Greek part of the CELEX database we came across 31 different ambiguity classes, which are utilised in the matching mechanism. The identification and grouping of the Greek fws was mainly done with the help of statistical tools applied to the CELEX database.\\n\\nWe tested the system on 8,000 sentences of the CELEX database. We are presenting results on two versions. One of 80 clusters (which accounts for the 1% of the number of the sentences of the corpus used) which resulted in 10,203 ``sentences'' (sentences or segments) in 2 iterations, and one of 160 clusters which resulted in 10,758 ``sentences'' in 2 iterations. To evaluate the system, we asked five translators to assign each translation proposal of the system (in our application these proposals sometimes refer to segments of the input sentence) to one of four categories :\\n\\nA : The proposal is the correct (or almost) translation B : The proposal is very helpful in order to produce the translation C : The proposal can help in order to produce the translation D : The proposal is of no use to the translator\\n\\nWe used as test suite 200 sentences of the CELEX database which were not included in the translation archive. The system proposed translations for 232 ``sentences'' (segments or whole input sentences) in the former case and for 244 in the latter case. The results are tabulated in table 1 (these results refer to the single best match located in the translation archive):\\n\\nTable 1\\n\\nThe table shows that in the case of 160 clusters, (1) at 62% the system will be very useful to the translator, and (2) some information can at least be obtained from 82% of the retrievals. In the case of 80 clusters the results do not change significantly. Hence, as far as the similarity metric is concerned the results seem quite promising (it should, however, be mentioned, that the CELEX database is quite suitable for EBMT applications, due to its great degree of repetitiveness).\\n\\nOn the other hand, the use of clustering of the corpus dramatically decreases the response time of the system, compared to the alternative of searching exhaustively through the corpus. Other methods for limiting the search space do exist (for example, using full-text retrieval based on content words), but are rather lossy, while clustering provides an effective means of locating the best available match in the corpus (in terms of the similarity metric employed). This can be seen in Table 2, where the column  ``MISSED'' indicates the percentage of the input ``sentences'' for which the best match in the corpus was not located in the favourite cluster, while the column ``MISSED BY'' indicates the average deviation of the located best matches from the actual best matches in the corpus for these cases.\\n\\nTable 2\\n\\nIn Table 1 as well as in Table 2 it can be seen that a quite important decrease in the number of clusters affected the results only slightly. This small deterioration in the performance of the system is due to ``hidden'' parts of sentences allocated to clusters (parts that are not represented by the cluster centres). Hence, the smaller the ``sentences'' contained in the database and the more the clusters, the better the performance of the proposed system. The number of clusters, however, should be constrained for the search space to be effectively limited.\\n\\nREFERENCES\\n\\n[BROWN 91] Brown P. F. et al, (1991). ``Aligning Sentences in Parallel Corpora''. Proc. of the 29th Annual Meeting of the ACL, pp 169-176.\\n\\n[BROWN 93] Brown P. F. et al, (June 1993). ``The mathematics of Statistical Machine Translation: Parameter Estimation''. Computational Linguistics, pp 263-311.\\n\\n[FURUSE 92] Furuse O. and H. Iida, (1992). ``Cooperation between Transfer and Analysis in Example-Based Framework''. Proc. Coling, pp 645-651.\\n\\n[GALE 91] Gale W. A. and K. W. Church, (1991). ``A Program for Aligning Sentences in Bilingual Corpora''. Proc. of the 29th Annual Meeting of the ACL., pp 177-184.\\n\\n[KAJI 92] Kaji H., Y. Kida and Y. Morimoto, (1992). ``Learning Translation Templates from Bilingual Text''. Proc. Coling., pp 672-678.\\n\\n[NAGAO 84] Nagao M., (1984). ``A framework of a mechanical translation between Japanese and English by analogy principle''. Artificial and Human Intelligence, ed. Elithorn A. and Banerji R., North-Holland, pp 173-180.\\n\\n[NEY 84] Ney H., (1984). ``The use of a One-stage Dynamic Programming Algorithm for Connected Word Recognition''. IEEE vol. ASSP-32, No 2.\\n\\n[NIRENBURG 93] Nirenburg S. et al, (1993). ``Two Approaches to Matching in Example-Based Machine Translation''. Proc. of TMI-93, Kyoto, Japan.\\n\\n[SADLER 90] Sadler V. and R. Vendelmans, (1990). ``Pilot Implementation of a Bilingual Knowledge Bank''. Proc. of Coling, pp 449-451.\\n\\n[SAKOE 78] Sakoe H. and S. Chiba, (1978). ``Dynamic Programming Algorithm Optimisation for Spoken Word Recognition''. IEEE Trans. on ASSP, vol. ASSP-26.\\n\\n[SATO 90] Sato S. and M. Nagao, (1990). ``Toward Memory-based Translation''. Proc. of Coling, pp 247-252.\\n\\n[SATO 92] Sato S., (1992). ``CTM: An Example-Based Translation Aid System''. Proc. of Coling, pp 1259-1263.\\n\\n[SUMITA 88] Sumita E. and Y. Tsutsumi, (1988). ``A Translation Aid System Using Flexible Text Retrieval Based on Syntax-Matching''. TRL Research Report, Tokyo Research Laboratory, IBM.\\n\\n[SUMITA 91] Sumita E. and H. Iida, (1991). ``Experiments and Prospects of Example-based Machine Translation''. Proc. of the 29th Annual Meeting of the Association for Computational Linguistics, pp 185-192.\\n\\n[WILPON 85] Wilpon J. and L. Rabiner, (1985). ``A Modified k-Means Clustering Algorithm for Use in Isolated Word Recognition''. IEEE vol. ASSP-33, pp. 587-594.\", metadata={'source': '../data/raw/cmplg-xml/9508005.xml'}),\n",
       " Document(page_content=\"The Use of Knowledge Preconditions in Language Processing1 Introduction SharedPlans\\n\\n. Recipes are modeled in Grosz and Kraus's definitions as sets of constituent acts and constraints. To perform an act\\n\\n, an agent must perform each constituent act in\\n\\n's recipe according to the constraints of that recipe. Actions themselves may be further decomposed into act-types and parameters. We will represent an action\\n\\nas a term of the form\\n\\nwhere\\n\\nrepresents the act-type of the action and the pi its parameters.\\n\\nKnowledge Preconditions\\n\\nOur axiomatization of knowledge preconditions is based on Morgenstern's observations, but adapted to the requirements of individual and shared mental-state plans. We use the predicates has.recipe and id.params to represent explicitly observations (1) and (3) above. The remaining observations are implicitly represented by the way in which these two knowledge precondition relations are defined. Observation (2) is modeled as the base case of has.recipe, and observation (5) is modeled by the use of has.recipe within the recursive plan definitions.\\n\\nObservation (4) requires that the knowledge precondition relations be intensional, rather than extensional; within their scope it should not be possible to freely substitute one representation of an action for another. We thus define has.recipe and id.params to hold of action descriptions, rather than actions. Action descriptions are intensional objects; one action description can be substituted for another only if the descriptions are the same. For example, although 555-1234 and phone-\\n\\nnumber(speech-lab) may be extensionally equivalent, the descriptions\\n\\n\\n\\nand\\n\\n\\n\\nnumber(speech-\\n\\nare not. By convention, we will omit the corner quote notation in what follows and simply take the appropriate arguments of the predicates to represent action descriptions rather than actions.\\n\\nDetermining Recipes: has.recipe\\n\\nFor an agent to be able to perform an act\\n\\n, it must know how to perform\\n\\n; i.e., it must have a recipe for the act. The relation\\n\\nis used to represent that agent G has a recipe\\n\\nfor an act\\n\\nat time T.  It is formalized as follows:\\n\\n(1)    [\\n\\n(2)\\n\\n(2a)\\n\\n(2a1)\\n\\n(2a2)\\n\\n(either a single agent (2a1) or a group of agents (2a2)) must believe that some set of acts,\\n\\n,\\n\\nand constraints,\\n\\n,\\n\\nconstitute a recipe for\\n\\n.\\n\\nIdentifying Parameters: id.params\\n\\nAn agent must also be able to identify the parameters of an act\\n\\nis used to represent that agent Gcan identify the parameters of act\\n\\nat time T.  If\\n\\nis of the form\\n\\n,\\n\\nthen\\n\\nis true if G can identify each of the pi. To do so, G must have a description of each pi that is suitable for\\n\\n. The relation id.params is defined as follows:\\n\\n. For example, in the case of sending a letter to John's residence, the constraint produced by the oracle function would be that John's residence be described by a postal address.\\n\\nFor an agent to suitably identify a parameter described as P, the agent must have a description,\\n\\n, of the parameter such that\\n\\nis of the appropriate sort. For example, for an agent to visit John's residence, it is not sufficient for the agent to believe that the description ``John's residence'' refers to the place where John lives. Rather, the agent needs another description of John's residence, one such as ``the only yellow house on Cherry Street,'' that is appropriate for the purpose of visiting him. To model an agent's ability to identify a parameter (described as P) for some purpose, we thus require that the agent have an individuating set for the parameter that contains a description\\n\\nsuch that\\n\\nsatisfies the identification constraint that derives from the purpose. The definition of has.sat.descr is thus as follows:\\n\\n[\\n\\nThe predicate\\n\\nis true if the constraint C applies to the parameter description\\n\\n.\\n\\nThe\\n\\noracle function\\n\\nin id.params produces the appropriate identification constraint on pi given\\n\\n.\\n\\nThe Role of Knowledge Preconditions in Language Processing\\n\\nSharedPlans as Intentional Structure\\n\\nThe utterances of a discourse are understood in terms of their contribution to the SharedPlans associated with the segments of the discourse. Those segments that have been completed at the time of processing an utterance have a full SharedPlan (FSP) associated with them (e.g., segment (2) in Figure 3), while those that have not have a partial SharedPlan (PSP) (e.g., segments (1) and (3) in Figure 3).\\n\\nFigure 3: Modeling Intentional Structure\\n\\nDSP1=\\n\\nwhere ac1 represents the air compressor the agents        are working on.\\n\\nAccounting for the Initiation of New Discourse Segments\\n\\nincludes full plans for each subact in\\n\\n's recipe as components (requirements (2c) and (3b)). The plans for the subacts thus contribute to the plan for\\n\\nand are therefore subsidiary to it.\\n\\n, an agent must (1) have a recipe for\\n\\n(\\n\\nhas.recipe), (2) be able to identify the parameters of\\n\\n(\\n\\nhas.sat.descr), and (3) have satisfied the constraints associated with performing\\n\\nThe purpose of this subdialogue is represented as      DSP2=\\n\\nAchieve(has.recipe(a,\\n\\nand can be glossed as ``the Apprentice intends that the agents collaborate on his obtaining a recipe for the act of removing the flywheel of the air compressor.'' To account for the Apprentice's initiation of this subdialogue, the Expert must determine the relationship of DSP2 to the purpose of the agents' preceding discourse, namely DSP1. In this case, the Expert can reason that the Apprentice wants to engage in the subdialogue to obtain a recipe for the act of removing the flywheel so that he will be able to perform that act as part of the agents' SharedPlan to remove the pump. The plan in DSP2 is thus subsidiary to that in DSP1 by virtue of a knowledge precondition requirement of the latter plan.\\n\\nDiscussion INTRODUCE-PLAN: introduce a new plan for discussion\\n\\nCONTINUE-PLAN: execute the next action in a plan\\n\\nTRACK-PLAN: talk about the execution of an action\\n\\nMODIFY-PLAN: introduce a new plan by modifying a previous one\\n\\nCORRECT\\n\\n\\n\\nPLAN: correct a plan\\n\\nIDENTIFY-PARAMETER: identify a parameter of an action in a plan\\n\\nConclusion\\n\\nBibliography\\n\\nD. Appelt and A. Kronfeld. A computational model of referring. In Proceedings of IJCAI-87, pages 640-647, Milan, Italy, 1987.\\n\\nD. E. Appelt. Some pragmatic issues in the planning of definite and indefinite noun phrases. In Proceedings of the 23rd Annual Meeting of the ACL, pages 198-203, Chicago, IL, 1985.\\n\\nM. E. Bratman, D. J. Israel, and M. E. Pollack. Plans and resource-bounded practical reasoning. Computational Intelligence, 14:349-355, 1988.\\n\\nH. P. Grice.\\n\\nUtterer's meaning and intentions.\\n\\nPhilosophical Review, 68(2):147\\n\\n\\n\\n177, 1969.\\n\\nB. J. Grosz and S. Kraus. Collaborative plans for group activities. In Proceedings of IJCAI-93, pages 367-373, Chambery, Savoie, France, 1993.\\n\\nB. J. Grosz and C. L. Sidner. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175-204, 1986.\\n\\nB. J. Grosz and C. L. Sidner. Plans for discourse. In P. R. Cohen, J. L. Morgan, and M. E. Pollack, editors,   Intentions in Communication, pages 417-444. MIT Press, Cambridge, MA, 1990.\\n\\nB. J. Grosz [Deutsch]. The structure of task-oriented dialogs. In IEEE Symposium on Speech Recognition: Contributed Papers, pages 250-253, Pittsburgh, PA, 1974.\\n\\nJ. R. Hobbs. Ontological promiscuity. In Proceedings of the 23rd Annual Meeting of the ACL, pages 61-69, Chicago, IL, 1985.\\n\\nA. Kronfeld. Donnellan's distinction and a computational model of reference. In Proceedings of the 24th Annual Meeting of the ACL, pages 186-191, New York, NY, 1986.\\n\\nL. Lambert and S. Carberry. A tripartite plan-based model of dialogue. In Proceedings of the 29th Annual Meeting of the ACL, pages 47-54, Berkeley, CA, 1991.\\n\\nD. J. Litman and J. F. Allen. A plan recognition model for subdialogues in conversations. Cognitive Science, 11:163-200, 1987.\\n\\nK. E. Lochbaum, B. J. Grosz, and C. L. Sidner. Models of plans to support communication: An initial report. In Proceedings of AAAI-90, pages 485-490, Boston, MA, 1990.\\n\\nK. E. Lochbaum. Using Collaborative Plans to Model the Intentional Structure of Discourse. PhD thesis, Harvard University, 1994.\\n\\nJ. D. Moore and C. L. Paris. Planning text for advisory dialogues: Capturing intentional and rhetorical information. Computational Linguistics, 19(4):651-694, December 1993.\\n\\nR. C. Moore. A formal theory of knowledge and action. In J. R. Hobbs and R. C. Moore, editors, Formal Theories of the Commonsense World, pages 319-358. Ablex Publishing Corp., Norwood, NJ, 1985.\\n\\nL. Morgenstern. Knowledge preconditions for actions and plans. In Proceedings of IJCAI-87, pages 867-874, Milan, Italy, 1987.\\n\\nL. Morgenstern. Foundations of a Logic of Knowledge, Action, and Communication. PhD thesis, New York University, 1988.\\n\\nM. E. Pollack. Plans as complex mental attitudes. In P. R. Cohen, J. L. Morgan, and M. E. Pollack, editors,   Intentions in Communication, pages 78-104. MIT Press, Cambridge, MA, 1990.\\n\\nL. A. Ramshaw. A three-level model for plan exploration. In Proceedings of the 29th Annual Meeting of the ACL, pages 39-46, Berkeley, CA, 1991.\\n\\nC. L. Sidner and D. J. Israel. Recognizing intended meaning and speakers' plans. In Proceedings of IJCAI-81, pages 203-208, Vancouver, British Columbia, Canada, 1981.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9508011.xml'}),\n",
       " Document(page_content=\"Parsing with Principles and Probabilities1 cmp-lg/9408004\\n\\nThis paper is an attempt to bring together two approaches to language analysis. The possible use of probabilistic information in principle-based grammars and parsers is considered, including discussion on some theoretical and computational problems that arise. Finally a partial implementation of these ideas is presented, along with some preliminary results from testing on a small set of sentences.\\n\\nIntroduction\\n\\nBoth principle-based parsing and probabilistic methods for the analysis of natural language have become popular in the last decade. While the former borrows from advanced linguistic specifications of syntax, the latter has been more concerned with extracting distributional regularities from language to aid the implementation of NLP systems and the analysis of corpora.\\n\\nThese symbolic and statistical approaches are beginning to draw together as it becomes clear that one cannot exist entirely without the other: the knowledge of language posited over the years by theoretical linguists has been useful in constraining and guiding statistical approaches, and the corpora now available to linguists have resurrected the desire to account for real language data in a more principled way than had previously been attempted.\\n\\nThis paper falls directly between these approaches, using statistical information derived from corpora analysis to weight syntactic analyses produced by a `principles and parameters' parser. The use of probabilistic information in principle-based grammars and parsers is considered, including discussion on some theoretical and computational problems that arise. Finally a partial implementation of these ideas is presented, along with some preliminary results from testing on a small set of sentences.\\n\\nGovernment\\n\\n\\n\\nBinding Theory\\n\\nA proper branch is a set of three nodes -- a mother and two daughters -- which are constructed by the parser, using a simple mechanism such as a shift-reduce interpreter, and then `licensed' by the principles of grammar. A complete phrase marker of the input string can then be constructed by following the manner in which the mother node from one proper branch is used as a daughter node in a dominating proper branch.\\n\\nBy using the proper branch method of axiomatising the grammar, the structure building section of the parser is only constrained in that it must produce proper branches; it is therefore possible to experiment with different interpreters (i.e. structure proposing engines) while keeping the grammar constant.\\n\\nThe Grammar and Parser\\n\\nExplanations of the knowledge contained within each grammar principle is given in the following sections.\\n\\nX Theory\\n\\nThe probabilities for occurrences of the X-bar schema were obtained from sentences from the preliminary Penn Treebank corpus of the Wall Street Journal, chosen because of their length and the head of their verb phrase (i.e. the main verbs were all from the set for which theta role data was obtained); the examples were manually parsed by the authors.\\n\\nThe probabilities were calculated using the following equation, where\\n\\nis a specific schema,\\n\\nis the set of X-bar schemata and A and B and C are variables over category,   SPEC and COMP feature bundles:\\n\\nThis is different to manner in which probabilities are collected for stochastic context-free grammars, where the identity of the mother node is taken into account, as in the equation below:\\n\\nThis would result in misleading probabilities for the X-bar schemata since the use of schemata (3), (4), and (5) would immediately bring down the probability of a parse compared to a parse of the same string which happened to use only (1) and (2).\\n\\nThe overall (X-bar) likelihood of a parse can then be computed by multiplying together all the probabilities obtained from each application of the schemata, in a manner analogous to that used to obtain the probability of a phrase marker generated by an SCFG. Using the schemata in this way suggests that the building of structure is category independent, i.e. it is just as likely that a verb will have a (filled) specifier position as it is for a noun. The work on stochastic context-free grammars suggests a different set of results, in that the specific categories involved in expansions are all important. While SCFGs will tend to deny that all categories expand in certain ways with the same probabilities, they make this claim while using a homogeneous grammar formalism. When a more modular theory is employed, the source of the supposedly category specific information is not as obvious. The use of lexical probabilities on specifier and complement co-occurrence with specific heads (i.e. lexical items) could exihibit properties that appear to be category specific, but are in fact caused by common properties which are shared by lexical items of the same category. Since it can be argued that the probabilistic information on lexical items  will be needed independently, there is no need to use category specific information in assigning probabilities to syntactic configurations.\\n\\nTheta Theory\\n\\nTheta theory is concerned with the assignment of an argument structure to a sentence. A verb has a number of the thematic (or `theta') roles which must be assigned to its arguments, e.g. a transitive verb has one theta role to `discharge' which must be assigned to an NP.\\n\\nIf a binary branching formalism is employed, or indeed any formalism where the arguments of an item and the item itself are not necessarily all sisters, the problem of when to access the probability of a theta application is presented. The easiest method of obtaining and applying theta probabilities will be with reference to whole theta grids. Each theta grid for a word will be assigned a probability which is not dependent on any particular items in the grid, but rather on the occurrence of the theta grid as a whole.\\n\\nThe probabilities for each of the verbs' theta grids were calculated using the equation below, where P(si|v) is the probability of the theta grid si occurring with the verb v, (v, si) is an occurrence of the items in si being licensed by v, and S ranges over all theta grids for v:\\n\\nCase Theory\\n\\nIn its simplest form, Case theory invokes the Case filter to ensure that all noun phrases in a parse are assigned (abstract) case. Case theory differs from both X-bar and Theta theory in that it is category specific: only NPs require, or indeed can be assigned, abstract case. If we are to implement a probabilistic version of a modular grammar theory incorporating a Case component, a relevant question is: are there multiple ways of assigning Case to  noun phrases in a sentence? i.e. can ambiguity arise due to the presence of two candidate Case assigners?\\n\\nCase theory suggests that the answer to this is negative, since Case assignment is linked to theta theory via visibility, and it is not possible for an NP to receive more than one theta role. As a result, the use of Case probabilities in a parser would be at best unimportant, since some form of ambiguity is needed in the module, i.e. it is possible to satisfy the Case filter in more than one way, for probabilities associated with the module to be of any use. While having a provision for using probabilities deduced from Case information, the implemented parser does not in fact use Case in its parse ranking operations.\\n\\nLocal Calculation\\n\\nThe use of a heterogeneous grammar formalism and multiple probabilities invokes the problem of their combination. There are  at least two ways in which each mother's probabilities can be calculated; firstly, the probability information of the same type can be used: the daughters' X-bar probabilities alone could be used in calculating the mother's X-bar probability. Alternatively, a combination of some or all of the daughters' probability features could be employed, thus making, e.g., the X-bar probability of the mother dependent upon all the stochastic information from the daughters, including theta and Case probabilities, etc.\\n\\nThe need for a method of combining the daughter probabilities into a useful figure for the calculation of the mother probabilities is likely to involve trial and error, since theory thus far has had nothing to say on the subject. The former method, using only the relevant daughter probabilities, therefore seems to be the most fruitful path to follow at the outset, since it does not require a way of integrating probabilities from different modules while the parse is in progress, nor is it as computationally expensive.\\n\\nGlobal Calculation\\n\\nThe manner in which the global probability is calculated will be partly dependent upon the information contained in the local probability calculations.\\n\\nIf the probabilities for partial analyses have been calculated using only probabilities of the same types from the subanalyses -- e.g. X-bar, Theta -- the probabilities at the top level will have been calculated using informationally distinct figures. This has the advantage of making `pure' probabilities available, in that the X-bar probability will reflect the likelihood of the structure alone, and will be `uncontaminated' by any other information. It should then be possible to experiment with different methods of combining these probabilities, other than the obvious `multiplying them together' techniques, which could result in one type of probabililty emerging as the most important.\\n\\nOn the other hand, if probabilities calculated during the parse take all the different types of probabilities into account at each calculation -- i.e. the X-bar, theta, etc. probabilities on daughters are all taken into account when calculating the mother's X-bar probability -- the probabilities at the top level will not be pure, and a lot of the information contained in them will be redundant since they will share a large subset of the probabilities used in their separate calculations. It will not therefore be easy to gain theoretical insight using these statistics, and their most profitable method of combination is likely to be more haphazard affair than when more pure probabilities are used.\\n\\nThe parser used in testing employed the first method and therefore produced separate module probabilities for each node. For the lack of a better, theoretically motivated method for combining these figures, the product of the probabilities was taken as the global probability for each parse.\\n\\nTesting the Parser\\n\\nThe parser was tested using sixteen sentences containing verbs for which data had been collected from the Penn Treebank corpus. The sentences were created by the authors to exhibit at least a degree of ambiguity when it came to attaching a post-verbal phrase as an adjunct or a complement. In order to force the choice of the `best' parse on to the verb, the probabilities of theta grids for nouns, prepositions, etc. was kept constant.\\n\\nOf these 16 highest ranked parses, 7 are the expected parse, with the other 9 exhibiting some form of mis-attachment. The fact that each string received multiple parses (the mean number of analyses being 9.135, and the median, 6) suggests that the probabilistic information did favourably guide the selection of a single analysis.\\n\\nIt is not really possible to say from these results how successful the whole approach of probabilistic principle-based parsing would be if it were fully implemented. The inconclusive nature of the results obtained was due to a number of limiting factors of the implementation including the simplicity of the grammar and the lack of available data.\\n\\nDiscussion\\n\\nLimitations of the Grammar\\n\\nThis approach could be viewed as putting the cart before the horse; the usefulness of stochastic information in parsers presumes that a certain level of accuracy can be achieved by the grammar alone. While GB is an elegant theory of cognitive syntax, it has yet to be shown that such a modular characteristion can be successfully employed in corpus analysis.\\n\\nStatistical Data and their Source\\n\\nThe use of the preliminary Penn Treebank corpus for the extraction of probabilities used in the implementation above was a choice forced by lack of suitable materials. There are still very few parsed corpora available, and none that contain information which is specified to the level required by, e.g., a GB grammar. While this is not an absolute limitation, in that it is theoretically possible to extract this information manually or semi-automatically from a corpus, time constraints entailed the rejection of this approach.\\n\\nIt would be ultimately desirable if the use of probabilities in principle-based parsing could be used to mirror the way that a syntactic theory such as Government-Binding handles constructions -- various modules of the grammar conspire to rule out illegal structures or derivations. It would be an elegant result if a construction such as the passive were to use probabilities for chains, Case assignment etc. to select a parse that reflected the lexical changes that had been undergone, e.g. the greater likelihood of an NP featuring in the verb's theta grid. It is this property of a number of modules working hand in hand that needs to be carried over into the probabilistic domain.\\n\\nThe objections that linguists once held against statistical methods are disappearing slowly, partly due to results in corpora analysis that show the inadequacy of linguistic theory when applied to naturally occurring data. It is also the case that the rise of the connectionist phoenix has brought the idea of weighted (though not strictly probabilistic) functions of cognition back to the fore, freeing the hands of linguists who believe that while an explanatorily adequate theory of grammar is an elegant construct, its human implementation, and its usage in computational linguists may not be straight forward. This paper has hopefully shown that an integration of statistical methods and current linguistic theory is a goal worth pursuing.\\n\\nBibliography\\n\\nR. C. Berwick and A. S. Weinberg. The Grammatical Basis of Linguistic Performance: Language Use and Acquistion. MIT Press, Cambridge, MA., 1984.\\n\\nJ. W. Bresnan. A realistic transformational grammar. In M. Halle, J. Bresnan, and G. Miller, editors, Linguistic Theory and Psychological Reality. MIT Press, Cambridge, MA., 1978.\\n\\nJ. G. Carbonell and P. J. Hayes. Recovery strategies for parsing extragrammatical language. American Journal of Computational Linguistics, 9(3-4):123-146, July-December 1983.\\n\\nN. Chomsky. Lectures on Government and Binding. Studies in Generative Grammar No. 9. Foris, Dordrecht, 1981.\\n\\nN. Chomsky. Knowledge of Language: Its Nature, Origin, and Use. Convergence. Praeger, New York, 1986.\\n\\nM. W. Crocker. A Logical Model of Competence and Performance in the Human Sentence Processor. PhD thesis, Dept. of Artificial Intelligence, University of Edinburgh, 1992.\\n\\nM. W. Crocker and I. Lewin. Parsing as deduction: Rules versus principles. In B. Neumann, editor, ECAI-92, Proceedings of the Tenth European Conference on Artificial Intelligence, pages 508-512, Vienna, Austria, 1992.\\n\\nS. Douglas and R. Dale. Towards robust PATR. In C. Boitet, editor, COLING-92, Proceedings of the Fourteenth International Conference on Computational Linguistics, pages 468-474, Nantes, France, 1992.\\n\\nD. Hindle. A parser for text corpora. In B. T. S. Atkins and A. Zampolli, editors, Computational Approaches to the Lexicon. 1993.\\n\\nD. Hindle and M. Rooth. Structural ambiguity and lexical relations. Computational Linguistics, 19(1), 1993.\\n\\nK. Jensen, G. E. Heidborn, L. A. Miller, and Y. Ravin. Parse fitting and prose fixing: Getting a hold on ill-formedness. American Journal of Computational Linguistics, 9(3-4):147-160, July-December 1983.\\n\\nC. S. Mellish. Some chart-based techniques for parsing ill-formed input. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pages 102-109, Vancouver, B.C., 1989.\\n\\nP. Muysken. Parameterizing the notion of head. Journal of Linguistic Research, 2:57-76, 1983.\\n\\nA. S. Weinberg. Mathematical properties of grammars. In F. J. Newmeyer, editor, Linguistics: the Cambridge Survey, Vol. 1, Linguistics Theory: Foundations, chapter 15, pages 415-429. Cambridge University Press, Cambridge, 1988.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9408004.xml'}),\n",
       " Document(page_content=\"On Using Selectional Restriction in Language Models for Speech Recognition\\n\\nIn this paper, we investigate the use of selectional restriction - the constraints a predicate imposes on its arguments - in a language model for speech recognition. We use an un-tagged corpus, followed by a public domain tagger and a very simple finite state machine to obtain verb-object pairs from unrestricted English text. We then measure the impact the knowledge of the verb has on the prediction of the direct object in terms of the perplexity of a cluster-based language model. The results show that even though a clustered bigram is more useful than a verb-object model, the combination of the two leads to an improvement over the clustered bigram model.\\n\\nIntroduction\\n\\nTraining and Testing Data\\n\\nV : the word is a verb (iff its tag starts with ``v'', ``b'' or ``h'')\\n\\nPP: the word is a preposition (iff its tag is ``in'')\\n\\nNC: the word starts a new clause (iff its tag is ``.'',``:'',``;'',``!'',``? '',``cs'' or begins with ``w'')\\n\\na direct object if we are currently in state 2; the corresponding verb is considered to be the one that caused the last transition into state 2;\\n\\npart of  a prepositional phrase if we are currently in state 3; the corresponding preposition is considered to be the one that caused the last transition into state 3;\\n\\nunconstrained by a predicate or preposition (for example a subject) if we are in state 1.\\n\\nThe Language Model\\n\\nMaximum\\n\\n\\n\\nLikelihood Criterion\\n\\nIn order to automatically find classification functions G1 and G2, which - as a shorthand - we will also denote as G, we first convert the classification problem into an optimisation problem. Suppose the function F(G) indicates how good the classification G (composed of G1 and G2) is. We can then reformulate the classification problem as finding the classification G that maximises F:\\n\\nIn the following, we will derive an optimisation function FML in terms of frequency counts observed in the training data. The likelihood of the training data FML is simply\\n\\nAssuming that the classification is unique, e.g. that G1 and G2 are functions, we have\\n\\nN(G2[i], Y[i])=N(Y[i])(because Y[i] always occurs with the same class G2[i]). Since we try to optimise FML with respect to G, we can remove any term that does not depend on G, because it will not influence our optimisation. It is thus equivalent to optimise\\n\\nIf, for two pairs\\n\\n(X[i], Y[i]) and\\n\\n(X[j], Y[j]), we have\\n\\nG1(X[i])=G1(X[j]) and\\n\\nG2(Y[i])=G2(Y[j]), then we know that\\n\\nf(X[i], Y[i])=f(X[j], Y[j]). We can thus regroup identical terms to obtain\\n\\nwhere the product is over all possible pairs\\n\\n(gx, gy). Because N(gx) does not depend on gy and N(gy) does not depend on gx, we can simplify this again to\\n\\nTaking the logarithm, we obtain the equivalent optimisation criterion\\n\\nLeaving\\n\\n\\n\\nOne\\n\\n\\n\\nOut Criterion\\n\\nLet Ti denote the data without the pair\\n\\n(X[i], Y[i]) and\\n\\npG,Ti(yl|xk) the probability estimates based on a given classification G and training corpus Ti. Given a particular Ti, the probability of the ``held-out'' part\\n\\n(X[i], Y[i]) is\\n\\npG,Ti(yl|xk). The probability of the complete corpus, where each pair is in turn considered the ``held-out'' part is the leaving-one-out likelihood LLO\\n\\nAs we saw before,\\n\\npG,Ti(gy, yl)=pG,Ti(yl) (if the classification G2is a function) and since\\n\\npTi(yl) is actually independent of G, we can drop it out of the maximization and thus need not specify an estimate for it.\\n\\nAs we will see later, we can guarantee that every class gx and gy has been seen at least once in the ``retained'' part and we can thus use relative counts as estimates for class uni-grams: p_{G,T_{i}}(g_{x})  =  \\\\frac{N_{T_{i}}(g_{x})}{N_{T_{i}}}\\\\\\\\ p_{G,T_{i}}(g_{y})  =  \\\\frac{N_{T_{i}}(g_{y})}{N_{T_{i}}}. \\\\end{eqnarray} -->\\n\\nClustering Algorithm\\n\\nWe will now determine the complexity of the algorithm. Let MX and MYbe the maximal number of clusters for X and Y, let |X| and |Y| be the number of possible values for X and Y, let\\n\\nM=max(MX, MY),\\n\\nW=max(|X|, |Y|) and let I be the number of iterations. When we move x from gx to g'x in the inner loop (the situation is symmetrical for y), we need to change the counts\\n\\nN(gx, gy) and\\n\\nN(g'x, gy) for all gy. The amount by which we need to change the counts is equal to the number of times X occurred with cluster gy. Since this amount is independent of g'x, we need to calculate it only once for each x. The amount can then be looked up in constant time within the loop, thus making the inner loop of order M. The inner loop is executed once for every cluster x can be moved to, thus giving a complexity of the order of M[2]. For each x, we needed to calculate the number of times x occurred with all clusters gy. For that  we have to sum up all the bigram counts\\n\\nN(x,y):G2(y)=gy, which is on the order of W, thus giving a complexity of the order of  W+M[2]. The two outer loops are executed I and W times thus giving a total complexity of the order of\\n\\nI\\n\\n\\n\\nW\\n\\n\\n\\n(W+M[2]).\\n\\nResults\\n\\nConclusions\\n\\nFrom a purely linguistic perspective, it would be slightly surprising to find out that the word immediately preceding a direct object can be used better to predict it than the preceding verb. However, this conclusion can not  be drawn from our results because of the noisy nature of the data. In other words, the data contains pairs like (is, chairman), which would usually not be considered as a verb-direct object pair. It is possible, that more accurate data (e.g. fewer, but only correct pairs) would lead to a different result. But the problem with fewer pairs would of course be that the model can be used in fewer cases, thus reducing the usefulness to a language model that would predict the entire text (rather than just the direct objects). The results thus support the common language modeling practice, in that bi-gram events (by themselves) seem to be more useful than this linguistically derived predictor (by itself). Nevertheless, the interpolation results also show that this linguistically derived predictor is useful as a complement to a standard class based bigram model. In the future, we hope to consolidate these early findings by more experiments involving a higher number of clusters and a larger data set.\\n\\nBibliography\\n\\nDoug Cutting, Julian Kupiec, Jan Perdersen, and Penelope Sibun. A practical part-of-speech tagger. In Conference on Applied Natural Language Processing, pages 133-140. Trento, Italy, 1992.\\n\\nR. O. Duda and P.E. Hart. Pattern Classification and Scene Analysis. Wiley, New York, 1973.\\n\\nFred Jelinek. Self-organized language modeling for speech recognition. In Alex Waibel and Kai-Fu Lee, editors, Readings in Speech Recognition, pages 450-506. Morgan Kaufmann, San Mateo, CA, 1990.\\n\\nReinhard Kneser and Hermann Ney. Improved clustering techniques for class-based statistical language modelling. In European Conference on Speech Communication and Technology, pages 973-976. Berlin, Germany, September 1993.\\n\\nHermann Ney and Ute Essen. Estimating `small' probabilities by leaving-one-out. In European Conference on Speech Communication and Technology, pages 2239-2242. Berlin, Germany, 1993.\\n\\nPhilip Stuart Resnik. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. Thesis, Computer and Information Science, University of Pennsylvania, PA, 1993.\\n\\nJoerg Peter Ueberla. Analyzing and Improving Statistical Language Models for Speech Recognition. PhD thesis, School of Computing Science, Simon Fraser University, Burnaby, B.C., V5A 1S6, Canada, May 1994.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9408010.xml'}),\n",
       " Document(page_content=\"Multi\\n\\n\\n\\nDimensional Inheritance\\n\\nIn this paper, we present an alternative approach to multiple inheritance for typed feature structures. In our approach, a feature structure can be associated with several types coming from different hierarchies (dimensions). In case of multiple inheritance, a type has supertypes from different hierarchies. We contrast this approach with approaches based on a single type hierarchy where a feature structure has only one unique most general type, and multiple inheritance involves computation of greatest lower bounds in the hierarchy. The proposed approach supports current linguistic analyses in constraint-based formalisms like HPSG, inheritance in the lexicon, and knowledge representation for NLP systems. Finally, we show that multi-dimensional inheritance hierarchies can be compiled into a Prolog term representation, which allows to compute the conjunction of two types efficiently by Prolog term unification.\\n\\nIntroduction\\n\\nIn the example, relative clauses are cross-classified according to two ``dimensions''; on the one hand according to the Phrase-Type (Headed or Non-Headed), and on the other hand according to the Clause-Type (Interrogative, Declarative, or Relative). The choices within one dimension are mutually exclusive: no structure can be described as both Headed and Non-Headed, or as more than one of {Int, Decl, Rel}. However, a structure can be assigned types from different dimensions, without the need for a subtype that inherits from both dimensions.\\n\\nMulti\\n\\n\\n\\nDimensional Inheritance\\n\\nX  >  [Y_{1.1},...,Y_{1.n}]. \\\\\\\\ \\\\vdots  \\\\vdots    \\\\vdots       \\\\nonumber \\\\\\\\ X  >  [Y_{m.1},\\\\ldots,Y_{m.k}]. \\\\nonumber \\\\end{eqnarray} -->\\n\\nMultiple inheritance is the case where some Y type occurs in the right-hand side of more than one type declaration. In this case, the type has several supertypes, which must be chosen from different dimensions in order to be consistent with each other.\\n\\n\\\\forall Y_i ([ \\\\hspace{-.5mm} [Y_i ] \\\\hspace{-.5mm} ] \\\\subseteq [ \\\\hspace{-.5mm} [X ] \\\\hspace{-.5mm} ])\\\\\\\\ \\\\forall Y_i \\\\forall Y_j (Y_i \\\\neq Y_j \\\\Rightarrow [ \\\\hspace{-.5mm} [Y_i ] \\\\hspace{-.5mm} ] \\\\cap [ \\\\hspace{-.5mm} [Y_j ] \\\\hspace{-.5mm} ] = \\\\emptyset) \\\\end{eqnarray} -->\\n\\nOur notion of feature typing and appropriateness is based on Carpenter's feature logic. Every feature is introduced for a unique most general type, and is appropriate for all subtypes of that type. In case of multiple inheritance, a type can inherit different features from its supertypes in different dimensions. A difference arises with type restrictions for feature values. In Carpenter's system, the value of a feature has one type as the type restriction, whereas in our system, the type restriction can be a conjunction of types from different dimensions.\\n\\nCompilation into a Prolog Term Representation\\n\\nWe start out by describing how the translation of type hierarchies into Prolog terms works, and then give an example. The translation to terms must be able to handle different dimensions of typing, mutually exclusive choices in a dimension, subtyping, multiple inheritance, features, and equality.\\n\\nDifferent dimensions: Each dimension occupies a different argument position in the resulting term representation, so that information from different dimensions can be combined by unification.\\n\\nMutually exclusive types: Mutually exclusive types in the same dimension have different functors at the same argument position, so that their unification fails.\\n\\nSubtype: The term which corresponds to the subtype is a further instantiation of the term corresponding to its supertype.\\n\\nMultiple inheritance: The term which corresponds to the subtype is a further instantiation of the unification of the terms which correspond to its supertypes.\\n\\nFeature: The term representation has an argument position for each feature introduced for a type. If a feature is introduced for a subtype, then an argument position is provided in that argument which further instantiates the supertype. Equality: In order to be able to distinguish structures that are identical from those which just happen to have the same value (i.e. their term representation is instantiated to the same ground term), an extra variable is introduced in the term representation (preventing instantiation to a ground term), which is only equal for two structures if they have been made identical by unification.\\n\\nIf the type Su-Wh-Rel has any subtypes, a choice must be made which of the two occurences of su_wh_rel in the Prolog terms should get argument positions for carrying this information. We always choose the leftmost occurence in a term for representing subtypes (and features). Further occurences then only serve to make a choice in a particular dimension of the hierarchy, and for that purpose, an atom which is distinct from other terms that can occur as alternatives in the same dimension is sufficient.\\n\\nApplication to Systemic Classification\\n\\nIn contrast, in our type system, the Prolog term translation has at most 8 nodes in the worst case, as in the following term which encodes the subjective case masculine (third person singular) personal pronoun.\\n\\nConclusion\\n\\nWe have presented a concept of inheritance which provides direct support for current linguistic descriptions making use of ``cross-classification'', and can be compiled into an efficient Prolog term representation.\\n\\nGiven the need for multiple dimensions in lingistic descriptions, we believe that multi-dimensional type hierarchies will remain important even when their compilation into Prolog terms is not needed any longer because unification of typed feature terms will be built-in in future logic programming languages.\\n\\nFor the time being, however, the combination of multi-dimensional inheritance and compilation into Prolog terms appears to give both the efficiency and the expressive power needed to develop larger-scale grammars and lexicons, and use existing Prolog-based technology (DCG parsers, left-corner, head-corner, or chart parsers, semantic-head driven or tabular generators) to build NLP systems. Such an approach can benefit from all the advantages of modern Prolog compilers (indexing, coroutining facilities, module systems etc.) that would need considerable effort to duplicate in a dedicated grammar formalism.\\n\\nBibliography\\n\\nHassan At-Kaci and Patrick Lincoln. LIFE, a natural language for natural language. T. A. Informations, 30(1-2):37 - 67, 1989.\\n\\nHassan At-Kaci and Roger Nasr. LOGIN: A logical programming language with built-in inheritance. In Proceedings of the 13th ACM Symposium an Principles of Programming Languages, pages 219-228, St. Petersburg, Florida, 1986.\\n\\nHassan At-Kaci and Andreas Podelski. Towards a meaning of  LIFE. Technical Report 11, DEC Paris Research Laboratory, Paris, June 1991.\\n\\nH. Alshawi, D. J. Arnold, R. Backofen, D. M. Carter, J. Lindop, K. Netter, J. Tsujii, and H. Uszkoreit. Eurotra 6/1: Rule formalism and virtual machine design study. final report. Technical report, SRI International, Cambridge, 1991.\\n\\nHiyan Alshawi, editor.\\n\\nThe Core Language Engine.\\n\\nMIT Press, 1991.\\n\\nBIM-SEMA. ALEP System Documentation: The ALEP Linguistic Subsystem, Version 1.0. Commission of the European Communities, March 1993.\\n\\nChris Brew. Systemic classification and its efficiency. Computational Linguistics, 17(4):375 - 408, 1991.\\n\\nBob Carpenter. The logic of typed feature structures. Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, Cambridge, 1992.\\n\\nMichael Covington. GULP 2.0: an extension of Prolog for unification-based grammar. Technical Report AI-1989-01, Advanced Computational Methods Center, University of Georgia, 1989.\\n\\nJochen Drre and Michael Dorna. CUF - A formalism for linguistic knowledge representation. In Jochen Drre, editor, Computational Aspects of Constraint-Based Linguistic Description. Deliverable R1.2.A. DYANA-2 - ESPRIT Basic Research Project 6852, 1993.\\n\\nGregor Erbach. ProFIT User's Manual - Version 1.05. Universitt des Saarlandes, Saarbrcken, July 1994.\\n\\nDan Flickinger and John Nerbonne. Inheritance and complementation: a case study of easy adjectives and related nouns. Computational Linguistics, 18(3):269 - 309, 1992.\\n\\nSusan Beth Hirsh. P-PATR: A compiler for unification-based grammars. Master's thesis, Stanford University, Stanford, CA, December 1986.\\n\\nHans-Ulrich Krieger and John Nerbonne. Feature-based inheritance networks for computational lexicons. In Ted Briscoe, Anne Copestake, and Valeria de Paiva, editors,   Default Inheritance within Unification-Based Approaches to the Lexicon. Cambridge University Press, 1992.\\n\\nChristopher S. Mellish. Implementing systemic classification by unification. Computational Linguistics, 14(1):40-51, 1988.\\n\\nChristopher S. Mellish. Term-encodable description spaces. In D. R. Brough, editor, Logic Programming: New Frontiers, pages 189 - 207. Intellect, Oxford, 1992.\\n\\nM. Andrew Moshier. A rational reconstruction of the domain of feature structures. Universitt des Saarlandes, Saarbrcken, 1994.\\n\\nCarl J. Pollard and Ivan A. Sag. Information-Based Syntax and Semantics. Volume 1: Fundamentals. CSLI Lecture Notes. Center for the Study of Language and Information, Stanford, CA, 1987.\\n\\nSusanne Riehemann. Word formation in lexical type hierarchies: A case study of   bar-adjectives in german. Master's thesis, University of Tbingen, Department of Linguistics, 1992.\\n\\nIvan Sag. Relative clauses: A multiple inheritance analysis. Stanford, 1994.\\n\\nAndreas P. Schter. Compiling feature structures into terms: A case study in Prolog. Technical Report RP-55, University of Edinburgh, Centre for Cognitive Science, 1993.\\n\\nTerry Winograd. Language as a Cognitive Process, volume I: Syntax. Addison Wesley, Reading, MA, 1983.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9411025.xml'}),\n",
       " Document(page_content=\"Intensional Verbs Without Type\\n\\n\\n\\nRaising or Lexical Ambiguity\\n\\nTheoretical Background\\n\\nAs a preliminary to presenting our analysis of intensional verbs, we outline our approach to semantic interpretation in LFG.\\n\\nMore complicated lexical entries give not only meanings for f-structures, but also instructions for assembling f-structure meanings from the meanings of other f-structures. We distinguish a meaning language, in which we represent the meanings of f-structures, and a composition language or glue language, in which we describe how to assemble the meanings of f-structures from the meanings of their substructures. Each lexical entry will contain a composition language formula, its meaning constructor, specifying how a lexical entry contributes to the meaning of any structure in which it participates.\\n\\nA Simple Example\\n\\nThe elements of the f-structure provide a set of formulas in the composition logic that introduce semantic elements and describe how they can be combined. For example, lexical items for words that expect arguments, like verbs, typically contribute a formula for combining the meanings of their arguments into a result. Once all the formulas are assembled, deduction in the logic is used to infer the meaning of the entire structure. Throughout this process we maintain a clear distinction between meanings proper and assertions about meaning combinations.\\n\\nQuantification\\n\\nWe now turn to an overview of our analysis of quantification (Dalrymple, Lamping, Pereira, and Saraswat 1993). As a simple example, consider the sentence Every man left. For conciseness, we will not illustrate the combination of the meaning constructors for every and man; instead, we will work with the derived meaning constructor for the subject every man, showing how it combines with the meaning constructor for left to produce a meaning constructor giving the meaning of the whole sentence.\\n\\n--that is, a function from a property, the scope of quantification, to a proposition. At the semantic composition level, we can understand that type as follows. If by assuming that x is the entity referred to by the NP we can derive S x as the meaning of the scope of quantification, where S is a property (a function from entities to propositions), then we can derive Q S as the meaning of the whole clause containing the NP.\\n\\nThe resource sensitivity of linear logic ensures that the scope of quantification is constructed and used exactly once.\\n\\nIntensional Verbs\\n\\nWe follow Montague (Montague:PTQ) in requiring intensional verbs like seek to take an object of NP type. What is interesting is that this is the only step required in our setting to obtain the appropriate ambiguity predictions for intensional verbs. The de re/de dicto ambiguity of a sentence like Bill seeks a unicorn:\\n\\nis a natural consequence, in our setting, of seek taking an NP-type argument.\\n\\nWe assign the following lexical entry to the verb seek:\\n\\nThe lexical entry for seek can be paraphrased as follows:\\n\\nThese are the premises for the deduction of the meaning of the sentence Bill seeks a unicorn. From the premises Bill and seeks, we can conclude by modus ponens:\\n\\nDifferent derivations starting from the premises Bill-seeks and a-unicorn will yield the different readings of Bill seeks a unicorn that we seek.\\n\\nDe Dicto Reading\\n\\nThe formula a-unicorn is exactly what is required by the antecedent of Bill-seeks provided that the following substitutions are performed:\\n\\nWe can thus conclude the desired de dicto reading:\\n\\nTo show how the premises also support a de re reading, we take first a short detour through a simpler example.\\n\\nNonquantified Objects\\n\\nThe meaning constructor for seek also allows for nonquantified objects as arguments, without needing a special type-raising rule. Consider the f-structure for the sentence Bill seeks Al:\\n\\nThe lexical entry for Al is analogous to the one for Bill. We begin with the premises Bill-seeks and Al:\\n\\ncan then be used to satisfy the antecedent of Bill-seeks to yield the desired result:\\n\\nType Raising and Quantifying In De Re Reading\\n\\nand the variable substitutions\\n\\nwe can apply modus ponens to derive the de re reading of Bill seeks a unicorn:\\n\\nA Comparison with Categorial Approaches Conclusion\\n\\nWe have shown that our deductive framework allows us to predict the correct set of readings for intensional verbs with quantified and nonquantified direct objects if we make a single assumption: that intensional verbs require a quantified direct object. This assumption is, of course, the starting point of the standard Montagovian treatment of intensional verbs. But that treatment depends on the additional machinery of quantifying in to generate de re readings of quantified direct objects, and that of explicit type raising to accommodate unquantified direct objects. In our approach those problems are handled directly by the deductive apparatus without further stipulation.\\n\\nThese results, as well as our previous work on quantifier scope, suggest the advantages of a generalized form of compositionality in which the semantic contributions of phrases are represented by logical formulas rather than by functional abstractions as in traditional compositionality. The fixed application order and fixed type requirements of lambda terms are just too restrictive when it comes to encoding the freer order of information presentation in natural language. In this observation, our treatment is closely related to systems of syntactic and semantic type assignment based on the Lambek calculus and its variants. However, we differ from those categorial approaches in providing an explicit link between functional structures and semantic derivations that does not depend on linear order and constituency in syntax to keep track of predicate-argument relations. Thus we avoid the need to force both syntax and semantics into an uncomfortably tight categorial embrace.\\n\\nAcknowledgments\\n\\nWe thank David Israel, Michael Moortgat and Stanley Peters for discussions on the subject of this paper.\\n\\nBibliography\\n\\nBarwise, Jon, and Robin Cooper. 1981. Generalized Quantifiers and Natural Language. Linguistics and Philosophy 4:159-219.\\n\\nCarpenter, Bob. 1993. Quantification and Scoping: a Deductive Account. Submitted for publication.\\n\\nDalrymple, Mary, Angie Hinrichs, John Lamping, and Vijay Saraswat. 1993. The Resource Logic of Complex Predicate Interpretation. In Proceedings of the 1993 Republic of China Computational Linguistics Conference (ROCLING). Hsitou National Park, Taiwan, September. Computational Linguistics Society of R.O.C.\\n\\nDalrymple, Mary, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. 1994. A Deductive Account of Quantification in LFG. In Quantifiers, Deduction, and Context, ed. Makoto Kanazawa, Christopher J. Pin, and Henriette de Swart. Stanford, California: Center for the Study of Language and Information.\\n\\nDalrymple, Mary, John Lamping, and Vijay Saraswat. 1993. LFG Semantics Via Constraints. In Proceedings of the Sixth Meeting of the European ACL. University of Utrecht, April. European Chapter of the Association for Computational Linguistics.\\n\\nGirard, J.\\n\\n\\n\\nY.\\n\\n1987.\\n\\nLinear Logic.\\n\\nTheoretical Computer Science 45:1\\n\\n\\n\\n102.\\n\\nHalvorsen, Per\\n\\n\\n\\nKristian.\\n\\n1983.\\n\\nSemantics for Lexical\\n\\n\\n\\nFunctional Grammar.\\n\\nLinguistic Inquiry 14(4):567\\n\\n\\n\\n615.\\n\\nHalvorsen, Per-Kristian, and Ronald M. Kaplan. 1988. Projections and Semantic Description in Lexical-Functional Grammar. In Proceedings of the International Conference on Fifth Generation Computer Systems, 1116-1122. Tokyo, Japan. Institute for New Generation Systems.\\n\\nKaplan, Ronald M. 1987. Three Seductions of Computational Psycholinguistics. In Linguistic Theory and Computer Applications, ed. Peter Whitelock, Harold Somers, Paul Bennett, Rod Johnson, and Mary McGee Wood. 149-188. London: Academic Press.\\n\\nKaplan, Ronald M., and Joan Bresnan. 1982. Lexical-Functional Grammar: A Formal System for Grammatical Representation. In The Mental Representation of Grammatical Relations, ed. Joan Bresnan. 173-281. Cambridge, MA: The MIT Press.\\n\\nLambek, Joachim. 1958. The Mathematics of Sentence Structure. American Mathematical Monthly 65:154-170.\\n\\nMontague, Richard. 1974. The Proper Treatment of Quantification in Ordinary English. In Formal Philosophy, ed. Richmond Thomason. New Haven: Yale University Press.\\n\\nMoortgat, Michael. 1988. Categorial Investigations: Logical and Linguistic Aspects of the Lambek Calculus. Doctoral dissertation, University of Amsterdam, Amsterdam, The Netherlands.\\n\\nMoortgat, Michael. 1992a. Generalized Quantifiers and Discontinuous Type Constructors. In Discontinuous Constituency, ed. W. Sijtsma and H. van Horck. Berlin, Germany: Mouton de Gruyter. To appear.\\n\\nMoortgat, Michael. 1992b. Labelled Deductive Systems for Categorial Theorem Proving. In Proceedings of the Eighth Amsterdam Colloquium, ed. P. Dekker and M. Stokhof, 403-423. Amsterdam. Institute for Logic, Language and Computation.\\n\\nMorrill, Glyn V. 1993. Type Logical Grammar: Categorial Logic of Signs. Studies in Linguistics and Philosophy. Dordrecht, Holland: Kluwer Academic Publishers. To appear.\\n\\nPereira, Fernando C. N.\\n\\n1990.\\n\\nCategorial Semantics and Scoping.\\n\\nComputational Linguistics 16(1):1\\n\\n\\n\\n10.\\n\\nPereira, Fernando C. N. 1991. Semantic Interpretation as Higher-Order Deduction. In Logics in AI: European Workshop JELIA'90, ed. Jan van Eijck, 78-96. Amsterdam, Holland. Springer-Verlag.\\n\\nSaraswat, Vijay A. 1989. Concurrent Constraint Programming Languages. Doctoral dissertation, Carnegie-Mellon University. Reprinted by MIT Press, Doctoral Dissertation Award and Logic Programming Series, 1993.\\n\\nScedrov, Andre. 1993. A Brief Guide to Linear Logic. In Current Trends in Theoretical Computer Science, ed. G. Rozenberg and A. Salomaa. World Scientific Publishing Co. Revised and expanded version of the article originally appearing in Bulletin of the European Assoc. for Theoretical Computer Science 41, 1990.\\n\\nvan Benthem, Johan. 1991. Language in Action: Categories, Lambdas and Dynamic Logic. Amsterdam: North-Holland.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9404010.xml'}),\n",
       " Document(page_content=\"An Empirically Motivated Reinterpretation of Dependency Grammar\\n\\nDependency grammar is usually interpreted as equivalent to a strict form of X-bar theory that forbids the stacking of nodes of the same bar level (e.g.,\\n\\nimmediately dominating\\n\\nwith the same head). But adequate accounts of one-anaphora and of the semantics of multiple modifiers require such stacking and accordingly argue against dependency grammar. Dependency grammar can be salvaged by reinterpreting its claims about phrase structure, so that modifiers map onto binary-branching X-bar trees rather than ``flat'' ones.\\n\\nIntroduction\\n\\nArguments for stacked X\\n\\n\\n\\nbar structures (such as\\n\\nimmediately\\n\\ndominating\\n\\nwith the same head) are arguments against dependency grammar as normally understood. This paper reviews the dependency grammar formalism, presents evidence that stacked\\n\\nstructures are required, and then proposes a reinterpretation of dependency grammar to make it compatible with the evidence.\\n\\nDependency grammar\\n\\nThe formalism\\n\\nThe fundamental relation in DG is between head and dependent. One word (usually the main verb) is the head of the whole sentence; every other word depends on some head, and may itself be the head of any number of dependents. The rules of grammar then specify what heads can take what dependents (for example, adjectives depend on nouns, not on verbs). Practical DGs distinguish various types of dependents (complement, adjunct, determiner, etc. ), but the details are not important for my argument.\\n\\nFigure 1 shows, in the usual notation, a dependency analysis of The old dog chased the cat into the garden. Here chased is the head of the sentence; dog and cat depend on chased; the and old depend on dog; and so on.\\n\\nConstituency in DG\\n\\nDependency grammar still recognizes constituents, but they are a defined rather than a basic concept. The usual definition is that a constituent consists of any word plus all its dependents, their dependents, and so on recursively. (Tesnire calls such a constituent a  NUD.) Thus the constituents in Figure 1 are (in addition to the individual words):\\n\\nare ruled out. Figure 2 shows Figure 1 recast into X-bar theory according to this interpretation.\\n\\nDifficulty 1: The proform one\\n\\n,\\n\\nand that\\n\\n's form stacked structures (Figure 3). Dependency grammar can do no such thing, because in dependency grammar as normally understood, all the modifiers hang from the same\\n\\nnode (Figure 4).\\n\\nFurther, the stacked\\n\\nanalysis predicts a structural ambiguity if there are modifiers on both sides of the head noun -- and the behavior of one shows that this ambiguity is real. Each\\n\\nin either tree in Figure 5 can be the antecedent of one: the long-haired student from Cambridge and a short-haired one from Oxford the long-haired student from Cambridge and a short-haired one the long-haired student from Cambridge and one from Oxford this long-haired student from Cambridge and the other one Again dependency grammar is left high and dry -- DG formalism can recognize neither the stacking nor the ambiguity, because all the modifiers have the same head.\\n\\nDifficulty 2: Semantics of multiple modifiers\\n\\nanalysis. But this grouping cannot be expressed by dependency grammar, because as far as DG is concerned, typical and French are dependents of house, and there is no intermediate syntactic structure.\\n\\n[[ knocked twice ] intentionally ] (acted on one intention, to knock twice) [[ knocked intentionally ] twice ] (had the intention two times) These argue strongly for stacking of\\n\\n's, or at least for something comparable on the semantic level.\\n\\nNote by the way that if there are modifiers on both sides of the verb, an ambiguity arises just as it did with nouns: intentionally knocked twice is ambiguous between [[ intentionally knocked ] twice ] and[ intentionally [ knocked twice ]].\\n\\nCrucially, these phenomena entail that if one adopts a non-stacked syntax such as that mandated by the standard interpretation of DG, then the semantic component of the grammar must know not only the grammatical relations recognized by the syntax, but also the comparative proximity of the various modifiers to the head.\\n\\nReinterpreting dependency grammar\\n\\nDependency grammar can be salvaged from this mess by reinterpreting its claims about phrase structure. Recall that in a dependency grammar, constituency is a defined concept. The solution is therefore to change the definition. Specifically, instead of being considered equivalent to flat X-bar trees, dependency structures can be mapped onto X-bar trees that introduce stacking in a principled way.\\n\\nHere is a sketch of such a reinterpretation, consistent with current X-bar theory. Given a head (X) and its dependents, attach the dependents to the head by forming stacked\\n\\nnodes as follows: 1. Attach subcategorized complements first, all under the same\\n\\nnode. If there are none, create the\\n\\nnode anyway. 2. Then attach modifiers, one at a time, by working outward from the one nearest the head noun, and adding a stacked\\n\\nnode for each.\\n\\n3.\\n\\nFinally, create an\\n\\nnode at the top of the stack, and attach the specifier (determiner), if any. Thus the dependency structure\\n\\nmaps, under the new interpretation, to the stacked structure:\\n\\nNote that if there are modifiers both before and after the head, the resulting X-bar tree is not unique -- and this non-uniqueness is desirable, because the resulting alternatives, such as =7.5in [[ long-haired student ] from Cambridge ]:[ long-haired [ student from Cambridge ]] [[ intentionally knocked ] twice ]:[ intentionally [ knocked twice ]]are exactly the ones required by the evidence.\\n\\nConclusion\\n\\nThe alert reader may wonder, at this point, whether dependency grammar has been salvaged or rather refuted, because under the new interpretation, DG is a notational variant of current X-bar theory. To this I have several replies: 1. It should not be surprising when separate theories of the same phenomena develop convergently. 2. DG always  WAS a notational variant of X-bar theory; I have merely brought its implicit X-bar theory up to date. 3. DG still imposes stricter requirements than transformational grammar, because in DG, violations of X-bar theory are flatly impossible, not just undesirable. In any case, the dependency perspective on sentence structure has proved its worth not only in syntactic theorizing, but also in language teaching, parsing, and other practical applications. Indeed, dependency concepts, such as government and c-command, are becoming increasingly prominent in transformational grammar. Dependency grammar can complement other approaches to syntax in much the same way that relational grammar, fifteen years ago, provided an organizing perspective on what had previously been a heterogeneous set of syntactic transformations.\\n\\nReferences\\n\\n, Avery, III (1983) A note on the constituent structure of modifiers. Linguistic Inquiry 14:695-697. Covington, Michael A. (1990) Parsing discontinuous constituents in dependency grammar. Computational Linguistics 16:234-236. Dahl, sten (1980) Some arguments for higher nodes in syntax: a reply to Hudson's `Constituency and dependency.' Linguistics 18:485-488. Hudson, Richard (1980a) Constituency and dependency. Linguistics 18:179-198. Hudson, Richard (1980b) A second attack on constituency: a reply to Dahl. Linguistics 18:489-504. Hudson, Richard (1990) English word grammar. Oxford: Basil Blackwell. Mel'cuk, Igor A. (1987) Dependency syntax: theory and practice. Albany: State University of New York Press. Radford, Andrew (1988) Transformational grammar. Cambridge: Cambridge University Press. Robinson, Jane J. (1970) Dependency structures and transformational rules. Language 46:259-285. Starosta, Stanley (1988) The case for lexicase. London: Pinter. Tesnire, Lucien (1959) lments de syntaxe structurale. Paris: Klincksieck.\\n\\nFootnotes\\n\\nBut it would be completely compatible with the formalism to postulate that the head of the sentence is a potentially empty INFL or the like. Then, in Fig. 2, the VP would be a constituent. This reinterpretation was suggested by Hudson's proposal (1980b:499-501, 1990:149-150) that the semantic effect of proximity of the head is due to a parsing effect. Since parsing is nothing if not syntactic, it seems desirable to incorporate this proposal into the syntactic theory. Actually, it is immaterial to my argument whether all the complements hang from the same node or whether they, too, are introduced by binary branching, like the adjuncts.\", metadata={'source': '../data/raw/cmplg-xml/9404004.xml'}),\n",
       " Document(page_content=\"OCCURRENCE VECTORS FROM CORPORA VS. DISTANCE VECTORS FROM DICTIONARIES\\n\\nA comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the inter-word distances in dictionary definitions. The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal (20M total words) was higher than that by using distance vectors from the Collins English Dictionary ( head words +  definition words). However, other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors.\\n\\nIntroduction\\n\\nDistance Vectors\\n\\nFor example, the definition for dictionary is ``a book in which the words of a language are listed alphabetically ... .'' The word dictionary is thus linked to the words book, word, language, and alphabetical.\\n\\nA word vector is defined as the list of distances from a word to a certain set of selected words, which we call origins. The words in Fig. marked with Oi (unit, book, and people) are assumed to be origin words. In principle, origin words can be freely chosen. In our experiments we used middle frequency words: the 51st to 1050th most frequent words in the reference Collins English Dictionary (CED).\\n\\nThe distance vector for dictionary is derived as follows:\\n\\nThe i-th element is the distance (the length of the shortest path) between dictionary and the i-th origin, Oi. To begin, we assume every link has a constant length of 1. The actual definition for link length will be given later.\\n\\nWe will now describe some technical details about the derivation of distance vectors.\\n\\nLink Length Distance measurement in a reference network depends on the definition of link length. Previously, we assumed for simplicity that every link has a constant length. However, this simple definition seems unnatural because it does not reflect word frequency. Because a path through low-frequency words (rare words) implies a strong relation, it should be measured as a shorter path. Therefore, we use the following definition of link length, which takes account of word frequency.\\n\\nThis shows the length of the links between words W\\n\\ni (i=1,2)in Fig., where Ni denotes the total number of links from and to Wiand n denotes the number of direct links between these two words.\\n\\nNormalization Distance vectors are normalized by first changing each coordinate into its deviation in the coordinate:\\n\\nwhere ai and\\n\\nare the average and the standard deviation of the distances from the i-th origin. Next, each coordinate is changed into its deviation in the vector:\\n\\nwhere\\n\\nand\\n\\nare the average and the standard deviation of\\n\\n.\\n\\nCo\\n\\n\\n\\noccurrence Vectors\\n\\nwhere\\n\\nis the occurrence density of word X in a whole corpus, and the conditional probability\\n\\nA co-occurence vector of a word is defined as the list of co-occurrence likelihood of the word with a certain set of origin words. We used the same set of origin words as for the distance vectors.\\n\\nCo\\n\\n\\n\\noccurrence Vector.\\n\\nWhen the frequency of X or Y is zero, we can not measure their co-occurence likelihood, and such cases are not exceptional. This sparseness problem is well-known and serious in the co-occurrence statistics. We used as a corpus the 1987 Wall Street Journal in the CD-ROM I ACL-CD-ROM-1, which has a total of 20M words. The number of words which appeared at least once was about 50% of the total 62K head words of CED, and the percentage of the word-origin pairs which appeared at least once was about 16% of total 62K\\n\\n1K (=62M) pairs. When the co-occurrence likelihood can not be measured, the value\\n\\nwas set to 0.\\n\\nExperimental Results\\n\\nWe compared the two vector representations by using them for the following two semantic tasks. The first is word sense disambiguation (WSD) based on the similarity of context vectors; the second is the learning of  or  meanings from example words.\\n\\nWith WSD, the precision by using co-occurrence vectors from a 20M words corpus was higher than by using distance vectors from the CED.\\n\\nWord Sense Disambiguation\\n\\nis\\n\\nThe similarity of contexts is measured by the angle of their vectors (or actually the inner product of their normalized vectors).\\n\\nLet word\\n\\nhave senses\\n\\n, and each sense have the following context examples.\\n\\nWe infer that the sense of word\\n\\nin an arbitrary context\\n\\nis\\n\\nif for some j the similarity,\\n\\n, is maximum among all the context examples.\\n\\nAnother possible way to infer the sense is to choose sense\\n\\nsuch that the average of\\n\\nover\\n\\nis maximum. We selected the first method because a peculiarly similar example is more important than the average similarity.\\n\\nFigure  (next page) shows the disambiguation precision for 9 words. For each word, we selected two senses shown over each graph. These senses were chosen because they are clearly different and we could collect sufficient number (more than 20) of context examples. The names of senses were chosen from the category names in Roget's International Thesaurus, except organ's.\\n\\nThe results using distance vectors are shown by dots (\\n\\n), and using co-occurrence vectors from the 1987 WSJ (20M words) by circles (  ).\\n\\nA context size (x-axis) of, for example, 10 means 10 words before the target word and 10 words after the target word. We used 20 examples per sense; they were taken from the 1988 WSJ. The test contexts were from the 1987 WSJ: The number of test contexts varies from word to word (100 to 1000). The precision is the simple average of the respective precisions for the two senses.\\n\\nThe results of Fig. show that the precision by using co-occurrence vectors are higher than that by using distance vectors except two cases, interest and customs. And we have not yet found a case where the distance vectors give higher precision. Therefore we conclude that co-occurrence vectors are advantageous over distance vectors to WSD based on the context similarity.\\n\\nThe sparseness problem for co-occurrence vectors is not serious in this case because each context consists of plural words.\\n\\nLearning of\\n\\n\\n\\nor-\\n\\nAnother experiment using the same two vector representations was done to measure the learning of  or  meanings. Figure  shows the changes in the precision (the percentage of agreement with the authors' combined judgement). The x-axis indicates the number of example words for each or  pair. Judgement was again done by using the nearest example. The example and test words are shown in Tables  and , respectively.\\n\\nIn this case, the distance vectors were advantageous. The precision by using distance vectors increased to about 80% and then leveled off, while the precision by using co-occurrence vectors stayed around 60%. We can therefore conclude that the property of -or- is reflected in distance vectors more strongly than in co-occurrence vectors. The sparseness problem is supposed to be a major factor in this case.\\n\\nSupplementary Data\\n\\nIn the experiments discussed above, the corpus size for co-occurrence vectors was set to 20M words ('87 WSJ) and the vector dimension for both co-occurrence and distance vectors was set to 1000. Here we show some supplementary data that support these parameter settings.\\n\\na. Corpus size (for co-occurrence vectors)\\n\\nFigure  shows the change in disambiguation precision as the corpus size for co-occurrence statistics increases from 200 words to 20M words. (The words are suit, issue and race, the context size is 10, and the number of examples per sense is 10.) These three graphs level off after around 1M words. Therefore, a corpus size of 20M words is not too small.\\n\\nb. Vector Dimension\\n\\nFigure  (next page) shows the dependence of disambiguation precision on the vector dimension for (i) co-occurrence and (ii) distance vectors. As for co-occurrence vectors, the precision levels off near a dimension of 100. Therefore, a dimension size of 1000 is sufficient or even redundant. However, in the distance vector's case, it is not clear whether the precision is leveling or still increasing around 1000 dimension.\\n\\nConclusion\\n\\nA comparison was made of co-occurrence vectors from large text corpora and of distance vectors from dictionary definitions.\\n\\nFor the word sense disambiguation based on the context similarity, co-occurrence vectors from the 1987 Wall Street Journal (20M total words) was advantageous over distance vectors from the Collins English Dictionary ( head words +  definition words).\\n\\nFor learning  or  meanings from example words, distance vectors gave remarkably higher precision than co-occurrence vectors. This suggests, though further investigation is required, that distance vectors contain some different semantic information from co-occurrence vectors.\\n\\nBibliography\\n\\nKenneth W. Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicography. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pages 76-83, Vancouver, Canada.\\n\\nJim Cowie, Joe Guthrie, and Louise Guthrie. 1992. Lexical disambiguation using simulated annealing. In Proceedings of COLING-92, pages 359-365, Nantes, France.\\n\\nIdo Dagan, Shaul Marcus, and Shaul Markovitch. 1993. Contextual word similarity and estimation from sparse data. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 164-171, Columbus, Ohio.\\n\\nJames Deese. 1962. On the structure of associative meaning. Psychological Review, 69(3):161-175.\\n\\nMarti A. Hearst. 1991. Noun homograph disambiguation using local context in large text corpora. In Proceedings of the 7th Annual Conference of the University of Waterloo Center for the New OED and Text Research, pages 1-22, Oxford.\\n\\nHideki Kozima and Teiji Furugori. 1993. Similarity between words computed by spreading activation on an english dictionary. In Proceedings of EACL-93, pages 232-239, Utrecht, the Netherlands.\\n\\nMark Liberman, editor. 1991. CD-ROM I. Association for Computational Linguistics Data Collection Initiative, University of Pennsylvania.\\n\\nYoshihiko Nitta. 1988. The referential structure of the word definitions in ordinary dictionaries. In Proceedings of the Workshop on the Aspects of Lexicon for Natural Language Processing, LNL88-8, JSSST, pages 1-21, Fukuoka University, Japan. (in Japanese).\\n\\nYoshihiko Nitta. 1993. Referential structure - a mechanism for giving word-definition in ordinary lexicons. In C. Lee and B. Kang, editors, Language, Information and Computation, pages 99-110. Thaehaksa, Seoul.\\n\\nYoshiki Niwa and Yoshihiko Nitta. 1993. Distance vector representation of words, derived from reference networks in ordinary dictionaries. MCCS 93-253, Computing Research Laboratory, New Mexico State University, Las Cruces.\\n\\nC. E. Osgood, G. F. Such, and P. H. Tannenbaum. 1957. The Measurement of Meaning. University of Illinois Press, Urbana.\\n\\nFernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of english words. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 183-190, Columbus, Ohio.\\n\\nPaul Procter, editor. 1978. Longman Dictionary of Contemporary English (LDOCE). Longman, Harlow, Essex, first edition.\\n\\nHinrich Schtze. 1993. Word space. In J. D. Cowan S. J. Hanson and C. L. Giles, editors, Advances in Neural Information Processing Systems, pages 895-902. Morgan Kaufmann, San Mateo, California.\\n\\nJohn Sinclair, editor. 1987. Collins COBUILD English Language Dictionary. Collins and the University of Birmingham, London.\\n\\nJean Vronis and Nancy M. Ide. 1990. Word sense disambiguation with very large neural networks extracted from machine readable dictionaries. In Proceedings of COLING-90, pages 389-394, Helsinki.\\n\\nYorick Wilks, Dan Fass, Cheng ming Guo, James E. McDonald, Tony Plate, and Brian M. Slator. 1990. Providing machine tractable dictionary tools. Machine Translation, 5(2):99-154.\\n\\nDavid Yarowsky. 1992. Word-sense disambiguation using statistical models of roget's categories trained on large corpora. In Proceedings of COLING-92, pages 454-460, Nantes, France.\", metadata={'source': '../data/raw/cmplg-xml/9503025.xml'}),\n",
       " Document(page_content=\"Connectivity in Bag Generation\\n\\nIntroduction\\n\\nBag generation is a form of natural language generation in which the input is a bag (also known as a multiset: a set in which repeated elements are significant) of lexical elements and the output is a grammatical sentence or a statistically most probable permutation with respect to some language model.\\n\\nTwo assumptions are made regarding lexical-semantic indexing.\\n\\nAssumption  2 All lexical signs must be connected to each other. Two lexical signs are connected if they are directly connected; furthermore, the connectivity relation is transitive.\\n\\nDefinition  1 Two signs, A, B, are directly connected if there exist at least two paths, PathA, PathB, such that A:PathA is token identical with B:PathB.\\n\\nThe indices involved in determining connectivity are specified as parameters for a particular formalism. For example, in HPSG, they would be indicated through paths such as SYNSEM:LOCAL:CONTENT:INDEX.\\n\\nTo ensure that only connected lexical signs are generated and analysed, the following assumption must also be made:\\n\\nAssumption  3 A grammar will only generate or analyse connected lexical signs.\\n\\nBag Generation Algorithms\\n\\nRedundancy in Bag Generation\\n\\nEx. 1\\n\\n# the dog\\n\\n# the dog barked\\n\\n# the brown dog\\n\\nFor simple cases in chart based generators such unnecessary strings do not create many problems, but for longer sentences, each additional substring implies a further branch in the search tree to be considered.\\n\\nTest: dog barked the brown big\\n\\nRewrite: __ barked the dog brown big\\n\\nTest: barked (the dog) brown big\\n\\nRewrite: __ (the dog) barked brown big\\n\\nTest: ((the dog) barked) brown big\\n\\nRewrite: the brown dog barked __ big\\n\\nTest: ((the (brown dog)) barked) big\\n\\nRewrite: the big (brown dog) barked __\\n\\nTest: ((the (big (brown dog))) barked) (terminate)\\n\\nIn this sequence double underscore (__) indicates the starting position of a moved constituent; the moved constituent itself is given in bold face; the bracketing indicates analysed constituents (for expository purposes the algorithm has been oversimplified, but the general idea remains the same).\\n\\nNow consider the step where `brown' is inserted between `the' and `dog'. This action causes the complete structure for `the dog barked' to be discarded and replaced with that for `the brown dog barked', which in turn is discarded and replaced by `the big brown dog barked'.\\n\\nPrevious Work\\n\\nTrujillo trujillo95c adapts some of Brew's ideas to phrase structure grammars by compiling Follow functions and constructing adjacency graphs. While this approach reduces the size of the search space, it does not prune it sufficiently for certain classes of modifiers.\\n\\nPhillips phillips93 proposes handling inefficiency at the expense of completeness. His idea is to maintain a queue of modifiable constituents (e.g. N1s) in order to delay their combination with other constituents until modifiers (e.g. PPs) have been analysed. While practical, this approach can lead to alternative valid sentences not being generated.\\n\\nConnectivity Restrictions\\n\\nIn searching for a mechanism that eliminates unnecessary wfss, it will be possible to use indices in lexical signs. As mentioned earlier, these indices play a major role in preventing the generation of incorrect translations.\\n\\nUsing Connectivity for Pruning\\n\\nTake the following bag:\\n\\nEx. 2\\n\\n{dog1,the1,brown1,big1}\\n\\nEx. 3\\n\\n{the1,dog1,with1,2,the2,brown2,collar2}\\n\\nA naive implementation of this deduction would attempt to expand the VP depth-first, left to right, in order to accommodate `brown' in a complete derivation. Since this would not be possible, the NP `the dog' would be discarded. This approach is grossly inefficient however. What is required is a more tractable algorithm which, given a wfss and its associated sign, will be able to determine whether all remaining lexical elements can ever form part of a complete sentence which includes that wfss.\\n\\nNote that deciding whether a lexical sign can appear outside a phrase is determined purely by the grammar, and not by whether the lexical elements share the same index or not. Thus, a more complex grammar would allow `the man' from the bag\\n\\nEx. 4\\n\\n{the1,man1,shavese,1,1,himself1}\\n\\neven though `himself' has the same index as `the man'.\\n\\nOuter Domains\\n\\nThe approach introduced here compiles the relevant information offline from the grammar and uses it to check for connectivity during bag generation. The compilation process results in a set of (Sign,Lex,Bindings) triples called outer domains. This set is based on a unification-based phrase structure grammar defined as follows:\\n\\nOuter domains are defined as follow:\\n\\nIn compiling outer domains, inner domains are used to facilitate computation. Inner domains are defined as follows:\\n\\nThe inner domains thus express all the possible terminal categories which may be derived from each nonterminal in the grammar.\\n\\nTo be able to exploit connectivity during generation, inner and outer domains contain only triples in which Binds has at least one element. In this way, only those lexical categories which are directly connected to the sign are taken into account; the implication of this will become clearer later.\\n\\nAs an example, the outer domain of NP as derived from the above grammar is: (NP[sem:arg1:X],Vtra[sem:arg2:Y], {[sem:arg1,sem:arg2]}) (NP[sem:arg1:X],Vtra[sem:arg3:Y], {[sem:arg1,sem:arg3]}) (NP[sem:arg1:X],P[sem:arg3:Y], {[sem:arg1,sem:arg3]})This set indicates that for any NP, the only terminal categories not contained in the subtree with root NP, and with which the NP shares a semantic index, are Vtra and P. For instance, the first triple arises from the following tree:\\n\\nPruning through Outer Domains and Connectivity\\n\\nThe pruning technique developed here operates on grammars whose analyses result in connected leaves.\\n\\nExample\\n\\nThis graph is built by using the outer domain of each lexical element to decide which of the remaining elements could possibly share an index with it in a complete sentence.\\n\\nCompiling Connectivity Domains\\n\\nDuring computation, the set of Binds is monotonically increased as different ways of directly connecting sign and lexeme are found.\\n\\nResults\\n\\nOnly one reading was generated for each bag, corresponding to one attachment site for PPs. The table shows that the technique can yield reductions in the number of edges (both active and inactive) and time taken, especially for longer sentences, while retaining the overheads at an acceptable level.\\n\\nConclusion\\n\\nA technique for pruning the search space of a bag generator has been implemented and its usefulness shown in the generation of different types of constructions. The technique relies on a connectivity constraint imposed on the semantic relationships expressed in the input bag. In order to apply the algorithm, outer domains needed to be compiled from the grammar; these are used to discard wfss by ensuring lexical signs outside a wfss can indeed appear outside that string.\\n\\nExploratory work employing adjacency constraints during generation has yielded further improvements in execution time when applied in conjunction with the pruner. If extended appropriately, these constraints could prune the search space even further. This work will be reported at a later date.\\n\\nAcknowledgments\\n\\nTwo anonymous reviewers provided very useful comments; we regret not being able to do justice to all their suggestions.\\n\\nBibliography\\n\\nA. V. Aho, R. Sethi, and J. D. Ullman. 1986. Compilers - Principles , Techniques, and Tools. Addison Wesley, Reading, MA.\\n\\nJ. L. Beaven. 1992. Lexicalist Unification Based Machine Translation. Ph.D. thesis, Department of Artificial Intelligence, University of Edinburgh, Edinburgh, UK.\\n\\nC. Brew. 1992. Letting the cat out of the bag: Generation for Shake-and-Bake MT. In Proceedings of the 14th COLING, pages 610-16, Nantes, France, August.\\n\\nJ. Calder, M. Reape, and H. Zeevat. 1989. An algorithm for generation in unification categorial grammar. In Proceedings of the Fourth European Conference of the ACL, pages 233-40, Manchester, England, April.\\n\\nH.H. Chen and Y.S. Lee. 1994. A corrective training algorithm for adaptive learning in bag generation. In New Methods in Language Processing, Manchester, UK.\\n\\nA. Copestake, D. Flickinger, R. Malouf, S. Riehemann, and I. Sag. 1995. Translation using minimal recursion semantics. In Proceedings of the 6th International Conference on Theoretical and Methodological Issues in Machine Translation, Leuven, Belgium, July.\\n\\nH. Kamp and U. Reyle. 1993. From Discourse to Logic - Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory, volume 42 of Studies in Linguistics and Philosophy. Kluwer Academic, Dordrecht, The Netherlands.\\n\\nKen Kennedy. 1981. A survey of data flow analysis techniques. In Muchnick and Jones muchnicketal81, chapter 1, pages 5-54.\\n\\nSteven S. Muchnick and Neil D. Jones, editors. 1981. Program Flow Analysis: Theory and Applications. Software. Prentice-Hall, Englewood Cliffs, NJ.\\n\\nJ. D. Phillips. 1993. Generation of text from logical formulae. Machine Translation, 8(4):209-35.\\n\\nC. Pollard and I. Sag. 1994. Head Driven Phrase Structure Grammar. Chicago University Press, IL.\\n\\nFred Popowich. 1995. Improving the efficiency of a generation algorithm for Shake and Bake machine translation using Head-Driven Phrase Structure Grammar. In Proceedings of Natural Language Understanding and Logic Programming V, Lisbon, Portugal, May.\\n\\nV. Poznanski, J. L. Beaven, and P. Whitelock. 1995. An efficient generation algorithm for lexicalist MT. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, Boston, MA, June.\\n\\nUwe Reyle. 1995. On reasoning with ambiguities. In Proceedings of the Seventh Conference of the European Chapter of the Association for Computational Linguistics, pages 1-15, Dublin, Ireland, March.\\n\\nC. J. Rupp, M. A. Rosner, and R. L. Johnson, editors. 1994. Constraints, Language and Computation. Academic Press, London.\\n\\nS. M. Shieber. 1986. An Introduction to Unification-based Approaches to Grammar, volume 4 of CSLI Lecture Notes. CSLI, Stanford, CA.\\n\\nA. Trujillo. 1994. Computing FIRST and FOLLOW functions for Feature-Theoretic grammars. In Proceedings of the 15th COLING, pages 875-80, Kyoto, Japan, August.\\n\\nA. Trujillo. 1995. Lexicalist Machine Translation of Spatial Prepositions. Ph.D. thesis, Computer Laboratory, University of Cambridge, April.\\n\\nPete Whitelock. 1994. Shake-and-bake translation. In Rupp et al. ruppetal94, pages 339-59.\\n\\nFootnotes\\n\\nNow at SHARP Laboratories of Europe, Oxford Science Park, Oxford OX4 4GA. E-mail: simon@sharp.co.uk\", metadata={'source': '../data/raw/cmplg-xml/9604024.xml'}),\n",
       " Document(page_content='Identifying Word Translations in Non-Parallel Texts\\n\\nCommon algorithms for sentence and word-alignment allow the automatic identification of word translations from parallel texts. This study suggests that the identification of word translations should also be possible with non-parallel and even unrelated texts. The method proposed is based on the assumption that there is a correlation between the patterns of word co-occurrences in texts of different languages.\\n\\nIntroduction\\n\\nIn a number of recent studies it has been shown that word translations can be automatically derived from the statistical distribution of words in bilingual parallel texts (e. g. Catizone, Russell  Warwick, 1989; Brown et al., 1990; Dagan, Church  Gale, 1993; Kay R\"oscheisen, 1993). Most of the proposed algorithms first conduct an alignment of sentences, i. e. those pairs of sentences are located that are translations of each other. In a second step a word alignment is performed by analyzing the correspondences of words in each pair of sentences.\\n\\nThe results achieved with these algorithms have been found useful for the compilation of dictionaries, for checking the consistency of terminological usage in translations, and for assisting the terminological work of translators and interpreters.\\n\\nHowever, despite serious efforts in the compilation of corpora (Church Mercer, 1993; Armstrong  Thompson, 1995) the availability of a large enough parallel corpus in a specific field and for a given pair of languages will always be the exception, not the rule. Since the acquisition of non-parallel texts is usually much easier, it would be desirable to have a program that can determine the translations of words from comparable or even unrelated texts.\\n\\nApproach\\n\\nIt is assumed that there is a correlation between the co-occurrences of words which are translations of each other. If - for example - in a text of one language two words A and B co-occur more often than expected from chance, then in a text of another language those words which are translations of A and B should also co-occur more frequently than expected. This assumption is reasonable for parallel texts. However, in this paper it is further assumed that the co-occurrence patterns in original texts are not fundamentally different from those in translated texts.\\n\\nSimulation\\n\\nA simulation experiment was conducted in order to see whether the above assumptions concerning the similarity of co-occurrence patterns actually hold. In this experiment, for an equivalent English and German vocabulary two co-occurrence matrices were computed and then compared. As the English vocabulary a list of 100 words was used, which had been suggested by Kent  Rosanoff (1910) for association experiments. The German vocabulary consisted of one by one translations of these words as chosen by Russell (1970).\\n\\nThe word co-occurrences were computed on the basis of an English corpus of 33 and a German corpus of 46 million words. The English corpus consists of the Brown Corpus, texts from the Wall Street Journal, Grolier\\'s Electronic Encyclopedia and scientific abstracts from different fields. The German corpus is a compilation of mainly newspaper texts from Frankfurter Rundschau, Die Zeit and Mannheimer Morgen. To the knowledge of the author, the English and German corpora contain no parallel passages.\\n\\nFor each pair of words in the English vocabulary its frequency of common occurrence in the English corpus was counted. The common occurrence of two words was defined as both words being separated by at most 11 other words. The co-occurrence frequencies obtained in this way were used to build up the English matrix. Equivalently, the German co-occurrence matrix was created by counting the co-occurrences of German word pairs in the German corpus. As a starting point, word order in the two matrices was chosen such that word n in the German matrix was the translation of word n in the English matrix.\\n\\nCo-occurrence studies like that conducted by Wettler  Rapp (1993) have shown that for many purposes it is desirable to reduce the influence of word frequency on the co-occurrence counts. For the prediction of word associations they achieved best results when modifying each entry in the co-occurrence matrix using the following formula:\\n\\nRegardless of the formula applied, the English and the German matrix were both normalized. Starting from the normalized English and German matrices, the aim was to determine how far the similarity of the two matrices depends on the correspondence of word order. As a measure for matrix similarity the sum of the absolute differences of the values at corresponding matrix positions was used.\\n\\nThis similarity measure leads to a value of zero for identical matrices, and to a value of 20 000 in the case that a non-zero entry in one of the 100 * 100 matrices always corresponds to a zero-value in the other.\\n\\nResults Discussion and prospects Computational limitations require the vocabularies to be limited to subsets of all word types in large corpora. With criteria like the corpus frequency of a word, its specificity for a given domain, and the salience of its co-occurrence patterns, it should be possible to make a selection of corresponding vocabularies in the two languages. If morphological tools and disambiguators are available, preliminary lemmatization of the corpora would be desirable.\\n\\nAmbiguities in word translations can be taken into account by working with continuous probabilities to judge whether a word translation is correct instead of making a binary decision. Thereby, different sizes of the two matrices could be allowed for.\\n\\nAcknowledgements\\n\\nI thank Susan Armstrong and Manfred Wettler for their support of this project. Thanks also to Graham Russell and three anonymous reviewers for valuable comments on the manuscript.\\n\\nBibliography\\n\\nArmstrong, Susan; Thompson, Henry (1995). A presentation of MLCC: Multilingual Corpora for Cooperation. Linguistic Database Workshop, Groningen.\\n\\nBrown, Peter; Cocke, John; Della Pietra, Stephen A.; Della Pietra, Vincent J.; Jelinek, Fredrick; Lafferty, John D.; Mercer, Robert L.; Rossin, Paul S. (1990). A statistical approach to machine translation. Computational Linguistics, 16(2), 79-85.\\n\\nCatizone, Roberta; Russell, Graham; Warwick, Susan (1989). Deriving translation data from bilingual texts. In: U. Zernik (ed. ): Proceedings of the First International Lexical Acquisition Workshop, Detroit.\\n\\nChurch, Kenneth W.; Mercer, Robert L. (1993). Introduction to the special issue on Computational Linguistics using large corpora. Computational Linguistics, 19(1), 1-24.\\n\\nDagan, Ido; Church, Kenneth W.; Gale, William A. (1993). Robust bilingual word alignment for machine aided translation. Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives. Columbus, Ohio, 1-8.\\n\\nKay, Martin; R\"oscheisen, Martin (1993). Text-Translation Alignment. Computational Linguistics, 19(1), 121-142.\\n\\nKent, G.H. ; Rosanoff, A.J. (1910). A study of association in insanity. American Journal of Insanity, 67, 37-96, 317-390.\\n\\nRussell, Wallace A. (1970). The complete German language norms for responses to 100 words from the Kent-Rosanoff word association test. In: L. Postman, G. Keppel (eds. ): Norms of Word Association. New York: Academic Press, 53-94.\\n\\nWettler, Manfred; Rapp, Reinhard (1993). Computation of word associations based on the co-occurrences of words in large corpora. In: Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, Columbus, Ohio, 84-93.\\n\\nFootnotes\\n\\nThe logarithm has been removed from the mutual information measure since it is not defined for zero co-occurrences. Normalization was conducted in such a way that the sum of all matrix entries adds up to the number of fields in the matrix. c = 1 is not possible and was not taken into account.', metadata={'source': '../data/raw/cmplg-xml/9505037.xml'}),\n",
       " Document(page_content=\"AN INTEGRATED HEURISTIC SCHEME FOR PARTIAL PARSE EVALUATION The GLR* Parser The GLR Parsing Algorithm The GLR* Parser\\n\\nThe parser accommodates the skipping of words of the input string by allowing shift operations to be performed from inactive state nodes in the Graph Structured Stack (GSS). Shifting an input symbol from an inactive state is equivalent to skipping the words of the input that were encountered after the parser reached the inactive state and prior to the current word that is being shifted. Since the parser is LR(0), previous reduce operations remain valid even when words further along in the input are skipped. Information about skipped words is maintained in the symbol nodes that represent parse sub-trees.\\n\\nTo guarantee runtime feasibility, the GLR* parser is coupled with a ``beam'' search heuristic, that dynamically restricts the skipping capability of the parser, so as to focus on parses of maximal and close to maximal substrings of the input. The efficiency of the parser is also increased by an enhanced process of local ambiguity packing and pruning. Locally ambiguous symbol nodes are compared in terms of the words skipped within them. In cases where one phrase has more skipped words than the other, the phrase with more skipped words is discarded in favor of the more complete parsed phrase. This operation significantly reduces the number of parses being pursued by the parser.\\n\\nThe Parse Evaluation Heuristics\\n\\nAt the end of the process of parsing a sentence, the GLR* parser returns with a set of possible parses, each corresponding to some grammatical subset of words of the input sentence. Due to the beam search heuristic and the ambiguity packing scheme, this set of parses is limited to maximal or close to maximal grammatical subsets. The principle goal is then to find the maximal parsable subset of the input string (and its parse). However, in many cases there are several distinct maximal parses, each consisting of a different subset of words of the original sentence. Furthermore, our experience has shown that in many cases, ignoring an additional one or two input words may result in a parse that is syntactically and/or semantically more coherent. We have thus developed an evaluation heuristic that combines several different measures, in order to select the parse that is deemed overall ``best''.\\n\\nOur heuristic uses a set of features by which each of the parse candidates can be evaluated and compared. We use features of both the candidate parse and the ignored parts of the original input sentence. The features are designed to be general and, for the most part, grammar and domain independent. For each parse, the heuristic computes a penalty score for each of the features. The penalties of the different features are then combined into a single score using a linear combination. The weights used in this scheme are adjustable, and can be optimized for a particular domain and/or grammar. The parser then selects the parse ranked best (i.e. the parse of lowest overall score).\\n\\nThe Parse Evaluation Features\\n\\nSo far, we have experimented with the following set of evaluation features: 1. The number and position of skipped words 2. The number of substituted words 3. The fragmentation of the parse analysis 4. The statistical score of the disambiguated parse tree\\n\\nThe penalty scheme for skipped words is designed to prefer parses that correspond to fewer skipped words. It assigns a penalty in the range of\\n\\n(0.95 - 1.05) for each word of the original sentence that was skipped. The scheme is such that words that are skipped later in the sentence receive the slightly higher penalty. This preference was designed to handle the phenomena of false starts, which is common in spontaneous speech.\\n\\nThe GLR* parser has a capability for handling common word substitutions when the parser's input string is the output of a speech recognition system. When the input contains a pre-determined commonly substituted word, the parser attempts to continue with both the original input word and a specified ``correct'' word. The number of substituted words is used as an evaluation feature, so as to prefer an analysis with fewer substituted words.\\n\\nThe grammars we have been working with allow a single input sentence to be analyzed as several grammatical ``sentences'' or fragments. Our experiments have indicated that, in most cases, a less fragmented analysis is more desirable. We therefore use the sum of the number of fragments in the analysis as an additional feature.\\n\\npenalty = (0.1\\n\\n\\n\\n(\\n\\n\\n\\nlog10(pscore)))\\n\\nThe penalty scores of the features are then combined by a linear combination. The weights assigned to the features determine the way they interact. In our experiments so far, we have fined tuned these weights manually, so as to try and optimize the results on a training set of data. However, we plan on investigating the possibility of using some known optimization techniques for this task.\\n\\nThe Parse Quality Heuristic\\n\\nThe utility of a parser such as GLR* obviously depends on the semantic coherency of the parse results that it returns. Since the parser is designed to succeed in parsing almost any input, parsing success by itself can no longer provide a likely guarantee of such coherency. Although we believe this task would ultimately be better handled by a domain dependent semantic analyzer that would follow the parser, we have attempted to partially handle this problem using a simple filtering scheme.\\n\\n(1) = simple heuristic, (2) = full heuristics\\n\\nThe filtering scheme's task is to classify the parse chosen as best by the parser into one of two categories: ``good'' or ``bad''. Our heuristic takes into account both the actual value of the parse's combined penalty score and a measure relative to the length of the input sentence. Similar to the penalty score scheme, the precise thresholds are currently fine tuned to try and optimize the classification results on a training set of data.\\n\\nParsing of Spontaneous Speech Using GLR*\\n\\nWe have recently conducted some new experiments to test the utility of the GLR* parser and our parse evaluation heuristics when parsing speech recognized spontaneous speech in the ATIS domain. We modified an existing partial coverage syntactic grammar into a grammar for the ATIS domain, using a development set of some 300 sentences. The resulting grammar has 458 rules, which translate into a parsing table of almost 700 states.\\n\\nA list of common appearing substitutions was constructed from the development set. The correct parses of 250 grammatical sentences were used to train the parse table statistics that are used for disambiguation and parse evaluation. After some experimentation, the evaluation feature weights were set in the following way. As previously described, the penalty for a skipped word ranges between 0.95 and 1.05, depending on the word's position in the sentence. The penalty for a substituted word was set to 0.9, so that substituting a word would be preferable to skipping the word. The fragmentation feature was given a weight of 1.1, to prefer skipping a word if it reduces the fragmentation count by at least one. The three penalties are then summed, together with the converted statistical score of the parse.\\n\\nWe then used a set of 120 new sentences as a test set. Our goal was three-fold. First, we wanted to compare the parsing capability of the GLR* parser with that of the original GLR parser. Second, we wished to test the effectiveness of our evaluation heuristics in selecting the best parse. Third, we wanted to evaluate the ability of the parse quality heuristic to correctly classify GLR* parses as ``good'' or ``bad''. We ran the parser three times on the test set. The first run was with skipping disabled. This is equivalent to running the original GLR parser. The second run was conducted with skipping enabled and full heuristics. The third run was conducted with skipping enabled, and with a simple heuristic that prefers parses based only on the number of words skipped. In all three runs, the single selected parse result for each sentence was manually evaluated to determine if the parser returned with a ``correct'' parse.\\n\\nOur results indicate that our full integrated heuristic scheme for selecting the best parse out-performs the simple heuristic, that considers only the number of words skipped. With the simple heuristic, good/close parses were returned in 24 out of the 53 sentences that involved some degree of skipping. With our integrated heuristic scheme, good/close parses were returned in 30 sentences (6 additional sentences). Further analysis showed that only 2 sentences had parses that were better than those selected by our integrated parse evaluation heuristic.\\n\\nFootnotes\\n\\nThe system can display the n best parses found, where the parameter n is controlled by the user at runtime. By default, we set n to one, and the parse with the lowest score is displayed.\", metadata={'source': '../data/raw/cmplg-xml/9405023.xml'}),\n",
       " Document(page_content='Automatic Inference of DATR Theories\\n\\nAn approach for the automatic acquisition of linguistic knowledge from unstructured data is presented. The acquired knowledge is represented in the lexical knowledge representation language DATR. A set of transformation rules that establish inheritance relationships and a default-inference algorithm make up the basis components of the system. Since the overall approach is not restricted to a special domain, the heuristic inference strategy uses criteria to evaluate the quality of a DATR theory, where different domains may require different criteria. The system is applied to the linguistic learning task of German noun inflection.\\n\\nIntroduction\\n\\na simple  DATR theory\\n\\nVERB: [mor past] == (\"[mor root]\" _ed) [mor pres tense] == \"[mor root]\" [mor pres tense sing three] == (\"[mor root]\" _s).\\n\\nLove: \\t\\t [] == VERB [mor root] == love.\\n\\nInference of DATR theories consistency with respect to the input data\\n\\ncompleteness with respect to the input data\\n\\nstructuring of the observed data by inheritance relationships\\n\\nstructuring of the observed data by generalizing them\\n\\nAcquisition of inheritance relationships\\n\\nThe observed data constitute a trivial  DATR theory which forms the initial hypothesis H0 of the learning task. This  DATR theory is complete and consistent with respect to the input but does not meet the other two criteria. This section addresses the question of how a given  DATR theory can be transformed into another theory that contains more inheritance descriptors or changes the latter in order to structure the domain. The knowledge of how a given  DATR theory can be transformed into a new one with different inheritance descriptors is defined by rewrite rules of the following format:\\n\\nform of a transformation rule\\n\\nrule for local node inheritance\\n\\nBy means of transformation rules all the different kinds of inheritance descriptors can be obtained with the exception of evaluable paths. Evaluable paths capture dependencies between properties with different values and therefore cannot be acquired by transformation rules that crucially depend on the existence of sentences which have the same RHSs. Therefore, they have here been excluded from the learning task.\\n\\nAcquisition of default information\\n\\nWhile inheritance relationships are represented with the RHSs of sentences, default information is basically expressed through paths of the left-hand sides (LHSs), namely by paths that cover more than one fact. Since transformation rules leave the LHSs of sentences unchanged, an additional device is necessary that operates on LHSs of sentences. For this purpose a default-inference algorithm (DIA) was developed that reduces any given  DATR theory that does not (yet) contain default information, where \\'\\'reduction\\'\\' means shortening the paths of sentences (by cutting off a path suffix) or deletion of whole sentences. Since extensive generalization is normally a desirable property, the resulting theory must be (and indeed is) maximally reduced. In order to acquire a  DATR default theory that remains consistent with respect to the input data the DIA has to check that a reduction of a sentence does not lead to any conflicts with the remaining sentences of the theory. Conflicts can only arise between sentences which have the same node and path, because in all other cases the longest matching path can be determined. Therefore, if a given sentence is to be shortened, it has to be checked whether the theory already contains another sentence with the same node and shortened path. If it does, and if the other sentence has a different RHS, the first sentence cannot be shortened and must remain in the resulting theory.\\n\\nIf the other sentence has the same RHS, the first sentence can be removed from the theory altogether. If the theory does not contain the shortened sentence, the shortening is a legitimate operation since no conflicts can arise. The following additional restrictions must be imposed to guarantee a theory that is complete and consistent with respect to the input data. First of all, the sentences of a node have to be considered in descending order according to the length of their paths. This guarantees that for every sentence, the sentences it can conflict with are still contained in the theory and are not shortened or removed. For similar reasons, sentences can only be shortened by one element (the last) at a time. In the case of path references or node-path pairs, some additional tests are carried out since potential conflicts arise from  DATR\\'s mechanism governing the inheritance of path extensions.\\n\\nInference strategy size of a  DATR theory, measured by the absolute or average number of sentences per object (useful only for default theories)\\n\\nhomogeneity of RHSs, measured by the number of different RHSs\\n\\ncomplexity of RHSs (length of paths and sequences)\\n\\nrelationships between objects (relative number of node references)\\n\\nrelationships within objects (relative number of path references)\\n\\nInference of German noun inflection\\n\\ninput sentence for German noun inflection\\n\\nConclusion\\n\\nThis paper has presented an approach to the acquisition of linguistic knowledge from unstructured data. The approach is general in the sense that it is not restricted to a specific linguistic domain. This has been achieved by choosing the general representation language DATR for the representation of the acquired knowledge and by postulating a learning strategy that is tailor-made for this formalism. A similar approach could be conceived for other knowledge-representation formalisms (e.g. KL-ONE, cf. Brachman and Schmolze (1985)) which are more familiar within the artificial-intelligence paradigm. The system was applied to a learning task involving German noun inflection. The results are sensible in that nouns are grouped into classes according to their inflectional behavior in such a way that generalizations are captured. The acquired theories are restricted in that they do not make use of evaluable paths; thus, although they are clearly non-trivial, the theories constitute a proper sublanguage of  DATR. In the future, further applications of the system within different domains must be made in order to get a more detailed view of its possibilities. This pertains especially to the criteria for guiding the search and selecting best hypotheses.\\n\\nFootnotes\\n\\nLight (1994) addresses a related topic, the insertion of a new object (described with extensional  DATR sentences) into an existing  DATR theory. In contrast to our approach the assumption of a structured initial theory is made. Barg (1995) gives a full account of all transformation rules. The question of what constitutes a good  DATR theory is addressed later. Assume for the moment that it has been defined and that two  DATR theories can be compared with each other with respect to quality. This presupposes that the search space is finite, which is guaranteed by further restricting the transformation rules (more precisely the rules for creating abstract sentences).', metadata={'source': '../data/raw/cmplg-xml/9601001.xml'}),\n",
       " Document(page_content=\"Integrating Gricean and Attentional Constraints Introduction\\n\\nAnalysis of a Coded Corpus\\n\\nIn this section, I present the results of an analysis of all discourse anaphoric NPs in a corpus of spoken narratives directed at the question of how informative NPs are, relative to their contexts of occurrence. The first subsection describes the corpus and coding features. The next subsection presents results showing that discourse anaphoric NPs in the corpus, whether pronominal or phrasal, are rarely more informative than necessary, and if so, tend to occur at shifts in global discourse structure.\\n\\nData Coding\\n\\nChafe chafe80 identified three types of prosodic phrases from graphic displays of intonation contours. A period indicates a phrase terminated by a pitch fall, a question mark indicates final level or rising pitch, and a comma indicates phrase final--not sentence-final--intonation. The transcriptions here show all repeated and incomplete words and phrases, non-lexical articulations such as ``uh, um, tsk'', and vowel lengthening as indicated by `-'. Pause locations are shown as `[ps]'.\\n\\nThe size and number of segments per subject per narrative varied widely, from a rate of 5.5% to 41.3% (Avg.=16%), with segment widths ranging from 1 to 49 phrases (Avg.=5.9). Despite this variation, the number of times 4 to 7 subjects assigned boundaries in the same place was extremely significant (using Cochran's Q cochran50; cf. []). We took agreement among at least 4 subjects as the threshold for empirically validated boundaries.\\n\\nAnalysis of Informational Constraints\\n\\nPotentially over-specified NPs were sorted into four mutually exclusive categories--well-specified, segment onset, attentional shift, and reiterative. A potentially over-specified NP is well-specified if a less explicit form would have been ambiguous or unclear. The containing utterance is included in the context since the proposition expressed in an utterance can disambiguate a referring expression. A potentially over-specified NP that is not well-specified, but which occurs in the first utterance of a new segment, is classified as a segment onset. The segments in the coded Pear corpus arguably contain intra-segmental shifts of attention associated with changes in temporal aspect, or shifts in discourse reference time (for definitions assumed here, cf. []). The third category, attentional shift, consists of these cases. A fourth catch-all category includes, e.g., repetitions, repairs, contrastive NPs and unexplained cases.\\n\\nFocused Attribute Sets\\n\\nHow to order the focussed attribute sets for a given discourse entity is a topic for further investigation. Here, I simply assume that the three types of attribute sets mentioned above--where applicable--are in focus. I also assume that the focussed attribute sets of an entity (FAVe) are updated as the discourse progresses.\\n\\nModelling Informativeness of NPs\\n\\nThe data reported above indicates that in the Pear corpus, definite pronouns and phrasal NPs are rarely over-specified or over-determined. In this section, I describe a processing model to account for this observation. In the next section, I discuss how centering can be integrated with this model to account for under-specified pronouns, and certain over-specified phrasal NPs. First, I briefly review Dale's dale89 dale92 model, including his more recent work with Reiter dalereiter94. Then I modify this model to apply to understanding as well as generation; to include the current utterance in the context of evaluation; to apply informational constraints uniformly to pronouns and phrasal NPs; and to select modifiers on the basis of focused attribute-value pairs.\\n\\nDistinguishing Descriptions\\n\\nDale dale89 requires phrasal NPs to be distinguishing descriptions. As in Webber webber78, Dale assumes that the discourse model represents the discourse entities that have already been evoked, and the attribute-value pairs describing them. For any set of entities U, Dale dale89 defines a distinguishing description of an entity e in U to be a set of attribute-value pairs that are true of e, and of no other members of U. This enforces adequacy. He defines a minimal distinguishing description to be one where the cardinality of the attribute-value pairs cannot be reduced. This addresses efficiency.\\n\\nC_describe\\n\\nCentering and Informativeness\\n\\nCentering\\n\\nIntegrated Model\\n\\nU9: the man (e1) is in the tree (e3), U10: and the boy (e2) gets off the bicycle (e4),\\n\\nConclusion\\n\\nI have presented an analysis of discourse anaphoric phrasal NPs in a corpus of narrative monologues showing that pronouns and phrasal NPs are rarely over-specified. Future research should indicate to what degree this generalization applies to other genres and modalities. Centering predicts conditions under which an under-specified pronoun can be used, but says little about the interpretation of phrasal NPs. I have outlined a processing model that integrates the attentional constraints of centering with aspects of Grice's maxims of quantity and quality. For enforcing the maxim of quantity, I rely on Dale's algorithm for constructing distinguishing descriptions dale89 dale92, which I apply uniformly to pronouns and phrasal NPs for both generation and understanding. For enforcing the maxim of quality, I combine aspects of Dale Reiter's dalereiter94 preferred attributes with the construct of focussed attribute sets derived from the corpus analysis. In contrast to Dale  Reiter dalereiter94, distinguishing descriptions are evaluated using the current utterance context as a filter, and by instantiating the discourse context successively to the Cf list of the preceding utterance, then the current focus space, then other focus spaces, until a solution is found.\\n\\nAcknowledgements\\n\\nThis work was partly supported by NSF grant IRI-91-13064. Thanks to Robert Dale, Ehud Reiter, and Megumi Kameyama for valuable comments on ideas presented here.\\n\\nBibliography\\n\\nW. L. Chafe. The Pear Stories. Ablex Publishing Corporation, Norwood, NJ, 1980.\\n\\nW. G. Cochran. The comparison of percentages in matched samples. Biometrika, 37:256-266, 1950.\\n\\nR. Dale and E. Reiter. Computational interpretations of the Gricean maxims in the generation of referring expressions. To appear in Cognitive Science.\\n\\nR. Dale. Cooking up references. In Proceedings of the 27th Annual ACL, 1989.\\n\\nR. Dale. Generating Referring Expressions.. MIT Press, Cambridge, MA, 1992.\\n\\nH. P. Grice. Logic and conversation. In Syntax and Semantics, Vol. 3: Speech Acts, pages 41-58. Academic Press, New York, 1975.\\n\\nB. J. Grosz and C. L. Sidner. Attention, intentions and the structure of discourse. Computational Linguistics, 12:175-204, 1986.\\n\\nB. J. Grosz, A. K. Joshi, and S. Weinstein. Providing a unified account of definite noun phrases in discourse. In Proceedings of the 21st ACL, 1983.\\n\\nBarbara J. Grosz, A. K. Joshi, and S. Weinstein. Towards a computational theory of discourse interpretation, 1986. Ms.\\n\\nM. Kameyama, R. J. Passonneau, and M. Poesio. Temporal centering. In Proceedings of the 31st ACL, 1993.\\n\\nM/ Kameyama. Zero Anaphora: The Case of Japanese. PhD thesis, Stanford University, 1985.\\n\\nM. Kameyama. A property-sharing constraint in centering. In Proceedings of the 24st ACL, 1986.\\n\\nD. Litman and R. Passonneau. Combining multiple knowledge sources for discourse segmentation. In Proceedings of the 33rd ACL, 1995.\\n\\nD. Litman and R. Passonneau. Developing algorithms for discourse segmentation. In Working Notes of AAAI Spring Symposium Series on Empirical Methods in Discourse Interpretation and Generation, 1995.\\n\\nR. J. Passonneau and D. Litman. Intention-based segmentation: Reliability and correlation with linguistic cues. In Proceedings of the 31st ACL, 1993.\\n\\nR. J. Passonneau and D. Litman. Empirical Analysis of Three Dimensions of Spoken Discourse: Segmentation, Coherence and Linguistic Devices. In E. Hovy and D. Scott, eds., Burning Issues in Discourse. Springer Verlag, To appear.\\n\\nRebecca J. Passonneau. Getting at discourse referents. In Proceedings of the 27th ACL, 1989.\\n\\nR. J. Passonneau. Protocol for coding discourse referential noun phrases and their antecedents. Technical report, Columbia University, 1994.\\n\\nR. J. Passonneau. Frame shifts: Perlocutionary meaning and discourse reference. In Conference on Focus and Natural Language Processing, Kassel,  Germany, 1994.\\n\\nR. J. Passonneau. Interaction of the segmental structure of discourse with explicitness of discourse anaphora. In E. Prince, A. Joshi, and M. Walker, eds.,   Proceedings of the Workshop on Centering Theory in Naturally Occurring Discourse. Oxford University Press, To appear.\\n\\nR. Reichman. Getting Computers to Talk Like You and Me. MIT Press, Cambridge, Massachusetts, 1985.\\n\\nE. Reiter. Generating appropriate natural language object descriptions. PhD thesis, Harvard University, 1990.\\n\\nC. L. Sidner. Towards a computational theory of definite anaphora comprehension inEnglish discourse. Technical report, MIT AI Lab, 1979.\\n\\nB. Webber. A formal approach to discourse anaphora. Technical Report 3761, Bolt Beranek and Newman Inc., 1978.\\n\\nFootnotes\\n\\nThe work reported here was not supported by Bellcore. Cf. Reiter reiter90 for a discussion of problems in generating maximally efficient NPs using Dale's framework, and Dale  Reiter dalereiter94 for an argument that maximal efficiency is psychologically implausible. For simplicity, the utterance context represents certain semantic arguments as quoted strings. For simplicity, I am ignoring the difference between grammatical gender and sex.\", metadata={'source': '../data/raw/cmplg-xml/9505036.xml'}),\n",
       " Document(page_content=\"GRAMMAR SPECIALIZATION THROUGH ENTROPY THRESHOLDS\\n\\nExplanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees. This allows very much faster parsing and gives a lower error rate, at the price of a small loss in coverage. Previously, it has been necessary to specify the tree-cutting criteria (or operationality criteria) manually; here they are derived automatically from the training set and the desired coverage of the specialized grammar. This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values.\\n\\nBACKGROUND\\n\\nA problem not fully explored yet is how to arrive at an optimal choice of tree-cutting criteria. In the previous scheme, these must be specified manually, and the choice is left to the designer's intuitions. This article addresses the problem of automating this process and presents a method where the nodes to cut at are selected automatically using the information-theoretical concept of entropy. Entropy is well-known from physics, but the concept of perplexity is perhaps better known in the speech-recognition and natural-language communities. For this reason, we will review the concept of entropy at this point, and discuss its relation to perplexity.\\n\\nEntropy\\n\\nEntropy is a measure of disorder. Assume for example that a physical system can be in any of N states, and that it will be in state si with probability pi. The entropy S of that system is then\\n\\nIn this case the entropy is simply the logarithm of the number of states the system can be in.\\n\\nIf all words have equal probability, the entropy is the logarithm of the branching factor at this point in the input string.\\n\\nPerplexity\\n\\nHere\\n\\np(w1,...,wn) denotes the probability of the word string\\n\\nw1,...,wn.\\n\\nSince we cannot experimentally measure infinite limits, we terminate after a finite test string\\n\\nw1,...,wM, arriving at the measured perplexity Pm:\\n\\nTree entropy\\n\\nWe now turn to the task of calculating the entropy of a node in a parse tree. This can be done in many different ways; we will only describe two different ones here.\\n\\nThe complete table is given here:\\n\\nIf we want to calculate the entropy of a particular node in a parse tree, we can either simply use the phrase entropy of the RHS node, or take the sum of the entropies of the two phrases that are unified in this node. For example, the entropy when the RHS NP of the rule pp_prep_np is unified with the LHS of the rule np_det_n will in the former case be 1.10 and in the latter case be\\n\\n1.10 + 1.33 = 2.43.\\n\\nSCHEME OVERVIEW\\n\\nIn the following scheme, the desired coverage of the specialized grammar is prescribed, and the parse trees are cut up at appropriate places without having to specify the tree-cutting criteria manually: 1. Index the treebank in an and-or tree where the or-nodes correspond to alternative choices of grammar rules to expand with and the and-nodes correspond to the RHS phrases of each grammar rule. Cutting up the parse trees will involve selecting a set of or-nodes in the and-or tree. Let us call these nodes ``cutnodes''. 2. Calculate the entropy of each or-node. We will cut at each node whose entropy exceeds a threshold value. The rationale for this is that we wish to cut up the parse trees where we can expect a lot of variation i.e. where it is difficult to predict which rule will be resolved on next. This corresponds exactly to the nodes in the and-or tree that exhibit high entropy values. 3. The nodes of the and-or tree must be partitioned into equivalence classes dependent on the choice of cutnodes in order to avoid redundant derivations at parse time. Thus, selecting some particular node as a cutnode may cause other nodes to also become cutnodes, even though their entropies are not above the threshold. 4. Determine a threshold entropy that yields the desired coverage. This can be done using for example interval bisection. 5. Cut up the training examples by matching them against the and-or tree and cutting at the determined cutnodes.\\n\\nDETAILED SCHEME\\n\\nFirst, the treebank is partitioned into a training set and a test set. The training set will be indexed in an and-or tree and used to extract the specialized rules. The test set will be used to check the coverage of the set of extracted rules.\\n\\nIndexing the treebank\\n\\nThen, the set of implicit parse trees is stored in an and-or tree. The parse trees have the general form of a rule identifier Id dominating a list of subtrees or a word of the training sentence. From the current or-node of the and-or tree there will be arcs labelled with rule identifiers corresponding to previously stored parse trees. From this or-node we follow an arc labelled Id, or add a new one if there is none. We then reach (or add) an and-node indicating the RHS phrases of the grammar rule named Id. Here we follow each arc leading out from this and-node in turn to accommodate all the subtrees in the list. Each such arc leads to an or-node. We have now reached a point of recursion and can index the corresponding subtree. The recursion terminates if Id is the special rule identifier lex and thus dominates a word of the training sentence, rather than a list of subtrees.\\n\\nFinding the cutnodes\\n\\nNext, we find the set of nodes whose entropies exceed a threshold value. First we need to calculate the entropy of each or-node. We will here describe three different ways of doing this, but there are many others. Before doing this, though, we will discuss the question of redundancy in the resulting set of specialized rules.\\n\\nWe must equate the cutnodes that correspond to the same type of phrase. This means that if we cut at a node corresponding to e.g. an NP, i.e. where the arcs incident from it are labelled with grammar rules whose left-hand-sides are NPs, we must allow all specialized NP rules to be potentially applicable at this point, not just the ones that are rooted in this node. This requires that we by transitivity equate the nodes that are dominated by a cutnode in a structurally equivalent way; if there is a path from a cutnode c1 to a node n1 and a path from a cutnode c2 to a node n2 with an identical sequence of labels, the two nodes n1 and n2 must be equated. Now if n1 is a cutnode, then n2 must also be a cutnode even if it has a low entropy value. The following iterative scheme accomplishes this: Function N[*(N0]) 1. i := 0; 2. Repeat i := i+1;\\n\\nN[i] := N(N[i\\n\\n\\n\\n1]);\\n\\n3.\\n\\nUntil\\n\\nN[i] = N[i-1]4. Return N[i]; Here N(N[j]) is the set of cutnodes N[j] augmented with those induced in one step by selecting N[j] as the set of cutnodes. In practice this was accomplished by compiling an and-or graph from the and-or tree and the set of selected cutnodes, where each set of equated nodes constituted a vertex of the graph, and traversing it.\\n\\nIn the simplest scheme for calculating the entropy of an or-node, only the RHS phrase of the parent rule, i.e. the dominating and-node, contributes to the entropy, and there is in fact no need to employ an and-or tree at all, since the tree-cutting criterion becomes local to the parse tree being cut up.\\n\\nIn a slightly more elaborate scheme, we sum over the entropies of the nodes of the parse trees that match this node of the and-or tree. However, instead of letting each daughter node contribute with the full entropy of the LHS phrase of the corresponding grammar rule, these entropies are weighted with the relative frequency of use of each alternative choice of grammar rule.\\n\\nIn a third version of the scheme, the relative frequencies of the daughters of the or-nodes are used directly to calculate the node entropy:\\n\\nFinding the threshold\\n\\nActually, we also need to handle the boundary case where no assignment of cutnodes gives the required coverage. Likewise, the coverages of the upper and lower bound may be far apart even though the entropy difference is small, and vice versa. These problems can readily be taken care of by modifying the termination criterion, but the solutions have been omitted for the sake of clarity.\\n\\nRetrieving the specialized rules\\n\\nADDITIONAL CONSTRAINTS\\n\\nAs mentioned at the beginning, the specialized grammar is compiled into LR parsing tables. Just finding any set of cutnodes that yields the desired coverage will not necessarily result in a grammar that is well suited for LR parsing. In particular, LR parsers, like any other parsers employing a bottom-up parsing strategy, do not blend well with empty productions. This is because without top-down filtering, any empty production is applicable at any point in the input string, and a naive bottom-up parser will loop indefinitely. The LR parsing tables constitute a type of top-down filtering, but this may not be sufficient to guarantee termination, and in any case, a lot of spurious applications of empty productions will most likely take place, degrading performance. For these reasons we will not allow learned rules whose RHSs are empty, but simply refrain from cutting in nodes of the parse trees that do not dominate at least one lexical lookup.\\n\\nEven so, the scheme described this far is not totally successful, the performance is not as good as using hand-coded tree-cutting criteria. This is conjectured to be an effect of the reduction lengths being far too short. The first reason for this is that for any spurious rule reduction to take place, the corresponding RHS phrases must be on the stack. The likelihood for this to happen by chance decreases drastically with increased rule length. A second reason for this is that the number of states visited will decrease with increasing reduction length. This can most easily be seen by noting that the number of states visited by a deterministic LR parser equals the number of shift actions plus the number of reductions, and equals the number of nodes in the corresponding parse tree, and the longer the reductions, the more shallow the parse tree.\\n\\nThe hand-coded operationality criteria result in an average rule length of four, and a distribution of reduction lengths that is such that only 17 percent are of length one and 11 percent are of length two. This is in sharp contrast to what the above scheme accomplishes; the corresponding figures are about 20 or 30 percent each for lengths one and two.\\n\\nAn attempted solution to this problem is to impose restrictions on neighbouring cutnodes. This can be done in several ways; one that has been tested is to select for each rule the RHS phrase with the least entropy, and prescribe that if a node corresponding to the LHS of the rule is chosen as a cutnode, then no node corresponding to this RHS phrase may be chosen as a cutnode, and vice versa. In case of such a conflict, the node (class) with the lowest entropy is removed from the set of cutnodes.\\n\\nEXPERIMENTAL RESULTS\\n\\nWith the mixed entropy scheme it seems important to include the restrictions on neighbouring cutnodes, while this does not seem to be the case with the RHS phrase entropy scheme. A potential explanation for the significantly higher average parsing times for all grammars extracted using the induced tree-cutting criteria is that these are in general recursive, while the hand-coded criteria do not allow recursion, and thus only produce grammars that generate finite languages.\\n\\nAlthough the hand-coded tree-cutting criteria are substantially better than the induced ones, we must remember that the former produce a grammar that in median allows 60 times faster processing than the original grammar and parser do. This means that even if the induced criteria produce grammars that are a factor two or three slower than this, they are still approximately one and a half order of magnitude faster than the original setup. Also, this is by no means a closed research issue, but merely a first attempt to realize the scheme, and there is no doubt in my mind that it can be improved on most substantially.\\n\\nSUMMARY\\n\\nThis article proposes a method for automatically finding the appropriate tree-cutting criteria in the EBG scheme, rather than having to hand-code them. The EBG scheme has previously proved most successful for tuning a natural-language grammar to a specific application domain and thereby achieve very much faster parsing, at the cost of a small reduction in coverage.\\n\\nInstruments have been developed and tested for controlling the coverage and for avoiding a large number of short reductions, which is argued to be the main source to poor parser performance. Although these instruments are currently slightly too blunt to enable producing grammars with the same high performance as the hand-coded tree-cutting criteria, they can most probably be sharpened by future research, and in particular refined to achieve the delicate balance between high coverage and a distribution of reduction lengths that is sufficiently biased towards long reductions. Also, banning recursion by category specialization, i.e. by for example distinguishing NPs that dominate other NPs from those that do not, will be investigated, since this is believed to be an important ingredient in the version of the scheme employing hand-coded tree-cutting criteria.\\n\\nACKNOWLEDGEMENTS\\n\\nThis research was made possible by the basic research programme at the Swedish Institute of Computer Science (SICS). I wish to thank Manny Rayner of SRI International, Cambridge, for help and support in matters pertaining to the treebank, and for enlightening discussions of the scheme as a whole. I also wish to thank the NLP group at SICS for contributing to a very conductive atmosphere to work in, and in particular Ivan Bretan for valuable comments on draft versions of this article. Finally, I wish to thank the anonymous reviewers for their comments.\\n\\nBibliography\\n\\nHiyan Alshawi, editor.\\n\\nThe Core Language Engine,\\n\\nMIT Press 1992.\\n\\nFred Jelinek. ``Self-Organizing Language Models for Speech Recognition'', in Readings in Speech Recognition, pp. 450-506, Morgan Kaufmann 1990.\\n\\nTom M. Mitchell, Richard M. Keller and Smadar T. Kedar-Cabelli. ``Explanation-Based Generalization: A Unifying View'', in Machine Learning 1, No. 1, pp. 47-80, 1986.\\n\\nJ. Ross Quinlan. ``Induction of Decision Trees'', in Machine Learning 1, No. 1, pp. 81-107, 1986.\\n\\nM. Rayner, H. Alshawi, I. Bretan, D. Carter, V. Digalakis, B. Gambck, J. Kaja, J. Karlgren, B. Lyberg, P. Price, S. Pulman and C. Samuelsson. ``A Speech to Speech Translation System Built From Standard Components'', in Procs. ARPA Workshop on Human Language Technology, Princeton, NJ 1993.\\n\\nChrister Samuelsson. Fast Natural-Language Parsing Using Explanation-Based Learning, PhD thesis, Royal Institute of Technology, Stockholm, Sweden 1994.\\n\\nChrister Samuelsson. ``Notes on LR Parser Design'' to appear in Procs. 15th International Conference on Computational Linguistics, Kyoto, Japan 1994.\\n\\nChrister Samuelsson and Manny Rayner. ``Quantitative Evaluation of Explanation-Based Learning as an Optimization Tool for a Large-Scale Natural Language System'', in Procs. 12th International Joint Conference on Artificial Intelligence, pp. 609-615, Sydney, Australia 1991.\\n\\nFootnotes\\n\\nOther more easily obtainable publications about this are in preparation. These are usually referred to as ``operationality criteria'' in the EBG literature. Since there is only one alternative, namely a lexical lookup. In fact, the scheme could easily be extended to encompass including lexical lookups of particular words into the specialized rules by distinguishing lexical lookups of different words; the entropy would then determine whether or not to cut in a node corresponding to a lookup, just as for any other node, as is described in the following. This can most easily be seen as follows: Imagine two identical, but different portions of the and-or tree. If the roots and leaves of these portions are all selected as cutnodes, but the distribution of cutnodes within them differ, then we will introduce multiple ways of deriving the portions of the parse trees that match any of these two portions of the and-or tree. Defined slightly differently, as described below. albeit in exponential time\", metadata={'source': '../data/raw/cmplg-xml/9405022.xml'}),\n",
       " Document(page_content=\"ProFIT: Prolog with Features, Inheritance and Templates\\n\\nProFIT is an extension of Standard Prolog with Features, Inheritance and Templates. ProFIT allows the programmer or grammar developer to declare an inheritance hierarchy, features and templates. Sorted feature terms can be used in ProFIT programs together with Prolog terms to provide a clearer description language for linguistic structures. ProFIT compiles all sorted feature terms into a Prolog term representation, so that the built-in Prolog term unification can be used for the unification of sorted feature structures, and no special unification algorithm is needed. ProFIT programs are compiled into Prolog programs, so that no meta-interpreter is needed for their execution. ProFIT thus provides a direct step from grammars developed with sorted feature terms to Prolog programs usable for practical NLP systems.\\n\\nIntroduction\\n\\na linguistic description\\n\\na processing model (parser, generator etc.)\\n\\nIn the past decade, there have been diverging trends in the area of linguistic descriptions and in the area of processing models. Most large-scale linguistic descriptions make use of sorted feature formalisms, but implementations of these formalisms are in general too slow for building practically usable NLP systems. Most of the progress in constructing efficient parsers and generators has been based on logic grammars that make use of ordinary Prolog terms. We provide a general tool that brings together these developments by compiling sorted feature terms into a Prolog term representation, so that techniques from logic programming and logic grammars can be used to provide efficient processing models for sorted feature grammars.\\n\\nIn this introductory section, we discuss the advantages of sorted feature formalisms, and of the logic grammar paradigm, and show how the two developments can be combined. The following sections describe the ProFIT language which provides sorted feature terms for Prolog, and its implementation.\\n\\nGrammar Development in Sorted Feature Formalisms\\n\\nSorted feature formalisms are often used for the development of large-coverage grammars, because they are very well suited for a structured description of complex linguistic data. Sorted feature terms have several advantages over Prolog terms as a representation langauge.\\n\\n1. They provide a compact notation. Features that are not instantiated can be omitted; there is no need for anonymous variables. 2. Features names are mnemonic, argument positions are not. 3. Adding a new feature to a sort requires one change in a declaration, whereas adding an argument to a Prolog functor requires changes (mostly insertion of anonymous variables) to every occurence of the functor. 4. Specification of the subsort relationship is more convenient than constructing Prolog terms which mirror these subsumption relationships.\\n\\nEfficient Processing based on Logic Grammars\\n\\nMuch work on efficient processing algorithms has been done in the logic grammar framework. This includes work on\\n\\nCompiling grammars into efficient parsers and generators: compilation of DCGs into (top-down) Prolog programs, left-corner parsers (BUP), LR parsers, head-corner parsers, and semantic-head driven generators.\\n\\nUsing coroutining (dif, freeze etc.) to provide more efficient processing models\\n\\nPartial deduction techniques to produce more efficient grammars\\n\\nUsing Prolog and its indexing facilities to build up a lexicon database\\n\\nSince much of this work involves compilation of grammars into Prolog programs, such programs can immediately benefit from any improvements in Prolog compilers (for example the tabulation provided by XSB Prolog can provide a more efficient implementation of charts) which makes the grammars more usable for NLP systems.\\n\\nCombining Logic Grammars and Sorted Feature Formalisms\\n\\nProFIT is not a grammar formalism, but rather extends any grammar formalism in the logic grammar tradition with the expressive power of sorted feature terms.\\n\\nThe ProFIT Language\\n\\nThe set of ProFIT programs is a superset of Prolog programs. While a Prolog program consists only of definite clauses (Prolog is an untyped language), a ProFIT program consists of datatype declarations and definite clauses. The clauses of a ProFIT program can make use of the datatypes (sorts, features, templates and finite domains) that are introduced in the declarations. A ProFIT program consists of:\\n\\nDeclarations for sorts\\n\\nDeclarations for features\\n\\nDeclarations for templates\\n\\nDeclarations for finite domains\\n\\nDefinite clauses\\n\\nSort Declarations\\n\\nIn addition to unsorted Prolog terms, ProFIT allows sorted feature terms, for which the sorts and features must be declared in advance.\\n\\nEvery sort must only be defined once, i.e. it can appear only once on the left-hand side of the connective ].\\n\\nThe immediate subsorts of top can be declared to be extensional. Two terms which are of an extensional sort are only identical if they have a most specific sort (which has no subsort), and if all features are instantiated to ground terms. If a sort is not declared as extensional, it is intensional. Two intensional terms are identical only if they have been unified.\\n\\nFeature Declarations\\n\\nThis notion of appropriateness is desirable for structuring linguistic knowledge, as it prevents the ad-hoc introduction of features, and requires a careful design of the sort and feature hierarchy. Appropriateness is also a prerequisite for compilation of feature terms into fixed-arity Prolog terms.\\n\\nThe following declaration defines a sort binary_tree with subsorts leaf and internal_node. The sort binary tree introduces the feature label and its subsort adds the features left_daughter and right_daughter. If a sort has subsorts and introduces features, these are combined in one declaration.\\n\\nbinary_tree ]  [leaf,internal_node]\\n\\nintro [label].\\n\\ninternal_node\\n\\nintro [left_daughter:binary_tree,\\n\\nright_daughter:binary_tree].\\n\\nSorted Feature Terms\\n\\nOn the basis of the declarations, sorted feature terms can be used in definite clauses in addition to and in combination with Prolog terms. A Prolog term can have a feature term as its argument, and a feature can have a Prolog term as its value. This avoids potential interface problems between different representations, since terms do not have to be translated between different languages. As an example, semantic representations in first-order terms can be used as feature values, but do not need to be encoded as feature terms.\\n\\nThe following clauses (based on  HPSG) state that a structure is saturated if its subcat value is the empty list, and that a structure satisfies the Head Feature Principle (hfp) if its head features are identical with the head features of its head daughter. Note that these clauses provide a concise notation because uninstantiated features can be omitted, and the sorts of structures do not have to be specified explicitly because they can be infered from use of the features.\\n\\nsaturated( synsem!local!cat!subcat! [elist ).\\n\\nhfp( synsem!local!cat!head!X\\n\\ndtrs!head_dtr!synsem!local!cat!head!X ).\\n\\nNote that conjunction also provides the possiblity to tag a Prolog term or feature term with a variable (Var  Term).\\n\\nFeature Search\\n\\nIn the organisation of linguistic knowledge, feature structures are often deeply embedded, due to the need to group together sets of features whose value can be structure-shared. In the course of grammar development, it is often necessary to change the ``location'' of a feature in order to get the right structuring of information.\\n\\nSuch a  change of the ``feature geometry'' makes it necessary to change the path in all references to a feature. This is often done by introducing templates whose sole purpose is the abbreviation of a path to a feature.\\n\\nProFIT provides a mechanism to search for paths to features automatically provided that the sortal restrictions for the feature values are strong enough to ensure that there is a unique minimal path. A path is minimal if it does not contain any repeated features or sorts.\\n\\nThe following clause makes use of feature search to express the Head Feature Principle (hfp).\\n\\nhfp( sign]]]head!X\\n\\ndtrs!head_dtr! ]]]head!X ).\\n\\nWhile this abbreviation for feature paths is new for formal description languages, similar abbreviatory conventions are often used in linguistic publications. They are easily and unambiguously understood if there is only one unique path to the feature which is not embedded in another structure of the same sort.\\n\\nTemplates\\n\\nThe purpose of templates is to give names to frequently used structures. In addition to being an abbreviatory device, the template mechanism serves three other purposes.\\n\\nAbstraction and interfacing by providing a fixed name for a value that may change,\\n\\nPartial evaluation,\\n\\nFunctional notation that can make specifications easier to understand.\\n\\nTemplates are called by using the template name prefixed with @ in a ProFIT term.\\n\\nAbstraction makes it possible to change data structures by changing their definition only at one point. Abstraction also ensures that databases (e.g. lexicons) which make use of these abstractions can be re-used in different kinds of applications where different datastructures represent these abstractions.\\n\\nAbstraction through templates is also useful for defining interfaces between grammars and processing modules. If semantic processing must access the semantic representations of different grammars, this can be done if the semantic module makes use of a template defined for each grammar that indicates where in the feature structure the semantic information is located, as in the following example for  HPSG.\\n\\nsemantics(synsem!local!cont!Sem) := Sem.\\n\\nPartial evaluation is achieved when a structure (say a principle of a grammar) is represented by a template that gets expanded at compile time, and does not have to be called as a goal during processing.\\n\\nWe show the use of templates for providing functional notation by a simple example, in which the expression @first(X) stands for the first element of list X, and  @rest(X) stands for the tail of list X, as defined by the following template definition.\\n\\nfirst([First|Rest]) := First.\\n\\nrest([First|Rest])  := Rest.\\n\\nThe member relation can be defined with the following clauses, which correspond very closely to the natural-language statement of the member relation given as comments. Note that expansion of the templates yields the usual definition of the member relation in Prolog.\\n\\n% The first element of a list % is a member of the list. member(@first(List),List). % Element is a member of a list % if it is a member of the rest of the list member(Element,List) :- member(Element,@rest(List)).\\n\\nThe expressive power of an n-place template is the same as that of an n+1 place fact.\\n\\nDisjunction\\n\\nDisjunction in the general case cannot be encoded in a Prolog term representation. Since a general treatment of disjunction would involve too much computational overhead, we provide disjunctive terms only as syntactic sugar. Clauses containing disjunctive terms are compiled to several clauses, one for each consistent combination of disjuncts. Disjunctive terms make it possible to state facts that belong together in one clause, as the following formulation of the Semantics Principle (sem_p) of  HPSG, which states that the content value of a head-adjunct structure is the content value of the adjunct daughter, and the content value of the other headed structures (head-complement, head-marker, and head-filler structure) is the content value of the head daughter.\\n\\nsem_p(  ([head_adj ]]]cont!X  ]]]adj_dtr! ]]]cont!X ) or ( (   [head_comp or [head_marker or [head_filler ) ]]]cont!Y  ]]]head_dtr! ]]]cont!Y ) ).\\n\\nFor disjunctions of atoms, there exists a Prolog term representation, which is described below.\\n\\nFinite Domains\\n\\nConsider the agreement features person (with values 1, 2 and 3) and number (with values sg and pl). For the two features together there are six possible combinations of values (1sg, 2sg, 3sg, 1pl, 2pl, 3pl). Any subset of this set of possible values can be encoded as one Prolog term. The following example shows the declaration needed for this finite domain, and some clauses that refer to subsets of the possible agreement values by making use of the logical connectives ~  (negation),  (conjunction), or (disjunction).\\n\\nagr fin_dom [1,2,3] * [sg,pl]. verb(sleeps,3sg). verb(sleep, ~(3sg)). verb(am,    1sg). verb(is,    3sg). verb(are,   2 or pl). np('I',     1sg). np(you,     2@agr).\\n\\nThis kind of encoding is only applicable to domains which have no coreferences reaching into them, in the example only the agreement features as a whole can be coreferent with other agreement features, but not the values of person or number in isolation. This kind of encoding is useful to avoid the creation of choice points for the lexicon of languages where one inflectional form may correspond to different feature values.\\n\\nCyclic Terms\\n\\nUnlike Prolog, the concrete syntax of ProFIT allows to write down cyclic terms by making use of conjunction:\\n\\nX  f(X).\\n\\nCyclic terms constitute no longer a theoretical or practical problem in logic programming, and almost all modern Prolog implementations can perform their unification (although they can't print them out). Cyclic terms arise naturally in NLP through unification of non-cyclic terms, e.g., the Subcategorization Principle and the Spec Principle of  HPSG.\\n\\nProFIT supports cyclic terms by being able to print them out as solutions. In order to do this, the dreaded occur check must be performed. Since this must be done only when results are printed out as ProFIT terms, it does not affect the runtime performance.\\n\\nFrom ProFIT terms to Prolog terms Compilation of Sorted Feature Terms The Prolog representation of a sort is an instance of the Prolog representation of its supersorts.\\n\\nFeatures are represented by arguments. If a feature is introduced by a subsort, then the argument is added to the term that further instantiates its supersort.\\n\\nMutually exclusive sorts have different functors at the same argument position, so that their unification fails.\\n\\nWe illustrate these principles for compiling sorted feature terms into Prolog terms with an example from  HPSG. The following declaration states that the sort sign has two mutually exclusive subsorts lexical and phrasal and introduces four features.\\n\\nsign ] [lexical,phrasal]\\n\\nintro [phon,\\n\\nsynsem,\\n\\nqstore,\\n\\nretrieved].\\n\\n$sign(Var,LexPhras,Phon,Synsem,Qstore,Retriev)\\n\\nThe following declaration introduces two sort hierarchy ``dimensions'' for subsorts of phrasal, and one new feature. The corresponding Prolog term representation instantiates the representation for the sort sign further, and leaves argument positions that can be instantiated further by the subsorts of phrasal, and for the newly introduced feature daughters.\\n\\nphrasal ] [headed,non_headed]\\n\\n\\n\\n[decl,int,rel]\\n\\nintro [daughters].\\n\\n$sign(Var,\\n\\n$phrasal(Phrasesort,Clausesort,Dtrs),\\n\\nPhon,\\n\\nSynsem,\\n\\nQstore,\\n\\nRetrieved)\\n\\nCompilation of Finite Domains\\n\\n$agr(1,A,B,C,D,E,0)\\n\\nNote that the first and last argument must be different. In the example, this is achieved by instantiation with different atoms, but an inequality constraint (Prolog  II's dif) would serve the same purpose. We assume that the domain element 1sg corresponds to the first and second arguments, 2sg to the second and third arguemnts, and so on, as illustrated below.\\n\\n$agr(      1   ,          A   ,          B   ,          C   ,          D   ,          E   ,          0  )  \\t\\t   1sg \\t\\t   2sg \\t\\t   3sg\\t\\t   1pl \\t\\t   2pl \\t\\t   3pl\\n\\nA domain description is translated into a Prolog term by unifying the argument pairs that are excluded by the description. For example, the domain description 2 or pl excludes 1sg and 3sg, so that the the first and second argument are unified (1sg), as well as the third and fourth (3sg).\\n\\n$agr(1,1,X,X,D,E,0)\\n\\nWhen two such Prolog terms are unified, the union of their excluded elements is computed by unificatation, or conversely the intersection of the elements which are in the domain description. The unification of two finite domain terms is successful as long as they have at least one element in common. When two terms are unified which have no element in common, i.e., they exclude all domain elements, then unification fails because all arguments become unified with each other, including the first and last arguments, which are different.\\n\\nImplementation\\n\\nProFIT has been implemented in Quintus and Sicstus Prolog, and should run with any Prolog that conforms to or extends the proposed ISO Prolog standard.\\n\\nAll facilities needed for the development of application programs, for example the module system and declarations (dynamic, multifile etc.) are supported by ProFIT.\\n\\nCompilation of a ProFIT file generates two kinds of files as output. 1. Declaration files that contain information for compilation, derived from the declarations. 2. A program file (a Prolog program) that contains the clauses, with all ProFIT terms compiled into their Prolog term representation.\\n\\nThe program file is compiled on the basis of the declaration files. If the input and output of the program (the exported predicates of a module) only make use of Prolog terms, and feature terms are only used for internal purposes, then the program file is all that is needed. This is for example the case with a grammar that uses feature terms for grammatical description, but whose input and output (e.g. graphemic form and logical form) are represented as normal Prolog terms.\\n\\nDeclarations and clauses can come in any order in a ProFIT file, so that the declarations can be written next to the clauses that make use of them. Declarations, templates and clauses can be distributed across several files, so that it becomes possible to modify clauses without having to recompile the declarations, or to make changes to parts of the sort hierarchy without having to recompile the entire hierarchy.\\n\\nSort checking can be turned off for debugging purposes, and feature search and handling of cyclic terms can be turned off in order to speed up the compilation process if they are not needed.\\n\\nError handling is currently being improved to give informative and helpful warnings in case of undefined sorts, features and templates, or cyclic sort hierarchies or template definitions.\\n\\nFor the development of ProFIT programs and grammars, it is necessary to give input and output and debugging information in ProFIT terms, since the Prolog term representation is not very readable. ProFIT provides a user interface which\\n\\naccepts queries containing ProFIT terms, and translates them into Prolog queries,\\n\\nconverts the solutions to the Prolog query back into ProFIT terms before printing them out,\\n\\nprints out debugging information as ProFIT terms.\\n\\nWhen a solution or debugging information is printed out, uninstantiated features are omitted, and shared structures are printed only once and represented by variables on subsequent occurences.\\n\\nA pretty-printer is provided that produces a neatly formatted screen output of ProFIT terms, and is configurable by the user. ProFIT terms can also be output in L[A]TEX format, and an interface to the graphical feature editor Fegramed is foreseen.\\n\\nIn order to give a rough idea of the efficiency gains of a compilation into Prolog terms instead of using a feature term unification algorithm implemented on top of Prolog, we have compared the runtimes with ALE and the Eisele-Drre algorithm for unsorted feature unification for the following tasks: (i) unification of (unsorted) feature structures, (ii) unification of inconsistent feature structures (unification failure), (iii) unification of sorts, (iv) lookup of one of 10000 feature structures (e.g. lexical items), (v) parsing with an  HPSG grammar to provide a mix of the above tasks.\\n\\nThe ProFIT system and documentation are available free of charge by anonymous ftp (server: ftp.coli.uni-sb.de, directory: pub/profit).\\n\\nConclusion\\n\\nProFIT allows the use of sorted feature terms in Prolog programs and Logic Grammars without sacrificing the efficiency of Prolog's term unification. It is very likely that the most efficient commercial Prolog systems, which provide a basis for the implementation of NLP systems, will conform to the proposed ISO standard. Since the ISO standard includes neither inheritance hierarchies nor feature terms (which are indispensible for the development of large grammars, lexicons and knowledge bases for NLP systems), a tool like ProFIT that compiles sorted feature terms into Prolog terms is useful for the development of grammars and lexicons that can be used for applications. ProFIT is not a grammar formalism, but rather aims to extend current and future formalisms and processing models in the logic grammar tradition with the expressive power of sorted feature terms. Since the output of ProFIT compilation are Prolog programs, all the techniques developed for the optimisation of logic programs (partial evaluation, tabulation, indexing, program transformation techniques etc.) can be applied straightforwardly to improve the performance of sorted feature grammars.\\n\\nAcknowledgements Deutsche Forschungsgemeinschaft, Special Research Division 314 ``Artificial Intelligence - Knowledge-Based Systems'' through project N3 ``Bidirectional Linguistic Deduction'' (BiLD), where it is used to compile typed feature grammars into logic grammars, for which bidirectional NLP algorithms are developed, and\\n\\nCray Systems (formerly PE-Luxembourg), with whom we had fruitful interaction concerning the future development of the ALEP system.\\n\\nBibliography\\n\\nHassan At-Kaci and Patrick Lincoln. 1989. Life, a natural language for natural language. T. A. Informations, 30(1-2):37 - 67.\\n\\nH. Alshawi, D. J. Arnold, R. Backofen, D. M. Carter, J. Lindop, K. Netter, J. Tsujii, and H. Uszkoreit. 1991. Eurotra 6/1: Rule formalism and virtual machine design study -- Final report. Technical report, SRI International, Cambridge.\\n\\nHiyan Alshawi, editor.\\n\\n1991.\\n\\nThe Core Language Engine.\\n\\nMIT Press.\\n\\nChris Brew. 1991. Systemic classification and its efficiency. Computational Linguistics, 17(4):375 - 408.\\n\\nBob Carpenter. 1992. The logic of typed feature structures. Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, Cambridge.\\n\\nMichael Covington. 1989. GULP 2.0: an extension of Prolog for unification-based grammar. Technical Report AI-1989-01, Advanced Computational Methods Center, University of Georgia.\\n\\nJochen Drre and Michael Dorna. 1993. CUF - A formalism for linguistic knowledge representation. In Jochen Drre, editor, Computational Aspects of Constraint-Based Linguistic Description. Deliverable R1.2.A. DYANA-2 - ESPRIT Basic Research Project 6852.\\n\\nMartin Emele and Rmi Zajac. 1990. Typed unification grammars. In Proceedings of the 13th International Conference on Computational Linguistics, Helsinki.\\n\\nGregor Erbach. 1994. Multi-dimensional inheritance. In H. Trost, editor, Proceedings of KONVENS '94, pages 102 - 111, Vienna. Springer.\\n\\nSusan Beth Hirsh. 1986. P-PATR: A compiler for unification-based grammars. Master's thesis, Stanford University, Stanford, CA.\\n\\nDraft ISO Standard for the Prolog language, ISO/IEC JTC1 SC22 WG17 N110 ``Prolog: Part 1, General core''.\\n\\nSuresh Manandhar. 1994. An attributive logic of set descriptions and set operations. In 32nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 255 - 262, Las Cruces, NM.\\n\\nSuresh Manandhar. 1995. Deterministic consistency checking of LP constraints. In Seventh Conference of the European Chapter of the Association for Computational Linguistics (EACL), Dublin.\\n\\nJohannes Matiasek. 1994. Conditional constraints in a CLP-based HPSG implementation. In Harald Trost, editor, KONVENS '94, pages 230 - 239, Vienna.\\n\\nChristopher S. Mellish. 1988. Implementing systemic classification by unification. Computational Linguistics, 14(1):40-51.\\n\\nChristopher S. Mellish. 1992. Term-encodable description spaces. In D. R. Brough, editor, Logic Programming: New Frontiers, pages 189 - 207. Intellect, Oxford.\\n\\nGnter Neumann and Gertjan van Noord. 1992. Self-monitoring with reversible grammars. In Proceedings of the 14th International Conference on Computational Linguistics, Nantes, F.\\n\\nGnter Neumann. 1994. A Uniform Computational Model for Natural Language Parsing and Generation. Ph.D. thesis, Universitt des Saarlandes, Saabrcken.\\n\\nChrister Samuelsson. 1994. Fast Natural-Language Parsing Using Explanation-Based Learning. Ph.D. thesis, The Royal Institute of Technology and Stockholm University, Stockholm.\\n\\nAndreas P. Schter. 1993. Compiling feature structures into terms: A case study in Prolog. Technical Report RP-55, University of Edinburgh, Centre for Cognitive Science.\\n\\nGert Smolka, Martin Henz, and Jrg Wrtz. 1995. Object-oriented concurrent constraint programming in Oz. In P. van Hentenryck and V. Saraswat, editors, Principles and Practice of Constraint Programming, chapter 2, pages 27-48. The MIT Press.\\n\\n160mm\\n\\nAppendix: BNF for ProFIT Terms\\n\\nPFT :=  [Sort                 [1. Term of a sort Sort                        ] | Feature!PFT           [2. Feature-Value pair                         ] | PFT  PFT             [3. Conjunction of terms                       ] | PROLOGTERM            [4. Any Prolog term                            ] | FINDOM                [5. Finite Domain term, BNF see below          ] | @Template             [6. Template call                              ] | ` PFT                 [7. Quoted term, is not translated             ] | `` PFT                [8. Double-quoted, main functor not translated ] | ]]]Feature!PFT        [9. Search for a feature                       ] | Sort]]]Feature!PFT    [10. short for [Sort  ]]]Feature!PFT           ] | PFT or PFT            [11. Disjunction; expands to multiple terms     ]\\n\\nFINDOM :=  FINDOM@FiniteDomainName\\n\\n| ~FINDOM\\n\\n| FINDOM  FINDOM\\n\\n| FINDOM or FINDOM\\n\\n| Atom\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9502003.xml'}),\n",
       " Document(page_content=\"Learning Part-of-Speech Guessing Rules from Lexicon: Extension to Non-Concatenative Operations Introduction\\n\\nThere are two kinds of word-guessing rules employed by the cascading guesser: morphological rules and ending guessing rules. Morphological word-guessing rules describe how one word can be guessed given that another word is known. In English, as in many other languages, morphological word formation is realised by affixation: prefixation and suffixation, so there are two kinds of morphological rules: suffix rules (A[s]) -- rules which are applied to the tail of a word, and prefix rules (A[p]) -- rules which are applied to the beginning of a word. For example, the prefix rule:\\n\\nA[p] :   [un (VBD VBN) (JJ)]\\n\\nA[s] :    [ed (NN VB) (JJ VBD VBN)]\\n\\nUnlike morphological guessing rules, ending-guessing rules do not require the main form of an unknown word to be listed in the lexicon. These rules guess a  POS-class for a word just on the basis of its ending characters and without looking up its stem in the lexicon. For example, an ending-guessing rule\\n\\nA[e]:  [ing -\\n\\n\\n\\n(JJ NN VBG)]\\n\\nsays that if a word ends with ``ing'' it can be an adjective, a noun or a gerund. Unlike a morphological rule, this rule does not ask to check whether the substring preceeding the ``ing''-ending is a word with a particular  POS-tag.\\n\\nNot surprisingly, morphological guessing rules are more accurate than ending-guessing rules but their lexical coverage is more restricted, i.e. they are able to cover less unknown words. Since they are more accurate, in the cascading guesser they were applied before the ending-guessing rules and improved the precision of the guessings by about 5%. This, actually, resulted in about 2% higher accuracy of tagging on unknown words.\\n\\nAlthough in general the performance of the cascading guesser was detected to be only 6% worse than a general-language lexicon lookup, one of the over-simplifications assumed at the extraction of the morphological rules was that they obey only simple concatenative regularities:\\n\\nNo attempts were made to model non-concatenative cases which are quite common in English, as for instance:\\n\\nSo we thought that the incorporation of a set of guessing rules which can capture morphological word dependencies with letter alterations should extend the lexical coverage of the morphological rules and hence might contribute to the overall guessing accuracy.\\n\\nThe Learning Paradigm\\n\\nRule Extraction Phase\\n\\nS is the affix itself;\\n\\nI is the  POS-class of words which should be looked up in the lexicon as main forms;\\n\\nR is the  POS-class which is assigned to unknown words if the rule is satisfied.\\n\\n[ S=  ied I=  (NN, VB) R=  (JJ VBD  VBN) M=y]\\n\\nor in short   [ied (NN VB) (JJ VBD VBN) y] says that if there is an unknown word which ends with ``ied'', we should strip this ending and append to the remaining part the string ``y''. If then we find this word in the lexicon as   (NN VB) (noun/verb), we conclude that the guessed word is of category   (JJ VBD VBN) (adjective, past verb or participle). This rule, for example, will work for word pairs like   specify - specified or   deny - denied.\\n\\nRule Scoring Phase\\n\\nSetting the Threshold\\n\\nThe task of assigning a set of  POS-tags to a particular word is actually quite similar to the task of document categorisation where a document should be assigned with a set of descriptors which represent its contents. The performance of such assignment can be measured in: recall - the percentage of  POS-tags which the guesser  assigned correctly  to a word;\\n\\nprecision - the percentage of  POS-tags the  guesser assigned correctly  over the total number of  POS-tags it assigned to the word;\\n\\ncoverage - the proportion of words which the guesser was able to classify, but not necessarily correctly.\\n\\nThere are two types of test-data in use at this stage. First, we measure the performance of a guessing rule-set against the actual lexicon: every word from the lexicon, except for closed-class words and words shorter than five characters, is guessed by the rule-sets and the results are compared with the information the word has in the lexicon. In the second experiment we measure the performance of the guessing rule-sets against the training corpus. For every word we measure its metrics exactly as in the previous experiment. Then we multiply these measures by the corpus frequency of this particular word and average them. Thus the most frequent words have the greatest influence on the final measures.\\n\\nLearning Experiment\\n\\nOne of the most important issues in the induction of guessing rule-sets is the choice of right data for training. In our approach, guessing rules are extracted from the lexicon and the actual corpus frequencies of word-usage then allow for discrimination between rules which are no longer productive (but have left their imprint on the basic lexicon) and rules that are productive in real-life texts. Thus the major factor in the learning process is the lexicon - it should be as general as possible (list all possible  POSs for a word) and as large as possible, since guessing rules are meant to capture general language regularities. The corresponding corpus should include most of the words from the lexicon and be large enough to obtain reliable estimates of word-frequency distribution.\\n\\n[ S=  ied I=  (NN, VB) R=  (JJ VBD  VBN) M=y]\\n\\n[ S=  ion I=   (NNS VBZ)  R=  (NN)  M=s]\\n\\nTable 1 presents some results of a comparative study of the cascading application of the new rule-set against the standard rule-sets of the cascading guesser. The first part of Table 1 shows the best obtained scores for the standard suffix rules (S) and suffix rules with alterations in the last letter (A). When we applied the two suffix rule-sets cascadingly their joint lexical coverage increased by about 7-8% (from 37% to 45% on the lexicon and from 30% to 37% on the corpus) while precision and recall remained at the same high level. This was quite an encouraging result which, actually, agreed with our prediction. Then we measured whether suffix rules with alterations (A) add any improvement if they are used in conjunction with the ending-guessing rules. Like in the previous experiment we measured the precision, recall and coverage both on the lexicon and on the corpus. The second part of Table 1 shows that simple concatenative suffix rules (S60) improved the precision of the guessing when they were applied before the ending-guessing rules (E75) by about 5%. Then we cascadingly applied the suffix rules with alterations (A80) which caused further improvement in precision by about 1%.\\n\\nAfter obtaining the optimal rule-sets we performed the same experiments on a word-sample which was not included into the training lexicon and corpus. We gathered about three thousand words from the lexicon developed for the Wall Street Journal corpus and collected frequencies of these words in this corpus. At this test-sample evaluation we obtained similar metrics apart from the coverage which dropped by about 7% for both kinds of suffix rules. This, actually, did not come as a surprise, since many main forms required by the suffix rules were missing in the lexicon.\\n\\nEvaluation\\n\\nThis metric gives us the exact measure of how the tagger has done when equipped with different guessing rule-sets. In this case, however, we do not account for the known words which were mis-tagged because of the unknown ones. To put a perspective on that aspect we measure the overall tagging performance:\\n\\nTo perform such evaluation we tagged several texts of different origins, except ones from the Brown Corpus. These texts were not seen at the training phase which means that neither the tagger nor the guesser had been trained on these texts and they naturally had words unknown to the lexicon. For each text we performed two tagging experiments. In the first experiment we tagged the text with the full-fledged Brown Corpus lexicon and hence had only those unknown words which naturally occur in this text. In the second experiment we tagged the same text with the lexicon which contained only closed-class and short words. This small lexicon contained only 5,456 entries out of 53,015 entries of the original Brown Corpus lexicon. All other words were considered as unknown and had to be guessed by the guesser. In both experiments we measured tagging accuracy when tagging with the guesser equipped with the standard Prefix+Suffix+Ending rule-sets and with the additional rule-set of suffixes with alterations in the last letter.\\n\\nTable 2 presents some results of a typical example of such experiments. There we tagged a text of 5,970 words. This text was detected to have 347 unknown to the Brown Corpus lexicon words and as it can be seen the additional rule-set did not cause any improvement to the tagging accuracy. Then we tagged the same text using the small lexicon. Out of 5,970 words of the text, 2,215 were unknown to the small lexicon. Here we noticed that the additional rule-set improved the tagging accuracy on unknown words for about 1%: there were 21 more word-tokens tagged correctly because of the additional rule-set. Among these words were: ``classified'', ``applied'', ``tries'', ``tried'', ``merging'', ``subjective'', etc.\\n\\nDiscussion and Conclusion\\n\\n[ S=  ging I=  (NN VB) R=  (JJ NN VBG) M=``'']\\n\\nBibliography\\n\\nE. Brill 1995. Transformation-based error-driven learning and Natural Language processing: a case study in part-of-speech tagging. In Computational Linguistics 21(4)\\n\\nW. Francis and H. Kucera 1982. Frequency Analysis of English Usage. Houghton Mifflin,  Boston.\\n\\nJ. Kupiec 1992. Robust Part-of-Speech Tagging Using a Hidden Markov Model. In Computer Speech and Language\\n\\nA. Mikheev and L. Liubushkina 1995. Russian morphology: An engineering approach. In Natural Language Engineering, 1(3)\\n\\nA. Mikheev 1996. Unsupervised Learning of Word-Category Guessing Rules. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL-96), Santa Cruz, USA.\\n\\nH. Schmid 1994. Part of Speech Tagging with Neural Networks. In Proceedings of the 15th International Conference on Computational Linguistics (COLING-94), Kyoto, Japan.\\n\\nR. Weischedel, M. Meteer, R. Schwartz, L. Ramshaw and J. Palmucci 1993. Coping with ambiguity and unknown words through probabilistic models. In Computational Linguistics, vol 19/2\\n\\nFootnotes\\n\\nusually we set this threshold quite low: 2-4. these words were not listed in the training lexicon articles, prepositions, conjunctions, etc. shorter than 5 characters\", metadata={'source': '../data/raw/cmplg-xml/9604025.xml'}),\n",
       " Document(page_content=\"Magic for Filter\\n\\nOptimization in Dynamic Bottom\\n\\n\\n\\nup Processing\\n\\nOff-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar. The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness. Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest.\\n\\nIntroduction\\n\\nIn natural language processing filtering is used to weed out those search paths that are redundant, i.e., are not going to be used in the proof tree corresponding to the natural language expression to be generated or parsed. Filter optimization often comprises an extension of a specific processing strategy such that it exploits specific knowledge about grammars and/or the computational task(s) that one is using them for. At the same time it often remains unclear how these optimizations relate to each other and what they actually mean. In this paper I show how starting from a definite clause characterization of filtering derived automatically from a logic grammar using Magic compilation, filter optimizations can be performed in a processor independent and logically clean fashion.\\n\\nAs a result of the definite clause characterization of filtering, Magic brings filtering into the logic underlying the grammar. I discuss two filter optimizations. These optimizations are direction independent in the sense that they are useful for both generation and parsing. For expository reasons, though, they are presented merely on the basis of examples of generation.\\n\\nDefinite Clause Characterization of Filtering Magic Compilation\\n\\ns(P0,P,VForm,SSem):-\\t\\tvp(P1,P,VForm,[CSem],SSem),\\t\\tnp(P0,P1,CSem). Step 2 of the algorithm results in the following modified version of the original grammar rule:\\n\\ns(P0,P,VForm,SSem):-\\t\\tmagic_s(P0,P,VForm,SSem),\\t\\tvp(P1,P,VForm,[CSem],SSem),\\t\\tnp(P0,P1,CSem). A magic literal is added to the right-hand side of the rule which 'guards' the application of the rule. This does not change the semantics of the original grammar as it merely serves as a way to incorporate the relevant bindings derived with the magic predicates to avoid redundant applications of a rule. Corresponding to the first right-hand side literal in the original rule step 3 derives the following magic rule:\\n\\nmagic_vp(P1,P,VForm,[CSem],SSem):-\\t\\tmagic_s(P0,P,VForm,SSem). It is used to derive from the guard for the original rule a guard for the rules defining the first right-hand side literal. The second right-hand side literal in the original rule leads to the following magic rule:\\n\\nmagic_np(P0,P1,CSem):-\\t\\tmagic_s(P0,P,VForm,SSem),\\t\\tvp(P1,P,VForm,[CSem],SSem). Finally, step 4 of the algorithm ensures that a seed is created. Assuming that the original rule is defining the start category, the query corresponding to the generation of the s ``John buys Mary a book'' leads to the following seed:\\n\\nmagic_s(P0,P,finite,buys(john,a(book),mary)). The seed constitutes a representation of the initial bindings provided by the query that is used by the magic predicates to derive guards. Note that the creation of the seed can be delayed until run-time, i.e., the grammar does not need to be recompiled for every possible query.\\n\\nExample\\n\\nFilter Optimization through Program Transformation Subsumption Checking\\n\\nOff\\n\\n\\n\\nline Abstraction\\n\\nmagic_vp(VForm,[CSem|Args],SSem):\\n\\n\\n\\nmagic_vp(VForm,Args,SSem).\\n\\nThis is the rule that is derived from the head-recursive vp rule when the partially specified subcategorization list is considered as filtering information (cf., fn. 1). The rule builds up infinitely large subcategorization lists of which eventually only one is to be matched against the subcategorization list of, e.g., the lexical entry for ``buys''. Though this rule is not cyclic, it becomes cyclic upon off-line abstraction:\\n\\nIndexing\\n\\ns(P0,P,VForm,SSem):-\\t\\tmagic_s(P0,P,VForm,SSem),\\t\\tvp(P1,P,VForm,[CSem],SSem),\\t\\tnp(P0,P1,CSem,index_1). magic_np(CSem,index_1):-\\t\\tmagic_s(P0,P,VForm,SSem),\\t\\tvp(P1,P,VForm,[CSem],SSem). The modified versions of the rules defining nps are adapted such that they percolate up the index of the guarding magic fact that licensed its application. This is illustrated on the basis of the adapted version of rule 14:\\n\\nRedundant Filtering Steps\\n\\nmagic_s(finite,SSem):-\\t\\tmagic_sentence(decl(SSem)). can be eliminated by  unfolding the magic_s literal in the modified s rule:\\n\\ns(P0,P,VFORM,SSem):-\\t\\tmagic_s(VFORM,SSem),\\t\\tvp(P1,P,VFORM,,[CSem],SSem),\\t\\tnp(P0,P1,CSem). This results in the following new rule which uses the seed for filtering directly without the need for an intermediate filtering step:\\n\\ns(P0,P,finite,SSem):-\\t\\t\\t\\tmagic_sentence(decl(SSem)),\\t\\t\\t\\tvp(P1,P,finite,[CSem],SSem),\\t\\t\\t\\tnp(P0,P1,CSem). Note that the unfolding of the magic_s literal leads to the instantiation of the argument VFORM to finite. As a result of the fact that there are no other magic_s literals in the remainder of the magic-compiled grammar the magic_s rule can be discarded.\\n\\nExample\\n\\nDependency Constraint on Grammar\\n\\nAlso with respect to the dependency constraint an optimization of the rules in the grammar is important. Through reordering the right-hand sides of the rules in the grammar the amount of nondeterminism can be drastically reduced as shown in Minnen et al. (1996). This way of following the intended semantic dependencies the dependency constraint is satisfied automatically for a large class of grammars.\\n\\nConcluding Remarks\\n\\nMagic evaluation constitutes an interesting combination of the advantages of top-down and bottom-up evaluation. It allows bottom-up filtering that achieves a goal-directedness which corresponds to dynamic top-down evaluation with abstraction and subsumption checking. For a large class of grammars in effect identical operations can be performed off-line thereby allowing for more efficient processing. Furthermore, it enables a reduction of the number of edges that need to be stored through unfolding magic predicates.\\n\\nAcknowledgments\\n\\nThe presented research was sponsored by Teilprojekt  B4 ``From Constraints to Rules: Efficient Compilation of  HPSG Grammars'' of the Sonderforschungsbereich 340 of the Deutsche Forschungsgemeinschaft. The author wishes to thank Dale Gerdemann, Mark Johnson, Thilo Gtz and the anonymous reviewers for valuable comments and discussion. Of course, the author is responsible for all remaining errors.\\n\\nBibliography\\n\\nFrancois Bancilhon. 1985. Naive Evaluation of Recursively Defined Relations. In Brodie and Mylopoulos, editors, On Knowledge Base Management Systems - Integrating Database and  AI Systems. Springer-Verlag.\\n\\nCatriel Beeri and Raghu Ramakrishnan. 1991. On the Power of Magic. Journal of Logic Programming 10.\\n\\nJochen Drre. 1993. Generalizing Earley Deduction for Constraint-based Grammars. Drre and Dorna, editors, Computational Aspects of Constraint-Based Linguistic Description I,  DYANA-2, Deliverable R1.2.A.\\n\\nDale Gerdemann. 1991. Parsing and Generation of Unification Grammars. Ph.D. thesis, University of Illinois,  USA.\\n\\nMark Johnson. forthcoming. Constraint-based Natural Language Parsing. Brown University, Richmond,  USA. Draft of 6 August 1995.\\n\\nGuido Minnen, Dale Gerdemann, and Erhard Hinrichs. 1996. Direct Automated Inversion of Logic Grammars. New Generation Computing 14.\\n\\nFernando Pereira and Stuart Shieber. 1987. Prolog and Natural Language Analysis. CSLI Lecture Notes, No. 10. Center for the Study of Language and Information, Chicago,  USA.\\n\\nAlberto Pettorossi and Maurizio Proietti. 1994. Transformations of Logic Programs: Foundations and Techniques. Journal of Logic Programming 19/20.\\n\\nRaghu Ramakrishnan, Divesh Srivastava, and S. Sudarshan. 1992. Efficient Bottom-up Evaluation of Logic Programs. In Vandewalle, editor, The State of the Art in Computer Systems and Software Engineering. Kluwer Academic Publishers.\\n\\nTaisuke Sato and Hisao Tamaki. 1984. Enumeration of Success Patterns in Logic Programs. Theoretical Computer Sience 34.\\n\\nStuart Shieber, Gertjan van Noord, Robert Moore, and Fernando Pereira. 1990. Semantic Head-driven Generation. Computational Linguistics 16.\\n\\nStuart Shieber. 1985. Using Restriction to Extend Parsing Algorithms for Complex Feature-based Formalisms. In Proceedings of the 23rd Annual Meeting Association for Computational Linguistics, Chicago,  USA.\\n\\nStuart Shieber. 1988. A Uniform Architecture for Parsing and Generation. In Proceedings of the 12th Conference on Computational Linguistics, Budapest, Hungary.\\n\\nStuart Shieber. 1989. Parsing and Type Inference for Natural and Computer Languages. Ph.D. thesis, Stanford University,  USA.\\n\\nHisao Tamaki and Taisuke Sato. 1984. Unfold/Fold Transformation of Logic Programs. In Proceedings of the 2nd International Conference on Logic Programming, Uppsala, Sweden.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9604019.xml'}),\n",
       " Document(page_content=\"Improving Language Models by Clustering Training Sentences\\n\\nMany of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences. I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduction, and calculating separate language model parameters for each cluster. This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model. It also offers a reasonably automatic means to gather evidence on whether a more complex, context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it: if clustering improves the performance of a model, this proves the existence of further context dependencies, not exploited by the unclustered model. As evidence for these claims, I present results showing that clustering improves some models but not others for the ATIS domain. These results are consistent with other findings for such models, suggesting that the existence or otherwise of an improvement brought about by clustering is indeed a good pointer to whether it is worth developing further the unclustered model.\\n\\nIntroduction\\n\\nIn speech recognition and understanding systems, many kinds of language model may be used to choose between the word and sentence hypotheses for which there is evidence in the acoustic data. Some words, word sequences, syntactic constructions and semantic structures are more likely to occur than others, and the presence of more likely objects in a sentence hypothesis is evidence for the correctness of that hypothesis. Evidence from different knowledge sources can be combined in an attempt to optimize the selection of correct hypotheses; see e.g. Alshawi and Carter (1994); Rayner et al (1994); Rosenfeld (1994).\\n\\nMany of the knowledge sources used for this purpose score a sentence hypothesis by calculating a simple, typically linear, combination of scores associated with objects, such as N-grams and grammar rules, that characterize the hypothesis or its preferred linguistic analysis. When these scores are viewed as log probabilities, taking a linear sum corresponds to making an independence assumption that is known to be at best only approximately true, and that may give rise to inaccuracies that reduce the effectiveness of the knowledge source.\\n\\nThe most obvious way to make a knowledge source more accurate is to increase the amount of structure or context that it takes account of. For example, a bigram model may be replaced by a trigram one, and the fact that dependencies exist among the likelihoods of occurrence of grammar rules at different locations in a parse tree can be modeled by associating probabilities with states in a parsing table rather than simply with the rules themselves (Briscoe and Carroll, 1993).\\n\\nHowever, such remedies have their drawbacks. Firstly, even when the context is extended, some important influences may still not be modeled. For example, dependencies between words exist at separations greater than those allowed for by trigrams (for which long-distance N-grams [Jelinek et al, 1991] are a partial remedy), and associating scores with parsing table states may not model all the important correlations between grammar rules. Secondly, extending the model may greatly increase the amount of training data required if sparseness problems are to be kept under control, and additional data may be unavailable or expensive to collect. Thirdly, one cannot always know in advance of doing the work whether extending a model in a particular direction will, in practice, improve results. If it turns out not to, considerable ingenuity and effort may have been wasted.\\n\\nIn this paper, I argue for a general method for extending the context-sensitivity of any knowledge source that calculates sentence hypothesis scores as linear combinations of scores for objects. The method, which is related to that of Iyer, Ostendorf and Rohlicek (1994), involves clustering the sentences in the training corpus into a number of subcorpora, each predicting a different probability distribution for linguistic objects. An utterance hypothesis encountered at run time is then treated as if it had been selected from the subpopulation of sentences represented by one of these subcorpora. This technique addresses as follows the three drawbacks just alluded to. Firstly, it is able to capture the most important sentence-internal contextual effects regardless of the complexity of the probabilistic dependencies between the objects involved. Secondly, it makes only modest additional demands on training data. Thirdly, it can be applied in a standard way across knowledge sources for very different kinds of object, and if it does improve on the unclustered model this constitutes proof that additional, as yet unexploited relationships exist between linguistic objects of the type the model is based on, and that therefore it is worth looking for a more specific, more powerful way to model them.\\n\\nThe use of corpus clustering often does not boost the power of the knowledge source as much as a specific hand-coded extension. For example, a clustered bigram model will probably not be as powerful as a trigram model. However, clustering can have two important uses. One is that it can provide some improvement to a model even in the absence of the additional (human or computational) resources required by a hand-coded extension. The other use is that the existence or otherwise of an improvement brought about by clustering can be a good indicator of whether additional performance can in fact be gained by extending the model by hand without further data collection, with the possibly considerable additional effort that extension would entail. And, of course, there is no reason why clustering should not, where it gives an advantage, also be used in conjunction with extension by hand to produce yet further improvements.\\n\\nAs evidence for these claims, I present experimental results showing how, for a particular task and training corpus, clustering produces a sizeable improvement in unigram- and bigram-based models, but not in trigram-based ones; this is consistent with experience in the speech understanding community that while moving from bigrams to trigrams usually produces a definite payoff, a move from trigrams to 4-grams yields less clear benefits for the domain in question. I also show that, for the same task and corpus, clustering produces improvements when sentences are assessed not according to the words they contain but according to the syntax rules used in their best parse. This work thus goes beyond that of Iyer et al by focusing on the methodological importance of corpus clustering, rather than just its usefulness in improving overall system performance, and by exploring in detail the way its effectiveness varies along the dimensions of language model type, language model complexity, and number of clusters used. It also differs from Iyer et al's work by clustering at the utterance rather than the paragraph level, and by using a training corpus of thousands, rather than millions, of sentences; in many speech applications, available training data is likely to be quite limited, and may not always be chunked into paragraphs.\\n\\nCluster\\n\\n\\n\\nbased Language Modeling\\n\\nMost other work on clustering for language modeling (e.g. Pereira, Tishby and Lee, 1993; Ney, Essen and Kneser, 1994) has addressed the problem of data sparseness by clustering words into classes which are then used to predict smoothed probabilities of occurrence for events which may seldom or never have been observed during training. Thus conceptually at least, their processes are agglomerative: a large initial set of words is clumped into a smaller number of clusters. The approach described here is quite different. Firstly, it involves clustering whole sentences, not words. Secondly, its aim is not to tackle data sparseness by grouping a large number of objects into a smaller number of classes, but to increase the precision of the model by dividing a single object (the training corpus) into some larger number of sub-objects (the clusters of sentences). There is no reason why clustering sentences for prediction should not be combined with clustering words to reduce sparseness; the two operations are orthogonal.\\n\\nOur type of clustering, then, is based on the assumption that the utterances to be modeled, as sampled in a training corpus, fall more or less naturally into some number of clusters so that words or other objects associated with utterances have probability distributions that differ between clusters. Thus rather than estimating the relative likelihood of an utterance interpretation simply by combining fixed probabilities associated with its various characteristics, we view these probabilities as conditioned by the initial choice of a cluster or subpopulation from which the utterance is to be drawn. In both cases, many independence assumptions that are known to be at best reasonable approximations will have to be made. However, if the clustering reflects significant dependencies, some of the worst inaccuracies of these assumptions may be reduced, and system performance may improve as a result.\\n\\nSome domains and tasks lend themselves more obviously to a clustering approach than others. An obvious and trivial case where clustering is likely to be useful is a speech understander for use by travelers in an international airport; here, an utterance will typically consist of words from one, and only one, natural language, and clusters for different languages will be totally dissimilar. However, clustering may also give us significant leverage in monolingual cases. If the dialogue handling capabilities of a system are relatively rigid, the system may only ask the user a small number of different questions (modulo the filling of slots with different values). For example, the CLARE interface to the Autoroute PC package (Lewin et al, 1993) has a fairly simple dialogue model which allows it to ask only a dozen or so different types of question of the user. A Wizard of Oz exercise, carried out to collect data for this task, was conducted in a similarly rigid way; thus it is straightforward to divide the training corpus into clusters, one cluster for utterances immediately following each kind of system query. Other corpora, such as Wall Street Journal articles, might also be expected to fall naturally into clusters for different subject areas, and indeed Iyer et al (1994) report positive results from corpus clustering here.\\n\\nFor some applications, though, there is no obvious extrinsic basis for dividing the training corpus into clusters. The ARPA air travel information (ATIS) domain is an example. Questions can mention concepts such as places, times, dates, fares, meals, airlines, plane types and ground transportation, but most utterances mention several of these, and there are few obvious restrictions on which of them can occur in the same utterance. Dialogues between a human and an ATIS database access system are therefore likely to be less clearly structured than in the Autoroute case.\\n\\nClustering Algorithms\\n\\nThere are many different criteria for quantifying the (dis)similarity between (analyses of) two sentences or between two clusters of sentences; Everitt (1993) provides a good overview. Unfortunately, whatever the criterion selected, it is in general impractical to find the optimal clustering of the data; instead, one of a variety of algorithms must be used to find a locally optimal solution.\\n\\nLet us for the moment consider the case where the language model consists only of a unigram probability distribution for the words in the vocabulary, with no N-gram (for N]1) or fuller linguistic constraints considered. Perhaps the most obvious measure of the similarity between two sentences or clusters is then Jaccard's coefficient (Everitt, 1993, p41), the ratio of the number of words occurring in both sentences to the number occurring in either or both. Another possibility would be Euclidean distance, with each word in the vocabulary defining a dimension in a vector space. However, it makes sense to choose as a similarity measure the quantity we would like the final clustering arrangement to minimize: the expected entropy (or, equivalently, perplexity) of sentences from the domain. This goal is analogous to that used in the work described earlier on finding word classes by clustering.\\n\\nThe adjustment process in the third step of the algorithm does not attempt directly to decrease entropy but to achieve a clustering with the obviously desirable property that each training sentence is best predicted by the cluster it belongs to rather than by another cluster. This heightens the similarities within clusters and the differences between them. It also reduces the arbitrariness introduced into the clustering process by the order in which the training sentences are presented. The approach is applicable with only a minor modification to N-grams for N ] 1: the probability of a word within a cluster is conditioned on the occurrence of the N-1 words preceding it, and the entropy calculations take this into account. Other cases of context dependence modeled by a knowledge source can be handled similarly. And there is no reason why the items characterizing the sentence have to be (sequences of) words; occurrences of grammar rules, either without any context or in the context of, say, the rules occurring just above them in the parse tree, can be treated in just the same way.\\n\\nExperimental Results\\n\\nExperiments were carried out to assess the effectiveness of clustering, and therefore the existence of unexploited contextual dependencies, for instances of two general types of language model. In the first experiment, sentence hypotheses were evaluated on the N-grams of words and word classes they contained. In the second experiment, evaluation was on the basis of grammar rules used rather than word occurrences.\\n\\nN\\n\\n\\n\\ngram Experiment\\n\\nIn the first experiment, reference versions of a set of 5,873 domain-relevant (classes A and D) ATIS-2 sentences were allocated to K clusters for\\n\\nK = 2, 3, 5, 6, 10 and 20 for the unigram, bigram and trigram conditions and, for unigrams and bigrams only, K=40 and 100 as well. Each run was repeated for ten different random orders for presentation of the training data. The unclustered (K=1) version of each language model was also evaluated. Some words, and some sequences of words such as ``San Francisco'', were replaced by class names to improve performance. The per-item entropy of the training set (i.e. the per-word entropy, but ignoring the need to distinguish different words in the same class) was 6.04 for a unigram language model, 2.96 for bigrams, and 1.97 for trigrams, giving perplexities of 65.7, 7.76 and 3.92 respectively. The greater the value of K, the more a clustering reduced the apparent training set per-item entropy (which, of course, is not the same thing as reducing test set entropy). The reductions for K=20 were around 20% for unigrams, 40% for bigrams and 50% for trigrams, with very little variation (typically 1% or less) between different runs for the same condition.\\n\\nThe improvement (if any) due to clustering was measured by using the various language models to make selections from N-best sentence hypothesis lists; this choice of test was made for convenience rather than out of any commitment to the N-best paradigm, and the techniques described here could equally well be used with other forms of speech-language interface.\\n\\nSpecifically, each clustering was tested against 1,354 hypothesis lists output by a version of the DECIPHER (TM) speech recognizer (Murveit et al, 1993) that itself used a (rather simpler) bigram model. Where more then ten hypothesis were output for a sentence, only the top ten were considered. These 1,354 lists were the subset of two 1,000 sentence sets (the February and November 1992 ATIS evaluation sets) for which the reference sentence itself occurred in the top ten hypotheses. The clustered language model was used to select the most likely hypothesis from the list without paying any attention either to the score that DECIPHER assigned to each hypothesis on the basis of acoustic information or its own bigram model, or to the ordering of the list. In a real system, the DECIPHER scores would of course be taken into account, but they were ignored here in order to maximize the discriminatory power of the test in the presence of only a few thousand test utterances.\\n\\nTo avoid penalizing longer hypotheses, the probabilities assigned to hypotheses were normalized by sentence length. The probability assigned by a cluster to an N-gram was taken to be the simple maximum likelihood (relative frequency) value where this was non-zero. When an N-gram in the test data had not been observed at all in the training sentences assigned to a given cluster, a ``failure'', representing a vanishingly small probability, was assigned. A number of backoff schemes of various degrees of sophistication, including that of Katz (1987), were tried, but none produced any improvement in performance, and several actually worsened it.\\n\\nThe unigram and bigram scores show a steady and, in fact, statistically significant increase with the number of clusters. Using twenty clusters for bigrams (score 43.9%) in fact gives more than half the advantage over unclustered bigrams that is given by moving from unclustered bigrams to unclustered trigrams. However, clustering trigrams produces no improvement in score; in fact, it gives a small but statistically significant deterioration, presumably due to the increase in the number of parameters that need to be calculated.\\n\\nThe random choice of a presentation order for the data meant that different clusterings were arrived at on each run for a given condition ((N,K) for N-grams and K clusters). There was some limited evidence that some clusterings for the same condition were significantly better than others, rather than just happening to perform better on the particular test data used. More trials would be needed to establish whether presentation order does in general make a genuine difference to the quality of a clustering. If there is one, however, it would appear to be fairly small compared to the improvements available (in the unigram and bigram cases) from increasing the numbers of clusters.\\n\\nGrammar Rule Experiment\\n\\nIn the second experiment, each training sentence and each test sentence hypothesis was analysed by the Core Language Engine (Alshawi, 1992) trained on the ATIS domain (Agns et al, 1994). Unanalysable sentences were discarded, as were sentences of over 15 words in length (the ATIS adaptation had concentrated on sentences of 15 words or under, and analysis of longer sentences was less reliable and slower). When a sentence was analysed successfully, several semantic analyses were, in general, created, and a selection was made from among these on the basis of trained preference functions (Alshawi and Carter, 1994). For the purpose of the experiment, clustering and hypothesis selection were performed on the basis not of the words in a sentence but of the grammar rules used to construct its most preferred analysis.\\n\\nThe simplest condition, hereafter referred to as ``1-rule'', was analogous to the unigram case for word-based evaluation. A sentence was modeled simply as a bag of rules, and no attempt (other than the clustering itself) was made to account for dependencies between rules.\\n\\nAnother condition, henceforth ``2-rule'' because of its analogy to bigrams, was also tried. Here, each rule occurrence was represented not in isolation but in the context of the rule immediately above it in the parse tree(its ``predecessor'' if the tree is traversed top-down). This choice was made on the assumption that the immediately dominating rule would be one important influence on the likelihood of occurrence of a particular rule. Other choices, involving sister rules and/or rules in less closely related positions, or the compilation of rules into common combinations (Samuelsson and Rayner, 1991) might have worked as well or better; our purpose here is simply to illustrate and assess ways in which explicit context modeling can be combined with clustering.\\n\\nThe training corpus consisted of the 4,279 sentences in the 5,873-sentence set that were analysable and consisted of fifteen words or less. The test corpus consisted of 1,106 hypothesis lists, selected in the same way (on the basis of length and analysability of their reference sentences) from the 1,354 used in the first experiment. The ``baseline'' score for this test corpus, expected from a random choice of (analysable) hypothesis, was 23.2%. This was rather higher than the 11.4% for word-based selection because the hypothesis lists used were in general shorter, unanalysable hypotheses having been excluded.\\n\\nThese results show that clustering gives a significant advantage for both the 1-rule and the 2-rule types of model, and that the more clusters are created, the larger  the advantage is, at least up to K=20 clusters. As with the N-gram experiment, there is weak evidence that some clusterings are genuinely better than others for the same condition.\\n\\nConclusions\\n\\nI have suggested that training corpus clustering can be used both to extend the effectiveness of a very general class of language models, and to provide evidence of whether a particular language model could benefit from extending it by hand to allow it to take better account of context. Clustering can be useful even when there is no reason to believe the training corpus naturally divides into any particular number of clusters on any extrinsic grounds.\\n\\nThe experimental results presented show that clustering increases the (absolute) success rate of unigram and bigram language modeling for a particular ATIS task by up to about 12%, and that performance improves steadily as the number of clusters climbs towards 100 (probably a reasonable upper limit, given that there are only a few thousand training sentences). However, clusters do not improve trigram modeling at all. This is consistent with experience (Rayner et al, 1994) that, for the ATIS domain, trigrams model inter-word effects much better than bigrams do, but that extending the N-gram model beyond N=3 is much less beneficial.\\n\\nFor N-rule modeling, clustering increases the success rate for both N=1 and N=2, although only by about half as much as for N-grams. This suggests that conditioning the occurrence of a grammar rule on the identity of its mother (as in the 2-rule case) accounts for some, but not all, of the contextual influences that operate. From this it is sensible to conclude, consistently with the results of Briscoe and Carroll (1993), that a more complex model of grammar rule interaction might yield better results. Either conditioning on other parts of the parse tree than the mother node could be included, or a rather different scheme such as Briscoe and Carroll's could be used.\\n\\nNeither the observation that trigrams may represent the limit of usefulness for N-gram modeling in ATIS, nor that non-trivial contextual influences exist between occurrences of grammar rules, is very novel or remarkable in its own right. Rather, what is of interest is that the improvement (or otherwise) in particular language models from the application of clustering is consistent with those observations. This is important evidence for the main hypothesis of this paper: that enhancing a language model with clustering, which once the software is in place can be done largely automatically, can give us important clues about whether it is worth expending research, programming, data-collection and machine resources on hand-coded improvements to the way in which the language model in question models context, or whether those resources are best devoted to different, additional kinds of language model.\\n\\nAcknowledgements\\n\\nThis research was partly funded by the Defence Research Agency, Malvern, UK, under assignment M85T51XX.\\n\\nI am grateful to Manny Rayner and Ian Lewin for useful comments on earlier versions of this paper. Responsibility for any remaining errors or unclarities rests in the customary place.\\n\\nA shorter version of this paper appears in the Proceedings of the ACL Conference on Applied Natural Language Processing, Stuttgart, October 1994, and is  Association for Computational Linguistics.\\n\\nAgns, M-S., et al (1994). Spoken Language Translator First Year Report. SRI International Cambridge Technical Report CRC-043.\\n\\nAlshawi, H., and D.M. Carter (1994). ``Training and Scaling Preference Functions for Disambiguation''. Computational Linguistics (to appear).\\n\\nBriscoe, T., and J. Carroll (1993). ``Generalized Probabilistic LR Parsing of Natural Language (Corpora) with Unification-Based Grammars'', Computational Linguistics, Vol 19:1, 25-60.\\n\\nCover, T.M., and J.A. Thomas (1991). Elements of Information Theory. New York: Wiley.\\n\\nEveritt, B.S. (1993). Cluster Analysis, Third Edition. London: Edward Arnold.\\n\\nIyer, R., M. Ostendorf and J.R. Rohlicek (1994). ``Language Modeling with Sentence-Level Mixtures''. Proceedings of the ARPA Workshop on Human Language Technology.\\n\\nJelinek, F., B. Merialdo, S. Roukos and M. Strauss (1991). ``A Dynamic Language Model for Speech Recognition'', Proceedings of the Speech and Natural Language DARPA Workshop, Feb 1991, 293-295.\\n\\nKatz, S.M. (1987). ``Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer'', IEEE Transactions on Acoustics, Speech and Signal Processing, Vol ASSP-35:3.\\n\\nLewin, I., D.M. Carter, S. Pulman, S. Browning, K. Ponting and M. Russell (1993). ``A Speech-Based Route Enquiry System Built From General-Purpose Components'', Proceedings of Eurospeech-93.\\n\\nMurveit, H., J. Butzberger, V. Digalakis and M. Weintraub (1993). ``Large Vocabulary Dictation using SRI's DECIPHER(TM) Speech Recognition System: Progressive Search Techniques'', Proceedings of the International Conference on Acoustics, Speech and Signal Processing, Minneapolis, Minnesota.\\n\\nNey, H., U. Essen and R. Kneser (1994). ``On Structuring Probabilistic Dependencies in Stochastic Language Modeling''. Computer Speech and Language, vol 8:1, 1-38.\\n\\nPereira, F., N. Tishby and L. Lee (1993). ``Distributional Clustering of English Words''. Proceedings of ACL-93, 183-190.\\n\\nRayner, M., D. Carter, V. Digalakis and P. Price (1994). ``Combining Knowledge Sources to Reorder N-best Speech Hypothesis Lists''. Proceedings of the ARPA Workshop on Human Language Technology.\\n\\nRosenfeld, R. (1994). ``A Hybrid Approach to Adaptive Statistical Language Modeling''. Proceedings of the ARPA Workshop on Human Language Technology.\\n\\nSamuelsson, C., and M. Rayner (1991). ``Quantitative Evaluation of Explanation-Based Learning as a Tuning Tool for a Large-Scale Natural Language System''. Proceedings of 12th International Joint Conference on Artificial Intelligence. Sydney, Australia.\\n\\nSiegel, S., and N.J. Castellan (1988). Nonparametric Statistics, Second Edition. New York: McGraw-Hill.\\n\\nFootnotes\\n\\n(Footnotes in this paper are used for the results of statistical significance tests and other technical details not essential to an understanding of the main argument).\", metadata={'source': '../data/raw/cmplg-xml/9410001.xml'}),\n",
       " Document(page_content=\"Extraction of V-N-Collocations from Text Corpora: A Feasibility Study for German\\n\\nThe usefulness of a statistical approach suggested by Church et al. (1991) is evaluated for the extraction of verb-noun (V-N) collocations from German text corpora. Some problematic issues of that method arising from properties of the German language are discussed and various modifications of the method are considered that might improve extraction results for German. The precision and recall of all variant methods is evaluated for V-N collocations containing support verbs, and the consequences for further work on the extraction of collocations from German corpora are discussed. With a sufficiently large corpus (\\n\\n6 mio. word-tokens), the average error rate of wrong extractions can be reduced to 2.2% (97.8% precision) with the most restrictive method, however with a loss in data of almost 50% compared to a less restrictive method with still 87.6% precision. Depending on the goal to be achieved, emphasis can be put on a high recall for lexicographic purposes or on high precision for automatic lexical acquisition, in each case unfortunately leading to a decrease of the corresponding other variable. Low recall can still be acceptable if very large corpora (i.e. 50 - 100 million words) are available or if corpora for special domains are used in addition to the data found in machine readable (collocation) dictionaries.\\n\\nIntroduction\\n\\nCollocations present an area that is important both for lexicography to improve their coverage in modern dictionaries as well as for lexical acquisition in computational linguistics, where the goal is to build either large reusable lexical databases (LDBs) or specific lexica for specialized NLP-applications. We have tested a statistical approach using Mutual Information (MI) and t-score, introduced into linguistics by Church and Hanks (1989) and Church et al. (1991), for the (semi-)automatic extraction of verb-noun (V-N) collocations from untagged German text corpora. At the time when this study was carried out in early 1993, no POS-tagged German corpora were available. In the meantime this has changed, even work on shallow parsers for German is in progress (Abney, University of Tbingen). Nevertheless it is interesting to answer the question how much can be done with an untagged corpus and what might be gained by lemmatizing, POS-tagging or shallow parsing.\\n\\nWhat Do We Mean by `Collocation'?\\n\\nCollocations in the sense of `frequently cooccurring words' can quite easily be extracted from corpora by statistic means. From a linguistic point of view, however, a more restricted use of the term is preferable which takes into account the difference between what Sinclair (1966) called casual vs. significant collocations. Casual word combinations show a normal, free syntagmatic behaviour. In this paper, collocations shall refer only to word combinations with a lexically (rather than syntactically or semantically) restricted combinatory potential, where at least one component has a special meaning that it cannot have in a free syntagmatic construction. For example, in Anstalten treffen (to make preparations/to take measures), treffen no longer has one of the meanings translateable with `to hit', `to hurt' or `to meet'; the noun (plural of Anstalt) will in other contexts always have its literal meaning `institution' but never the meaning of `measure' or `preparation'.\\n\\nverbal phrasemes (idioms) (e.g. Brundage et al. 1992)\\n\\nsupport verb constructions (SVCs) (v.Polenz 1989 or Danlos 1992)\\n\\ncollocations in the narrower sense (Hausmann 1989)\\n\\nCollocations are well suited for statistical corpus studies. According to Fleischer (1982:63f) the semantics of a collocation in the narrower sense is ``given by the phrase-external semantics of its components, but it differs in an unpredictable way from the pure sum of these component meanings -- however small this difference may be. [...] Language use turns such word combinations into phrase-like stereotypes. A decisive cause for this [conversion] is the frequency of occurrence and the probability with which the occurrence of one component determines the occurrence of the other''. The high cooccurrence frequency of its components compared to the relative frequency of the single words thus -- at least partly -- causes the stereotype status of a collocation, which in turn leads to its special, phrase-internal semantics. For SVCs and phrasemes this is even more true because in the course of time they have turned from stereotypes to completely lexicalised expressions, leading to their (partly) non-compositional semantics and fixedness.\\n\\nRelated Work\\n\\nDuring the last couple of years, various collocation extraction tools have been developed for English using frequency thresholds, t-score, z-score or MI values (see Fontenelle et al. (1994) for a survey), many of them inspired by Berry-Rogghe (1973), Choueka (1988) or the work of Church and colleagues. All of them extract some type of `collocation', but -- with the exception of Smadja (1991b) -- not much can be found in the literature about the actual quality and usefulness of the extracted data. Nor have to our knowledge alternative methods been suggested for German.\\n\\nChoueka (1988) describes how to automatically extract adjacent word combinations from English corpora as a preselection of collocation candidates to ease a lexicographer's search for collocations. He only uses quantitative selection criteria, no statistical ones, his main extraction criterion being n-gram frequency with a lower threshold of at least one occurrence of the collocation in one million words. He mentions plans to define a `binding degree' on how strong the words of a collocation attract each other, which would be similar in spirit to the calculation of MI.\\n\\nThe work on Xtract, described in Smadja and McKeown (1990) and Smadja (1991a, 1991b, 1993) is along the same lines as that of Church and his colleagues, trying to improve over a pure frequency approach such as that of Choueka; but Smadja uses different statistical calculations, a z-score and variance of distribution, and tagged, lemmatized corpora. His work goes a step further in also calculating consecutive n-grams to recognize `rigid noun phrases' and `phrasal templates'. In addition, syntactic relations determined with shallow parsing can be taken into account to get a better selection of word combinations. Using information on syntactic relations, 80% precision is achieved compared to 40% without, recall is only 6% less than with the simpler method.\\n\\nCalzolari and Bindi (1990) use MI to extract compounds, fixed expressions and collocations from an Italian corpus, but to our knowledge have not evaluated their results so far.\\n\\nGrefenstette (1992) has developed Sextant, a system that can (among other things) produce bigrams for verb and subject, direct or indirect object combinations, adjective-noun and noun-noun combinations for English and French. The system itself does not include additional statistics to determine collocations but the output can directly be used as input for such calculations.\\n\\nResources and Methods Used in the Study Statistical Calculations\\n\\nMI is a function well suited for the statistical characterization of collocations because it compares the joint probability p(w1,w2) that two words occur together within a predefined distance with the independent probabilities p(w1) and p(w2) that the two words occur at all in the corpus:\\n\\nMI(x,y) = log2( p(x,y) / (p(x)p(y) )\\n\\n(for a more detailed description see Church et al. (1991:120) or Breidt (1993:18)). Several methods are possible for the calculation of probabilities (cf. Gale and Church 1990); for our purposes we use the simplest one, where the frequency of occurrence in the corpus is divided by the size N of the corpus, p(x) = f(x)/N. Distance is defined as the window-size in which bigrams are calculated.\\n\\nMI does not give realistic figures for very low frequencies. If a relatively unfrequent word occurs only once in a certain combination, the resulting very high MI value suggests a strong link between the words although the cooccurrence might well be simply by chance. To compensate for this, either a lower frequency bound can be used as an approximation or better a standard significance test such as a t-test (e.g. Hatch and Farhady 1982). The t-score indicates whether the difference between the probability for a collocational occurrence and the probability for an independent occurrence of the two words is significant or not.\\n\\nThe `Standard' Method\\n\\nIn German, common nouns and proper names start with an uppercase letter (sentence beginnings are changed to lowercase in the corpus) which makes it possible to extract V-N collocations even from untagged corpora if the verb is used as the key word. The results give good indications how promising the retrieval of collocations is with POS-tagged corpora. We use verbs that can occur in SVCs as key words because they provide examples for all three types of V-N collocations; besides, the chosen potential support verbs anyway belong to the most frequent verbs in the corpus. V-N collocations have been extracted for the following 16 verbs (no translations are given because they differ depending on the N argument): bleiben, bringen, erfahren, finden, geben, gehen, gelangen, geraten, halten, kommen, nehmen, setzen, stehen, stellen, treten, ziehen.\\n\\nBigram tables of all words that occur within a certain distance of these verbs, together with their cooccurrence frequencies, form the basis for the calculation of MI. A span of 5 words to the left and right is said to capture 95% of significant collocations in English (Martin et al. 1983). So we started with bigram calculations in a 5-word window, but only to the left of the verb (for a motivation see next section). We will refer to these with BI5. For combinations that occur at least 3 times, MI was calculated together with a t-score. From these lists, candidates for V-N collocations were automatically extracted, sorted by MI. For the evaluation, all of these were manually checked by means of KWIC-listings and classified by the author w.r.t. their collocational status. The classification was in most cases very obvious. If a combination potentially formed a collocation but was not used as such in the corpus it did not count; a couple of times, where some of the usages were indeed collocations and others not, the decision was made in favour of the predominant case.\\n\\nNo prepositions are extracted in addition to verb and noun, only bigrams, though MI calculation would also be possible for n-grams, because bigrams give enough information to test this statistical approach. In the examples below, prepositions are added manually.\\n\\nApplication for German: Some Problems\\n\\nAnother point concerns the variable word order in German (e.g. Uszkoreit 1987) which makes it more difficult to locate the parts of a V-N collocation, regardless whether the corpus is POS-tagged or not. In a main clause (verb-second order), a noun preceding a finite verb usually is the subject, but it may also be a topicalized complement; in sentences where the main verb occurs at the end (nonfinite verb or subordinate clause) the preceding noun is mostly a direct object or other complement, or possibly an adjunct. In a main clause, a noun to the right of a finite verb can be any of subject, object or other argument due to topicalization or scrambling. The object is located almost at the end of the phrase whereas the main verb is at verb-second position in the front of the phrase. Therefore, the assumption that a ``semantic agent [...] is principally used before the verb'' and a ``semantic object [...] is used after it'' as suggested by Smadja (1991a:180) does not hold for German.\\n\\nThis has consequences for the choice of a window for bigram calculations. Most nouns within 5 words to the right of a verb are not its object or prepositional object and therefore are not potential collocation partners. On the other hand, in subordinate clauses and in main clauses with a complex tense or modal verb, nouns to the left of a verb are quite likely to be its (prepositional) object.\\n\\nAs a conclusion, with an unparsed corpus we restrict our search to V-N combinations where the noun precedes the verb within two to five words, because this is most likely to capture complements of main verbs in verb-final position. Furthermore, except for the experiment simulating lemmatization, we only extract collocations for verbs in the infinitive form. The infinitive form is used as nonfinite verb in complex tenses (modal, conditional, future) and is identical in form with 1st/3rd pers. pl. present tense and thus covers more occurrences of the verb than other inflection forms. Besides, the infinitive is only used in verb-final position.\\n\\nEvaluation of the Results\\n\\nBelow, the top bigrams with kommen (come) are shown, and some of the nonsignificant ones (t\\n\\n1.65\\n\\nPrecision and Recall\\n\\nThe question how much is extractable fully automatically can be answered by an evaluation of precision and recall of the described method as it is done in information retrieval. Following Smadja (1991a), we define precision as the number of correctly found collocations divided by the number of V-N combinations found at all. Recall reflects the ratio of the number of correctly found collocations and the maximal number of collocations that could possibly have been found. The latter is difficult to determine, because the total number of collocations occurring in the whole corpus needs to be known. Thus, we decided to use instead the number of collocations as determined by the standard method (BI5) as the basis for recall comparisons, i.e. 100% `recall' is set to this number. Note that this leads to `recall' percentages above 100% in case more collocations are extracted than with BI5.\\n\\nAnother possibility had to be discarded, viz to take all collocations that are mentioned in a collocation dictionary as the maximal number of valid collocations: a comparison with Agricola (1970) or Drosdowski (1970) is not really possible because the collocations found in the corpus are not a subset of those mentioned in the dictionaries. Only 22 of the 43 collocations found with the lemma bring- in the MK1 (BI5) belong to the 135 combinations mentioned in the lexical entry for bringen in Agricola (1970). Of the remaining 21 in the MK1, 9 can be found in the corresponding noun entries, and 12 do not appear at all though they are `significant' collocations, e.g. Klarheit bringen (to clarify), zur Entfaltung bringen (to develop), zur Wirkung bringen (to bring the effect), in Schwierigkeiten bringen (to create difficulties), ins Gesprch bringen (to bring into discussion).\\n\\nResults of the Standard Method\\n\\n6, precision goes up to 81.6% with a loss in recall of 10%.\\n\\nFor bringen, the approximate absolute number of collocations in the MK1 was manually determined. Out of 585 different V-N combinations, 71 can generously be classified as collocations. Of these, 31 are automatically extracted with BI5, so absolute recall in this case is 43.7% (precision 67.4%).\\n\\nExperiment 1: Variation of Window-Size\\n\\nExperiment 2: Simulating Lemmatizing\\n\\nRegarding lemmatization our study shows that one gets more collocations, but at the expense of more uninteresting combinations as well. One explanation for this is that 3rd pers. sg. present/past and 1st/3rd pers. pl. past verbforms occur to the right of their noun argument only in subordinate clauses; the nonfinite form, identical with 1st/3rd pers. pl. present, and the past participle additionally occur in verb-final position to the right of the noun argument in main clauses with a finite auxiliary or modal verb and in infinitive clauses. Therefore, with an unparsed corpus, only the infinitive and past participle should be used for extractions.\\n\\nExperiment 3: Varying Corpus Size\\n\\nV-N combinations within 2 words to the left of the infinitive were also calculated for a larger corpus consisting of the MK1 and BZK together (6.4 mio word tokens). The number of V-N collocations found with BI2 Inf is almost twice as big (186% `recall'), with a slightly lower precision than for the small corpus. So, larger corpora considerably improve recall. Raising the frequency threshold to five improves precision but drastically cuts down recall to its half. With an MI threshold instead of frequency results are much better, almost the same precision with 154% recall, so using an MI measure rather than pure frequencies has clear advantages. If both are combined, less than 3 out of 100 combinations are wrongly extracted, of course with a low recall of less than half of the unfiltered BI2 Inf. Considering that the corpus is untagged, this is quite a good result for the automatic acquisition of collocations.\\n\\nExperiment 4: Simulating Syntactic Tagging\\n\\nIn main clauses with simple tense, most often the subject is to the left of the inflected verb, so it is likely that for verbs in 1st/3rd pers. pl. present (identical with the infinitive form), the nouns extracted to the left are in fact subjects of the verb and thus do not form a collocation with it. Note that a POS-tagged but unparsed corpus would not solve this problem because the correct distinction of infinitives and inflected 1st/3rd pers. pl. present tense forms is one of the difficulties in automatic POS-tagging. In order to see how much the precision could possibly be improved by determining syntactic relations as done by Smadja (1991a,b) for English, we conducted another test where we manually excluded from BI2 Inf those wrongly extracted combinations in which the nouns were in fact used in subject position of the verb.\\n\\nThese results point in the same direction as Smadja's who reports an improvement from 40 to 80% precision if syntactic relations are considered, with a 94% recall of all collocations that had been found regardless of syntactic relations. However, this cannot as easily be achieved in a large scale for German due to the complicated parsing techniques necessary for the varying word order.\\n\\nConclusions and Outlook\\n\\nThe task of extracting V-N collocations can be split into two sub-tasks: (i) the extraction of possible candidates (ii) the filtering of undesired combinations. For (i), a general basic requirement is a POS-tagged corpus, unless verbs or adjectives are given as key words and combinations with nouns are to be extracted. With an unparsed corpus, restricting the extraction to nouns within 2 words to the left of an infinitive or past participle yields the best results for task (i) with a precision of around 80% (BI2 Inf+Part). Lemmatization is not very useful unless parsed corpora are available (BI2 Lemma). Larger corpora improve recall without a serious decline in precision, though some additional noise seems to come along with the bigger amount of data (BI2 Inf Mk+Bz). Of course, the determination of syntactic relations would open up still better possibilities to find collocation candidates.\\n\\nMI and t-score thresholds work quite satisfactorily as filters for task (ii), but at least half of the actually occurring collocations are filtered out, too. MI sorts the extracted combinations in such a way that the collocations are the better the higher the MI-score is (with a few exceptions which often reflect highly significant, but linguistically uninteresting word combinations from one of the texts; this could hopefully be avoided with a more balanced corpus). Possibly, the likelyhood ratio method suggested by Dunning (1993) might be an interesting alternative to MI.\\n\\nVery high precision rates that do not cut down too much on recall, an indispensible requirement for lexical acquisition, can best be achieved for German with parsed corpora. As long as such resources are not widely available, using a large corpus and high MI and t-score filters should produce satisfactory results ( 97.8% precision, `BI2 Inf Mk+Bz,MI,f\\n\\n5'). A good lexicographical support where a better recall is important can be provided with less restrictive filters. The usefulness of the resulting lists should not be underestimated both for manually built NLP lexica and for printed dictionaries. The simple variant BI2 Inf together with MI is successfully used in the EEC-funded COMPASS project (LRE 62-080) to support lexicographic work on augmenting a large German-English bilingual dictionary with missing collocations and idioms and checking the relevance of already included multi-word expressions.\\n\\nFurthermore, the described approach seems to be a good method for corpora with texts from restricted domains, where a special terminology is used which will thus show up strongly against `normal' combinations.\\n\\nWork is currently in progress to calculate trigrams and larger n-grams to check for prepositions in SVCs or for specific (or no) determiners for phrasemes. This will give indications to distinguish SVCs and lexicalized, phraseological SVCs from other collocations. In addition, we plan to consider the variation in span position of the noun within the searched window in order to distinguish fixed phrasemes from flexible ones.\\n\\nReferences\\n\\nAgricola, E., H. Grner, R. Kfner (eds.) (1962/1970). Wrter und Wendungen. Wrterbuch zum deutschen Sprachgebrauch. Leipzig, Mnchen: Verlag Enzyklopdie.\\n\\nBerry-Rogghe, G. (1973). The computation of collocations and their relevance in lexical studies. In: Aitken, A. J., R. W. Bailey, N. Hamilton-Smith (eds.). The Computer and Literary Studies. Edinburgh: University Press. 103-112.\\n\\nBreidt, E. (1993). Extraktion von Verb-Nomen-Verbindungen aus dem Mannheimer Korpus I. SfS-Report 03-93. University of Tbingen.\\n\\nBrundage, J., M. Kresse, U. Schwall, A. Storrer (1992). Multiword lexemes: a monolingual and contrastive typology for NLP and MT. IWBS-Report 232, September 1992. IBM Germany, Scientific Centre Heidelberg.\\n\\nCalzolari, N., R. Bindi (1990). Acquisition of lexical information from a large textual italian corpus. 13th COLING 1990, Helsinki. 54-59.\\n\\nChoueka, Y. (1988). Looking for needles in a haystack, or: locating interesting collocational expressions in large textual databases. Proceedings of the RIAO. 609-623.\\n\\nChurch, K. W., P. Hanks (1989). Word Association Norms, Mutual Information and Lexicography. 27th ACL, Vancouver. 76-83.\\n\\nChurch, K. W., W. A. Gale, P. Hanks, D. M. Hindle (1991). Using statistics in lexical analysis. In: Zernik, U. (ed.). Lexical acquisition: exploring on-line resources to build a lexicon. Hillsdale, NJ.\\n\\nDanlos, L. (1992). Support verb constructions. Linguistic properties, representation, translation. Journal of French Linguistic Study, Vol. 2, No. 1. CUP.\\n\\nDrosdowski, G. et al. (eds.) (1970). Duden Stilwrterbuch der deutschen Sprache: Die Verwendung der Wrter im Satz. 6th completely revised and extended edition. Mannheim.\\n\\nDunning, T. (1993). Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics, Vol. 19, No. 1. 61-74.\\n\\nFleischer, W. (1982). Phraseologie der deutschen Gegenwartssprache. Leipzig.\\n\\nFontenelle, Th. W. Bruls, L. Thomas, T. Vanallemeersch, J. Jansen (1994). Survey of collocation extraction tools. Deliverable D-1a, MLAP-Project 93-19 DECIDE. University of Lige, Belgium, July 1994.\\n\\nGale, W., K. W. Church (1990). What's wrong with adding one? IEEE Transactions on Acoustics, Speech and Signal Processing.\\n\\nGrefenstette, G. (1992). Use of Syntactic Context to Produce Term Association Lists for Text Retrieval. 15th ACM SIGIR Conference on Research and Development in Information Retrieval. Copenhagen, Denmark. 89-97.\\n\\nHatch, E., H. Farhady (1982). Research design and statistics for applied linguistics. Rowley.\\n\\nHausmann, F. J. (1989). Le dictionnaire de collocations. In: Hausmann, F. J. et al. (eds.). Dictionaries: an international handbook for lexicography. Part I. HSK 5.1. Berlin: De Gruyter. 1010-1019.\\n\\nMartin, W., B. Al, P. van Sterkenburg (1983). On the processing of a text corpus. In: Hartmann, R. R. K. (ed.). Lexicography: principles and practice. London. 77-87.\\n\\nv.Polenz, P. (1989). Funktionsverbgefge im allgemeinen einsprachigen Wrterbuch. In: Hausmann, F. J. et al. (eds.). Dictionaries: an international handbook for lexicography. Part I. HSK 5.1. 882-887.\\n\\nSinclair, J. M. (1966). Beginning the study of lexis. In: Bazell, C. E. et al. (eds.) (1966). In memory of J. R. Firth. London. 410-430.\\n\\nSmadja, F. A., K. R. McKeown (1990). Automatically extracting and representing collocations for language generation. 28th ACL 1990. 252-259.\\n\\nSmadja, F. A. (1991a). Macrocoding the lexicon with co-occurrence knowledge. In: Zernik, U. (ed.). Lexical acquisition: exploring on-line resources to build a lexicon. Hillsdale, NJ.\\n\\nSmadja, F. A. (1991b). From n-grams to collocations: an evaluation of Xtract. 29th ACL, Berkeley, CA. 279-284.\\n\\nSmadja, F. A. (1993). Retrieving collocations from text: XTRACT. Computational Linguistics, Vol. 19, No. 1. 143-177.\\n\\nUszkoreit, H. (1987). Word order and constituent structure. CSLI Lecture Notes 8.\\n\\nFootnotes\\n\\n``[...] sind Wortverbindungen, deren Gesamtsemantik durch die wendungsexterne Semantik ihrer Komponenten gegeben ist, die sich aber doch noch auf nicht voraussagbare Weise -- und sei dies noch so geringfgig -- von der einfachen Summe dieser Komponentenbedeutungen unterscheiden. [...] Der Sprachgebrauch wandelt derartige Wortverbindungen in eine Art phrasenhafter Stereotype um. Magebend dafr sind die Hufigkeit des Vorkommens und die Wahrscheinlichkeit, mit der das Auftreten einer Komponente das Auftreten der anderen determiniert''. We greatfully acknowledge that the work reported here would not have been possible without the supplied tools and corpora. Recall figures are above 100% because the absolute number of collocations found is higher than for BI5 Inf, the basis for our recall calculations.\", metadata={'source': '../data/raw/cmplg-xml/9603006.xml'}),\n",
       " Document(page_content=\"Conciseness through Aggregation in Text Generation\\n\\nAggregating different pieces of similar information is necessary to generate concise and easy to understand reports in technical domains. This paper presents a general algorithm that combines similar messages in order to generate one or more coherent sentences for them. The process is not as trivial as might be expected. Problems encountered are briefly described.\\n\\nMotivation\\n\\nAggregation is any syntactic process that allows the expression of concise and tightly constructed text such as coordination or subordination. By using the parallelism of syntactic structure to express similar information, writers can convey the same amount of information in a shorter space. Coordination has been the object of considerable research (for an overview, see []). In contrast to linguistic approaches, which are generally analytic, the treatment of coordination in this paper is from a synthetic point of view -- text generation. It raises issues such as deciding when and how to coordinate. An algorithm for generating coordinated sentences is implemented in PLANDoc [,], an automated documentation system.\\n\\nPLANDoc generates natural language reports based on the interaction between telephone planning engineers and LEIS-PLAN, a knowledge based system. Input to PLANDoc is a series of messages, or semantic functional descriptions (FD, Fig. 1). Each FD is an atomic decision about telephone equipment installation chosen by a planning engineer. The domain of discourse is currently limited to 31 message types, but user interactions include many variations and combinations of these messages. Instead of generating four separate messages as in Fig. 2, PLANDoc combines them and generates the following two sentences: ``This refinement activated DLC for CSAs 3122 and 3130 in the first quarter of 1994 and ALL-DLC for CSA 3134 in 1994 Q3. It also activated DSS-DLC for CSA 3208 in 1994 Q3.''\\n\\nSystem Architecture\\n\\nCombining Strategy\\n\\nBecause PLANDoc can produce many paraphrases for a single message, aggregation during the syntactic phase of generation would be difficult; semantically similar messages would already have different surface forms. As a result, aggregation in PLANDoc is carried out at the content planning level using semantic FDs. Three main criteria were used to design the combining strategy:\\n\\n1. domain independence: the algorithm should be applicable in other domains. 2. generating the most concise text: it should avoid repetition of phrases to generate shortest text. 3. avoidance of overly-complex sentences: it should not generate sentences that are too complex or ambiguous for readers.\\n\\nThe first aggregation step is to identify semantically related messages. This is done by grouping messages with the same action attribute. Then the system attempts to generate concise and unambiguous text for each action group separately. This reduces the problem size from tens of messages into much smaller sizes. Though this heuristic disallows the combination of messages with different actions, the messages in each action group already contain enough information to produce quite complex sentences.\\n\\nThe system combines the maximum number of related messages to meet the second design criterion-generating the most concise text. But such combination is blocked when a sentence becomes too complex. A bottom-up 4-step algorithm was developed:\\n\\n1. Sorting: putting similar messages right next to each other. 2. Merging Same Attribute: combining adjacent messages that only have one distinct attribute. 3. Identity Deletion: deletion of identical components across messages. 4. Sentence Breaking: determining sentence breaks.\\n\\nStep 1: Sorting\\n\\nThe system first ranks the attributes to determine which are most similar across messages with the same action. For each potential distinct attribute, the system calculates its rank using the formula m - d, where m is the number of messages and d is the number of distinct attributes for that particular attribute. The rank is an indicator of how similar an attribute is across the messages. Combining messages according to the highest ranking attribute ensures that minimum text will be generated for these messages. Based on the ranking, the system reorders the messages by sorting, which puts the messages that have the same attribute right next to each other. In Fig. 2, equipment has rank 1 because it has 3 distinct equipment values - ALL-DLC, DLC, and DSS-DLC; date has rank 2 because it has two distinct date values - 1994 Q1 and 1994 Q3; site has rank 0. Attribute class and action (Fig. 1) are ignored because they are always the same at this stage. When two attributes have the same rank, the system breaks the tie based on a priority hierarchy determined by the domain experts. Because the final sorting operation dominates the order of the resulting messages, PLANDoc sorts the message list from the lowest rank attribute to the highest. In this case, the ordering for sorting is site, equipment, and then date. The resulting message list after sorting each attribute is shown in Fig. 4.\\n\\nStep 2: Merging Same Attribute\\n\\nThe list of sorted messages is traversed. Whenever there is only one distinct attribute between two adjacent messages, they are merged into one message with a conjoined attribute, which is a list of the distinct attributes from both messages. What about messages with two or more distinct attributes? Merging two messages with two or more distinct attributes will result in a syntactically valid sentence but with an undesirable meaning: ``*This refinement activated ALL-DLC and DSS-DLC for CSAs 3122 and 3130 in the third quarter of 1993.''\\n\\nBy tracking which attribute is compound, a third message can be merged into the aggregate message if it also has the same distinct attribute. Continue from Step 1, (E2 S1 D1) and (E2 S2 D1) are merged because they have only one distinct attribute, site. A new FD, (E2 (S1 S2) D1), is assembled to replace those two messages. Note that although (E1 S3 D2) and (E3 S4 D2) have the date in common, they are not combined because they have more than one distinct attribute, site and equipment.\\n\\nStep 2 is applied to the message list recursively to generate possible crossing conjunction, as in the following output which merges four messages: ``This refinement activated ALL-DLC and DSS-DLC for CSAs 3122 and 3130 in the third quarter of 1993.'' Though on the outset this phenomenon seems unlikely, it does happen in our domain.\\n\\nStep 3: Identity Deletion\\n\\nAfter merging at step 2, the message list left in an action group either has only one message, or it has more than one message with at least two distinct attributes between them. Instead of generating two separate sentences for (E2 (S1 S2) D1) and (E1 S3 D2), the system realizes that both the subject and verb are the same, thus it uses deletion on identity to generate ``This refinement activated DLC for CSAs 3122 and 3130 in 1994 Q1 and [this refinement activated] ALL-DLC for CSA 3134 in 1994 Q3.'' For identical attributes across two messages (as shown in the bracketed phrase), a ``deletion'' feature is inserted into the semantic FD, so that SURGE will suppress the output.\\n\\nStep 4: Sentence Break\\n\\nApplying deletion on identity blindly to the whole message list might make the generated text incomprehensible because readers might have to recover too much implicit information from the sentence. As a result, the combining algorithm must have a way to determine when to break the messages into separate sentences that are easy to understand and unambiguous.\\n\\nHow much information to pack into a sentence does not depend on grammaticality, but on coherence, comprehensibility, and aesthetics which are hard to formalize. PLANDoc uses a heuristic that always joins the first and second messages, and continues to do so for third and more if the distinct attributes between the messages are the same. This heuristics results in parallel syntactic structure and the underlying semantics can be easily recovered. Once the distinct attributes are different from the combined messages, the system starts a new sentence. Using the same example, (E2 (S1 S2) D1) and (E1 S3 D2) have three distinct attributes. They are combined because they are the first two messages. Comparing the third message (E3 S4 D2) to (E1 S3 D2), they have different equipment and site, but not date, so a sentence break will take place between them. Aggregating all three messages together will results in questionable output. Because of the parallel structure created between the first 2 messages, readers are expecting a different date when reading the third clause. The second occurrence of ``1994 Q3'' in the same sentence does not agree with readers' expectation thus potentially confusing.\\n\\nFuture Directions\\n\\nAcknowledgements\\n\\nThe author thanks Prof. Kathleen McKeown, and Dr. Karen Kukich at Bellcore for their advice and support. This research was conducted while supported by Bellcore project #CU01403301A1, and under the auspices of the Columbia University CAT in High Performance Computing and Communications in Healthcare, a New York State Center for Advanced Technology supported by the New York State Science and Technology Foundation.\\n\\nBibliography\\n\\nDalianis, Hercules, and Hovy, Edward. 1993. Aggregation in Natural Language Generation. In Proceedings of the Fourth European Workshop on Natural Language Generation, Pisa, Italy.\\n\\nElhadad, Michael.\\n\\n1991.\\n\\nFUF: The universal unifier\\n\\n\\n\\nuser manual, version 5.0.\\n\\nTech Report CUCS\\n\\n\\n\\n038\\n\\n\\n\\n91, Columbia Univ.\\n\\nRobin, Jacques. 1994. Revision-Based Generation of Natural Language Summaries Providing Historical Background: Corpus-based analysis, design, implementation and evaluation. Ph.D. thesis, Computer Science Department, Columbia Univ.\\n\\nKukich, K., McKeown, K., Morgan, N., Phillips, J., Robin, J., Shaw, J., and Lim, J. 1993. User-Needs Analysis and Design Methodology for an Automated Documentation Generator. In Proceedings of the Fourth Bellcore/BCC Symposium on User-Centered Design, Piscataway, NJ.\\n\\nMcKeown, Kathleen, Kukich, Karen, and Shaw, James. 1994. Practical Issues in Automatic Documentation Generation. In Proceedings of the 4th Conference on Applied Natural Language Processing, Stuttgart, p.7-14.\\n\\nvan Oirsouw, Robert.\\n\\n1987.\\n\\nThe Syntax of Coordination\\n\\nBeckenham: Croom Helm.\\n\\nFootnotes\\n\\nLEIS is a registered trademark of Bell Communications Research, Piscataway, NJ.\", metadata={'source': '../data/raw/cmplg-xml/9505008.xml'}),\n",
       " Document(page_content=\"The Semantics of Resource Sharing inLexical-Functional Grammar Introduction\\n\\nBill supported, and Hillary opposed, NAFTA. Since the hallmark of the linear logic approach is to ensure that f-structure contributions are utilized exactly once in a derivation, such constructions would at first glance appear to be problematic for the approach.\\n\\nWe argue that the resource sharing that is commonly manifest in the treatment of coordination in other approaches is appropriately handled by exploiting the structure-sharing in LFG f-structures. We refine our previous analysis to account for cases where an f-structure is reached by multiple paths from an enclosing f-structure.\\n\\nPrevious Work\\n\\nCombinatory Categorial Grammar\\n\\nPartee and Rooth\\n\\nCitizens who seek, paraded against politicians who have, a decent health insurance policy.\\n\\nLFG and Linear Logic\\n\\nLexical entries specify syntactic constraints on f-structures as well as semantic information:\\n\\nExamples\\n\\nRNR with Coordination\\n\\nThe meaning constructors contributed by the lexical items are as follows:\\n\\nHere, we treat and as a binary relation. This suffices for this example, but in general we will have to allow for cases where more than two constituents are conjoined. Therefore, a second meaning constructor and2 is also contributed by the appearance of and, prefixed with the linear logic operator `! ', so that it may be used as many times as necessary (and possibly not at all, as is the case in this example).\\n\\nWe combine the antecedents and consequents of the foregoing formulae to yield:\\n\\nConsuming the meaning of and and R-relations (i) and (ii), and using Axiom I, we derive:\\n\\nUsing Axiom I and  R-relations (iv) and (vi), the following implication can be derived:\\n\\nUsing these last two formulae, by transitivity we obtain:\\n\\nFinally, consuming the contribution of NAFTA, by universal instantiation and modus ponens we obtain a meaning for the whole sentence:\\n\\nAt this stage, all accountable resources have been consumed, and the deduction is complete.\\n\\nRNR with Coordination and Quantified NPs\\n\\nThe derivation is just as before, up until the final step, where we have derived the formula labeled bill-supported-and-hillary-opposed2. This formula matches the antecedent of the quantified NP meaning, so by universal instantiation and modus ponens we derive:\\n\\nIntensional Verbs\\n\\nIn the interest of space, again we only show a few steps of the derivation. Combining the meanings for Hillary, found, supported, and and, Axiom I, and R-relations (ii), (iii), (v), (vi), (viii), and (ix), we can derive:\\n\\nWe duplicate the meaning of two candidates using QNP Duplication, and combine one copy with the foregoing formula to yield:\\n\\nWe then combine the other meaning of two candidates with the meanings of Hillary and wanted, and using Axiom I and R-relations (i), (iv), and (vii) we obtain:\\n\\nFinally, using and2 with the two foregoing formulae, we deduce the desired result:\\n\\nConclusions and Future Work\\n\\nHere we have separated the issue of arriving at the appropriate f-structure in the syntax from the issue of deriving the correct semantics from the f-structure. We have argued that this is the correct distinction to make, and have given a treatment of the second issue. A treatment of the first issue will be articulated in a future forum.\\n\\nAcknowledgements\\n\\nWe would like to thank Sam Bayer, John Maxwell, Fernando Pereira, Dick Oehrle, Stuart Shieber, and especially Ron Kaplan for helpful discussion and comments. The first author was supported in part by National Science Foundation Grant IRI-9009018, National Science Foundation Grant IRI-9350192, and a grant from the Xerox Corporation.\\n\\nBibliography\\n\\nMary Dalrymple, Angie Hinrichs, John Lamping, and Vijay Saraswat. 1993a. The resource logic of complex predicate interpretation. In Proceedings of the 1993 Republic of China Computational Linguistics Conference (ROCLING), Hsitou National Park, Taiwan, September. Computational Linguistics Society of R.O.C.\\n\\nMary Dalrymple, John Lamping, and Vijay Saraswat. 1993b. LFG semantics via constraints. In Proceedings of the Sixth Meeting of the European ACL, University of Utrecht, April. European Chapter of the Association for Computational Linguistics.\\n\\nMary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. 1994a. A deductive account of quantification in LFG. In Makoto Kanazawa, Christopher J. Pin, and Henriette de Swart, editors, Quantifiers, Deduction, and Context. Center for the Study of Language and Information, Stanford, California. To appear.\\n\\nMary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. 1994b. Intensional verbs without type-raising or lexical ambiguity. In Conference on Information-Oriented Approaches to Logic, Language and Computation, Moraga, California. Saint Mary's College.\\n\\nJ.\\n\\n\\n\\nY. Girard.\\n\\n1987.\\n\\nLinear logic.\\n\\nTheoretical Computer Science, 45:1\\n\\n\\n\\n102.\\n\\nHerman Hendriks. 1993. Studied Flexibility: Categories and Types in Syntax and Semantics. ILLC dissertation series 1993--5, University of Amsterdam, Amsterdam, Holland.\\n\\nRichard A Hudson. 1976. Conjunction reduction, gapping, and right-node raising. Language, 52(3):535-562.\\n\\nRonald M. Kaplan and Joan Bresnan. 1982. Lexical-Functional Grammar: A formal system for grammatical representation. In Joan Bresnan, editor, The Mental Representation of Grammatical Relations, pages 173-281. The MIT Press, Cambridge, MA.\\n\\nRichard T. Oehrle. 1990. Categorial frameworks, coordination, and extraction. In Aaron Halpern, editor, Proceedings of the Ninth West Coast Conference on Formal Linguistics, pages 411-425.\\n\\nBarbara Partee and Mats Rooth. 1983. Generalized conjunction and type ambiguity. In Bauerle, Schwarze, and von Stechow, editors, Meaning, Use, and Interpretation of Language, pages 361-383. de Gruyter.\\n\\nMark J. Steedman. 1985. Dependency and coordination in the grammar of Dutch and English. Language, 61:523-568.\\n\\nMark J. Steedman. 1987. Combinatory grammars and parasitic gaps. Natural Language and Linguistic Theory, 5:403-439.\\n\\nMark J. Steedman. 1989. Constituency and coordination in a combinatory grammar. In Mark Baltin and Anthony Kroch, editors, Alternative Conceptions of Phrase Structure, pages 201-231. Chicago University Press.\\n\\nMark J. Steedman.\\n\\n1990.\\n\\nGapping as constituent coordination.\\n\\nLinguistics and Philosophy, 13(2):207\\n\\n\\n\\n263.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9502015.xml'}),\n",
       " Document(page_content=\"Semantic Ambiguity and Perceived Ambiguity1\\n\\nI explore some of the issues that arise when trying to establish a connection between the underspecification hypothesis pursued in the NLP literature and work on ambiguity in semantics and in the psychological literature. A theory of underspecification is developed `from the first principles', i.e., starting from a definition of what it means for a sentence to be semantically ambiguous and from what we know about the way humans deal with ambiguity. An underspecified language is specified as the translation language of a grammar covering sentences that display three classes of semantic ambiguity: lexical ambiguity, scopal ambiguity, and referential ambiguity. The expressions of this language denote sets of senses. A formalization of defeasible reasoning with underspecified representations is presented, based on Default Logic. Some issues to be confronted by such a formalization are discussed.\\n\\nThe Combinatorial Explosion Puzzle\\n\\nThe alternative syntactic readings of a sentence such as synamb probably number in the hundreds, whereas sentences such as hobbs would have hundreds of thousands scopally distinct readings if all permutations of scope-taking sentence constituents were considered admissible readings. Yet, human beings appear able to deal with these sentences effortlessly.\\n\\nThe existing theories of underspecification, however, have been motivated almost exclusively by computational considerations. For example, the semantics assigned to underspecified representations is designed so as to support those inferences that are deemed useful for an economical approach to disambiguation, rather than being motivated by an analysis of the phenomenon of ambiguity. In this paper I explore some of the issues that arise when trying to establish a connection between work on underspecification and, on the one side, work on ambiguity in semantics; on the other side, work on ambiguity in the psychological literature. A theory of underspecification is developed `from the first principles', i.e., starting from a definition of what it means for a sentence to be semantically ambiguous and from what we know about the way humans deal with ambiguity. The goal is to arrive at a linguistically and cognitively plausible theory of ambiguity and underspecification that, in addition to computational gains, may provide a better understanding of how humans process language.\\n\\nAmbiguity in Natural Language\\n\\nAmbiguity and Grammar\\n\\nCharacterizing Ambiguity\\n\\nMeaning, Sense, and Ambiguity\\n\\nOne problem to be tackled in attempting to make precise the definition of ambiguity is to say what `meanings' and `senses' are. In modern semantic theory, the meaning assigned to an expression by a grammar is a function from contexts (or discourse situations) to senses. Roughly speaking, the discourse situation provides a value for all context-dependent aspects of the sentence; the sense of a sentence (what we get once we resolve its context-dependent aspects) tells us under which circumstances in the world the sentence is true or false.\\n\\nNot all notions of `sense' employed in the literature can serve as the basis for a definition of ambiguity. For example, of the various notions of proposition (the sense of sentences), the simplest is the one according to which propositions are truth values. But if we were to use this notion of sense, the sentence Kermit croaked, ambiguous between a reading in which Kermit utters a frog-like sound and a reading in which he dies, would be classified as unambiguous with respect to all models in which Kermit has both the property of dying and the property of producing a frog-like sound, or he (it) has neither property. In other words, in providing a definition of ambiguity we find the same need for a fine-grained notion of sense that has been observed in connection with the semantics of attitude reports. A model-theoretic definition of ambiguity requires a finer-grained notion of proposition than simply truth values. In most recent semantic theories, senses are intensional objects; the simplest way of achieving intensionality is to use functions from possible worlds or situations to referents as one finds in Montague Grammar, where, for example, propositions are functions from possible worlds to truth values. This simple form of intensionality will be sufficient for the purposes of the present paper.\\n\\nA Semantic Theory of Ambiguity\\n\\nPrecisification Principle : A sentence is of indefinite truth value in a context if and only if it can be precisified alternatively to ``true'' or to ``false''. which Pinkal also reformulates as follows: Extended Precisification Principle An expression is semantically indefinite in a context iff it can assume different senses in that context. Pinkal does not equate ambiguity with vagueness. His theory includes, in addition to the notion of precisification, additional criteria to differentiate different forms of ambiguity, as well as differentiating `pure' ambiguity from `pure' vagueness. The intuition he is trying to capture is that ``...whether an expression is ambiguous or only vague is a question that cannot be cleared once and for all. Indefiniteness is perceived as ambiguity when alternative precisifications are predominant, as vagueness when an unstructured continuum presents itself:''\\n\\nAmbiguity (Pinkal) : If the precisification spectrum of an expression is perceived as discrete, we may call it ambiguous; if it is perceived as continuous, we may call it vague. Pinkal identifies two fundamental types of ambiguity, according to whether an expression has, or does not have, a `wider' sense that could be taken as most `basic'. For example, ball does not have a wide sense of `round object and dancing party', whereas American may either mean `person from the US' or `person from the American continent'. He classifies expressions like American which have a wider sense as having a multiplicity of use, whereas expressions such as ball or green which do require precisification are called narrowly ambiguous. The cases of ambiguity in the narrow sense are further distinguished in two classes, depending on whether they are subject to the Precisification Imperative. Although the two interpretations of green are distinct, it is possible of an object to be both green in the `ripe' sense and green in the `color' sense: for example, a green apricot. An object cannot, however, be a `band' both in the musical group sense and in the piece of tape sense. Pinkal proposes that polysemous expressions behave like green, and calls all of these expressions P-type ambiguous; expressions like band, however, are true homonyms, and therefore he calls them H-type ambiguous. These latter are defined as follows:\\n\\nTo summarize, a sentence is H-type ambiguous iff the grammar assigns to it distinct precisifications (senses) in a given discourse situation, and if the `base level' of the expression requires precisification. Thus, the sentence Kermit croaked is considered ambiguous since in the `empty context' that provides all the senses of the expression according to the grammar G for English, that sentence has two senses: the proposition that attributes to Kermit the property of producing the sound that frogs produce, and the proposition that attributes to Kermit the property of dying. (I am assuming here that terms like Kermit refer unambiguously.) On the other hand, the sentence Kermit kissed Miss Piggy would be considered unambiguous with respect to the same context.\\n\\nAlthough Pinkal is only concerned with lexical ambiguity, the precisification approach can also be used to classify as ambiguous sentences which have more than one structural analysis (like the sentence They saw her duck) or are scopally ambiguous (cfr. the sentence Everybody didn't leave) whenever the grammar assigns to them more than one sense. I will discuss below how Pinkal's system can be extended to scopal and referential ambiguity.\\n\\nThe Disjunction Fallacy\\n\\nIt is important to realize that saying that a sentence is ambiguous in a context if it has distinct precisifications is not the same as saying that an ambiguous sentence is equivalent to the disjunction of its distinct precisifications. Intuitively, in uttering S, whose two precisifications are the propositions P and Q, a speaker may have meant P or she may have meant Q, but the following does not hold:\\n\\n[[A means that P] [A means that Q]] [A means that [P Q]] To treat an ambiguous sentence in such a way would be tantamount to propose that an ambiguous sentence has a single sense in any given discourse situation, namely, the proposition that is true at a situation if either of the distinct interpretations of the sentence is true at that situation; but according to the definition above, an ambiguous sentence is one which has more than one sense at a discourse situation. For example, according to the definition of ambiguity discussed above, the listener of an utterance of They saw her duck could either interpret the speaker as saying that the contextually determined set of individuals denoted by the pronoun they saw a contextually specified female person lowering herself, or as saying that that set of individuals saw the pet waterfowl of that female person. According to the disjunction theory, instead, the listener would attribute to the speaker of that sentence a single meaning, albeit a disjunctive one; namely, that it was either the case that they saw a contextually specified female person lowering herself, or it was the case that they saw the pet waterfowl of that female person.\\n\\nThe Role of Syntactic and Semantic Constraints\\n\\nAlthough the number of logical form permutations that one can obtain for a particular sentence by, e.g., considering all the permutations of its operators may be rather large, constraints of a syntactic and/or semantic nature drastically reduce this number.\\n\\nPerceived  Ambiguity\\n\\nThe opposite is true, as well: when clarity is a goal, writers and speakers tend to construct their sentences in such a way as to avoid ambiguity. Thus, most sentences one runs across in scientific texts or in transcripts of task-oriented conversations have a clearly preferred interpretation. This interpretation is sometimes suggested by the context, sometimes by means of disambiguation markers--expressions such as each, a different, or the same that suggest which interpretation is preferred. Thus, a writer will use sentences such as Every kid climbed the same tree, rather than Every kid climbed a tree, when we/she wants to make sure that the reader arrives at the interpretation in which there is a single tree.\\n\\nI will call the situation in which a listener arrives at more than one interpretation for an utterance perceived ambiguity. A situation in which B perceives an utterance as ambiguous may result in B's appreciating the joke, the poetic phrase, or the point of the linguistic example; if the ambiguity is not perceived as intended, B may say saying something like This is not very clear, or perhaps This sentence is ambiguous. This situation can be informally characterized as follows:\\n\\nSemantic Ambiguity versus Perceived Ambiguity\\n\\nA preliminary and, I hope, uncontroversial conclusion I intend to draw from the discussion on deliberate ambiguity and ambiguity processing is that a theory of ambiguity that aims at explaining the Combinatorial Explosion Puzzle needs to be concerned both with the interpretation that the grammar assigns to a sentence--i.e., what it means for a sentence to be semantically ambiguous--and with the process by which interpretations are generated, i.e., with what it means for an utterance to be perceived as ambiguous. On the one hand, the theory must explain why the disambiguation process will not generate all semantically available interpretations; on the other hand, it must predict that more than one interpretation will be generated. This conclusion is the central idea of this paper, indeed, what gives the paper its title. The inclusion of a theory of disambiguation will also remedy one of the omissions in Pinkal's theory, namely, how to formalize the Precisification Imperative.\\n\\nThe discussion of perceived ambiguity supports a stronger claim, namely, that semantic ambiguity and perceived ambiguity are distinct notions, in the sense that whereas a model of semantic ambiguity has to express the truth-conditional properties of an expression, the reasoning processes involved in disambiguation, and that may lead to a perceived ambiguity, consist of defeasible inferences that are not supported by the semantics of ambiguous expressions.\\n\\nThe distinction I intend to draw, then, is as follows. Semantic ambiguity is part of the specification of the grammar of a language; most, if not all, sentences are semantically ambiguous, but their ambiguity need not be noticed by listeners, and in fact it is typically discovered only by linguistic research. Perceived ambiguity, on the other hand, is a result of the interpretation process, that is defeasible in nature, and may therefore result in more than one interpretation in cases of miscommunication or when the speaker constructs the context appropriately to serve a rhetorical purpose, as in the puns presented above.\\n\\nSome readers may wonder why the developer of a  system should be concerned with perceived ambiguity, i.e., with generating all of the contextually available interpretations of a sentence. The answer is that certain applications need this information. Consider the following example, again from the  domain. Say that the user utters move the engine to Avon, and say that two different engines have been discussed during the elaboration of the current part of the plan. Clearly, we do not want the system to just come out with a plausible guess about which engine was meant: instead, we want it to recognize the ambiguity and ask for clarification. In general, all systems that engage in conversations with their users need to be able to recognize an ambiguity, to ask for clarifications when necessary rather than guess one possible interpretation, and to make their own output unambiguous. (Of course, the theory of contextual disambiguation must be such that no spurious ambiguities are obtained.)\\n\\nThe Underspecification Hypothesis\\n\\nUnderspecified representations were originally conceived as a way to solve a problem in system implementation, namely, separating `context-independent' from `context dependent' aspects of the interpretation, thus making either part reusable for different applications. Since the motivation was strictly computational, the underspecified representations used in most  systems are little more than data structures, in the sense that they do not have a interpretation other than the one provided by the procedures that interpret them. These representations `encode' the ambiguity of a sentence in the sense that that sentence has the reading r iff that reading can be generated by repeatedly applying `construction rules' to the underspecified representation.\\n\\nIn recent years, there has been growing interest for the hypothesis that the ability to encode multiple interpretations in an underspecified language may be (part of) the explanation of the Combinatorial Explosion Puzzle. The idea is that humans, as well, make use of an underspecified language that can encode distinct meanings implicitly, and therefore do not need to generate all of these meanings. A semantically ambiguous sentence, therefore, need not cause problems for a human to process, because it is not necessarily perceived as ambiguous in the sense discussed in the previous section. I will call this assumption the Underspecification Hypothesis:\\n\\nUnderspecification Hypothesis : Human beings represent semantic ambiguity implicitly by means of underspecified representations that leave some aspects of interpretation unresolved. My goal in the rest of the paper is to  spell out  the Underspecification Hypothesis both as a theory of grammar and as a theory of discourse interpretation. I assume, that is, that the hypothesis is correct, and try to answer questions such as: what kind of language are underspecified representations? what is their semantics? and, what kind of inferences are done with them?\\n\\nThe novel aspect of this work is that the answers I give are based on the discussion of semantic ambiguity and perceived ambiguity in the previous section. I hypothesize that underspecified representations are used by humans as the translation of expressions that are indefinite in the sense of Pinkal, and assign them a semantics that reflects this hypothesis. I assume that the disambiguation process is consists of defeasible inferences, and examine the characteristics of defeasible reasoning with underspecified representations. Although the same position towards disambiguation and defeasible has been adopted in the Core Language Engine, most of the issues I discuss have not been mentioned so far in the discussion on underspecified representations.\\n\\nAn Underspecified Theory of Ambiguity, Part I:  Lexical           Ambiguity\\n\\nThe simplest way to illustrate my implementation of the Underspecification Hypothesis is to start with lexical ambiguity. I present in this section a theory of grammar which makes use of an underspecified language to encode the `ambiguity potential' of lexically ambiguous expressions, as well as a simple formalization of lexical disambiguation as defeasible inference over underspecified representations. In the next section I will show how to extend the approach presented here to deal with expressions that exhibit other forms of semantic ambiguity.\\n\\nA Lexically Underspecified Grammar\\n\\nDiscourse Interpretation and Perceived Ambiguity\\n\\nDiscourse Interpretation and Defeasible Reasoning\\n\\nA theory of ambiguity processing solves the Combinatorial Explosion Puzzle if it does not require that all distinct interpretations of a semantically ambiguous sentence are actually generated. A grammar consistent with the Underspecification Hypothesis such as the one just discussed moves us one step towards that goal, since it only imposes the constraint that a single underspecified interpretation be generated.\\n\\nOn the other hand, we can conclude from the discussion of deliberate ambiguity and of the psychological work on ambiguity that a psychologically plausible theory of ambiguity must also predict that more than one interpretation may become available in a given context, although the number of such interpretations will in general be much smaller than the number of possible semantic interpretations.\\n\\nLexical Disambiguation Using Defaults\\n\\nA default theory always has an extension as long as all defaults are normal, but it may have more than one extension if the set of Discourse Interpretation Principles contains two inference rules that both apply but generate a conflict. Consider, for example, the default theory consisting of a set of discourse interpretation principles DI that includes, in addition to CROAK1-IF-FROG, a second discourse interpretation principle (let's call it CROAK2-IF-HUMAN-LIKE) stating that the croak2 interpretation is plausible for human-like beings; and of a set of wffs UF including the fact that Kermit is a human-like being.\\n\\nPerceived ambiguity can now be redefined more precisely as the state that obtains when the default theory `encoding' the listener's discourse interpretation processes has more than one extension; and the cases of deliberate ambiguity discussed in section ambiguity_section can be formalized as cases in which the speaker has `reasoned about the other agent's reasoning,' as it were.\\n\\nConstraints on Discourse Interpretation and the               Anti-Random Hypothesis\\n\\nA theory of lexical disambiguation of this kind would simply produce all semantically justified interpretations of a sentence, and the Combinatorial Explosion Puzzle would remain a puzzle. To solve the puzzle, a theory of disambiguation must therefore supplement the Underspecification Hypothesis with constraints on discourse interpretation that ensure that only a few extensions are generated.\\n\\nAnti-Random Hypothesis (Informal) Humans do not randomly generate alternative interpretations of an ambiguous sentence; only those few interpretations are obtained that (i) are consistent with syntactic and semantic constraints and (ii) are suggested by the context. The Anti-Random Hypothesis should be thought of as a `meta-constraint' on theories of interpretation: if we intend to account for the Combinatorial Explosion Puzzle, we have to develop theories of interpretation (e.g., theories of parsing, or theories of definite description interpretation) that satisfy this constraint, i.e., in which discourse interpretation principles like CROAK1-AT-RANDOM and CROAK2-AT-RANDOM are not allowed.\\n\\nIn order to illustrate more concretely the difference between theories of discourse interpretation that satisfy the Anti-Random Hypothesis, and theories that do not, let us consider how one could formalize a theory of pronominal interpretation. A `random' theory of pronoun interpretation would go as follows: first, compute all possible antecedents of the pronoun in the discourse. Then, generate an hypothesis for each of them, stating that the pronoun refers to that antecedent. Finally, rank these hypotheses according to their plausibility. A random hypothesis generation process usually leaves the task of choosing one hypothesis to plan recognition; the problem is that most often, the alternatives are equally plausible.\\n\\nThe Anti-Random Hypothesis can be made more formal in the framework for discourse interpretation adopted here by introducing a slightly different syntax for default inference rules, one in which the underspecified condition is syntactically separated from additional contextual requirements such as the requirement in CROAK1-IF-FROG that the object in question be a frog:\\n\\nAnti-Random Hypothesis A discourse interpretation theory (DI,UF) is Anti-Random iff for all  discourse interpretation principles ::/  in DI,  is not satisfied in every situation.\\n\\nThe Condition on Discourse Interpretation\\n\\nThe framework just introduced can also be used to formalize the `second order' aspects of Pinkal's theory, such as the Precisification Imperative. The Precisification Imperative can be seen as imposing a constraint on the extensions of a discourse interpretation theory, namely, as the requirement that extensions include a `disambiguating wff' like croak1k for each H-type ambiguous constituent of the set UF such as croakUk. I will call this constraint Condition on Discourse Interpretation. In first instance, the Condition on Discourse Interpretation might be formulated as follows, for the case of lexical ambiguity:\\n\\nCondition on Discourse Interpretation (Preliminary): Each extension E of a discourse interpretation theory (DI,UF) must include, for each literal L in UF whose predicate is H-type ambiguous, a distinct disambiguating literal, i.e., a literal whose denotation is a single function among those in the denotation of L. The definition of the Condition on Discourse Interpretation just given is not very general: it depends on the assumption that all cases of H-type ambiguity are originated by predicates. A simpler, and more general, formulation of the Condition on Discourse Interpretation can be obtained by generalizing the format for the discourse interpretation principles once more.\\n\\nDefault inference rules are typically used to augment a set of wffs with additional facts inferred by default: the fact that a particular bird flies, for example. But the purpose of discourse interpretation rules used for disambiguation, like CROAK1-IF-FROG, is to restrict the interpretation by eliminating certain readings. In this perspective, leaving the underspecified wffs around doesn't make much sense. I propose therefore to allow discourse interpretation principles to rewrite their `triggering wff' whenever this wff encodes an H-type ambiguity, in addition to adding new wffs to a set. The more general format for discourse interpretation principles is as follows:\\n\\nCondition on Discourse Interpretation : An extension E of a discourse interpretation theory (DI,UF) cannot contain an H-type ambiguous wff. Notice that the statement of the Condition on Discourse Interpretation as a condition on pragmatic reasoning gives it the status of a felicity condition rather than of a hard constraint on interpretation.\\n\\nExtensions, closure and consistency checking in an  underspecified default theory\\n\\nSo far, I've been using the terminology from default logic as if the shift to an underspecified representation had no side effects, but this is not the case. Consider the way in which Reiter defines the notion of extension of a (closed) default theory, for example:\\n\\nIn fact, each extension of a discourse interpretation theory under this new definition will, in general, be H-type ambiguous, some of the interpretations being inconsistent. However, if we adopt the `rewriting' version of disambiguation discussed above, and impose the Condition on Discourse Interpretation, each extension will have a single interpretation, and therefore its consistency can be checked. I propose therefore to define the notion of extension of a discourse interpretation theory as follows:\\n\\nOther Forms of Ambiguity\\n\\nThe theory of ambiguity introduced in the previous sections can be straightforwardly extended to obtain a treatment of two other classes of semantic ambiguity: scopal ambiguity and referential ambiguity. These extensions preserve the basic ideas of the theory, semantic ambiguity as multiplicity of meanings, and perceived ambiguity as multiple extensions of a default theory; what changes is that on the one hand, a more complex underspecified language is introduced, capable of encoding other forms of ambiguity; on the other hand, more complex inference rules are used.\\n\\nScopal Ambiguity\\n\\nI will call the sentence constituents that modify the parameters of evaluation, and therefore affect the interpretation of other sentence constituents `in their scope', operators. Examples of operators are quantifiers (that affect the choice of the variable assignment used to evaluate expressions in their scope) and modals (that affect the choice of the world / situation at which expressions in their scope are evaluated). As it is well-known, one cause of semantic ambiguity is that sentences may contain more than one operator, and their relative scope is not completely determined by the sentence's syntactic structure. Sentences that have more than one meaning due to the interaction between operators are called scopally ambiguous.\\n\\nThese representations are typically justified in terms of ease of processing, and their ability to represent `intermediate' readings. It is clear however that for the purposes of developing a `principled' theory of ambiguity processing, it would be much better to stick to as few new `levels of representation' as possible.\\n\\nIn fact, there is no need to introduce a new level of representation. The two requirements on a scopally underspecified representation--that it allow representing the structural information provided by a sentence, and representing the intermediate steps of disambiguation--can be satisfied by using as an underspecified representation the syntactic structure of the sentence, augmented with information about the semantic interpretation of word-forms. In this way we can also maintain semantic translation of lexical items used in Montague grammar, that determine how they combine with other sentence constituents to determine a sentence's meaning.\\n\\nA Lexically and Scopally Underspecified Language An Example of a Scopally and Lexically Underspecified Grammar The Denotation of Logical Forms\\n\\nThe CV function used to define the interpretation of logical forms is based on an implementation of the storage idea less general than Cooper's, but simpler. In order to arrive at a uniform specification of the Cooper Value of all constructs, it is useful to define construct-specific versions of application in which to `bury' the differences in storage manipulation. These operations are defined as follows:\\n\\nLambda Abstracts Quantification Scope Disambiguation by Defeasible Inference Referential Ambiguity Referential Expressions as Cases of Semantic Ambiguity\\n\\nYet another way in which the semantics of sentences is `underspecified' by their syntax is in the interpretation of anaphoric expressions and other expressions whose interpretation has to be fixed in context. In semantics, referential expressions are traditionally translated as free variables whose interpretation depends on the choice of an assignment function (for the cases of deictic anaphora) or by assigning them the same variable bound by the quantifier that serves as their antecedent (for the cases of bound anaphora). This translation does capture the intuition that the truth conditions of a sentence containing a referential expression can only be evaluated after fixing the value of the referential expressions. It is also clear, however, that distinct propositions are obtained depending on the value assigned to these expressions, much as distinct propositions are obtained depending on the choice of an interpretation for lexical items, or of a scope for operators: in other words, a sentence which includes a referential expression is semantically ambiguous much in the way a sentence containing a lexically ambiguous item is.\\n\\nA complete discussion of reference interpretation would require introducing a formalization of context, so I will only consider here the issue of providing an underspecified treatment of intra-clausal and deictic anaphora. I propose that referential expressions are cases of semantic ambiguity, and translate into a special kind of underspecified object that I will call parameters. Semantically, a parameter is a type e expression that, in a discourse situation d, denotes a set of functions from situations to elements of e in d. For example, the pronoun he would translate into a parameter x which, in a discourse situation d with constituents a1 ...an, and given the set  of situations, will denote a set of functions {f1, ..., fm, ...} from situations in  to a1 ...an, including at least the set of all constant functions that map each situation s into aj if aj is a constituent of that situation (see below), and the set of all variable denotations. The reader will immediately realize that parameters are the equivalent for type e expressions of `underspecified predicates' like croakU introduced above.\\n\\nThe grammar presented in the previous section can be straightforwardly extended as follows to generate sentences such as It croaked:\\n\\nParameters and Discourse Interpretation\\n\\nReferential ambiguity gets `resolved' by anchoring a parameter. A parameter is anchored if only one among the functions in its denotation results in a consistent interpretation of the set of sentences in which the parameter occurs; a parameter can be anchored by means of equality statements of the form =xa, where a is not parametric, or is already anchored: such equality statements make all but one of the interpretations of the parameter inadmissible. Once a parameter is anchored, it can be `replaced' by a term that denotes the one function among those in the interpretation of the parameter that does not result in an inconsistent interpretation, much as in the previous discussion of lexical disambiguation, an H-type ambiguous predicate could be replaced by a disambiguated version. So, the discourse interpretation principles formalizing pronoun disambiguation involve a rewriting operation, just as the discourse interpretation principles formalizing lexical disambiguation.\\n\\nSyntactic Ambiguity\\n\\nDiscussion\\n\\nIn the theory, semantic ambiguity is characterized model-theoretically in terms of multiplicity of sense, whereas perceived ambiguity is characterized in terms of inference. One may wonder if the distinction is really necessary; i.e., if it is really the case that the meaning of natural language expressions can be specified a priori. Two arguments in favor of a distinction are that it provides for a clean distinction between the role of grammar and the role of discourse interpretation; and that perceived ambiguity may also reflect non-semantic distinctions, e.g., distinctions in speech act interpretation; this question is not however totally resolved in the paper.\\n\\nBibliography\\n\\nAllen, J. F. 1987. Natural Language Understanding. Menlo Park, CA: Benjamin Cummings.\\n\\nAllen, J. F., L. K. Schubert, G. Ferguson, P. Heeman, C. H. Hwang, T. Kato, M. Light, N. Martin, B. Miller, M. Poesio, and D. R. Traum. 1995. The TRAINS project: a case study in building a conversational planning agent. Journal of Experimental and Theoretical AI 7:7-48.\\n\\nAlshawi, H. (ed.). 1992.\\n\\nThe Core Language Engine.\\n\\nThe MIT Press.\\n\\nAltmann, G. T. M. (ed.). 1989. Parsing and Interpretation. Hove, East Sussex, UK: Lawrence Erlbaum.\\n\\nAltmann, G. T. M., and M. Steedman. 1988. Interaction with Context during Human Sentence Processing. Cognition 30:191-238.\\n\\nAsher, N., and M. Morreau. 1991. Common Sense Entailment: A Modal Theory of Commonsense Reasoning. In Proc. 12th IJCAI.\\n\\nBarwise, J., and R. Cooper. 1981. Generalized Quantifiers and Natural Language. Linguistics and Philosophy 4(2):159-219.\\n\\nBarwise, J., and R. Cooper. 1993. Extended Kamp Notation. In Situation Theory and its Applications, v.3, ed. P. Aczel, D. Israel, Y. Katagiri, and S. Peters. Chap. 2, 29-54. CSLI.\\n\\nBarwise, J., and J. Perry. 1983. Situations and Attitudes. Cambridge, MA: MIT Press, Cambridge Mass.\\n\\nCharniak, E., and R. P. Goldman. 1988. A Logic for Semantic Interpretation. In Proc. ACL-88, 87-94. Buffalo, NY.\\n\\nChierchia, G., and S. McConnell-Ginet. 1990. Meaning and Grammar: An Introduction to Semantics. Cambridge, MA: The MIT Press.\\n\\nChomsky, N. 1981. Lectures on Government and Binding. Dordrecht: Foris.\\n\\nCooper, R. 1983. Quantification and Syntactic Theory. Dordrecht, Holland: D. Reidel Publishing Company.\\n\\nCrain, S., and M. Steedman. 1985. On not being led up the garden path: the use of context by the psychological syntax processor. In Natural Language Parsing: Psychological, Computational and Theoretical perspectives, ed. D. R. Dowty, L. Karttunen, and A. M. Zwicky. 320-358. New York: Cambridge University Press.\\n\\nCrouch, R. 1995. Ellipsis and Quantification: A Substitutional Approach. In Proceedings 7th Conference of the European Chapter of the Association for Computational Linguistics, 229-236. Dublin. Dublin City University.\\n\\nDalrymple, M., S. M. Shieber, and F. C. N. Pereira. 1991. Ellipsis and Higher-Order Unification. Linguistics and Philosophy 14(4):399-452.\\n\\nDowty, D. R., R. E. Wall, and S. Peters. 1981. Introduction to Montague Semantics. Dordrecht, Holland: D. Reidel.\\n\\nFenstad, J.E., P.K. Halvorsen, T. Langholm, and J. van Benthem. 1987. Situations, Language and Logic. Dordrecht: D.Reidel.\\n\\nFine, K.\\n\\n1975.\\n\\nVagueness, Truth, and Logic.\\n\\nSynthese 30:265\\n\\n\\n\\n300.\\n\\nFrazier, L., and J. D. Fodor. 1978. The sausage machine: A new two-stage parsing model. Cognition 6:291-295.\\n\\nGawron, J. M., and S. Peters. 1990. Anaphora and Quantification in Situation Semantics. Lecture Notes, Vol. 19. CSLI.\\n\\nGibson, E. 1991. A Computational Theory of human linguistic processing: memory limitations and processing breakdown. Doctoral dissertation, Carnegie Mellon University, Pittsburgh.\\n\\nGillon, B. 1990. Ambiguity, generality, and indeterminacy: Tests and definitions. Synthese 85:391-416.\\n\\nGrosz, B.J., A.K. Joshi, and S. Weinstein. 1983. Providing a Unified Account of Definite Noun Phrases in Discourse. In Proc. ACL-83, 44-50.\\n\\nHaegeman, L. 1991. An Introduction to Government and Binding Theory. Basil Blackwell. First edition.\\n\\nHamblin, C.\\n\\n1973.\\n\\nQuestions in Montague English.\\n\\nFoundations of Language 10:41\\n\\n\\n\\n53.\\n\\nHeim, I. 1982. The Semantics of Definite and Indefinite Noun Phrases. Doctoral dissertation, University of Massachusetts at Amherst.\\n\\nHirst, G. 1987. Semantic Interpretation and the Resolution of Ambiguity. Studies in Natural Language Processing. Cambridge, UK: Cambridge University Press.\\n\\nHobbs, J. R. 1983. An Improper Treatment of Quantification in Ordinary English. In Proc. ACL-83, 57-63. Cambridge, MA, June.\\n\\nHobbs, J. R., and S. M. Shieber. 1987. An Algorithm for Generating Quantifier Scopings. Computational Linguistics 13(1-2):47-63.\\n\\nHobbs, J. R., M. Stickel, P. Martin, and D. Edwards. 1990. Interpretation as Abduction. Technical Note 499. Menlo Park, CA: SRI International, December.\\n\\nHwang, C. H., and L. K. Schubert. 1993. Episodic Logic: A Situational Logic for Natural Language Processing. In Situation Theory and its Applications, v.3, ed. P. Aczel, D. Israel, Y. Katagiri, and S. Peters. 303-338. CSLI.\\n\\nIoup, G. 1975. Some Universals for Quantifier Scope. In Syntax and Semantics 4, ed. J. Kimball. 37-58. New York: Academic Press.\\n\\nKamp, H., and U. Reyle. 1993. From Discourse to Logic. Dordrecht: D. Reidel.\\n\\nKaplan, D. 1977. Demonstratives. An Essay on the Semantics, Logic, Metaphysics and Epistemology of Demonstratives and Other indexicals. Unpublished manuscript, University of California, Los Angeles.\\n\\nKeller, W. R. 1988. Nested Cooper Storage: The Proper Treatment of Quantification in Ordinary Noun Phrases. In Natural Language Parsing and Linguistic Theories, ed. U. Reyle and C. Rohrer. 432-447. Dordrecht: D. Reidel.\\n\\nKempson, R., and A. Cormack. 1981. Ambiguity and Quantification. Linguistics and Philosophy 4(2):259-310.\\n\\nKurtzman, H. 1985. Studies in Syntactic Ambiguity Resolution. Doctoral dissertation, MIT, Cambridge, MA.\\n\\nKurtzman, H. S., and M. C. MacDonald. 1993. Resolution of Quantifier Scope Ambiguities. Cognition 48:243-279.\\n\\nLakoff, G. P. 1970. A note on vagueness and ambiguity. Linguistic Inquiry 1(3):357-359.\\n\\nLascarides, A., N. Asher, and J. Oberlander. 1992. Inferring Discourse Relations in Context. In Proc. ACL-92, 1-8. University of Delaware.\\n\\nLewis, D. K. 1979. Scorekeeping in a language game. Journal of Philosophical Logic 8:339-359.\\n\\nMay, R. 1985. Logical Form in Natural Language. The MIT Press.\\n\\nPartee, B. H., and M. Rooth. 1983. Generalized Conjunction and Type Ambiguity. In Meaning, Use and Interpretation of Language, ed. R. Bauerle, C. Schwarze, and A. von Stechow. Berlin, West Germany: Walter de Gruyter.\\n\\nPereira, F. C. N.\\n\\n1990.\\n\\nCategorial Semantics and Scoping.\\n\\nComputational Linguistics 16(1):1\\n\\n\\n\\n10.\\n\\nPereira, F. C. N., and M. E. Pollack. 1991. Incremental Interpretation. Artificial Intelligence 50:37-82.\\n\\nPinkal, M. 1985. Logik und Lexikon: Die Semantik des Unbestimmten. Berlin: de Gruyter.\\n\\nPinkal, M. 1995.\\n\\nLogic and Lexicon.\\n\\nLondon: Oxford.\\n\\nPoesio, M. 1991. Relational Semantics and Scope Ambiguity. In Situation Semantics and its Applications, vol.2, ed. J. Barwise, J. M. Gawron, G. Plotkin, and S. Tutiya. Chap. 20, 469-497. Stanford, CA: CSLI.\\n\\nPoesio, M. 1994. Discourse Interpretation and the Scope of Operators. Doctoral dissertation, University of Rochester, Department of Computer Science, Rochester, NY.\\n\\nPoesio, M. 1995. A Model of Conversation Processing Based on Micro Conversational Events. In Proceedings of the Annual Meeting of the Cognitive Science Society. Pittsburgh.\\n\\nRaskin, V. 1985. Semantic Mechanisms of Humor. Dordrecht and Boston: D. Reidel.\\n\\nReiter, R. 1980. A Logic for Default Reasoning. Artificial Intelligence 13(1-2):81-132.\\n\\nReyle, U. 1993. Dealing with ambiguities by underspecification: Construction, Representation and Deduction. Journal of Semantics 3.\\n\\nRodman, R. 1976. Scope Phenomena, ``Movement Transformations,'' and Relative Clauses. In Montague Grammar, ed. Barbara Partee. 165-176. Academic Press.\\n\\nRooth, M. 1985. Association with Focus. Doctoral dissertation, University of Massachusetts, Amherst.\\n\\nSchubert, L. K. 1986. Are There Preference Trade-Offs in Attachment Decisions. In Proceedings of the Fifth National Conference on Artificial Intelligence, 601-605. Philadelphia, Pennsylvania, August. American Association for Artificial Intelligence.\\n\\nSchubert, L. K., and F. J. Pelletier. 1982. From English to Logic: Context-Free Computation of 'Conventional' Logical Translations. American Journal of Computational Linguistics 10:165-176.\\n\\nSpivey-Knowlton, M., J. Sedivy, K. Eberhard, and M. Tanenhaus. 1994. Psycholinguistic Study of the Interaction between Language and Vision. In Proceedings of 12th National Conference on Artificial Intelligence (AAAI-94). Seattle.\\n\\nStallard, D. 1987. The Logical Analysis of Lexical Ambiguity. In Proceedings of the 25th Meeting of the ACL, 179-185.\\n\\nSu, S. P. 1994.\\n\\nLexical Ambiguity in Poetry.\\n\\nLondon: Longman.\\n\\nSwinney, D. A. 1979. Lexical Access During Sentence Comprehension: (Re)consideration of Context Effects. Journal of Verbal Learning and Verbal Behavior 18:545-567.\\n\\nThomason, R. H. (ed.). 1974. Formal Philosophy: Selected Papers of Richard Montague. New York: Yale University Press.\\n\\nTurner, R. 1992. Properties, propositions and semantic theory. In Computational Linguistics and Formal Semantics, ed. M. Rosner and R. Johnson. CUP, Cambridge.\\n\\nvan Deemter, K. 1991. On the Composition of Meaning. Doctoral dissertation, University of Amsterdam.\\n\\nVerkuyl, H. J. 1992. Some Issues in the Analysis of Multiple Quantification with Plural NPs. OTS Working Papers OTS-WP-TL-92-005. The Netherlands: University of Utrecht, Research Institute for Language and Speech. To appear in F. Hamm and E. Hinrichs, editors, Plural Quantification, Kluwer.\\n\\nZwicky, A., and J. Sadock. 1975. Ambiguity Tests and How to Fail Them. In Syntax and Semantics 4, ed. J. Kimball. 1-36. New York: Academic Press.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9505034.xml'}),\n",
       " Document(page_content='Report of the Study Group on Assessment and Evaluation\\n\\nThis is an interim report discussing possible guidelines for the assessment and evaluation of projects developing speech and language systems. After a discussion of terminology, the report focusses on combining user-centred and technology-centred assessment, and on how meaningful comparisons can be made of a variety of systems performing different tasks for different domains. The report outlines the kind of infra-structure that might be required to support comparative assessment and evaluation of heterogenous projects, and also the results of a questionnaire concerning different approaches to evaluation.\\n\\nIntroduction\\n\\nThis is an interim report of the study group on evaluation set up to work out guidelines for assessment and validation of projects in the Language Engineering sector (LE) of the TELEMATICS Fourth Framework Programme (FP-4).\\n\\nThe Study Group on Assessment\\n\\nIn October 1994 a number of experts were invited by the CEC to a meeting whose purpose was to initiate a Study Group on Assessment. The task of this group is to work out guidelines and specifications for assessment and validation of LE projects in the Fourth Framework Programme (FP-4), specifically those responding to the first and second call for proposals in December 1994 and September 1995 respectively.\\n\\nThe SG has its organisational framework within EAGLES. It is chaired by EC staff and supported by three to four part-time editors or rapporteurs, whose task is to coordinate the actions of the study group, to support the composition of trigger papers on various topics, and to work out the proposals for actions within FP-4. The study group will have completed its task by end of June 1995 and cease to exist thereafter.\\n\\nMandate and Scope of the Study Group\\n\\nAssessment in the FP-4 covers a broad range of activities, ranging from the TELEMATICS Programme level, through the sector level to the level of individual projects.\\n\\nAt programme level, assessment is meant to address issues such as the following:\\n\\nassessing value for money\\n\\nassessing the programme\\'s contribution to competitiveness of Community industry\\n\\nidentifying ways and means to maximise effectiveness of RTD policy\\n\\nPreparatory work is under way to provide the basic assessment frameworks required to identify the main assessment and evaluation issues for Programme Support Actions (in particular SU 1.3 of the TELEMATICS Application Work Plan), and to determine expertise requirements and likely assessment issues needed.\\n\\nThe SU 1.3. assessment task has connections with SU 1.1. Strategic Marked and Innovation Watch, as well as with sector-based tasks concerned with assessment and evaluation.\\n\\nWhile recognising the importance of the above and the need for a coherent approach to all assessment issues, the mandate and scope of the study group is limited to issues at the level of the LE sector. Specifically it concentrates on the preparation of a proposal for setting up infrastructure and guidelines for technology assessment and performance analysis, including a comparative evaluation where possible, taking into account the broader context of project assessment, user validation and field testing and the user driven paradigm of the TELEMATICS APPLICATIONS in general and the LE sector in particular.\\n\\nThe activities of the study group are thus limited to assessment issues at the level of individual projects and project clusters in the LE sector, as outlined by the following:\\n\\nWithin individual projects: Projects responding to calls for proposals have to take assessment issues into account from the very beginning. Proposals must outline methods for verifying and validating project results,  and dedicate a certain amount of effort to evaluation effort during project cycles. Assessment and evaluation is accordingly conducted primarily by the project consortium members themselves, but additionally by peer reviewers and EC consultants who will monitor work on an ad hoc basis.\\n\\nWithin project clusters: Project clusters may be formed on the basis of related objectives and applications (e.g. technical authoring, information access, information management), or on the basis of common user groups. Shared tasks within project clusters could include, amongst others, market research and user requirement definitions, the establishment of user forums, data gathering, the testing and assessment of generic technologies, and the development of user validation guidelines. Appropriate organisational structures to support these shared tasks need to be identified.\\n\\nAssessment issues can be divided into those concerning user-centered assessment, and those concerning technology assessment.\\n\\nUser-centered assessment involves the testing the operational feasibility and functional adequacy of system, with regard to its intended user population, and to gauge user acceptance and the socio-economic impact of the system in a real-life situation. More specifically, it concerns such questions as:\\n\\nHow is market research performed and how are user requirements defined?\\n\\nHow are different classes of user taken into account?\\n\\nWhat are the factors in the general environment the system is used in that significantly affect its usability?\\n\\nHow is evaluation of adequacy in terms of user satisfaction to be performed?\\n\\nWhat are the methods of quality management?\\n\\nHow are usability, scalability, portability and maintainability ensured?\\n\\nWhat are the migration paths and methodologies for porting applications?\\n\\nTechnology assessment refers to activities undertaken to measure the performance of the technology(ies) used within a system, with an aim to: assessing progress development within a project; establishing the suitability of a given technology to a set of stated goals; to compare methods and results with a view to stimulating technology transfer; to establish the potential of a given technology, e.g. in terms of portability, scalability and integratability with other technologies. More specifically, it concerns such questions as:\\n\\nHow is progress measured?\\n\\nWhat are the technological baselines and starting conditions required for progress measurement?\\n\\nWhat are the criteria for choosing a given technology?\\n\\nHow is the fulfilment of functional specifications determined?\\n\\nHow does the technology relate to the intended application and user population?\\n\\nGiven the emphasis of FP-4 on user-led rather than technology-driven projects, it is important that technology assessment should be tied in with user-centered assessment. One needs to be able to assess, and predict, what kinds and what properties of technology lend themselves best to different applications and user groups. Little is to be gained from measuring technology performance if this either has no bearing on, or even an inverse relation to, the user adequacy of systems employing the technology. Taking an example from another area, one could measure the technological performance of (formula 1 racing) cars in terms of how fast they go; but this would not necessarily be the primary or most appropriate technology measure for domestic motorists. One of the primary aims of technology assessment is to provide the means to abstract away from individual users, and to gauge the applicability of systems or system components to other types of user.\\n\\nProvenance of the Interim Report\\n\\nThis report is based around, and includes material from, four trigger papers written on various aspects of evaluation and assessment, and responses to the papers. The trigger papers, and one reaction paper, are\\n\\nSparck Jones  Crouch: General Technology Assessment\\n\\nNetter: Technology Assessment for Written NL Applications\\n\\nSteeneken: Speech Technology Assessment\\n\\nAdriaens: User-Centered Assessment for RTD in Language Engineering\\n\\nKing: Reactions to G. Adriaens\\n\\nThe editors have made free use of this material, and have recycled some of it more or less verbatim. However, the overall way in which this material has been used (or misused) remains the responsibility of the editors.\\n\\nThe report divides into four other sections. The first introduces terminology to be used in the report. The second discusses user-centered assessment, followed by an extensive discussion of technology assessment. In section 5., some recommendations are given about the kind of organisation and infrastructure required for carrying out user-centered and technology assessment, both at a project internal level and for the purposes of comparative evaluation.\\n\\nBackground and Terminology\\n\\nIn this section some of the terminology used throughout this report is introduced. This is, unfortunately, quite important. When there is little agreement on common terminology, as is the case in evaluation, the definitions invoked are usually a good predictor of a person\\'s final views. From the terminology here, one can predict a strong predisposition towards a form of comparative technological assessment that takes user-centered considerations into account. For wider ranging discussions of terminology and illustrative examples, see Galliers  Sparck Jones 1993, and EAGLES 1994.\\n\\nSystems\\n\\nA language engineering application system or just LE system is a set of software components constructed to permit a user to carry out some language-related task or function in a specific real-world environment. We will assume that all pilot applications funded under the FP-4 call in Language Engineering will be LE systems.\\n\\nTasks\\n\\nJust as an LE system can typically be broken down into a number of individual components and sub-components, so can the task (or function) it performs be broken down into a number of tasks and sub-tasks. Generally, we can talk of a system or system component as implementing a task. However, it is important to realise that this need not always be the case, and that task structure need not always exactly match system structure.\\n\\nFor example, a (hypothetical) connectionist language analyser might not have any separately identifiable components implementing the tasks of morphological analysis, syntactic parsing and semantic interpretation; these are all wrapped up in one monolithic system component. From one point of view, say that of someone designing a modular language analyser, the system still performs these separate tasks. But one could well imagine the connectionist system designer arguing that these do not correspond to any genuine tasks, as witnessed by the lack of any separate system components. For the purposes of evaluation, we want to avoid the ensuing factional arguments. By separating tasks from components, we have the option of super-imposing different task structures on top of a single system structure in order to promote a variety of meaningful comparisons and evaluations.\\n\\nThus, identifying task structure rather than system architecture is the first step towards defining an evaluation framework. System architecture will of course normally provide much useful information about the appropriate task structures to employ.\\n\\nA task is a mapping from an input object or set of input objects to an output object or set of output objects. Tasks can be largely defined by these input-output mappings. Some examples of language engineering (LE) tasks are: translation (texts in, texts out); parsing (sentences or word lattices in, parse trees, or labelled bracketings or dependency structures out); speech recognition (sounds in, lists of words or word lattices out).\\n\\nVisibility and Transparency:\\n\\nIt is helpful to distinguish user-visible and user-transparent tasks. User-visible (or user-significant) tasks are those where both the input and output objects have some kind of direct functional significance to a system user. User-transparent tasks are ones where the input and/or output are of no direct interest to the user, so that typically a user-transparent task is part of a wider user-visible task. In a machine translation system, the translation task will be user-visible: the input and output texts are both significant objects from the user\\'s point of view. But parsing, carried on as part of translation, is likely to be user-transparent: the user does not know and does not care about parse trees. Whether a task is user-visible or transparent can depend on the user as much as on the task. In a multi-lingual information access system, translation may well be user-transparent: the user does not know and does not care about what language the information originated in, and thus only cares about the output of translation.\\n\\nIndividuating Tasks:\\n\\nAs the parsing task illustrates, input-output mappings do not completely define tasks. This is particularly a problem for intrinsically user-transparent tasks, like parsing, since the inputs and/or outputs are theoretical objects. Depending on one\\'s perspective , one can hold different views on what kinds of theoretical objects should be involved. This is perhaps even more marked in the case of semantic interpretation, where the output might variously be discourse representation structures, situation schemata, quasi logical forms, montagovian logical forms, etc. So strict identity of input or output objects is not required for task identity. What does seem to be required is that 1. There is some degree of shared information between input objects and between output objects. There should be a relatively trivial purely syntactic transformation that converts one input (or output) representation to another input (or output) representation, perhaps with considerable loss of information. For example, one can convert word lattices to word lists (by just taking one path through the lattice), and word lists are a trivial form of lattice. Similar transformations, again with often considerable loss of information, are possible for different syntactic and semantic representations. 2. The input and output objects have basically the same functional role within similar larger task structures.\\n\\nThus, whatever the details of parsing output, it is typically has the role of input to a further task of semantic interpretation, which in turn might feed into some sort of inference task. Eventually, these chains of roles have to bottom out in obviously user-visible objects (sentences, texts, answers, etc.). Fortunately, there is fairly widespread agreement about what constitute the main language engineering tasks, despite theoretical variation.\\n\\nTask Attributes: The language  of the input or output (e.g. French, English);\\n\\nThe subject area (e.g. weather reports, financial newswire stories);\\n\\nText type (e.g. newswire stories, technical manuals, spoken dialogue, dictation);\\n\\nText length (e.g. sentence, paragraph, article);\\n\\nSpontaneity (e.g. read speech, spontaneous speech);\\n\\nChannel conditions (e.g. telephone, wide band);\\n\\nAccent (e.g. native, non\\n\\n\\n\\nnative, regional);\\n\\nSpeaker (e.g. speaker dependent, speaker independent).\\n\\nInternal Objects: Tasks can rely on internal objects (or perhaps concealed input objects), such as grammars, lexicons, language bigrams, statistical preference weightings, etc. Often, these internal objects can be altered to take account of differences in I/O attributes. For example, in a parser different grammars and lexicons for different languages or even for different theoretical frameworks.\\n\\nDepth, Accuracy and Robustness: A task can vary in the degree of depth, accuracy and robustness with which it maps inputs onto outputs. A task is done shallowly if certain details of the output representation are not captured, and is done deeply if they are. A task is done accurately to the extent that the details of the output object that are represented are or are not ones that are really there. A task is done robustly if it can produce some sort of output for any input, rather than just failing on some inputs.\\n\\nOne might wish e.g. to consider different settings of the depth, accuracy and robustness attributes for parsers to be employed in different settings. Accuracy and robustness are often also attributes of components or systems implementing tasks. But along with depth they are definitely also task attributes: one can deliberately aim to implement a shallow, robust parser, regardless of implementation.\\n\\nEfficiency: The speed, and the resources consumed in performing a task are often important attributes relating to the user adequacy of systems. Efficiency is often more of a component (i.e. implementation) attribute. But some tasks are also inherently time or resource consuming, regardless of implementation, while others are inherently efficient.\\n\\nTask Decomposition: There may be more than one way of decomposing a task into constituent sub-tasks. Different task decompositions may be more or less appropriate to different constraints on the linguistic features, the depth and accuracy, and the environment within which a task is to be performed.\\n\\nOf course an LE system may perform many tasks that are not linguistic -- for instance it may provide a user with an interface to select files to be processed for translation, to store the output of translation operations in files, and so on. Proper performance of these tasks may be as essential to the success of the system as carrying out its linguistic tasks.\\n\\nApplications:\\n\\nA useful piece of terminology is that an application is a task plus a domain, where the domain covers such things as subject matter and text type (more generally, the universe of discourse). Thus, we might have two applications for the same task, e.g. a translation system for weather reports and a translation system for airline enquiries. Similarly, we can have two applications for a similar domains, e.g. translation of airline enquiries and automatic answering of such queries.\\n\\nEnvironments\\n\\nTasks, and the systems that perform them, do not exist in isolation. They are embedded within a wider environment. For user-transparent sub-tasks within a system, the environment is determined by the sub-tasks surrounding it. For user-visible tasks, the environment is determined both by the other user-visible and -transparent system tasks surrounding it, but also by the properties of the users, e.g. degree of training, degree of urgency in getting the task done, importance of getting the task done exactly right, the other non-system based tasks they need to perform, and so on.\\n\\nEnvironments thus display a concentric, onion-like organisation, where what constitutes a system, task or component at one level may correspond to part of an environment at a lower level. One can consider an entire setup (computational system plus end-users) within the wider environment or corporate or national economies. One can consider a computational system within the environment constituted by its end-users. One can look at a major task components in a system (e.g. translation, document retrieval, speech recognition) in the context of other major task components in the system. One can view lower level tasks (parsing, coreference resolution) within the environment of the broader linguistic tasks they contribute to (e.g. translation, summarisation, message routing). And so on.\\n\\nIt is important to realise that it is not just systems that have environments. Any task or component (including non-linguistic ones)  within a system also has an environment. Evaluative principles stressing the need to match a task or its implementation to its environment do not apply just at system level. The nature of the environment might change as one descends to ever deeper levels within the system, but the same general principles apply. It is therefore a mistake to draw a completely hard line between user-centered / system  assessment on the one hand, and technology / individual component assessment on the other.\\n\\nEnvironment Attributes:\\n\\nTo match a task to its environment, we need to compare task attributes and environment attributes. By and large the categorisation of task attributes carries over to environment attributes. Thus the environment can determine the kinds of inputs and outputs expected of tasks (which language, subject domain, text type, etc); the depth, accuracy and robustness required of the task; the efficiency required. In other words, environmental attributes impose requirements on the task, and one needs to check whether the task (and/or its implementation) has the appropriate attributes to meet these requirements.\\n\\nIt is useful to identify environment and task attributes that can vary, and those that (for a given evaluation or setup) are fixed. The variable attributes will be called environment variables and task (or system/component) parameters. For example, suppose the intended users of a translation system may variously want quick and dirty translations, or clean but slow translations. Then the environmental attributes of depth, accuracy and efficiency constitute environmental variables. If the system is to cope with these varying requirements, the system/task attributes of depth, accuracy and efficiency should also be variable, and constitute system/task parameters.\\n\\nIn many cases, it may not be so obvious which task parameters need to be varied to cope with different values of environmental variables. A grid evaluation methodology naturally emerges. This involves carrying out different runs of a system or component, varying environmental attributes (e.g. language, subject domain, text type) against different settings of task parameters. Depending on what the evaluation is meant to show, one might wish to vary more than just the environmental variables, e.g. if one wishes to investigate how a system of component would fare outside its intended environment.\\n\\nEnvironment attributes tend to percolate down as one increases the granularity of tasks being considered. Thus a system environment attribute concerning speed of processing will typically surface as a similar environmental attribute for components or sub-tasks within the system. This means that user-centered aspects will usually need to be taken into account when considering the environments in which system sub-components or sub-tasks are to be evaluated. So even user-transparent sub-tasks are subject to user-centered considerations.\\n\\nAt this point the task/component distinction is useful. There may be two reasons why environmental attributes are not matched by system attributes. First, the components implementing the system task have not been coded with sufficient generality or flexibility; in this case the system may be doing the right thing, but not well enough. Second, the task itself does not have the right kind of attributes; in this case, the system may be trying to do the wrong thing. (See validation and verification).\\n\\nUsers\\n\\nUsers, who have been somewhat neglected in most speech and language assessment, come in two major varieties 1. The people who actually use the system on a day-to-day basis (end-users), and 2. The people who are paying for the system (procuring users) The two, and their respective interests, may or may not coincide. End-users will probably want something that makes their lives easier, more interesting or more enjoyable. Procuring users will be more concerned with productivity increases, and may not much care whether the system improves the lot of the hands-on users, so long as it makes them more productive (though one would hope that former leads to the latter).\\n\\npresent users, accessible for collection of user data\\n\\nprospective users, from whom such data may or may not be obtainable\\n\\nidealised users, to who future users will hopefully correspond\\n\\nindividual users: if a system is being tailored to a specific individual their idiosyncratic needs should be taken into account\\n\\ncollective users (e.g. home, professional, or mobile): most users within a collective group will exhibit broadly similar profiles, though with exceptions.\\n\\nUser Tasks\\n\\nUser Tasks or User Roles can be seen as an abstraction from users as individuals or groups for the purpose of modelling certain tasks or roles that a prototypical user has to perform in a given scenario or domain. These tasks can be defined on the basis of a work-flow analysis and can form the basis for measuring improvement of effectiveness in the performance of the tasks.\\n\\nValidation and Verification Verification: are we building the system right?\\n\\nValidation: are we building the right system?\\n\\nValidation ultimately comes down to user validation; namely that a system, the tasks it aims to perform, and the components implementing those tasks all contribute to meeting the needs of its intended users. To carry out the validation, it is vital to have a profile of the users and to determine the environment variables and range of possible settings these profiles give rise to. Having determined these, one way of subjecting a system to user validation is by carrying out a grid evaluation, varying environment variables alongside system parameters. But this kind of laboratory test is no substitute for real user-acceptance testing.\\n\\nThanks to the concentric onion-like organisation of environments, one can abstract away, to some extent,  from the needs of specific users and talk about task or technology validation. Ultimately, a task or technology only has validity if it can produce results that are beneficial to some user. So one cannot have `pure\\' task validation, independent of any user requirements. But some tasks and technologies are so far from being user-visible that it is impossible to provide direct user validation. Instead, one needs to validate them against the environments set up by other tasks, to ensure that the task in question really does deliver the kind of results that are wanted for other tasks. User requirements still have an attenuated influence at this level of validation, through the effects they have on surrounding tasks.\\n\\nTechnology\\n\\nA technology is a task for which there exists one or more alternative components implementing it, and where substantial variation of internal task parameters to match possible environmental variables is permitted. For the task of parsing, there exist a number of implementations (chart parsers, shift reduce parsers, statistical parsers). These can be parameterised by grammars, lexicons, statistical preference ratings, etc to tailor them to the different environmental variables of language, subject domain, text-type, etc. Thus we talk of parsing technologies.\\n\\nTechnologies are re-usable in different situations. They can be deployed to implement a task within a wide variety of environments. A technology has user validity if it can be deployed within some system meeting genuine user needs. The more such systems it can be deployed in, the greater the re-usability and validity of the technology.\\n\\nTechnology Assessment\\n\\nThere is a tendency to draw a strong contrast between `technology\\' and user-centered assessment. What is usually meant by technology assessment here is the evaluation of mainly user-transparent tasks or components of a system, in a way that either abstracts away from (or more likely ignores) user-centered factors. Not all sub-tasks or components within a system meet the criteria above for being a technology; some can be quite specific to a particular application or system. Therefore, the term technology assessment can be somewhat misleading: (user-transparent) task assessment might be better terminology. However, the term is entrenched, and we will continue to use it to mean a more general form of task assessment.\\n\\nIn the light of the preceding discussion of onion-like environments, it would a mistake to view technology assessment as being entirely divorcible from user-centered factors. A task or technology has to be assessed in the light of environmental factors, and these will include factors ultimately stemming from system users.\\n\\nTo be sure, many previous efforts at technology assessment have proposed abstract measures (such as matching labelled bracketings, predicate argument structure, word error rates etc) and applied them to system components without regard to user issues. This is not necessarily incorrect, merely incomplete. For the assessment to enjoy proper validity, one also needs to match the results of these measures up against identified environment attributes for the task.\\n\\nIn other words, there is a pressing need for many of the technology assessment measures currently being advocated to be subjected to (environmental) and user-centered validation. One needs to establish that the measure being used do correlate with useful properties, especially from the user\\'s point of view. Technology validation is just as important as technology verification.\\n\\nInternal Assessment\\n\\nInternal assessment refers to user and technology verification and validation that is carried out solely with regard to the particular needs of an individual project. Internal assessment typically has a diagnostic as well as an evaluative element. One does not only wish to establish the degree to which the system does the right thing in the right way. One also wants to identify, during the project lifecycle, where things are going wrong, what needs to be done to correct them, and whether attempts at correction have been successful.\\n\\nInternal assessment will normally make use of evaluation data specific to the needs of the project: either the user needs, or the technological needs. User-specific data will be determined by the nature of the intended application: the subject matter (weather reports, airline queries, aerospace technical documentation), and the task (translation, information access, document management). Given that applications will vary from project to project, this user-specific data may not be directly amenable for use as comparative evaluation data\\n\\nTechnological evaluation data will depend on the precise instances of technology used within the project. One might for example have data for assessing parser performance, which involves sentences paired with their intended parser output. The details of the parser and its output might be quite specific to the project. So again, the technological data may not be directly amenable for use as comparative evaluation data.\\n\\nThere are some areas where the border between internal and comparative assessment becomes a little grey. Important properties of any system are maintainability, adaptability and portability, which all pertain to the ability to use the system outside of its initially intended specification and environment. Changes in environment might result from: different user groups, different domains, addition of further task requirements, and so on. Assessment of these properties gauges how the present system compares to slight variants of itself, and is a form of comparative assessment.\\n\\nComparative Assessment\\n\\nUser profile (system assessment)\\n\\nOverall surrounding task (component/sub\\n\\n\\n\\ntask assessment)\\n\\nDomain, text type, etc\\n\\nChannel conditions, speaker accent, etc\\n\\nOther non\\n\\n\\n\\nlinguistic factors\\n\\nTask: One can vary the task or system goals\\n\\nDepth, accuracy, robustness and efficiency\\n\\nGrammars, lexicons, language models etc\\n\\nTask decomposition\\n\\nARPA: The ARPA MUC and TREC evaluations are characterised by having the system environment and task fixed, while varying task attributes and implementation.\\n\\nFlexibility: For an individual system one can vary environment factors such as user profile, domain, etc to see how flexible a system is. Assessing domain independence may be important here.\\n\\nUsefulness: By fixing just the user profile,  one can assess how useful various systems are for a particular user group\\n\\nPortability: e.g. by varying the hardware platform, operating system.\\n\\nFP\\n\\n\\n\\n4(? ): Let everything vary\\n\\nCross-Fertilization: By comparing different systems, technologies and implementations, projects can learn from and exploit best practices adopted in other projects.\\n\\nMaintainability and portability: Comparative assessment inevitably extends a system\\'s performance beyond its originally intended domain, fostering the properties of maintainability and portability.\\n\\nIdentifying promising technologies.\\n\\nEvaluation Data\\n\\nThe core evaluation activity involves providing a system or system component with some input and comparing its output with the results expected. The comparison may takes several forms besides a simple yes or no depending on whether the results were as required. One could measure the time and resources consumed in obtaining the results; if the actual results differ from the desired ones, one could attempt to quantify the difference in terms of both depth and accuracy; one could even measure the amount of effort involved in porting a system from one subject domain to another. But for all these modes of comparison, evaluation data is required.\\n\\nEvaluation data comes in three forms 1. Test, or input data 2. Answer, or output data 3. Training data Training data may not always be required, but for any system employing statistical  methods it is likely to be essential. Very often, old input and output data can be used as training data.\\n\\nRequirements on evaluation data are that it be realistic, and representative (Sparck Jones 1994). For test data to be realistic, it must be the kind of input data that the system or component would actually receive in real use. For it to be representative, it should contain instances from the full range of input data that would be received. Moreover, the data should either be easy to acquire, or if not then widely reusable for other purposes. This issue especially affects answer data, since acquiring it may often involve laborious hand (or semi-automatic) annotations of texts or other linguistic objects.\\n\\nA distinction should also be drawn between diagnostic data and adequacy data. Diagnostic data may be set up very carefully to pinpoint particular points at which a system is failing. Adequacy data is used to check the degree to which a system or component performs as required, without necessarily attempting to pinpoint the sources of any failings. The requirements of realism and  representativeness are somewhat weaker for diagnostic data than for adequacy data.\\n\\nIn collecting evaluation data for assessment, a decision must be made about the level of: 1. granularity at which systems will be evaluated, e.g. at the level of user-significant tasks only, or at some level of tasks that are user-transparent) 2. generality at which systems will be evaluated (e.g. how much do we vary the linguistic features of the input/output data we provide/expect relative to the features of the data in the intended application; e.g do we evaluate the system against data from different languages, different domains etc.). These two dimensions are orthogonal: the decision to evaluate at a high level of granularity is independent of the decision about which linguistic features of the input/output data are to be generalised, if any, and to what extent. The choices made will in part depend on whether internal or comparative assessment is being contemplated.\\n\\n--ilities Functionality: should satisfy stated or implied needs Covers: suitability, accuracy, interoperability, compliance, security\\n\\nReliability: should behave in a predictable and consistent way Covers: maturity, fault tolerance, recoverability\\n\\nUsability: minimise effort required to use system Covers: Understandability, learnability, operability\\n\\nEfficiency: cost effective use of resources Covers: time behaviour, resource behaviour\\n\\nMaintainability: ease of making modification Covers: analysability, changeability, stability, testability\\n\\nPortability: ease of transferring from one environment to another Covers: adaptability, installability, conformance, replaceability\\n\\nMeasures and Metrics\\n\\nAssessment metrics need to be specified at three levels 1. (Qualitative) Criteria: what kind of thing is to be measured, e.g. translation quality, productivity improvements, ... 2. (Quantitative) Measures: how the criteria are to be measured, e.g. amount of post-editing required, increased throughput of documents, ... 3. Application Methodology: how the measures are to be applied in a uniform and consistent way.\\n\\nUser\\n\\n\\n\\nCentered Assessment\\n\\nThis section addresses user-centered  assessment. As such it is primarily, though not exclusively, concerned with project internal evaluation; the next section on technology assessment takes up the issue of comparative assessment.\\n\\nThe Project Level\\n\\nEvolutionary Life\\n\\n\\n\\nCycles\\n\\nThe classical project development life-cycle consists of the following major phases: 1. requirements analysis and definition (in LE terms: preparation stage) 2. system and software design  (in LE terms: development / implementation verification  and testing  stage) 3. delivery and maintenance   (in LE terms: demonstration, documentation, exploitation stage) In its simple form, this model has advantages, but also major drawbacks. One advantage is that it allows for imposing clear milestones (assessment points) at the  transition from one major phase to another. A major drawback is that users, if consulted at all, are brought in either too presupposes that the different stages are independent, can nicely be finished before proceeding to a next stage (with an all-or-nothing delivery at the end), and hardly need feedback-plus-change iterations\\n\\nMaximal user involvement: for each relevant system increment, users can and should participate in  all stages (from analysis to test), and trigger a number of iterations before accepting a component. Users are no longer forced to state all their requirements (which they may only know when they see a concrete system component!) beforehand, and to accept a fully developed system of which they can at most suggest some cosmetic changes at the end.\\n\\nComplete analysis, design, build, test and document in each step; much tighter interwovenness of all phases (for instance, one can no longer afford to postpone writing a test plan corresponding to requirements, or to postpone documentation writing forever)\\n\\nMaximal adjustment flexibility: if mistakes are made, they can be detected and corrected early; if certain design or tool options change (given the rapid evolution in the software world), they can be incorporated more easily\\n\\nInherent need for open-endedness and extendibility: all increments must be fitted seamlessly, and hence maximally modular, compatible and interoperable (which should also improve reusability)\\n\\nBottom\\n\\n\\n\\nup early\\n\\n\\n\\nresult orientation, not top\\n\\n\\n\\ndown last\\n\\n\\n\\nminute software\\n\\ndevelopment orientation\\n\\nThe number of iterations at different evolutionary stages must be kept under control (say, maximally three). For instance, users may tend to keep changing their minds as they see different proposals for interface solutions. At some point, they should formally accept a solution so as to allow the project to continue its course, and not go around in circles.\\n\\nFeedback loops require a solid management of change (by experienced project managers), and a carefully designed verification and validation approach; regression tests, for instance, become crucial (see below)\\n\\nThe evolutionary approach implies a lot of activity at the micro-level of a project, but the risk is that one is unable to see the wood for the trees. Hence, it must be controlled; one way of doing this is by still imposing the classical development milestones (end-of-analysis, end-of-design, end-of-laboratory-development, end-of-alpha-test, end-of-beta-test) on the evolutionary cycle, although the point in time may be kept flexible. And of course, as for any type of project, resource constraints (time, money, people, tools) largely determine what is possible for each step.\\n\\nThe evolutionary model also has implications for an external evaluation and review process; rather than only having major mid-term and end-of-term reviews, less extensive but more frequent checkpoints or spy-points may have to be imposed on the project. Although at first sight this looks like a lot of overhead, it allows for timely adjustment and better overall project results.\\n\\nUser Profiles\\n\\nUser can either be end-users or procurers. They may be characterised individually, collectively, prospectively or hypothetically. But in all cases, information profiling users is a necessary starting point for user-centered assessment.\\n\\nBackground factors (education, training, experience, knowledge, language abilities, skills, etc.)\\n\\nPhysiological factors (vision, hearing, touching, dexterity, speed)\\n\\nPsychological/cognitive factors (learning, memory, attitudes, beliefs, expectations, mental models; the latter constitute the framework of concepts, objects, actions, structures, tasks, metaphors etc. the user has in mind for subsequent visualisation and use in interaction with a computer)\\n\\nApplication interest and use factors (frequency of use, goal in using)\\n\\nUser opinions, preferences, subjective data (obtained by interviews or questionnaires) These data correspond to the expressed wants of users and need not be consistent with the objective data. Confronting both may already help in finding the \"real\" needs; the next type of data can also be very helpful in determining these real needs.\\n\\nMeasured or observed data, obtained by recording how the user actually performs in interaction with the computer for a particular application These data correspond to demonstrated needs and wants. Practically, they can be obtained by video taping, or better still by internal recording mechanisms added to an application. This can range from simple modules gathering usage statistics   to sophisticated software tools that record all on-screen user manipulations in a kind of movie. The latter could be complemented by video taping to also record off-screen activity. Of course, not every application should go so far, but the module  gathering usage statistics should not represent a major effort and still be a very useful instrument in trying to objectively determine what the users really want on the basis of their behaviour with a system (in order to produce improved subsequent versions)\\n\\nSystem Attributes\\n\\nUser profiles determine system-wide environment attributes and variables. The system attributes that should be matched against environment attributes should be identified, and success criteria and measures for meeting the system attributes must be set up.\\n\\nTwo very broad system attributes are usability and integratability. These attributes can be refined, and tested in the following manner.\\n\\nUsability\\n\\nLevel of personal ability to enter training courses (if any) for the product.\\n\\nTraining time required to attain a pre-determined level of productivity with the product\\n\\nThe specified amount of work to be produced by a person so trained\\n\\nThe rate of errors made by a trained user, operating at the normal work rate\\n\\nThe opinion of the users as to how well they liked the product\\n\\nIntegratability\\n\\nNext to user profiles on the microlevel, corporate organisation profiles (companies, institutions, etc.) or subdivision profiles (documentation division, translation division, EDP division) could be drawn up at the macro level to get an idea of the global  environment in which an application will be integrated\\n\\nInformation and work flow analysis and the effect of new technologies on these in terms of changes in processes, structures, tasks; changes in user profile requirements (new types of occupation, need to retrain or hire specialised personnel); changes in computer facilities, etc. could be investigated\\n\\nTesting and Acceptance\\n\\nThe following test types occur during development of a system or system increment; they are ordered chronologically, by decreasing involvement of system creators and increasing involvement of users, and by increasing importance of integration matters in ever-broadening contexts: 1. component test (unit, module) 2. integration test (in the laboratory) 3. alpha test (system or system increment test by other people than the developers, typically prospective end users, possibly still in a controlled environment) 4. acceptance test (a formal test initiated by the prospective procurer/buyer of a system or system increment, typically at the installation site; for custom-built solutions, this is a final test in the test chain for a system or system increment) 5. beta test (in case of a development which has to become a product at large -- as opposed to a custom-built solution - further field testing of a completed system with selected prospective users) At each level of testing, different iterations following feedback and test results may be needed, i.e. regression tests must be carried out as a way of monitoring progress. Regression testing presupposes a rigid approach in which test data are well-defined, and evaluation tools exist to determine changes from one test run to another. Planning of regression tests is also notoriously difficult, because it is unpredictable whether and how fast a system will converge to a stable state.\\n\\nCare should be taken to foresee time for these regression loops inside the different testing phases. In the context of EC projects with partners in different countries and ever more complex cluster structures, it may be important to apply acceptance tests internally to the project. After all, from an industrial perspective, the typical project structure is one of a prime contractor (with final responsibility), and a number of partners required to deliver subcomponents. Hence, the prime contractor should have a formal acceptance procedure for outsourced components as part of his quality assurance plan. This may not be a trivial task, because different companies may have different quality assurance plans and acceptance criteria; moreover, the prime contractor cannot impose his own approach on the other partners. To avoid conflicts in this respect, it may be a good thing that partners make explicit their quality assurance approaches; if a partner does not have an in-house approach, one could decide beforehand to adopt the approach of another partner (typically the prime contractor). In these situations, adherence to overall standards (such as ISO-9000) would be a solution, but we are not that far yet. In the meantime, however, ANSI/IEEE standards do exist for different stages of a development cycle; they provide a backbone or checklist of activities and procedures that should not be overlooked.\\n\\nWhat are the organisations involved and their responsibilities in the acceptance test?\\n\\nHow can requirements be traced to test cases?\\n\\nHow will the acceptance process be administered?\\n\\nAre the test cases and test procedures complete?\\n\\nHow will error reporting and error analysis be done?\\n\\nWhat will be the location, testing approach, facilities, equipment, training for the tests?\\n\\nWhat are the time and money resource implications of the tests?\\n\\nWhat are the items to be tested for?\\n\\nWhat are the objectives and constraints for each subtest?\\n\\nHow can the test design and test cases be traced to system requirements (i.e. are they already considered in the requirements and specifications documents?)\\n\\nWhat are the supporting tools required for each test?\\n\\nWhat are the expected inputs and outputs of each test case?\\n\\nWhat are the initialisation and stopping conditions for testing?\\n\\nWhat are the concrete criteria to say that the system passed the test?\\n\\nWhat test procedure corresponds to the test design and test cases?\\n\\nWho (what particular user(s)) will do the testing where?\\n\\nWhat are the required pretest conditions in the test environment?\\n\\nHow are test results reported?\\n\\nIn case of problems, how are problems reported and what is the procedure for handling them?\\n\\nUser Interfaces\\n\\nThe importance of user-interfaces, and the difficulty of assessing them, should not be overlooked in user-centered evaluation. However good the underlying system, a poor user interface can make it practically unusable. Adriaens\\'s trigger paper contains many valuable comments on interface design. From the point of view of testing, a major difficulty arises from the fact that it is difficult to precisely reduplicate test circumstances. Within an interactive system, the user\\'s knowledge and circumstances inevitably change over time, so that one cannot usefully repeat individual tests.\\n\\nThe Project Cluster Level\\n\\nSince different sets of users are involved at the project cluster level, it is not immediately obvious what role user-centered assessment has there, or even whether it is possible.\\n\\nBut most of the reasons for pursuing comparative evaluation do persist at the user level: e.g. cross-fertilization through the recognition and adoption of promising approaches and technologies; promotion of maintainability and portability by evaluating a system outside of its immediately intended environment to gauge the ease with which it could move between environments.\\n\\nOne possible mode of user-centered comparative evaluation would be to try the user group from one project out on a system from another project having a broadly similar,  though not necessarily identical, functionality. As well as providing further information about learnability, it is possible that users will find features in other systems that they would like to see incorporated in their own.\\n\\nAnother mode of evaluation would be to exchange subject domains between projects in the cluster (assuming that suitable evaluation data can be made available), to assess the flexibility and portability of systems. However, if systems are especially unportable, the cost of changing domain may be prohibitive. Domain change could either go in tandem with, or be separate from, exchanges of user groups.\\n\\nFinally, and again probably assuming that systems can change domains, integratability could be addressed by actually trying to combine either parts of systems or entire systems.\\n\\nAll of these modes of comparative evaluation have an over-optimistic ring to them. The problem with user-centered comparative evaluation comes down to the fact that user groups may simply be too heterogeneous to permit much meaningful comparison. This being so, there is a danger that user groups within projects will be resistant to this form of comparative evaluation.\\n\\nWhat appears to be feasible at the present stage, however, is to draw on the expertise and experience that different projects have to and will build up during project internal validation and to encourage a strictly bottom up development of standards. Although user-centered evaluation will always be geared towards the individual users, it is quite likely that users will be interested in profiting from experiences with validation methods gathered in comparable projects and applications.\\n\\nFor this purpose, the user-centered evaluations methods applied by the different projects should be made publically available and/or collected, compared and processed by some independent institution. What could be distilled from these data are abstractions from comparable evaluation scenarios, evaluation metrics etc. In the medium and long run this could result in a kind of evaluation library containing user models, abstract scenarios, evaluation methods, test suites, tools, metrics etc., which have been successfully applied in user validation.\\n\\nClearly, this will not result in a comparative evaluation proper, however, such a collection and its continuous improvement and refinement could lay the foundation for more broadly accepted standards in the area of user-centered evaluation.\\n\\nTechnology Assessment\\n\\nTechnology and Comparison\\n\\nPurely user-centered comparison, viewing systems as black boxes, is a doubtful proposition. User groups (and systems) tend to be too heterogeneous to make meaningful comparisons feasible. However, if one looks at the structure of different systems, one usually finds that they have tasks and components in common. Of course the same task/component in two different systems might have quite different attributes in terms of user-visibility, depth, accuracy, robustness, efficiency, language, text-type, etc etc. Any comparison between the task performance in the two systems will need to take these differences into account, and also try to relate performance to user attributes.\\n\\nIt is vital that one does not abstract away completely from user-centered considerations. `Pure\\' technology evaluation metrics require user-centered validation. There is little benefit to be gained if the technology metrics bear no relation to the ability of a system to perform its chosen task for its chosen user group. Technology must be evaluated relative to the environment in which the technology is to be used, and this brings in a reference to user requirements, albeit indirect.\\n\\nUser-centered validation of technology evaluation metrics has received scant attention. The metrics proposed in the course of such things as ParsEval and SemEval (labelled bracketings, predicate-argument structures, co-reference relations, etc) make sense from a technologist\\'s point of view. But beyond intuition (which may well turn out to be reliable), not much is known about how these metrics correlate with the performance of systems, taken from a user\\'s point of view. Although highly unlikely, it is conceivable that that there is no correlation. Rather more likely is that for some tasks there is an inverse correlation. To take an admittedly fanciful example, success in identifying co-reference relations may have an inverse correlation with the adequacy of spell-checkers; this might reflect the fact that co-reference imposes few if any constraints on spelling, and that effort which would have been more profitably directed elsewhere has been wastefully devoted to co-reference processing.\\n\\nIf user-centered validation is to be taken seriously, this means that the initial stages of any comparative technology evaluation exercise would need to be devoted to it. The aim would be to propose a number of candidate technology metrics, and to measure them on a variety of systems in parallel with carrying out direct user-centered evaluation. Validation would consist of correlating the two evaluations, in the hope of finding which kinds of technology are best suited to which kinds of task. It is possible that this would require a number of iterations in order to find the most informative technology measures, and this could well be a long-term undertaking.\\n\\nAfter initial phases of user-centered validation, benefits would start to flow. One being the prospect  of being able to make informed choices about the choice of technologies in the initial design of a system for a given task. On the assumption that technology assessment can be carried out using standardised test data, users and systems designers can identify appropriate technological tools without first having to perform user-specific evaluation, where the collection of test data is likely to be expensive. This is not to say that user-specific testing can be dispensed with; far from it. Merely that in the initial feasibility and design stages it may be dispensable. Another benefit is that users and systems designers would have a much better idea of the ways in which a system can be modified and adapted to deal with different tasks.\\n\\nCross-fertilization One of the bye-products  of the ARPA MUC and TREC comparative evaluations has been cross-fertilization between different systems. If one system employs a technique that is shown perform particularly well on a certain sub-task, then the other systems in the evaluation have tended to make use of that technique to improve their own performance. Admittedly, the ARPA evaluations all compare systems performing the same task, on the same subject domain, for the same hypothetical user group. This makes cross-fertilization particularly easy. But systems designed for a more disparate range of tasks are still likely to have at least some sub-tasks in common (e.g. spell-checkers and information retrieval systems will probably involve word segmentation and morphological processing). Comparative evaluation means that funding user groups can be more confident that the most appropriate components have been employed to perform various sub-tasks. More generally, the competition that comparative evaluation engenders means that technologists developing systems are kept on their toes and don\\'t become complacent. Given that most users will probably not have the technical expertise to detect such complacency, this is bound to be in their interests.\\n\\nMaintainability and portability Over the course of time, the precise tasks that a system is called upon to perform tend to shift. It is important that the system be maintainable and portable in order to deal with these shifts in use. One way of promoting this kind of flexibility is to test and evaluate a system by consideration of tasks and application domains lying outside its immediately intended areas. A properly set up comparative evaluation can do this, since components of a given system will be run on data from different domains, and where the data arise in response to different sets of user requirements.\\n\\nIdentifying promising technologies Of longer term interest is the fact that comparative evaluations should serve to identify promising technologies. This may not be of immediate interest if a given system has been set up in such a way that it is committed to a particular, less than optimal, technology. But for developing future systems, the information may be valuable.\\n\\nComparative assessment involves a degree of abstraction away from specific tasks, domains and user groups. Some kind of general technology assessment is required to achieve this abstraction.\\n\\nPerforming meaningful technology assessment entails the identification of relevant environment and user attributes.\\n\\nAny technology measures used must have a user-centered rationale.\\n\\nUser-centered validation has not previously been carried out to any satisfactory degree. Therefore, an important initial component of any comparative evaluation exercise for the FP-4 should be this kind of validation. This involves proposing general technology metrics, applying them to individual systems, and comparing the results with user-centered assessment of the systems.\\n\\nOne consequence of this is that comparative evaluation is unlikely to lead to a league table saying which of the participating systems is best. Rather, it should lead to a grid identifying what it is about different systems that makes them well suited to their task.\\n\\nConstraints on a Comparison Exercise\\n\\nSubstantial comparison exercises have been undertaken by ARPA. While having certain flaws, these exercises have been of benefit. However, it is unclear whether comparative evaluation under FP-4 can or should follow the same path. The main feature of the ARPA exercises is that, while growing from small beginnings, they have been primarily top-down. Systems have been constrained to operate on the same application (and for the same hypothetical user group). But within FP-4 there will be a wide variety of applications and user groups. A top-down, ARPA style approach is not applicable to all the FP-4 projects in their entirety.\\n\\nOne possibility would be to set up a small ARPA-like evaluation project under FP-4. However, this would (a) exclude remaining FP-4 projects from comparative evaluation, and (b) does not sit well with emphasis on users in FP-4 -- the evaluation task would doubtless involve a large degree of artificiality. Nevertheless, amongst many technologists there is enthusiasm for a top-down approach.\\n\\nAnother way of tackling things would be to build an evaluation exercise bottom-up from existing FP-4 projects, hopefully building on the work done for internal project evaluation. Not all of the FP-4 projects need be involved. Indeed, it would be more practical to start with a limited number aimed at connected, though not identical, applications. In what follows, we assume that a primarily bottom-up scenario is more in tune with the aims of FP-4.\\n\\nIf a more bottom-up approach to comparative evaluation is to succeed, it is important that participation be relatively painless and inexpensive. Especially so, given that in its initial stages the benefits of participating in a comparative evaluation may be slow to accrue. This in turn means that the evaluation exercise may start off rather small but grow, so that the exercise should have a structure that allows for development over time.\\n\\nIn reality, the distinction between a top-down and bottom-up evaluation exercise is somewhat artificial. A `left-corner\\' framework, combining both top-down and bottom-up features is preferable. In this, one tries to build up from individual projects, but in order to do this a certain amount of standardisation (in terms of the types of evaluation data used, and the measures employed) needs to be imposed top-down.\\n\\nMore specifically, constraints on the exercise are that: 1. The exercise should be incremental for the future, with respect to data, tests and experience: it should be possible to roll the materials, experience and results of earlier stages forward into later ones. 2. The exercise should have low entry and working costs to allow and encourage participation. 3. The evaluation should address tasks with multilingual and multimodal (spoken and written language) aspects, falling within the areas covered by approved LE projects. 4. The materials should be cheap, including both working data e.g. corpora and, more importantly, the evaluation data with `answers\\' for chosen tasks. 5. The materials should as far as possible be reusable, or multipurpose, for other SLP research. 6. The evaluation program for computing performance measures should be relatively easy to provide and apply. 7. The evaluation structure should allow both technological and user-centered evaluation. 8. As far as possible the comparative evaluation exercise should sit on top of, and make use of, project internal evaluation. In addition, as argued previously the evaluation exercise should, in its initial stages if not throughout, address itself to the question of validating technology metrics in user-centered terms.\\n\\nBraided Evaluation\\n\\nA braided evaluation structure is a candidate meeting the requirements set out above. The braid model starts from the observation that tasks of any substantial complexity can be decomposed into a number of linked sub-tasks.\\n\\nSome of these tasks will be user-significant, others will be user-transparent For example, a document authoring and management system may perform a number of distinct user-significant tasks (spelling correction, grammar correction, dictionary/terminology access, document retrieval), and these may depend in turn on a number of user-transparent tasks (word segmentation, morphological analysis, parsing, selection of index terms for document retrieval, etc).\\n\\nSub-tasks within an overall task will be linked by language objects, which might be texts, sentences, parse trees, predicate-argument structures, phoneme lattices, etc. A given sub-task will have one or more input objects and one or more output objects. The input and output objects provide obvious material for black-box evaluation of the sub-task (which will become glass-box evaluation if the sub-task is further decomposed).\\n\\nSometimes, all sub-tasks within an overall task will be linked in a simple, linear, nose-to-tail structure. But more often, the sub-tasks will exhibit a branching, or even braided structure. The more complex structure reflects the fact that in a complex task (like creating a document, say), there are different routes through the task structure, and different options that one might employ or ignore on different occasions.\\n\\nDifferent overall tasks will often have overlapping sub-tasks. The more similar the overall tasks, the greater the degree of overlap. When two or more tasks are decomposed and put together, what emerges is a braid-like structure. At some points, the task decompositions will diverge, maybe to partially converge at a later point, or maybe never to join up again. A given sub-task may have alternative decompositions, so that although it has the same input and output objects, it may pass through quite different intermediate objects under different decompositions.\\n\\nThe point of decomposing tasks and putting them together in a braid structure is that it allows one to identify natural common evaluation points. These are where two or more overall tasks have common sub-tasks, even if the sub-tasks themselves are not necessarily composed in the same way. However, care must be exercised here. The fact that two tasks share a common sub-task does not mean that those tasks have identical input and output properties; merely comparable ones. It is quite possible that two tasks sharing a common sub-task pursue the sub-tasks to different levels of detail. For example, a parsing sub-task may be required to give only a shallow analysis in the context of one overall task, while a deeper more detailed parse tree is required for the other.\\n\\nThe braid consists of sequences of (linguistic) tasks, linked through common (language) objects -- e.g. sentences, texts.\\n\\nThere are natural evaluation points, not only at the end points of the whole braid, but at other intermediate points referring to one or more of the preceding member tasks.\\n\\nWhen a sub-task is user significant, then user-centered evaluation metrics can be applied.\\n\\nWhen a sub-task is specific to a particular kind of task, e.g. retrieving documents matching a query or filling in slots in a standardised template, then metrics specific to that particular kind of task can be applied (e.g. precision, recall, etc).\\n\\nWhen a sub-task can be seen as an instance of a general SLP task (parsing, morphology, word recognition), then general technology metrics can be applied\\n\\nA braided structure provides a flexible framework, allowing comparison of different tasks where possible.\\n\\nCommon Resources for Braided Evaluation\\n\\nA braided evaluation structure allows for comparative and individual evaluation of different systems at different levels (user-centered, task-specific, general technology). It has the flexibility to incorporate new tasks and systems as it goes along. Unfortunately, it does not make comparative evaluation a simple and trivial matter, requiring no central resources or effort. While every attempt should be made to bring project internal and comparative evaluation as close together as possible, it would be unrealistic to expect a comparative evaluation to run itself, with the only impetus coming from within individual projects.\\n\\nOne of the important insights of previous attempts at evaluation on a large scale has been that it always turned out to be a very cost and time consuming enterprise. Both cost and time can be reduced if methods, test data, and interpretation results can be shared by a larger community. For many institutions, the possibility of sharing in such an infrastructure for an evaluation scheme has, by itself, proved to be sufficient attraction to voluntarily undergo an evaluation.\\n\\nObstacles to Shared Infrastructure\\n\\nHowever, the sharing of such an infrastructure is hampered by various factors, such as diversity of application tasks, domains, system architectures, text types and--crucially in a multi-lingual environment--diversity of languages. A braid model is not, in itself, a solution to these problems.\\n\\n(a) Tasks\\n\\nResearch politics in the past (and possibly present) has favoured a diversification of technologies and applications, in the sense that projects had better chances for funding when they suggested not only a different solution and technology for a known problem, but also when they suggested a new and interesting problem. ``No parallel research\\'\\' has lead to a vast amount of diverse technological developments in NLP, which on the surface are hardly comparable and are difficult to assess against each other, mainly because they are claimed to serve their purpose only within specific and different applications.\\n\\nTask specific assessment of technologies will thus be feasible only if there is a large enough number of systems performing the same tasks. Task oriented assessment of technologies would require that the contributions of a module to an application could be factored out in a reliable way.\\n\\n(b) Domains\\n\\nEven if more than one project works on the same task, it is almost invariably the case that they will not involve the same subject domain. Typical examples here are small to medium scale MT systems, which are often constrained to specific domains.\\n\\nWhat is more frequently found though is different tasks being for the same domain. However, here the obvious problem arises, that domain specific test data typically do not carry over between different tasks if they are annotated in a task-specific way.\\n\\n(c) Evaluation Data\\n\\nEvaluation data has to be chosen that is sufficiently representative for the task or application to be assessed. Normally, this problem is circumvented by collecting a sample which is large enough to fulfill this requirement. In very few cases test suites, with fully controlled and more compact test material, have been constructed. To our knowledge, none of these test suites have been applied to evaluation as opposed to diagnosis; test suites are generally designed to meet criteria other than user-centered evaluation.\\n\\nLarge corpora by themselves, however natural and representative, do not suffice as the basis of an automatic assessment procedure, since they only provide the input but not the output against which the performance has to be assessed. What is required in most cases is therefore some kind of annotation to the corpus, providing answer data. Depending on the type of technology assessed, these annotations can range from tagging, phrase structure annotations, etc., to content classifications in some abstract way.\\n\\nThe problem is that most reference corpora and annotations of this kind are tailored towards the assessment of some specific technology or task. Again, to some degree this appears to be justified if the text sort or the domain are very specific to the application. However, this also means of course that the efforts for collecting and annotating corpora have to be duplicated, even for those cases where annotations could be shared at least in part.\\n\\n(d) Languages\\n\\nIn a multi-lingual RD environment comparability across languages represents another important factor. Trivially, parallel multi-lingual corpora could play a role in the assessment of MT systems, although adequacy of MT systems is hardly ever tested by comparing system output with a predefined translation on the level of corpora.\\n\\nParallel test corpora in different languages with comparable annotations could become relevant, if portability of systems from one language to another is to be assessed.\\n\\nLayered Corpora\\n\\nWhat appears to be required to address the issues and problems mentioned above is the provision of an infrastructure which allows an open competitive assessment on the basis of predefined reference material which supports glass box evaluation of tasks and technologies wherever this is feasible.\\n\\nThis involves not only the development of an appropriate organisational framework, but also some considerable effort in constructing corpora serving as the basis of the assessment. Klaus Netter\\'s trigger paper argued that these should be assigned layered annotations at different levels of abstraction matching the envisaged intermediate reference levels for glass box evaluation. These would correspond to the input and output language objects linking tasks in a braided evaluation structure\\n\\nThese annotations could include all kinds of information, such as morpho-syntactic tagging, word sense disambiguation, phrase structures, relational structures, semantic representations including resolved references, but also specific annotations for a range of envisaged tasks. What these reference levels are would have to be agreed on by the groups involved. Ideally these corpora should also be constructed in parallel for different languages.\\n\\nThe idea of corpora with layered annotations (rather than different annotations distributed over a wide number of corpora) would be that projects can enter a competition on the basis of a corpus without being constrained by either their application or by the specific architecture of their system. Even if it would not necessarily provide the assessment basis for some specific application, the technology of some subcomponents or modules could still be evaluated. Of course, systems would also not be forced to follow a strict architectural setup, since they could equally well be assessed at only those subsets of the reference levels, which are relevant for the respective applications and which are employed in the individual architectures.\\n\\nWhile it might be possible to provide a single layered corpus, this would probably be a mistake. First, one would have to exercise great care in selecting a corpus that lends itself equally well to a wide variety of different tasks. Second, a single corpus would not provide the means necessary for assessing the domain independence of systems. One would therefore hope to be able to build up a handful of layered corpora, uniformly annotated.\\n\\nOther Resources\\n\\nProvision of layered corpora is liable to be expensive, and some ways of reducing the cost by building on project internal corpora are suggested below. But in any case, more than corpora will be required to support comparative evaluation using common data. This includes further linguistic data and resources, as well as non-linguistic resources\\n\\nExtra linguistic resources needed might include: lexicons, terminological databases, or  statistical domain models. Non linguistic resources might include: the data held in exemplar databases, logical axiomatisations of world knowledge, and so forth. Not all the extra resources would be required for all systems being evaluated. However, some kind of database or information access system would require, in addition to a corpus of possible information requests, some kind of database containing the answers to the requests. A document authoring aid would probably not require this kind of resource, but if the system was intended to allow the incorporation of material from other documents, it would require data about those documents.\\n\\nSoftware tools for scoring results, and possibly also the semi-automatic annotation of answer data would also be required.\\n\\nExploiting Project Internal Resources\\n\\nAny project that tackles internal evaluation seriously is likely to build up evaluation data in the form of layered corpora, lexicons, terminological databases, databases, etc, as described above. If at all possible, it makes sense to build on this kind of test data to provide material for comparative assessment.\\n\\nHowever, even if individual projects can be persuaded to part with their own, hard won test data (in exchange for equally hard won data from other projects, perhaps), this alone is not enough. The problem is not that the data will be specific to a particular domain. In fact, this is an advantage, since evaluation across a range of domains is a good way of ensuring that systems do not contain ad hoc domain-specific short cuts. The problem is that the test data will only be appropriate for certain tasks and sub-tasks. It will typically not be of much help in evaluating those parts of other systems dealing with quite different tasks applied to the same domain.\\n\\nFor example, a mono-lingual information access system for airline reservations simply will not provide the kind of test data required for evaluating the multi-lingual aspects of a multi-lingual access system for airline reservations. Nor is it likely to provide the kind of annotated test data required for evaluating some authoring tool dealing with an airline reservation domain.\\n\\nTest data provided by individual projects is thus a starting point for building up shared evaluation resources, but it has to be built upon. It needs to be extended to cover tasks not relevant to the system from which the data originated. In addition, the project data itself needs to be subjected to some kind of quality control: a test suite of half a dozen questions and their expected answers does not even provide a good starting point for building shared resources.\\n\\nTo further facilitate the re-use of project internal test data, a degree of top-down imposition is desirable. If a core of technology metrics can be decided upon in advance (e.g. labelled bracketings, predicate-argument structure, co-reference relations, word sense identification), then projects can be encouraged to produce internal test data appropriate to these kinds of metric. This does not mean that projects cannot also develop test data annotated towards further requirements of their own, nor that they are prevented from arguing for the inclusion of these further annotations as part of the common test data. One needs to retain flexibility and room for development in what should constitute common test data.\\n\\nRelation to ARPA\\n\\nStandards for technology assessment of written NL applications are only gradually emerging. The most prominent instances of practical applications in this area in recent years have been the MUC and TREC conferences for message understanding and text retrieval respectively, and possibly schemes such as ParsEval and its upcoming successor SemEval (some of which will be part of MUC-6) for the testing of broad coverage text analysis components.\\n\\nFor MUC and TREC the main cornerstones of the assessment methods were black box evaluation, the usage of naturally occurring corpora as the test material, and the employment of ``recall\\'\\' and ``precision\\'\\' as evaluation metrics. Both conferences led to advances in the field not only by challenging competitions but also by exchanges of experiences of technologies. An important factor on the success side appears to have been that these conferences, although sponsored by ARPA, were not limited to ARPA sponsored projects, but organized on the basis of an open competition.\\n\\nThe main criticism that has been levelled against the evaluation methods in these conferences is that it did not support the development of strategically relevant promising technologies (this is somewhat truer of MUC than TREC). Since what was evaluated by these conferences was task performance rather than technologies in the narrower sense, they supported task and application oriented short-cuts to some degree.\\n\\nIn contrast to the task oriented MUC and TREC, Parseval was an approach which attempts to evaluate at the module level by using a benchmark method. Its basis was corpora which were annotated by some syntactic structure agreed on by a representative number of experts. The output of analysis components was measured against these annotations.\\n\\nThe main criticisms of Parseval are on the one hand that the syntactic annotations had to be very abstract to allow mapping of the output of different analysis systems onto these annotations. Systems which over-performed and produced more specific output where thus at a disadvantage relative to systems which would produce just about the required result. Second, phrase structure analyses still favour a certain theory specific convergence on what is assumed to be an accepted structure. SemEval is similar to Parseval in these respects, e.g. over-performing systems with a deeper semantic understanding will still be at a disadvantage, and even if predicate argument structure is more abstract it will still not be completely theory neutral.\\n\\nWhat appear to be the main limitations of current evaluation methods, if viewed from the point of technology assessment are the following aspects.\\n\\nMany of the performance evaluation methods, such as employed in MUC and TREC, are task specific. Thus, it is practically impossible to compare technologies unless they are integrated into an environment which tests the respective task.\\n\\nThere is little methodology for evaluating the portability of technologies. This partially results from the concentration on application specific assessment, but partly also from the limitation to pure task-independent assessment. In a multi-lingual environment the latter criticism can be applied in a similar way to the porting of technologies from one language to another. Important factors for the strategic advancement of technologies are thus practically ignored at present.\\n\\nAn Illustrative Evaluation Scenario\\n\\nThis section expands on a scenario for a braided evaluation exercise outlined in an earlier trigger paper. We should emphasise that this is for illustrative purposes. We are not proposing that this particular scenario should form the basis for an evaluation exercise set up under the FP-4. It would be possible to use the scenario in this way; but it is preferable to pursue a more bottom-up approach to deciding on an evaluation structure. In order to involve as many LE projects as possible in evaluation, it is better to wait and see what tasks and projects are actually to be worked upon, and then build the task structure up from that. In this scenario, we merely try to show how various types of evaluation can be combined, with the possibility of validating them against each other.\\n\\nThe task scenario centres on document retrieval and translation, assuming that document requests couched in one language may need to be searched against document files in other languages, and that retrieved document titles in one language may need to be translated to others. This is something that could very easily form a sub-task within various LE projects: e.g. those dealing with document management and authoring and those dealing with information access. In addition, the translation aspects will have points in common with machine translation projects. One could therefore envisage a considerable range of further tasks and sub-tasks being added to the braid. But here we deliberately consider only a relatively simple task structure.\\n\\nWe will start by summarising the main path through the task structure: L-OBJ 0             Request sentences for documents TASK 1              Select and translate index terms for document retrieval L-OBJ 1         ]]  Search term lists TASK 2              Document retrieval L-OBJ 2        ]]   Retrieved documents TASK 3              Translate retrieved document titles and/or abstracts L-OBJ 3        ]]   Translated document titles/abstracts This gives a very coarse breakdown of the task structure, and all of the tasks could be sub-divided into further sub-tasks.\\n\\nTask 1 is likely to be user-transparent: in many cases, users will not be interested in index terms selected for file searching by the system, only in the documents retrieved as a result of them. The pair L-OBJ 0 and L-OBJ 1 therefore will not provide suitable material for user-centered evaluation. However, it will provide material for a task specific evaluation, which e.g. checks on the soundness and completeness of the index terms selected (soundness: proportion of correct indexes selected, and incorrect indexes not selected; completeness: proportion of total number of  correct  indexes actually selected).\\n\\nThis task-specific evaluation needs to be treated with care. What counts as the correct index terms may very well depend on the nature of the retrieval system carrying out Task 2. So direct, task-specific comparisons between systems employing different retrieval systems may very well not be possible. This illustrates one of the problems with task-specific evaluation: the nature of a task may depend on its surrounding context, so that even two apparently identical tasks may not be directly comparable when their contexts differ.\\n\\nDepending on the nature of the systems being tested, Task 2 may not in fact be regarded as an SLP task, although it may very well include some sort of language processing. The pair L-OBJ 1 and L-OBJ 2 provides material for evaluation of the retrieval system. But from the point of view of the task structure as a whole, the pair L-OBJ 0 and L-OBJ 2 is perhaps of more interest. The transition between the two is very likely to constitute a user-visible task, so that user-centered assessment is possible. This would involve not only comparing the language objects, but e.g. assessing the extent to which the document retrieval improve the speed and quality of whatever wider task the combination of Tasks 1 and 2 is embedded in. The language objects also permit a relatively context neutral task-specific assessment of overall retrieval, since the form of the desired results are liable to be largely independent of constraints imposed by the following translation task.\\n\\nTask 3 may very well be entered from other directions, without first going through document retrieval. One could envisage user, task and general technology measures being applied to Task 3. In addition, the combination of Tasks 1-3 is similar to that of Tasks 1 and 2, permitting task-specific and user centered assessment.\\n\\nThe coarse task decomposition above provides little space for technology assessment. We will illustrate how it can be included with a further decomposition of Task 1. There are various ways in which one might map requests onto sets of index terms, ranging from keyword spotting to full syntactic, semantic and pragmatic processing. This in part depends on whether requests are short, one sentence questions, or paragraph long statements of interest. Hence, the decomposition of Task 1 braids into a number of different paths. There is one path going through e.g. : (1) word segmentation and morphology; (2) parsing; (3) semantic interpretation; (4) reference resolution and pragmatic processing; (5) identifying key concept combinations; (6) producing index terms. Some of these may be decomposed further; thus (2), (3) and (4) may very well involve further disambiguation tasks. Various steps on the path may be missed out. One might go straight from word segmentation and morphology to either the production of index terms or identifying key concept combinations, or do the same from parsing, semantic interpretation, or reference resolution. Or one may sidestep the entire path by doing simple keyword spotting. Since most of the tasks on the path (1)-(5) correspond to standard linguistic functionalities, one can apply general technology assessment measures, e.g. those identified by Parseval and SemEval.\\n\\nHowever, different systems may perform these tasks to different depths of analysis and levels of detail. It is therefore important that the metrics are able to give some measure of analysis depth, as well as of correctness at the chosen level of detail.\\n\\nValidating technology assessment will involve taking (i) the presence or absence of a particular task, (ii) the depth of analysis to which the task is performed, and (iii) the proportion of correct to incorrect results, at the chosen level of detail, and correlating these with the various dimensions along which Task 1 can be deemed to succeed at a task-specific level, and Tasks 1 plus 2 at the task and user level.\\n\\nPossible amplifications of this chain, assumed so far based entirely on written language, are an alternative entry to Job 1, with spoken requests; and an alternative exit after Job 2, when retrieved document texts are processed to extract key concepts: depending on how this extraction task was defined, it could be a natural extension of the evaluation after the first stage. Note, however, that own-language document retrieval should probably not be treated as a separately evaluable task for short requests. This is because, on current test findings, it does not require extensive SLP for typical retrieval situations and would thus not be an attractive subject for SLP evaluation: the presumption is that request translation, especially for some languages e.g. compounding ones, could be linguistically more exacting and not be limited to simple dictionary lookup. Own-language retrieval would be done only to provide evaluation data. However the retrieval task with translated requests could be varied with a `baseline\\' version designed to study translation effectiveness in its own right, and a `souped-up\\' version where actual retrieval performance exploiting all search resources like weighting could be assessed.\\n\\nIn relation to speech processing evaluation in particular, various speech input conditions are possible for Task 1, and there are also speech input and output possibilities at Task 3. At this sort of level, one can address question of how deep (full parsing etc) and shallow (keyword spotting) approaches to index selection fare with different word accuracy rates. Does keyword spotting work better with poorer word accuracy in recognition since it is inherently less brittle, or does the extra context imposed by deeper linguistic processing allow one to recover more easily from word errors?\\n\\nThe other subpaths allow for less or more exigent SLP, e.g. according to whether title/abstract translation is undertaken. It is possible to enter and exit the chain at different points, but never without a rational job or task and evaluation step for this.\\n\\nSpoken Language Issues\\n\\nSince the illustrative evaluation scenario above is somewhat biassed towards text processing, this section raises some  issues more specific to the evaluation of spoken language systems.\\n\\nIntegration of speech products in daily applications is growing (e.g. dictation systems, banking and travel information inquiries, and dialogue systems as used for training and education). Three topics of particular interest in the evaluation of the present state-of-the-art speech technology: user appreciation of spoken language systems; assessment of technology modules;  and the interaction between spoken language and natural language systems.\\n\\nMany development centres and industries are interested in comparing their in-house systems with current state-of-the-art systems. To facilitate this, data bases used for assessment and the description of the procedures, scoring metrics and results need to be disseminated within the community. For most of the assessment procedures it is crucial that test date are ``unseen\\'\\', so that systems are not adapted or tuned by making use of the test data. This requires a continuous stream of new test data. This may be feasible for certain conditions but for a wide range of applications availability of these type of data will be limited. Nevertheless for representative tests, data for various languages, speech types and recording and transmission conditions are required. A central coordination point might be required as well as general accepted classification criteria.\\n\\nA point of consideration concerns the items to be assessed and possible assessment methods. Items to be assessed include:\\n\\n1. Applications\\n\\na1 Command and control: generally related to the operation of ``hands and eyes busy\\'\\' systems. Speaker dependent recognition and a small text output vocabulary are used. The major issue is total system performance and error correction.\\n\\na2 Document creation: requires large vocabulary recognition and robust error correction. Systems may be adaptive to speaker, environment, vocabulary, speaking rate and other variables. This adaptation requires specific assessment methods.\\n\\na3 Information retrieval: may be a public service (telephone speech, speaker independent, dialogue structured, speech understanding). Total system performance (does the user get the required information efficiently), user appreciation and the performance of different technologies are main issues for assessment.\\n\\na4 Tools for disabled: includes a wide variety of specific applications. Speaker dependent recognition for retarded speakers which may deliver deteriorated speech are the major issues.\\n\\na5 Training and education: can be characterized by command and control type applications but also (such as with second language teaching) combined with written text applications.\\n\\na6 Security control: includes speaker identification, speaker verification, and language identification. The assessment is very much application related and generally requires specific data bases.\\n\\na7 Translation systems: include multilingual speech I/O (in general large vocabulary applications). Many aspects are to be assessed such as input/output modules and the translation module itself.\\n\\na8 For simulation: (including virtual environment applications) all aspects are covered by a1, a2, a3, and a5.\\n\\n2. Modules\\n\\nm1 Speech input: can be divided into isolated/connected word recognition (for which many methods are developed) and large vocabulary recognizer assessment. For large vocabulary recognition in general ``untouched\\'\\' test data are required and specification of the vocabulary and lexicon type.\\n\\nm2 Speech output: for pre-recorded speech may rely on existing speech intelligibility tests. For text-to-speech the dimensions intonation and speaker and style variation should be added. This normally requires subjective tests.\\n\\nm3 Transmission: in adverse input/output conditions (back-ground noise, telephone speech) transmission of the speech signal requires a robust physical specification of the conditions. Standardization, by making use of simulation, offers a possibility. Multilingual use requires a variety of identical (language specific) data bases. To make this feasible a ``universal\\'\\' application can be used (i.e. travel information) as well as extension of existing robust corpora to other languages.\\n\\n3. Standardization\\n\\ns1 Bench Marks: in order to compare the performance of systems evaluated within different projects (a thread that pulls together projects within a robust programme such as sponsored by the CEU) should include standardized measuring methods, sharing of tests material, and uniform scoring methods. Also bench marks or reference conditions can be considered. Robust bench marking allows new developers or industrial competitors to scale the performance of their system within the range of performance of state-of-the-art systems.\\n\\ns2 Data bases: should be (within commercial confidence) accessible by other projects within the same frame work. General coordination should include merging of tools between related projects.\\n\\nPossible assessment methods include: User appreciation: Human factor studies normally require subjective measures based on queries opinion scores, rating, (pair-wise) comparison, etc. These techniques are well developed as well as the statistical tests which are applied to analyze the responses of subjects. However, the application of these techniques to quantify the user appreciation of NLP products is not well developed.\\n\\nTherefore, specification and development of controlled repeatable user appreciation tests is required.\\n\\nbenefit vs. human performance\\n\\nsuccessful trials\\n\\nhandling time\\n\\nerror analysis\\n\\nease of use\\n\\nTechnology assessment: Progress during development, competitive comparison and progress estimation are normally quantified within a specific application or technology (e.g., large vocabulary recognition). For technology assessment the conditions are carefully controlled (laboratory conditions) in order to obtain reproducible results. Depending on the purpose of the assessment a black versus a glass box approach can be selected. Some test designs are competitive which pushes the technology foreword. The test paradigm should include the share of results and methodology after the test (used with ARPA and SQALE) in order to keep participants interested. Interaction between a human and a system may fail. Therefore, the method of error correction and an analysis of the errors occurring in man-machine dialogues is an aspect to be addressed.\\n\\nRealistic adverse conditions can be included in laboratory tests (artificial, reproducible, inexpensive) or in field tests (representative, uncontrolled, expensive). In general development makes use of laboratory test conditions followed by a final verification based on a field test.\\n\\nIt is not easy to produce a general recipe for the evaluation of a certain group of applications. Specific requirements of a certain application may lead to a different assessment methods, and at least to different metrics and criteria. Therefore, it is impossible to give detailed examples of possible assessment projects. In general a robust evaluation experiment should include experiments on various items of user appreciation, system performance or technology.\\n\\nThree examples are given:\\n\\nApplication oriented: The evaluation of a system using speech input and speech output in a dialogue concept, such as used for a travel information system, allows for the assessment of the total system (user appreciation and system performance). Also the assessment of individual modules may be relevant, consider the robustness of the recognizer for telephone quality speech and the intelligibility of the (text-to-)speech output system. If the assessment is focussed on modules, many parameters should be controlled. However, for application oriented assessments one can  use representative evaluation data where variables are uncontrolled but more or less representative for the application. In many cases a representative set of test data may be too big and therefore not realistic. But with a limited set of test data valuable results can still be obtained. Also selective analysis of the results (individual module responses) may produce diagnostic information on the performance of the system.\\n\\nTechnology oriented: The present ARPA yearly competitive tests and the present CEC-LRE SQALE project, in which various recognizers are evaluated for multi-lingual use, are examples of technology oriented projects. The present needs for performance under realistic conditions require to cope with adverse conditions such as: telephone speech, noise conditions, spontaneous speech, etc. Also text-to-speech systems for many languages covered by the EU requires more attention. A competitive project in which various systems are compared will push the technology. It is also recommended that test data are made available to the community. Participation at a moment after such a project is launched should be made possible. It may be obvious that a general consensus on this matter between partners and the CEC is required and that dissemination is guaranteed. A possible example may be the results of the SQALE project.\\n\\nCombination of Spoken Language and Natural Language Projects: In general SL and NLP use different assessment methods and scoring metrics. Therefore it is not possible to give a realistic example of such a combined project. However, the use of common data bases for testing of a combined system (e.g a translation system with speech input and/or output, a spelling checker with voice control, or a information system with an interpreter) may be a useful first step. Original text fragments compared with distorted recognition outputs may be used for assessment of the NLP module.\\n\\nTechnology Assessment at Project Level\\n\\nProgress Evaluation\\n\\nThe role of project internal technology assessment is likely to be primarily one of producing diagnostic information. This can be used to facilitate progress in system development, by identifying gaps in the technology that need to be filled in order to improve system performance. It would serve to identify precise points in processing where something is going wrong. Internal technology assessment may or may not form part of progress evaluation, which would in any case require a large element of user-centered assessment.\\n\\nTo a large degree, internal technology assessment is a matter of internal project policy. It would be unwise to impose many general requirements. First, the specifics of the technologies employed may vary quite widely from project to project, with a concomitant variety in the forms of assessment producing useful diagnostic information. Second, some projects may aim to take speech or language processing modules off the shelf and fit them into the system  with minimal adaptation. Other projects may aim at more substantial refinement and development of existing technologies. In the first case, it is not even clear that technology assessment has any useful diagnostic role, and if it does it is likely to take a different form from that in the second case.\\n\\nPerhaps the most one can say is that projects should specify a progress evaluation plan, and that where applicable this should include some form of diagnostic technological assessment. Technological assessment is appropriate where a project envisages either (i) refining or developing speech and/or language processing techniques, or (ii) adapting data -- such as grammars, language models, lexicons -- for a particular domain, task or language. The form of the technological assessment can vary, but in most cases one would expect some form of test-suite or test-corpus, along with a specification of the results that should be obtained by processing the test data.\\n\\nRelating Project Internal and Comparative Assessment\\n\\nThere is an obvious tension between two claims that have been made above. Namely, (a) that the form  of project internal technology evaluation can vary from project to project, and (b) that technology evaluation provides the core of comparative assessment. The second claim would seem to demand uniformity between projects in technology evaluation, while the first denies it. This suggests that internal and comparative technology assessment are very different kinds of animal. But what one would like is for comparative technology assessment to build on the back of project internal assessment. If comparative assessment requires a vast expenditure of extra effort, this will act as a strong disincentive to participation.\\n\\nFortunately, the tension can be reduced. While internal technology evaluation will tend to be more specific and detailed than the comparative version, this is still compatible with there being an a core of internal assessment that is common to a variety of projects. In particular, different projects might very well use standard annotations, such as labelled syntactic bracketings, basic predicate argument structures, etc (as defined under Parseval and SemEval), to formulate test data. They may wish to extend the annotations, and produce other kinds of data besides. But if the standard components can be identified and kept separable, then project internal test data may, with appropriate extra effort, be re-usable for comparative purposes.\\n\\nAnother obvious point to make is that if an evolutionary system development cycle is followed, then snapshots of a system at different times can profitably be regarded, for the purposes of comparative evaluation as different systems for the same domain, task and user group.\\n\\nThus, in order to facilitate comparison and reusability of evaluation data, a limited degree of standardisation is required for project internal technology assessment.\\n\\nInput From the Speech and Language Community\\n\\nAs part of the exercise of preparing this report, a questionnaire was designed to sample opinion about technology assessment across the European Speech and Language community. The questionnaire was not meant to be scientific, nor exhaustive. It was intended to provoke response, which earlier trigger papers has failed to do, and to see if there were very strong opinions in certain directions.\\n\\nThe first question was intended to determine the attitude of researchers towards two forms that technology assessment might take: project-internal and comparative. The overwhelming majority of respondents expressed keenness to conduct both sorts of technology assessment (some comments indicated a confusion between project-internal technology assessment, and other forms of project internal assessment, such as simple progress evaluation - the questionnaire may have failed to make this distinction sufficiently clearly). There were worries expressed, however, about the cost of comparative technology assessment and the possible effect that it might divert resources away from other research and assessment activities.\\n\\nThe second, multipart question sought to explore the possible content of project-internal technology assessment. The first subpart asked whether there should be quantitative technology assessment of projects, in addition to any (quantitative or non-quantitative) user validation that might be carried out. Again, the overwhelming response was in favour of quantitative technology assessment, independently of what user assessment might or might not take place. The comments indicated that user assessment alone was not sufficient to drive the technology or to promote reusability. The second subpart asked about the level of granularity at which project-internal technology assessment should take place, at the level of user-significant components or at finer levels ? The majority preference was for assessment at finer levels. The final subpart asked whether in project-internal assessment systems should be assessed against test data that differed in various linguistic features (e.g. language, domain etc) from the application for which it was designed. Response was more cautious here. While the majority were in favour, comments indicated that although it was clearly preferable to do this it might be too soon.\\n\\nThe third multipart question asked about implementing project-internal technology assessment. The question aimed at determining whether researchers were in favour of the evaluation exercise being largely run by the projects themselves or whether it would be better for persons independent of the project to be involved as evaluators and as evaluation data collectors. The results indicated a clear preference for the evaluators to be project members - reasons cited were primarily the expense and bureaucracy the alternative would entail. However, there was almost an even split for and against the evaluation data collectors being members of projects.\\n\\nThe fourth multipart question aimed at determining views on the content of comparative evaluation. The first subpart asked for preferences concerning an integrated speech and language comparative evaluation exercise versus separate speech and language evaluations. The split in responses was almost even, with some people arguing for both. The second subpart asked about granularity of comparative assessment (as with project-internal assessment), whether it should be at the level of user-significant components only, or at finer levels too. The bulk of the respondents supported finer levels of assessment, again arguing that it would advance the technology; however fears were expressed that development could be slowed by evaluation forcing a uniform modularity across systems.\\n\\nThe next three subparts of the fourth question asked respondents to prioritise user-significant system components they thought were candidates for evaluation, sub-user-significant components suitable for evaluation, and dimensions along which generality should be evaluated (and hence promoted). For user-significant, written language components there was little consensus, though document retrieval had perhaps the most support; for user-significant, spoken language components, speech recognition was a clear winner with topic spotting next. For sub-user-significant, written language components there was again little consensus with part-of-speech tagging, morphological analysis, named entity recognition and parsing attracting most support; for sub-user-significant, spoken language components word lattice production, phone lattice production and prosodic marking all attracted support but no overwhelming preference was indicated. Finally, concerning dimensions along which written language systems should generalised, domain was overwhelmingly selected as most important, with language being placed second; for spoken language, speaker-independence was clearly selected as the most important generalisation required, while little consensus emerged about which other features it was most important to generalise with respect to.\\n\\nTo make a summary of the summary, one might tentatively draw the following broad conclusions from the responses to questionnaire: 1. technology assessment, as differentiated from user validation, is viewed as very important and needs to be carried out quantitatively, both within projects and comparatively, and at a lower level than user-significant components - it is seen as an important force in driving technology and encouraging reuse; 2. projects themselves should carry out project internal technology assessment, though there may be a case for having evaluation data assembled by independent `experts\\'; 3. there is no consensus about how comparative assessment should be implemented; 4. with the exception of speech recognition, there is no consensus about the most important evaluation exercises either for written or spoken language at either user-significant or user-transparent level; 5. with the exception of domain for written language, there is no consensus about the sort of generalisation that evaluation exercises ought to be encouraging.\\n\\nRecommendations\\n\\nOverall Recommendation\\n\\nThe ARPA evaluations have shown that there is much to be gained from comparative evaluation, and the preceding discussion has indicated how one can set up an evaluation exercise that can accommodate a variety of different applications. While technology evaluation must form the core of the exercise, user-centered issues may readily be catered for by paying due attention to the environmental aspects of technology assessment. Therefore we make the following general suggestions:\\n\\nIt should build as far as possible on materials gathered for project internal assessment plus other pre-existing resources where applicable.\\n\\nAlthough it should start on a small scale, it is important that it has the flexibility to grow over time.\\n\\nA flexible evaluation structure capable of accommodating a variety of tasks and systems, and with an emphasis on environmental / user-centered validation of technology measures should be employed.\\n\\nThe evaluation exercise should be open to LE sites not directly funded within the FP-4 LE programme, to encourage the spread of annotation standards, to bring in fresh ideas and experience, and to prevent the exercise becoming the preserve of a clique of established sites.\\n\\nWhile steps should be taken to minimise costs, both for projects participating in the evaluation and for central administration, it should be recognised that such an evaluation exercise cannot come for free. Therefore, it may be desirable to provide limited extra resources to those projects participating to cover additional efforts necessary for participation. However, participating projects should also gain from economies of scale in setting up a core of common metrics and resources for evaluation. In due course, other projects within the FP-4 LE programme should also benefit from these common resources and standards.\\n\\nProvision would need to be made for a small group of people -- independent of any specific project -- to oversee the collection, refinement and dissemination of common evaluation material, as well as the specification of evaluation architectures, standards and metrics. This group could be either loosely organised in a support project or they could be affiliated to some existing or to-be-established organisation. The coordinators would need to act in close cooperation with participating projects in order to ensure that the exercise is built up in such a way as to stay in touch with the technical and user needs of the individual projects. The profile of expertise of this group should reflect both the needs of technology assessment and of user validation.\\n\\nWe add some more details to these recommendations below.\\n\\nProject Internal Evaluation\\n\\nTo a large degree, methods of project internal evaluation will be a matter for negotiation between users and system developers within individual projects. However, there are a number of points that are not only desirable for internal evaluation, but which would also facilitate comparative evaluation:\\n\\nProjects should be encouraged to develop a well defined, evolutionary evaluation strategy.\\n\\nProjects should identify from the outset the kinds of evaluation data that they will require, and the means by which this data is to be acquired. Early acquisition (and use) of evaluation data should be encouraged.\\n\\nProjects should provide functional specifications that not only identify tasks and sub-tasks, but also the environmental attributes pertinent to those tasks.\\n\\nWherever possible, projects should be encouraged to make their internal evaluation strategies, test data, user profiles etc. available to a wider community, even if they are not willing to participate in a comparative evaluation exercise.\\n\\nInitiating Comparative Evaluation\\n\\nThe comparative evaluation exercise needs to be set up in a way that is part top-down and part bottom-up. A number of steps need to be taken to initiate the exercise\\n\\n1. A small number of projects should be selected to take part in the exercise. The projects must be willing to participate, and they must all be geared to tasks that have a substantial degree of overlap. Clear agreement must be reached at the outset about public accessibility of the evaluation data that projects will be supplying.\\n\\n2. A braided task structure should be identified to accommodate the participating projects. This may involve further refinement of the functional specifications employed by some of the projects.\\n\\n3. Natural evaluation points in the braided structure should be identified. Under realistic assumptions, their number will not exceed half a dozen such points.\\n\\n4. Appropriate corpus annotations for the selected evaluation points should be specified. The specification should be carried out in consultation with participating projects, but where possible standard annotations (word strings, morphosyntactic tags, labelled syntactic bracketings, predicate-argument structure, co-reference relations, scope relations) should be used. It is important that the annotations allow for the measurement of both accuracy and depth of analysis.\\n\\n5. Projects whose task structure includes a particular evaluation point are expected to employ the annotation relevant to that point as part of their internal assessment regime. (They may of course also apply additional assessment measures).\\n\\n6. Tools should be devised for (a) producing (target) annotations, and (b) measuring actual against target annotations. One should aim to exploit existing tools where possible, and otherwise distribute development effort over different projects and the evaluation coordinators.\\n\\n7. Using these tools, individual projects should produce annotated answer data from their own evaluation corpora. These should be passed on to the evaluation coordinators. It is then the job of the coordinators to assess the suitability of the material as a basis for common evaluation data. Producing common evaluation data will typically involve the coordinators, perhaps in conjunction with other participating projects, refining and extending the range of annotations on the material delivered.\\n\\n8. Individual projects should make available as much additional linguistic and non-linguistic data pertinent to evaluation as possible. Presenting this data in a form accessible to other projects will be problematic unless some kind of interchange format is agreed.\\n\\nHaving obtained this initial infrastructure it is then necessary to attempt to validate the chosen common annotations and evaluation metrics. This involves correlating metric scores with the adequacy or inadequacy of task performance under the different environmental attributes imposed by different systems. This correlation would be greatly assisted if systems were assessed at different stages of development. This would allow comparison of technology measures and system adequacy under relatively fixed environmental constraints. It should be borne in mind that this form of validation may well show that some of the originally selected technology metrics and annotations are of limited or zero validity.\\n\\nCentral Coordination and Resources\\n\\nEven a small comparative evaluation exercise cannot be expected to be self-running and self-regulating. Some central coordinating organisation, independent of the participating projects, is needed. The function of this coordinating organisation would be to negotiate with the projects participating in the comparative exercise, to collect, administer and disseminate evaluation material, and to survey and synchronise the evaluation exercise.\\n\\nThe central coordination in the initial stage could be in the responsibility of some support project, e.g., the European Linguistic Resource Agency (ELRA) or a comparable organisation involving a group of independent experts. The profile of these experts should include industrial and academic expertise in the areas of evaluation and standardisation, language technology, linguistic and software engineering, as well as knowledge of sectors and application areas where user centered assessment is concerned.\\n\\nWhere possible, the evaluation exercise should build on pre-existing materials, such as data (and expertise) from the various ARPA evaluations, the Penn tree-bank, various national European exercises (e.g., the French GRACE exercise), results of LRE projects such as MULTEXT and TSNLP, and so forth. For the common annotations employed, it could be desirable to draw on the annotation schemes devised under Parseval and SemEval, where appropriate.\\n\\nQuestionnaire\\n\\nText of the Questionnaire\\n\\nThe text of the questionnaire follows in teletype font. After each question selected comments from respondents have been included in italic font, prefixed by the respondent\\'s numeric response, if any, to the question.\\n\\nQUESTIONNAIRE ON ASSESSMENT AND EVALUATION ========================================== IN LANGUAGE ENGINEERING ======================= 1.0 Background ============== The EC has set up a Study Group on Assessment and Evaluation (A  E) in Language Engineering (LE) with a view to establishing productive ways forward within Framework IV (and beyond) in the area of assessing and evaluating both particular systems and underlying LE technologies. Your views on this subject are sought. As you will no doubt be aware, the ARPA-sponsored evaluation programmes in the United States (CSR, ATIS, MUC, TREC, etc.) have become extremely influential in shaping the direction of LE research there. The Commission is now considering whether there should be a set of European actions in A  E and if so, what form they should take. To that end the Study Group has been set up and rapporteurs for a number of subgroups appointed. Several trigger papers have been prepared which some of you may have seen (these are available by non-anonymous ftp from cl-ftp.dfki.uni-sb.de with username \\'assessment\\' and password \\'tnemssessa\\'). However, these have not led to much response as yet. In order to provoke more response a questionnaire has been developed and is attached below. I would be most obliged if you could complete the questionnaire and return it to me as soon as possible.\\n\\nResponses will feed into the production of Commission policy concerning requirements to be placed on individual projects concerning A  E and also concerning specific initiatives in this area. Due to time constraints on the production of position papers, responses must be received no later than March 10 to be taken fully into account. In the current Telematics Language Engineering call, pilot applications (which form the bulk of the projects under Framework IV) are being assessed along the two broad dimensions of: 1. user validation; and 2. technology assessment and system evaluation (TA  SE). This questionnaire pertains chiefly to the latter dimension. 2.0 Questionnaire ================= 2.1 Introduction and Terminology ================================ Terms later used in the specific sense introduced here are capitalised. An LE APPLICATION SYSTEM or just LE SYSTEM is a set of software components constructed to permit a user to carry out some language-related function in a specific real-world environment. LE systems and system components may be described in terms of: 1)  the LINGUISTIC FUNCTION they carry out (e.g.\\n\\ntranslation, speech recognition, parsing, summarising, coreference resolution) -- components which have some broadly agreed linguistic function will be termed LINGUISTIC PROCESSING (LP) COMPONENTS (so, e.g., a parser is a linguistic processing component whereas a binary search routine is not) 2)  LINGUISTIC FEATURES of the input or output data they operate on or produce (e.g. language (French,English), subject area or domain (weather reports, financial newswire stories), text type (newswire stories, technical manuals), text length (paragraph,article), spontaneity (read speech vs. spontaneous speech), channel conditions (telephone vs. wide-band), and so  on) 3)  whether or not the inputs and outputs are objects which are of functional significance to the user of the system -- systems or components whose inputs and whose outputs are of such significance will be termed USER-SIGNIFICANT (e.g. a tokeniser whose input is an English newswire story but whose output is a sequence of pointer pairs indicating token start-end positions is not a user-significant component; a system with the same input whose output is the same newswire story in French is user-significant). To evaluate a system and the technology it embodies, a decision must be made about the level of: 1)  GRANULARITY at which it will be evaluated (e.g. at the level of user-significant components only, or at some level of LP components which are sub-user-significant) 2)  GENERALITY at which it will be evaluated (e.g.\\n\\nhow much do we vary the linguistic features of the input/output data we provide/expect relative to the features of the data in the intended application; e.g do we evaluate the system against data from different languages, different domains etc.). These two dimensions are orthogonal: the decision to evaluate at a high level of granularity is independent of the decision about which linguistic features of the input/output data are to be generalised, if any, and to what extent. By PROJECT INTERNAL TA  SE is meant assessment measures devised for and applied to individual projects, with no attempt made at isolating and comparing technologies or LP components across projects. By COMPARATIVE TA  SE is meant the complement: assessment measures which attempt to isolate and compare LP components which recur in different systems. Example ======= To make these distinctions clearer consider this example. MET is a hypothetical LE system that translates spoken English reports composed by the British Metereological office into French text suitable for reading by French announcers on local radio stations in Britanny. The linguistic functions of the overall system, and those which define user-significant components, may be described as transcription and translation.\\n\\nBut we may suppose the system contains linguistic processing subcomponents which are not user-significant and which accomplish such functions as word lattice interpretation, part-of-speech tagging, parsing, English-French syntax tree transformation, etc. Relevant linguistic features of the input data include: English language, single speaker, read speech, limited domain, short passages, formulaic style; relevant linguistic features of the output are French language, formulaic style, limited domain. To assess the system we may want only to see how the user-significant components behave on unseen data whose linguistic features match those of the intended application; to assess the technology we may want to evaluate both at a lower level of granularity (so e.g. the word lattice interpretation component may be of interest) and by generalising the values of the linguistic features of the input/output data (so, e.g. seeing if the techniques employed may be generalised other languages, other sorts of brief report, etc.). 2.2 General Form of Assessment ============================== 1)  PROJECT INTERNAL TA  SE What is your attitude towards conducting project internal TA  SE ? 1. Would not want to conduct it at all 2. Would conduct it, but only if necessary 3. Would be willing to conduct it 4. Would be keen to conduct it Response:       (1 - 4) Comment:\\n\\n4: It is essential at the technology level to know where we are\\n\\n4: Project internal evaluation is absolutely necessary. Without systematic evaluation it is impossible to measure progress. Maybe you should have an option 5. I see quantitative evaluation as indispensible, even if it is very time consuming\\n\\n3:      Presumably this only has a point if you do it several times within a project, as a measure of progress (well, change at least). For an applied project, it ought to be part of the basic research methodology. For more theoretical work its more problematic since you might not know what to measure, and it may change as the project progresses.\\n\\n4:   [A] project that does not involve some form of at least progress evaluation would not be properly managed\\n\\n2: Any project has to have some way of deciding how well it is doing. I\\'m assuming you mean something that has been regimented from outside.\\n\\n2)  COMPARATIVE TA  SE ? What is your attitude towards participation in comparative TA  SE ? 1. Would not want to participate at all 2. Would participate, but only if necessary 3. Would be willing to participate 4. Would be keen to participate Response:       (1 - 4) Comment:\\n\\n4: It is more difficult (therefore be very careful in defining reasonable goals), but necessary\\n\\n4:I\\'m more in favour of comparative TA/SE if broadened and funded to include non-LE specific projects. If this doesn\\'t happen then from among only the EU-funded projects there would be insufficient overlap so no point in trying to compare heterogeneous systems.\\n\\n4: We already actively participate in common comparative evaluations. This is an important means for us to compare our work with what others do, to learn from their techniques, and to improve our research and system.\\n\\n4:      Properly resourced participation would be a useful discipline. Not convinced how useful it would be, but that\\'s not what you asked. In common (I suspect) with much of the community, however, I have much more interest in participating than in developing the substantial framework that would be required to allow participation - maybe there\\'s another community out there who might be interested in doing that kind of thing?\\n\\n4:  This assumes that 1. comparative assessment is not going to divert substantial resources from internal assessment and development; 2. that comparative assessment is likely to bring about project internal improvements, in the same way that internal evaluation should; 3. a sensible framework can be found for conducting comparative assessment across a variety of projects covering different tasks and application domains; in particular, methods should not be biassed by the adoption of one particular linguistic theory or another.\\n\\n2:  I\\'m not keen, but if all money comes with this sort of strings attached, I haven\\'t got much choice.\\n\\n4:  Concern that too much research time would be directed towards setting up evaluations (but this had good results in the ARPA community)\\n\\n2.3 Content of Project Internal TA  SE ======================================= 1)  USER VALIDATION  TECHNOLOGY ASSESSMENT Should projects 1.  rely on user validation only (i.e. whatever validation the users deem sufficient) ? 2.  rely on user validation and additionally, if not specified in user validation, produce test data (input/output pairings) and carry out tests which allow quantitative assessment of performance ? Response:       (1 - 2) Comment:\\n\\n2: Assessment should be as quantitative as possible. Very important here is the number and representability of users tested on and that each test should be made on new data with frequency of about once a year.\\n\\n2: Quantified numbers tell the truth, or appear to.\\n\\n2: qualitative assessment is not enough. (only user validation would require a huge number of users of all types to have a statistically meaningful result. It is well known that most users do not know what they want or need)\\n\\n2:      Hey, you\\'re changing the rules here. In your intro you specifically separated User Validation (UV) from TA  SE, and now up pops UV as part of TA  SE. But it also changes the rules in another way, because I don\\'t see UV as project internal - if two projects attempt to do the same thing, then their UV results can be compared. Once you can do that, the considerations change. I actually think it may be dangerous to place too much emphasis on UV, because depending on users\\' existing preconceptions introduces a huge inertia into technological development. So even if one wants to steer projects into user-oriented directions (as the EC clearly does), one needs to allow for assessment independent of any existing user base and encourage a degree of technology-pull in users.\\n\\n2:  User validation alone is unlikely to provide sufficient diagnostic leverage for technological development. Test data at least provides some moves in this direction.\\n\\n2:  User validation alone won\\'t promote generality and reusability of components.\\n\\n2)  GRANULARITY Supposing quantitative assessment against test data, at what level of granularity should linguistic processing components be evaluated ? 1. at the level of user-significant components only 2. at finer levels as well Response:       (1 - 2) Comment:\\n\\n2: User validity is very important but often very difficult to define quantitatively. Therefore technological assessment levels are undetournable.\\n\\n2: Finer evaluation is necessary for system development\\n\\n1,2:    So 2 was the right answer to Q1 eh? Well you\\'ve broken the tidy model now so there\\'s no clear answer. For semi-objective UV it only makes to look a user-significant components. As a project internal measure you would probably be well advised to look at finer levels too, although you might sometimes feel you didn\\'t need to.\\n\\n2: Again, for the purposes of diagnostic leverage. But clearly, the measures would not be of interest to users.\\n\\n2:  User validation alone won\\'t promote generality and reusability of components.\\n\\n1: Difficult to meaningfully evaluate at finer levels\\n\\n3)  GENERALITY Supposing quantitative assessment at what level of generality should components be evaluated ? 1.  only on unseen input-output pairings whose linguistic features match those of the project application 2.  on input-output pairings whose linguistic features differ from those of the project application,  as well as on those whose linguistic features match those of the project application Response:       (1 - 2) Comment:\\n\\n1: Answer 2 might be interesting in at least 2-3 years time. We could try to push more and ask for multilinguality from now on (like in SQALE) and (more difficult because it needs more data) on different domains, but we might encounter the risk of frightening people.\\n\\n1: While I agree that 2 would be nice, I do not think that we are there. Most of the results would be pretty meaningless. When possible, 2 is clearly preferable.\\n\\n1,2:    This is a silly question. You will achieve different things depending which you do. It just depends on what you want to know about your system. True blue user-orientation would argue that 1 was enough. More generally-oriented work would do well do think about 2. Some recent EC calls seem to want to do both, which isn\\'t obviously sensible - that is, doing user-oriented work and then looking to see how general it turns out to be is not a good way to make useful progress.\\n\\n2: This answer needs to be qualified. What it would be useful to measure is how readily a system  can be ported from one domain to another. This reflects a system\\'s maintainability, portability and flexibility. However, provision should be made for spending a certain amount of time porting between domains, and not just doing it straight off\\n\\n2:  User validation alone won\\'t promote generality and reusability of components.\\n\\n2: Case 2 can only give more information compared with case 1.\\n\\n2.4 Implementation of Project Internal TA  SE ============================================== 1)  EVALUATORS Should tests be carried out 1. internally by project participants (developers and users) ? 2. externally by independent project evaluators ? Response:       (1 - 2) Comment:\\n\\n1: Answer 2 is interesting for projects that are very near to products (within 6 months on the market)\\n\\n1:makes it seem too forced. Projects should want to do this.\\n\\n1: we need to have confidence in the participants now. In our area (spoken) it would be extremely difficult to have external evaluators. Maybe in the written areas it is easier. 1:      Again, this depends what you want. But this time I\\'ll express an opinion about what I want and its 1. I think 2 is overkill given the fairly limited objective utility of project-internal TA SE. It should just be part of the research team\\'s methodology and they should do it primarily for project-management purposes.\\n\\n(If anyone thinks there\\'s a problem about integrity and trust of research teams I don\\'t think 2 will actually make the teams better, though it might make the administrators feel better.)\\n\\n1: Assuming that the intended users count as project internal. External evaluators are likely to be a bureaucratic nightmare, and expensive\\n\\n: Depends on cases again. 2 will involve a lot of organising\\n\\nand doesn\\'t make sense for every project.\\n\\n: If the infrastructure was in places then (2) would be\\n\\npreferable, but I\\'m not convinced that it is the best use of\\n\\nresources to set up such an infrastructure effectively from scratch.\\n\\n2)  EVALUATION DATA COLLECTION Should test data be compiled 1. internally by project participants (developers and users) ? 2. externally by independent specialists ? Response:       (1 - 2) Comment:\\n\\n2: That would be nice if we could find independent specialists.\\n\\n1: makes it seem too forced. Projects should want to do this.\\n\\n2: but these specialists must do so in close coordination with the partners to ensure compatibility/applicability\\n\\n1:      See previous comment. But there\\'s the additional proviso I mentioned above that its not obvious that the kind of people who typically propose the research are the same as the kind of people who want to construct test data - maybe projects should be encouraged to include \\'independent specialists\\' in their consortia.... (if such specialists really exist?) 1:  It ought to be cheaper to collect this stuff internally. But not for data going beyond the project application, as in 2.3.3 above. Here it would be useful if some other project could collect the data internally, and make it available.\\n\\n: Depends on cases again. 2 will involve a lot of organising\\n\\nand doesn\\'t make sense for every project.\\n\\n2.5 Content of Comparative TA  SE ================================== 1)  INTEGRATED VS NON-INTEGRATED SPEECH AND LANGUAGE EVALUATION The Commission should aim to develop 1.  an integrated speech and language evaluation. 2.  independent evaluations for spoken and written language engineering. Response:       (1 - 2) Comment:\\n\\n1 for technology pushed projects, 2 for projects where user validation is required (application oriented projects).\\n\\n2:insufficient overlap between the two to date ... too early\\n\\nBoth: it really depends on the application. when possible the paradigm should be integrated, though many issues are very different. However, spoken and written language have a lot to learn from each other.\\n\\n2:      I think there will always be sharp distinctions between the user-significant bits of speech and text systems, even if internal modules can be shared. I also think there will be for a long time some tasks which are specifically speech and others that are specifically text (plus some which might be both). So it would be wasting effort to try and force and integrated view in this way - the actual applications would still end up being polarised one way or the other.\\n\\nBut actually we should be considering a three-way distinction: speech, text, and common (\\'language\\'?) - most of the latter not being user-significant. Then there\\'s handwriting as well....\\n\\n1: However, speech only and language only systems should be able to fit in as well\\n\\n:  Not a sensible pair of alternatives. A single integrated\\n\\nform of evaluation to cover all possible projects is impossible.\\n\\nIt doesn\\'t follow that speech should be split off from the rest of\\n\\nlanguage.\\n\\n:    This depends on the application but in general you need\\n\\nboth\\n\\n1: Option 1 is more risky in terms of complexity and possible biases.\\n\\n: Integration is over strong; there ought to be some connection\\n\\nbut independent cases are also desirable.\\n\\n2)  GRANULARITY At what level of granularity should linguistic processing components be evaluated ? 1. at the level of user-significant components only 2. at finer levels as well Response:       (1 - 2) Comment:\\n\\n2: I am of the opinion technology is still to be improved, which means the finer levels are of high importance.\\n\\n2:   Finer evaluation is necessary for system development\\n\\n1:  I know MUC-6 has gone for 2, but my tendency is to think that you can\\'t pin down internal structure sharply enough to be useful without compromising development. You might think you can (as in Hobb\\'s generic MUC system), but in practice aligning real modules with these ideals doesn\\'t really work out. I\\'m all for modularity, and component re-use, for components that are not the focus of development in a project. But evaluation requires modularity to be enforced everywhere, and that will just block innovation.\\n\\n2: We need to bear in mind that finer granularity may differ substantially between systems, so that in may cases comparative evaluation may not be possible. But where it is, it is likely to provide useful diagnostic information\\n\\n3)  CANDIDATE USER-SIGNIFICANT COMPONENTS FOR EVALUATION Given the following user-significant linguistic functions, select those in which you would be willing to participate in a comparative evaluation and prioritise them (do this by writing your priority number after the task; feel free to extend the list): For written language: Your priority 1. translation                                            ? 2. summarisation                                          ? 3. document retrieval by topic                            ? 4. template filling or information extraction             ? 5. question answering                                     ? 6. generation                                             ? For spoken language: 1. recognition                                            ? 2. topic spotting                                         ? 3. generation                                             ?\\n\\nI don\\'t know what \\'willing\\' is supposed to mean here - this is a very resource-dependent thing.\\n\\n4)  CANDIDATE SUB-USER-SIGNIFICANT COMPONENTS FOR EVALUATION Given the following language processing functions which are probably below the level of user-significance, select those in which you would be willing to participate in a comparative  evaluation and prioritise them (do this by writing your priority number after the task; feel free to extend the list or to specify more precisely the form the analysis would need to take for you to be interested): For written language: Your priority 1. named entity recognition (company names,               ? locations, personal names) 2. part-of-speech tagging                                 ? 3. morphological analysis                                 ? 4. parsing                                                ? 5. coreference resolution                                 ? 6. word sense identification                              ? 7. predicate-argument structure identification            ? For spoken language: 1. word lattice production                                ? 2. phone lattice production                               ? 3. prosodic marking                                       ?\\n\\nmany spoken language systems wont produce any of these.\\n\\n5)  GENERALITY With respect to which linguistic features of the input/output should a comparative evaluation attempt to encourage generality ? (write your priority number after the feature; feel free to extend the list or to replicate it for different linguistic functions -- e.g. language independence may be more important for advancing translation technology than for advancing summarisation research): For written language: 1. language                                               ? 2. subject area or domain                                 ? 3. text type                                              ? 4. text length                                            ? For spoken language: 1. speaker-dependence                                     ? 2. spontaneity                                            ? 3. channel bandwidth                                      ? 4. native-non-native speakers                             ?\\n\\n2.6 Implementation of Comparative TA  SE ========================================= Two possible scenarios for developing an evaluation programme are the following. Bottom-up scenario: pilot applications funded in the Telematics LE 1st call (closing March 15) are required, where appropriate, to participate in the provision of data and the definition of one or more comparative evaluation exercises perhaps with assistance from external specialists (these exercises might be designed around project `clusters\\'). A project is funded under the `resources\\' heading in the 2nd call (to be announced Sept 95) to coordinate the definition of the evaluation exercises and the preparation of test data and scoring software. Selected pilot applications receive funding, where appropriate, under the final call (Sept 96) to participate evaluation exercises. In this scenario the type of evaluation evolves out of actual funded pilot applications. Top-down scenario: an evaluation exercise is defined through consultation with the community, a project is funded to coordinate it and to produce test data, and several projects (a very small number) are funded exclusively to take part in it. This would be done in the 2nd and final calls. In this scenario the evaluation is initially designed separately from actual LE applications, but the aim is to produce a framework which could expand to attract a range of participants involved in application work.\\n\\n1)  COMPARATIVE EVALUATION IMPLEMENTATION SCENARIOS Which of the two scenarios do you believe is the best approach for establishing a European comparative evaluation exercise ? 1. the bottom-up scenario ? 2. the top-down scenario ? Response:       (1 - 2) Comment:\\n\\n2: The top-down scenario is very important for technology assessment, but user validation plays also a role and some selected projects should receive funding to make a user validation, which is totally different in spirit to technology assessment and very project or prototype dependent.\\n\\n1: ... bottom up ... there will be too few LE projects to impose, a priori, an evaluation scenario. TREC works \\'cos there are 57 groups taking part in TREC4 and they all do the same ad hoc a/o routing experiments, i.e. homogeneous. LE projects will be heterogeneous.\\n\\n2:  I do not believe that 1 will work. What does \"are required, where appropriate,  to participate ...\" I am afraid that this will not lead anywhere. Also, the type of evaluation will likely be only valid for actual pilot applications, and may not extend to future projects. 2 is more oriented to technology push, and should provide a more general framework. However, for it to work the whole evaluation process a learning experience for all participants, providing detailed information exchange.\\n\\n2:      I don\\'t think either of theses is an effective way to achieve this goal, but 2 comes closest. 1 is just hopeless - the idea that you will be able to induce useful evaluation scenarios from the projects that happen to get through the 1st call is not realistic - there won\\'t be enough of them, and they will be too diverse (almost by definition - LE won\\'t want to fund many groups doing the same things, unless it ALREADY has a reason to do so.) But the description of 2 suggests it will be hopelessly underresourced and if so it will be a waste of effort too - just one little evaluation will result, plus perhaps a methodology for doing more. But is there a reason to believe that will tell us more than we already know from the American experience?\\n\\n1: It would be nice if some top-down scenario could be developed, with several groups working on exactly the same problems. However, given the structure of the LE call, this clearly won\\'t happen, or if it did it would exclude most of the LE projects. A bottom-up approach seems the only way forward.\\n\\n1: 1 is not great, but 2 looks disastrous.\\n\\n1: Top\\n\\n\\n\\ndown sounds like mega\\n\\n\\n\\nmachine bureaucratic nightmare...\\n\\n: I do not believe one should make a priori choices here\\n\\nfor example a great deal depends on what is actually\\n\\nsubmitted as pilots.\\n\\n2.7 Other Comments ================== If you have any other comments you would like to make about the way you think TA  SE should be carried out in the EC, or about aspects of this questionnaire (omissions or commissions) then please make them here. General comments:\\n\\nas a member of the study group\\n\\nas somebody with specific feelings about this whole field of LE assessment\\n\\nor as an actual or imaginary partner in ongoing or future LE projects\\n\\nIt is for me much too complex to have the pretension to make a user\\n\\nvalidation\\n\\nassessment and a technological evaluation at the same time: emphasize should be\\n\\nmade on one or the other. User validation is very important for near to\\n\\nproduct projects: it should also include technological evaluation to\\n\\nquantify with different objective measures the user validation process.\\n\\nTechnology evaluation is important to push the technology further, for\\n\\nexample in speech recognition to push to spontaneity or multilingualism or to\\n\\nlarger domains e.g. general business letter dictation. Technology development\\n\\nproject are important in order to have a good basis for future more ambitious\\n\\napplication oriented projects.\\n\\nI think that the most important issue is that evaluation be taken\\n\\nseriously in order to iteratively improve technology and applications.\\n\\nMy answers are very much those of someone interested in developing,\\n\\nrather than using, written language technology. From that point of\\n\\nview, evaluation is a waste of time unless it can provide diagnostic\\n\\ninformation pointing out gaps in the technology. It would be nice\\n\\nif it could also be established that filling these technological gaps\\n\\nalso leads to better systems from a user point of view. Or at any\\n\\nrate, which (if any) technological gaps detract from user\\n\\nadequacy.\\n\\nThis all looks dangerously bureaucracy-driven. Administrators will find\\n\\nit convenient to have numbers that can be put in rank order. I think there\\n\\nis a great risk of stifling new work that doesn\\'t fit the pattern. An\\n\\noccasional open competition of the DARPA sort sounds like a fine idea, but\\n\\nmaking it a model for all work does not.\\n\\nThere is also too much scope for creating a class of professional evaluators,\\n\\nwho will be people that couldn\\'t make it in research. There are already\\n\\nsome groups like this around. The first item on their agenda is, naturally\\n\\nenough, to keep themselves in business, which is not the same as promoting\\n\\ngood work.\\n\\nThe background material above talks about \"independent experts\", but real ones\\n\\nwill be extremely hard to come by. For one thing, serious experts have their\\n\\nown work to do. For another, in a small community like this, people with\\n\\nenough expertise to be of use are very likely to be either colleagues or\\n\\ncompetitors - or a bit of both - of the people being evaluated.\\n\\n\\n\\nWhile interested in trying to do it, I have considerable difficulties\\n\\nin trying to answer the questions in your questionnaire for the following\\n\\nreasons:\\n\\n1. you set up false dichotomies, where one actually has a gradation eg for 2.2, one end is assessing just one individual application in its own right and the other is comparing systems regardless of contexts of use . Clearly some mix is required.\\n\\n2. its not obvious whether your questions should be answered from a realistic or an idealistic point of view about what sort of evaluation programme one might go for.\\n\\n3. there\\'s too much of a top-down flavour about things, as if one is saying, OK lets evaluate (for its own sake), so whats the best way of doing things. A more appropriate view would be to have some desiderata and consider various possible evaluation suggestions or scenarios against these: this seems to me the only sensible way of getting a fix on what LP systems/components are of value in relation to the conditions that make them of value.\\n\\nTabulation of Responses\\n\\nThe total number of responses to the questionnaire was 11 out of 35. The following tables are an attempt to summarise the results in tabular/numeric form. Because of the recalcitrance of the respondents, who frequently refused to play by the rules (this awkwardness was encouraged) these numeric summaries should not be taken too seriously, or interpreted too literally. In many cases the comments attached to the responses are of far more importance.\\n\\nNumeric choice questions\\n\\nNot all respondents replied to all questions. In some cases respondents indicated that they thought there was no sensible answer to the question, or that they had no opinion, or that more than one answer was appropriate depending on various factors.\\n\\nRanked priority questions\\n\\nThese tables show how many selected which option in which priority position. I.e., for each item the number of persons selecting that item at a given priority ranking is shown. Only top five priorities are listed.\\n\\nNot all respondents answered all questions. Some did not use numeric ranking, but instead introduced terms such as `important\\', `very important\\' etc. sometimes putting several items into the same category. I have mapped these onto numeric ranking as follows. Whatever category ordering was adopted has been mapped onto the numeric ranking from 1 down in sequence (e.g. `very important\\', `important\\', `fairly important\\' are mapped onto 1, 2, 3 respectively.). If several items are listed at the same ranking level that level\\'s count of 1 is distributed proportionally over the items (e.g. if parsing and tagging are both `very important\\' the count in position 1 for each of parsing and tagging goes up by .5). In at least one case one item was deemed \\'uninteresting\\'; this was recorded by introducing a `-\\' column heading, to indicate a negative priority.\\n\\nThere were three ranked priority questions in the questionnaire all part of section 2.5 Content of Comparative TA  SE. Their purpose was to ascertain crudely what sort of activity people would like to see in comparative evaluation exercises.\\n\\nFootnotes\\n\\nHow much variation a task can survive before becoming a completely different task is touched on above. There may be different types of end-user, e.g. teachers and students using some kind of educational system. In the ARPA evaluations, `test data\\' referred to the combination of what we have here called input and output data. Most of this section is taken, more or less verbatim, from Geert Adriaens\\' trigger paper. These point are intended to address some doubts raised by Maghi King, who has said ``there seems to me to be a tension between the FP-4\\'s emphasis on involving specific user communities throughout the life-time of a project and and any attempt to impose some sort of comparative technology evaluation. This is because user communities have their own very specific interests, and I think it will be hard, if not impossible, to  convince them to collaborate in any evaluation initiative which they perceive as falling outside their direct  interests. Thus, for example, I suspect that they will not be interested in using common test data unless that data matches their own concerns.\\'\\' The latter were called jobs by Sparck Jones and Crouch in their trigger paper. Not all tasks performed by a system will be linguistic, e.g. low level file access, graphical interfaces, etc. We are implicitly confining attention to speech and language related tasks here. This section is derived from a contribution of Herman Steeneken\\'s.', metadata={'source': '../data/raw/cmplg-xml/9601003.xml'}),\n",
       " Document(page_content=\"GENERIC RULES AND NON\\n\\n\\n\\nCONSTITUENT COORDINATION\\n\\nWe present a metagrammatical formalism, generic rules, to give a default interpretation to grammar rules. Our formalism introduces a process of dynamic binding interfacing the level of pure grammatical knowledge representation and the parsing level. We present an approach to non-constituent coordination within categorial grammars, and reformulate it as a generic rule. This reformulation is context-free parsable and reduces drastically the search space associated to the parsing task for such phenomena.\\n\\nIntroduction\\n\\nGrammatical theories always make a distinction between unmarked and marked phenomena. The general case is to use certain mechanisms which are likely to deal with regular behaviour. More complex mechanisms should be available when exceptional behaviour is present. However, this difference is not formally expressed in most grammar accounts of complex phenomena. The need to restrict licensing of additional grammar resources is commonly asserted outside of the formal framework used. As exceptional rules involve a higher computational cost, parsing processes associated to such formalisms become computationally intractable.\\n\\nGeneric Rules\\n\\nUnderlying Ideas\\n\\nThe second one is to rewrite the hierarchy, in order to keep the number of rules invariant:\\n\\nNone of these options is an incremental way of capturing exceptions in a grammar. The introduction of an exception forces the revision of previously described lexical types and grammar rules.\\n\\nWe propose a formalism to express and handle default and exceptional rules within a phrase-structure approach that permits this kind of representations. Our formalism describes rules as object-oriented functions that map daughters information into mother features. As in object-oriented programming, precedence between rules is not given statically in the linguistic signature, but it is dynamically established upon the types of the input structures of the compositional process. When combining two constituents, a metagrammatical mechanism establishes a partial order for the candidate rules and selects the most appropriate rule (or set of rules) to be applied.\\n\\nIn each case, the following partial rules have been applied:\\n\\nIn that way, we have the ability to express default rules, and rules with different degrees of specificity in an incremental way, without a need to give them different formal status. Also, only minor changes have to be made to a CFG parsing algorithm in order to work with generic rules.\\n\\nDefinition of a Generic Rule\\n\\nWell\\n\\n\\n\\nformedness\\n\\nParsing with Generic Rules\\n\\nA particular case of generic rule is that in which the combination functions simply return a type, as in our first example. In such a rule, the role of every partial rule is similar to that of a context-free rule, and the generic rule works as a context-free grammar in which rules has a default interpretation. For this kind of generic rules, that combine syntactic information, it is senseless to consider their interaction with a context-free grammar, as they are parsable by themselves. Such parsing does not differ substantially from context-free parsing. The main restriction is that, due to the functional interpretation of partial rules, bottom-up algorithms are best suited that top-down mechanisms, that would require the definition of inverse functions.\\n\\nThe reduce rule that takes account of syntactic parsing with a generic rule can be described as follows:\\n\\nThe computational cost of parsing processes associated to this kind of generic rules carrying on syntactic information is obviously the same as the cost of parsing a context-free grammar. The only difference between both processes is the way to select the appropriate rule when the Reduce step is called. In context-free parsing, this step involves looking up the available rules and matching the objects involved with the right-hand side of the rules. In the worst case, this process introduces a factor G in the overall complexity of parsing, being G the size of the grammar. For a generic rule, the reduce rule involves introducing a new type in the hierarchy of rules. Again, this implies a factor G in the overall complexity, being G the number of partial rules associated to the generic rule. The dependence with the length of the string is obviously the same, as the surface behaviour of a context-free grammar and a generic rule is exactly the same for bottom-up parsing.\\n\\nLet us see a (linguistically weird ) example. Consider the grammar made up of the following generic rules:\\n\\nthe application of the deductive system above to the string `Betty got angry' would be as follows:\\n\\nIn the only application of the reduce rule, the following terms were used:\\n\\nThis example illustrates the point, stated before, that it is not possible to attach a semantic rule to each syntactic rule at compile time. It is, essentially, a dynamic binding process.\\n\\nA phrase-structure grammar with a one-to-one correspondence between syntactic and semantic rules would need 12 pairs of rules to get the same behaviour as the two generic rules above. The incrementality and modularity of the generic rule approach is evident in this case.\\n\\nNon-Constituent Coordination and Categorial Grammar\\n\\nThe general scheme for coordination corresponds to the conjunction of constituents belonging to the same type: `Nothing is certain, except death and taxes', `Take the money and run', `the long and winding road'. Within the categorial grammar framework, such cases are solved using the basic function application rules (corresponding to the non-associative Lambek calculus in a sequent calculus presentation). Nevertheless, there are cases of the so-called non-constituent coordination, where the conjoined expressions are not constituents in the classical sense: `John met Jane yesterday and Chris today', `John read a book about linguistics on Monday and a journal about computers on Tuesday' (Left-node raising) , `John made and Peter painted a wooden chair (Right-node raising). The most common solution is to postulate extra-grammatical levels of representation and/or special purpose parsing algorithms.\\n\\nThe model-theoretic definition for the sequence product is as follows:\\n\\nThis approach avoids using type-raising (a rule that can be applied on any category at any time), but still suffers from intractability, as it is available for every combination of types. Again, the problem relies on the exceptional status that should be given to the rules that deal with non-canonical phenomena.\\n\\nA Generic Rule to Parse Non-Constituent Coordination\\n\\nThe framework of generic rules offers a natural way to express the grammar to deal with non-constituent coordination as an arrangement of default rules, where each combination of types is performed according to the most specific rule available.\\n\\nThe essential rules we want to express are:\\n\\nBy default, two types are combined using functional application and, if functional application is not applicable, they cannot be combined.\\n\\nWhen we try to combine two or more verbal complements and there is a conjunction inmediately preceding them, a sequence product can be formed with them.\\n\\nWhen we try to combine a noun phrase and a verb followed by a conjunction, a sequence product can be formed with them.\\n\\nSuch rules would guarantee that a sequence product is formed only in the relevant cases. We only need an additional rule to scan optimally the elements that match the sequence product on the left-side of the coordination.\\n\\nThe first step is to express the operations related to the grammar as binary rules:\\n\\nThat is the fragment of hierarchy that we need for our present purposes:\\n\\nGiven that hierarchy, we propose the following generic rule to parse sentences including non-constituent coordination:\\n\\nNote, again, that no precedence has to be defined by the grammar writer to control the interaction of the rules. An easy, natural analysis of the suitable arguments for each rule has implicitly defined a partial ordering between them.\\n\\nThe Hasse diagram of the cartesian poset associated to that generic rule is:\\n\\nac\\n\\nad\\n\\nA parser with heuristics or daemons to control coordination processes could achieve a similar efficiency. The clear advantage of a generic rule is that the knowledge that reduces the search space is declaratively introduced at the grammar level, and controlled at an intermediate level between the grammar and the parser (by means of dynamic binding). This enhances linguistic motivation, modularity and incrementality (both to extend the grammar and to control the parsing processes).\\n\\nConclusions\\n\\nThe possibility of formally stating grammar rules with a default interpretation has some advantages from a parsing perspective and from the point of view of knowledge representation. On one side, it provides a declarative and modular way to reduce the search space of parsing processes without altering parsing algorithms with heuristic recipes. On the other side, it provides a linguistically motivated account of exceptional behaviour that is particularly appealing for lexicalized grammar formalisms where the lexicon is the repository of most of the linguistic information, and there are only a few, very general rules that govern linguistic phenomena.\\n\\nOur account of non-constituent coordination illustrates the advantages of such default arrangements for grammar rules. We have presented a categorial account of non-constituent coordination in which incombinable constituents on both sides of conjunction are treated as tuples of elements by the introduction of a sequence type in the conjunction type. Once we have formed a single tuple constituent, we can combine it with the remaining elements. This approach of non-constituent coordination within Categorial Grammar needs some rules of introduction and elimination of the sequence operator which are not commonly needed for other simpler linguistic phenomena, and that makes the parsing process intractable in its original Lambek-style formulation. When reformulated as a generic rule, the type conditions on the arguments for each rule are used by the dynamic binding process to fire the most appropriate grammar rule at every parsing step. The process turns to be context-free parsable, and exhibits a highly restricted search space.\\n\\nBibliography\\n\\nBouma, G. (1992).\\n\\nFeature structures and nonmonotonicity.\\n\\nComputational Linguistics, 18\\n\\n\\n\\n2.\\n\\nCarpenter, R. (1993). Skeptical and credulous default unification with application to templates and inheritance. In Inheritance, Defaults and the Lexicon. Cambridge University Press.\\n\\nDaelemans, W., De Smedt, K., and Gazdar, G. (1992). Inheritance in natural language processing. Computational Linguistics, vol 18-2.\\n\\nDowty, D. (1988). Type raising, functional composition, and non-constituent conjunction. In Categorial Grammars and Natural Language Structures. Reidel Publishing Company.\\n\\nGonzalo, J. (1995). An object-oriented approach to treat exceptions in grammar. In Topics in Constraint Grammar Formalism for Computational Linguistics. Seminar fr Sprachwissenschaft: Universitt Tbingen.\\n\\nMorrill, G. and Solas, T. (1993). Tuples, discontinuity and gapping in categorial grammar. In EACL-93.\\n\\nRoss, J. (1967). Constraints on variables in Syntax. PhD thesis, MIT.\\n\\nShieber, S., Schabes, Y., and Pereira, F. (1994). Principles and implementation of deductive parsing. Technical Report TR-11-94, Center for Research in Computing Technology, Harvard University. Also as cmp-lg/9404008 .\\n\\nSolas, M. (1992). Gramticas Categoriales, Coordinacin Generalizada y Elisin. PhD thesis, Universidad Autnoma de Madrid.\\n\\nSolas, T. (1993). Sequence product, gapping and multiple wrapping. In Proceedings of the workshop on Linear Logic and Lambek Calculus.\\n\\nSteedman, M. (1985). Dependency an coordination in the grammar of dutch and english. Language, 61(3).\\n\\nWittenburg, K. and Wall, R. (1990). Parsing with categorial grammar in predictive normal form. In Current Issues in Parsing Technologies. Wiley.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9601002.xml'}),\n",
       " Document(page_content=\"SEGMENTING SPEECH WITHOUT A LEXICON: THE ROLES OF PHONOTACTICS AND SPEECH SOURCE\\n\\nInfants face the difficult problem of segmenting continuous speech into words without the benefit of a fully developed lexicon. Several sources of information in speech might help infants solve this problem, including prosody, semantic correlations and phonotactics. Research to date has focused on determining to which of these sources infants might be sensitive, but little work has been done to determine the potential usefulness of each source. The computer simulations reported here are a first attempt to measure the usefulness of distributional and phonotactic information in segmenting phoneme sequences. The algorithms hypothesize different segmentations of the input into words and select the best hypothesis according to the Minimum Description Length principle. Our results indicate that while there is some useful information in both phoneme distributions and phonotactic rules, the combination of both sources is most useful.\\n\\nINTRODUCTION\\n\\nInfants must learn to recognize certain sound sequences as being words; this is a difficult problem because normal speech contains no obvious acoustic divisions between words. Two sources of information that might aid speech segmentation are: distribution--the phoneme sequence in cat appears frequently in several contexts including thecat, cats and catnap, whereas the sequence in catn is rare and appears in restricted contexts; and phonotactics--cat is an acceptable syllable in English, whereas pcat is not. While evidence exists that infants are sensitive to these information sources, we know of no measurements of their usefulness. In this paper, we attempt to quantify the usefulness of distribution and phonotactics in segmenting speech. We found that each source provided some useful information for speech segmentation, but the combination of sources provided substantial information. We also found that child-directed speech was much easier to segment than adult-directed speech when using both sources.\\n\\nTo date, psychologists have focused on two aspects of the speech segmentation problem. The first is the problem of parsing continuous speech into words given a developed lexicon to which incoming sounds can be matched; both psychologists (e.g., Cutler  Carter, 1987; Cutler  Butterfield, 1992) and designers of speech-recognition systems (e.g., Church, 1987) have examined this problem. However, the problem we examined is different--we want to know how infants segment speech before knowing which phonemic sequences form words. The second aspect psychologists have focused on is the problem of determining the information sources to which infants are sensitive. Primarily, two sources have been examined: prosody and word stress. Results suggest that parents exaggerate prosody in child-directed speech to highlight important words (Fernald  Mazzie, 1991; Aslin, Woodward, LaMendola Bever, in press) and that infants are sensitive to prosody (e.g., Hirsh-Pasek et al., 1987). Word stress in English fairly accurately predicts the location of word beginnings (Cutler  Norris, 1988; Cutler  Butterfield, 1992); Jusczyk, Cutler and Redanz (1993) demonstrated that 9-month-olds (but not 6-month-olds) are sensitive to the common strong/weak word stress pattern in English. Sensitivity to native-language phonotactics in 9-month-olds was recently reported by Jusczyk, Friederici, Wessels, Svenkerud and Jusczyk (1993).\\n\\nHow do children combine the information they perceive from different sources? Aslin et al. speculate that infants first learn words heard in isolation, then use distribution and prosody to refine and expand their vocabulary; however, Jusczyk (1993) suggests that sound sequences learned in isolation differ too greatly from those in context to be useful. He goes on to say, ``just how far information in the sound structure of the input can bootstrap the acquisition of other levels [of linguistic organization] remains to be determined.'' In this paper, we measure the potential roles of distribution, phonotactics and their combination using a computer-simulated learning algorithm; the simulation is based on a bootstrapping model in which phonotactic knowledge is used to constrain the distributional analysis of speech samples.\\n\\nWhile our work is in part motivated by the above research, other developmental research supports certain assumptions we make. The input to our system is represented as a sequence of phonemes, so we implicitly assume that infants are able to convert from acoustic input to phoneme sequences; research by Kuhl (e.g., Grieser  Kuhl, 1989) suggests that this assumption is reasonable. Since sentence boundaries provide information about word boundaries (the end of a sentence is also the end of a word), our input contains sentence boundaries; several studies (Bernstein-Ratner, 1985; Hirsh-Pasek et al., 1987; Kemler Nelson, Hirsh-Pasek, Jusczyk  Wright Cassidy, 1989; Jusczyk et al., 1992) have shown that infants can perceive sentence boundaries using prosodic cues. However, Fisher and Tokura (in press) found no evidence that prosody can accurately predict word boundaries, so the task of finding words remains. Finally, one might question whether infants have the ability we are trying to model--that is, whether they can identify words embedded in sentences; Jusczyk and Aslin (submitted) found that 7 1/2-month-olds can do so.\\n\\nThe Model\\n\\nTo gain an intuitive understanding of our model, consider the following speech sample (transcription is in IPA): particular segmentation is called a segmentation hypothesis). Two such hypotheses are: lexicons: frequent words whereas Segmentation 2 yields a much larger lexicon of infrequent words. Also note that a lexicon contains only the words used in the sample--no words are known to the system a priori, nor are any carried over from one hypothesis to the next. Given a lexicon, the sample can be encoded by replacing words with their respective indices into the lexicon: sizes of the lexicon and encoded sample. This approach is called the Minimum Description Length (MDL) paradigm and has been used recently in other domains to analyze distributional information (Li Vitnyi, 1993; Rissanen, 1978; Ellison, 1992, 1994; Brent, 1993). For reasons explained in the next section, the system converts these character-based representations to compact binary representations, using the number of bits in the binary string as a measure of size.\\n\\nPhonotactic rules can be used to restrict the segmentation hypothesis space by preventing word boundaries at certain places; for instance, /ktspz/ (``cat's paws'') has six internal segmentation points (k tspz, k tspz, etc), only two of which are phonotactically allowed (kt spz and kts pz). To evaluate the usefulness of phonotactic knowledge, we compared results between phonotactically constrained and unconstrained simulations.\\n\\nSIMULATION DETAILS\\n\\nTo use the MDL principle, as introduced above, we search for the smallest-sized hypothesis. We must have some well-defined method of measuring hypothesis sizes for this method to work. A simple, intuitive way of measuing the size of a hypothesis is to count the number of characters used to represent it. For example, counting the characters (excluding spaces) in the introductory examples, we see that Hypothesis 1 uses 48 characters and Hypothesis 2 uses 75. However, this simplistic method is inefficient; for instance, the length of lexical indices are arbitrary with respect to properties of the words themselves (e.g., in Hypothesis 2, there is no reason why /jul/ was assigned the index `10'--length two--instead of `9'--length one). Our system improves upon this simple size metric by computing sizes based on a compact representation motivated by information theory.\\n\\nWe imagine hypotheses represented as a string of ones and zeros. This binary string must represent not only the lexical entries, their indices (called code words) and the coded sample, but also overhead information specifying the number of items coded and their arrangement in the string (information implicitly given by spacing and spatial placement in the introductory examples). Furthermore, the string and its components must be self-delimiting, so that a decoder could identify the endpoints of components by itself. The next section describes the binary representation and the length formul derived from it in detail; readers satisfied with the intuitive descriptions presented so far should skip ahead to the Phonotactics sub-section.\\n\\nRepresentation and Length Formul\\n\\nThe representation scheme described below is based on information theory (for more examples of coding systems, see, e.g., Li  Vitnyi, 1993 and Quinlan  Rivest, 1989). From this representation, we can derive a formula describing its length in bits. However, the discrete form of the formula would not work well in practice for our simulations. Instead, we use a continuous approximation of the discrete formula; this approximation typically involves dropping the ceiling function from length computations. For example, we sometimes use a self-delimiting representation for integers (as described in Li  Vitnyi, pp. 74-75). In this representation, the number of bits needed to code an integer x is given by\\n\\nHowever, we use the following approximation:\\n\\nThe lexicon lists words (represented as phoneme sequences) paired with their code after the other; the first column is called the word inventory column; the second column is called the code word inventory column.\\n\\nThe length of the representation of the integer n is given by the function\\n\\nTo be fully self-delimiting, the width of a field must be represented in a self-delimiting way; we use a unary representation--i.e., write an extra field consisting of only `1' bits followed by a terminating `0'. There are n fields (one for each word), plus the unary prefix, so the combined length of the fields plus prefix (plus terminating zero) is:\\n\\nThe sample can be represented most compactly by assigning short code words to frequent words, reserving longer code words for infrequent words. To satisfy this property, code words are assigned so that their lengths are frequency-based; the length of the code word for a word of frequency f(w) will not be greater than:\\n\\nThe total length of the code word list is the sum of the code word lengths over all lexical entries:\\n\\nAs in the word inventory column (described above), the length of each code word is represented in a fixed-length field. Since the least frequent word will have the longest code word (a property of the formula for\\n\\nlen([wi])), the longest possible code word comes from a word of frequency one:\\n\\nSince the fields contains integers between one and this number, we define the length of a field to be:\\n\\nAs above, we represent the width of a field in unary, so there are a total of n+1 elements of this size (n fields plus the unary representation of the field width). The combined length of the fields plus prefix (and terminating zero) is:\\n\\nThe length of the representation of the integer m is given by the function\\n\\nThis system of computing hypothesis sizes is efficient in the sense that elements are thought of as being represented compactly and that code words are assigned based on the relative frequencies of words. The final evaluation given to a hypothesis is an estimate of the minimal number of bits required to transmit that hypothesis. As such, it permits direct comparison between competing hypotheses; that is, the shorter the representation of some hypothesis, the more distributional information can be extracted and, therefore, the better the hypothesis.\\n\\nPhonotactics\\n\\nPhonotactic knowledge was given to the system as a list of licit initial and final consonant clusters of English words; this list was checked against all six samples so that the list was maximally permissive (e.g., the underlined consonant cluster in explore could be divided as ek-splore or eks-plore). In those simulations which used the phonotactic knowledge, a word boundary could not be inserted when doing so would create a word initial or final consonant cluster not on the list or would create a word without a vowel. For example (from an actual sample--corresponds to the utterance, ``Want me to help baby? ''): marked with dots. The boundary between /w/ and /a/ is illegal because /w/ by itself is not a legal word in English; the boundary between /a/ and /n/ is illegal because /ntm/ is not a valid word initial consonant cluster; the boundary between /m/ and /i/ is illegal because /ntm/ is also not a valid word final consonant cluster; the boundary between /p/ and /b/ is legal because /lp/ is a valid word final cluster and /b/ is a valid word initial cluster. Note that using the phonotactic constraints reduces the number of potential word boundaries from fifteen to six in this example.\\n\\nAfter the system inserts a new word boundary, it updates the list of remaining valid insertion points--adding a point may cause nearby points to become unusable due to the restriction that every word must have a vowel. For example (corresponding to the utterance ``green and''): /i/ and /n/ becomes invalid because inserting a word boundary there would produce a word with no vowel (/n/).\\n\\nInputs and Simulations\\n\\nTwo speech samples from each of three subjects were used in the simulations; in one sample a mother was speaking to her daughter and in the other, the same mother was speaking to the researcher. The samples were taken from the CHILDES database (MacWhinney Snow, 1990) from studies reported in Bernstein (1982). Each sample was checked for consistent word spellings (e.g., 'ts was changed to its), then was transcribed into an ASCII-based phonemic representation. The transcription system was based on IPA and used one character for each consonant or vowel; diphthongs, r-colored vowels and syllabic consonants were each represented as one character. For example, ``boy'' was written as b7, ``bird'' as bRd and ``label'' as lebL. For purposes of phonotactic constraints, syllabic consonants were treated as vowels. Sample lengths were selected to make the number of available segmentation points nearly equal (about 1,350) when no phonotactic constraints were applied; child-directed samples had 498-536 tokens and 153-166 types, adult-directed samples had 443-484 tokens and 196-205 types. Finally, before the samples were fed to the simulations, divisions between words (but not between sentences) were removed.\\n\\nThe space of possible hypotheses is vast, so some method of finding a minimum-length hypothesis without considering all hypotheses is necessary. We used the following method: first, evaluate the input sample with no segmentation points added; then evaluate all hypotheses obtained by adding one or two segmentation points; take the shortest hypothesis found in the previous step and evaluate all hypotheses obtained by adding one or two more segmentation points; continue this way until the sample has been segmented into the smallest possible units and report the shortest hypothesis ever found. Two variants of this simulation were used: (1)  DIST-FREE was free of any phonotactic restrictions on the hypotheses it could form ( DIST refers to the measurement of distributional information), whereas (2)  DIST-PHONO used the phonotactic restrictions described above. Each simulation was run on each sample, for a total of twelve DIST runs.\\n\\nFinally, two other simulations were run on each sample to measure chance performance: (1) RAND-FREE inserted random segmentation points and reported the resulting hypothesis, (2) RAND-PHONO inserted random segmentation points where permitted by the phonotactic constraints. Since the  RAND simulations were given the number of segmentation points to add (equal to the number of segmentation points needed to produce the natural English segmentation), their performance is an upper bound on chance performance. In contrast, the DIST simulations must determine the number of segmentation points to add using MDL evaluations. The results for each  RAND simulation are averages over 1,000 trials on each input sample.\\n\\nRESULTS\\n\\nEach simulation was scored for the number of correct segmentation points inserted, as compared to the natural English segmentation. From this scoring, two values were computed: recall, the percent of all correct segmentation points that were actually found; and accuracy, the percent of the hypothesized segmentation points that were actually correct. In terms of hits, false alarms and misses, we have:\\n\\nThe performance of  DIST-PHONO on child-directed speech shows that this system goes a long way toward solving the segmentation problem. However, comparing the average performances of simulations is also useful. The effect of phonotactic information can be seen by comparing the average performances of  RAND-FREE and  RAND-PHONO, since the only difference between them is the addition of phonotactic constraints on segmentations in the latter. Clearly phonotactic constraints are useful, as both recall and accuracy improve. A similar comparison between  RAND-FREE and  DIST-FREE shows that distributional information alone also improves performance. Note in all the results of DIST-FREE that using distributional information alone favors recall over accuracy; in fact, the segmentation hypotheses produced by  DIST-FREE have most words broken into single phoneme units with only a handful of words remaining intact. Two comparisons are needed to show that the combination of distributional and phonotactic information performs better than either source alone:  DIST-PHONO compared to  RAND-PHONO, to see the effect of adding distributional analysis to phonotactic constraints, and  DIST-PHONO compared to DIST-FREE, to see the effect of adding phonotactic constraints to distributional analysis. The former comparison shows that the sources combined are more useful than phonotactic information alone.\\n\\nThe latter comparison is less obvious--the trade-off between recall and accuracy seems to have reversed, with no clear winner. Data on discovered word types helps make this comparison:  DIST-FREE found 12% of the words with 30% accuracy and  DIST-PHONO found 33% of the words with 50% accuracy. Whereas the segmentation point data are inconclusive, word type data demonstrate that combining information sources is more useful than using distributional information alone.\\n\\nThere is no obvious difference in performance between child- and adult-directed speech, except in  DIST-PHONO (combined information sources) in which the difference is striking: accuracy remains high and recall rate more than triples for child-directed speech. This difference is again supported by word type data: 14% recall with 30% accuracy for adult-directed speech, 56% recall with 65% accuracy for child-directed speech.\\n\\nDISCUSSION\\n\\nOur technique segments continuous speech into words using only distributional and phonotactic information more effectively than one might expect--up to 66% recall of segmentation points with 92% accuracy on one sample, which yields 58% recall of word types with 67% accuracy (the relatively low type accuracy is mitigated by the fact that most incorrect words are meaningful concatenations of correct words--e.g., `thekitty'). This finding confirms the idea that distribution and phonotactics are useful sources of information that infants might use in discovering words (e.g., Jusczyk et al., 1993b). In fact, it helps explain infants' ability to learn words from parental speech: these two sources alone are useful and infants have several others, like prosody and word stress patterns, available as well. It also suggests that semantics and isolated words need not play as central a role as one might think (e.g., Jusczyk, 1993, downplayed the utility of words in isolation). It is difficult, if not impossible given currently available methods, to determine which sources of information are necessary for infants to segment speech and learn words; only this sort of indirect evidence is available to us.\\n\\nThe results show a difference between adult- and child-directed speech, in that the latter is easier to segment given both distribution and phonotactics. This lends quantitative support to research which suggests that motherese differs from normal adult speech in ways possibly useful to the language-learning infant (Aslin et al.). In fact, the factors making motherese more learnable might be elucidated using this technique: compare the results of several different models, each containing a different factor or combination of factors, looking for those in which a substantial performance difference exists between child- and adult-directed speech.\\n\\nOur model uses phonotactic constraints as absolute requirements on the structure of individual words; this implies that phonotactics have been learned prior to attempts at segmentation. We must therefore show that phonotactics can indeed be learned without access to a lexicon--without such a demonstration, we are trapped in circular reasoning. Gafos and Brent (1994) demonstrate that phonotactics can be learned with high accuracy from the same unsegmented utterances we used in our simulations. In general, two methods exist for combining information sources in the MDL paradigm: one is to have absolute requirements on plausible hypotheses (like our phonotactic constraints)--these requirements must be independently learnable; the other method of combination is to include an information source in the internal representation of hypotheses (like our distributional information)--all components of the representation are learned simultaneously (see Ellison, 1992, for an example of multiple components in a representation).\\n\\nWe would like to extend the system by using a more detailed transcription system. We expect that this would help the system find word boundaries for reasons detailed in Church (1987)--in brief, that allophonic variation may be quite useful in predicting word boundaries. Another simpler extension of this research will be to increase the length of the speech samples used. Finally, we will try the current system on samples from other languages, to make sure this method generalizes appropriately.\\n\\nThis research program will provide complementary evidence supporting hypotheses about the sources of information infants use in learning their native languages. Until now, research has focused on demonstrations of infants' sensitivity to various sources; we have begun to provide quantitative measures of the usefulness of those sources.\\n\\nBibliography\\n\\n1 Richard N. Aslin, Julide Z. Woodward, Nicholas P. LaMendola, and Thomas G. Bever. In press. Models of word segmentation in fluent maternal speech to infants. In Morgan  Demuth (Eds. ), Signal to Syntax: Bootstrapping from Speech to Syntax in Early Acquisition. Erlbaum, Hillsdale, NJ.\\n\\n2 Nan Bernstein. 1982. Acoustic study of mothers' speech to language-learning children: An analysis of vowel articulatory characteristics. Unpublished doctoral dissertation, Boston University.\\n\\n3 Nan Bernstein-Ratner. 1985. Cues which mark clause-boundaries in mother-child speech. Paper presented at the meeting of the American Speech-Language Hearing Association, Washington DC.\\n\\n4 Michael R. Brent 1993. Minimal generative explanations: A middle ground between neurons and triggers. In Proceedings of the 15th Annual Conference of the Cognitive Science Society, pages 28-36, Boulder, Colorado.\\n\\n5 Kenneth Church. 1987. Phonological parsing and lexical retrieval. Cognition, 25:53-69.\\n\\n6 Anne Cutler, and Sally Butterfield. 1992. Rhythmic cues to speech segmentation: Evidence from juncture misperception. Journal of Memory  Language, 31:218-236.\\n\\n7 Anne Cutler, and David M. Carter. 1987. The predominance of strong initial syllables in the English vocabulary. Computer Speech and Language, 2:133-142.\\n\\n8 Anne Cutler, and D. G. Norris. 1988. The role of strong syllables in segmentation for lexical access. Journal of Experimental Psychology: Humen Perception and Performance, 14:113-121.\\n\\n9 T. Mark Ellison. 1992. The Machine Learning of Phonological Structure. Unpublished doctoral dissertation, University of Western Australia.\\n\\n10 T. Mark Ellison. In press. The iterative learning of phonological rules. Computational Linguistics.\\n\\n11 Anne Fernald, and Claudia Mazzie. 1991. Prosody and focus in speech to infants and adults. Developmental Psychology, 27:209-221.\\n\\n12 Cindy Fisher, H. Tokura. In press. Acoustic cues to clause boundaries in speech to infants: Cross-linguistic evidence. In Morgan  Demuth (Eds. ), Signal to Syntax: Bootstrapping from Speech to Syntax in Early Acquisition, Erlbaum, Hillsdale, NJ.\\n\\n13 Adamantios Gafos, and Michael R. Brent. 1994. Learning syllable structure without word boundaries. Paper presented at the 1994 Stanford Child Language Research Forum, Stanford, CA.\\n\\n14 DiAnne Grieser, and Patricia K. Kuhl. 1989. The categorization of speech by infants: Support for speech-sound prototypes. Developmental Psychology, 25:577-588.\\n\\n15 Kathy Hirsh-Pasek, Deborah G. Kemler Nelson, Peter W. Jusczyk, K. Wright Cassidy, B. Druss, and L. Kennedy. 1987. Clauses are perceptual units for young infants. Cognition, 26:269-286.\\n\\n16 Peter W. Jusczyk. 1993. Discovering sound patterns in the native language. In Proceedings of the 15th Annual Conference of the Cognitive Science Society, pages 49-60, Boulder, Colorado.\\n\\n17 Peter W. Jusczyk, and Richard N. Aslin. Submitted for publication. Recognition of familiar patterns in fluent speech by 7 1/2-month-old infants.\\n\\n18 Peter W. Jusczyk, Anne Cutler, and Nancy J. Redanz. 1993. Infants' preference for the predominant stress patterns of English words. Child Development, 64:675-687.\\n\\n19 Peter W. Jusczyk, Angela D. Friederici, Jeanine M. Wessels, Vigdis Y. Svenkerud, and A. M. Jusczyk. 1993. Infants' sensitivity to the sound patterns of native language words. Journal of Memory  Language, 32:402-420.\\n\\n20 Peter W. Jusczyk, Kathy Hirsh-Pasek, Deborah G. Kemler Nelson, Lori J. Kennedy, Amanda Woodward, and Julie Piwoz. 1992. Perception of acoustic correlates of major phrasal units by young infants. Cognitive Psychology, 24:252-293.\\n\\n21 Deborah G. Kemler Nelson, Kathy Hirsh-Pasek, Peter W. Jusczyk, and K. Wright Cassidy. 1989. How the prosodic cues in motherese might assist language learning. Journal of Child Language, 16:55-68.\\n\\n22 Ming Li, and Paul Vitnyi. 1993. An Introduction to Kolmogorov Complexity and its Applications., Springer-Verlag, New York, NY.\\n\\n23 Brian MacWhinney, and C. Snow. 1990. The Child Language Data Exchange System: An update. Journal of Child Language, 17:457-472.\\n\\n24 J. R. Quinlan, and R. L. Rivest. 1989. Inferring decision trees using the minimum description length principle. Information and Computing, 80:227-248.\\n\\n25 J. Rissanen. 1978. Modeling by shortest data description. Automatica, 14:465-471.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9412005.xml'}),\n",
       " Document(page_content=\"DUAL-CODING THEORY AND CONNECTIONIST LEXICAL SELECTION\\n\\nWe introduce the bilingual dual-coding theory as a model for bilingual mental representation. Based on this model, lexical selection neural networks are implemented for a connectionist transfer project in machine translation.\\n\\nIntroduction\\n\\nPsycholinguistic knowledge would be greatly helpful, as we believe, in constructing an artificial language processing system. As for machine translation, we should take advantage of our understandings of (1) how the languages are represented in human mind; (2) how the representation is mapped from one language to another; (3) how the representation and mapping are acquired by human.\\n\\nDual\\n\\n\\n\\nCoding Theory\\n\\nBased on the above structural assumption, dual-coding theory proposes a parallel set of processing assumptions. Activation of connections between referentially related imagens and logogens is called referential processing. Naming objects and imaging to words are prototypical examples. Activation of associative connections between logogens is called associative processing. Lexical translation is an example of associative processing between two languages.\\n\\nConnectionist Lexical Selection\\n\\nLexical Selection\\n\\nIn next subsections, we propose information-theoretical networks based on the bilingual dual-coding theory for lexical selection.\\n\\nInformation\\n\\n\\n\\nTheoretical Networks\\n\\nInformation-theoretical network is a neural network formalism that is capable of doing associations between two layers of representations. The associations can be obtained statistically according to the network's experiences.\\n\\nAn information-theoretical network has two layers. Each unit of a layer represents an element in the input or output of a training pattern, which might be a logogen or a word. Units in different layers are connected. The weight of the connection between unit i in one layer and unit j in the other layer is assigned with the mutual information between the elements represented by the two units\\n\\nEach layer also contains a bias unit, which is always activated. The weight of the connection between the bias unit in one layer and unit j in the other layer is\\n\\nLexical Selection as an Associative Process\\n\\nWe replaced the back-propagation networks for lexical selection with information-theoretical networks simulating the associative process in the dual-coding theory. The networks have two layers of units. Each source (target) language lexical item is represented by a unit in the input (output) layer. One network is constructed for each phrasal category (NP, VP, AP, etc. ).\\n\\nThe networks works in the following way: for a target-language f-structure to be generated, the transfer system knows its phrasal category and its corresponding source-language f-structure from the networks that perform the sub-task 1. It then activates the lexical selection network for that phrasal category with the input units that correspond to the heads of the source language f-structure and its sub-structures. Through the connections between the two layers, the output units are activated, and the lexical item that corresponds to the most active output unit is selected as the head of the target f-structure. The following example illustrates how the system selects the head anmelden for the German XCOMP sub-structure when it does the transfer from\\n\\nPreliminary Result\\n\\nFrom the 300 sentential f-structure pairs, every German VP sub-structure is extracted and labeled with its English counterpart. The English counterpart's head and its immediate sub-structures' heads serve as the input in a sample of VP association, and the German f-structure's head become the output of the association. For the above example, the association ([input I, register, conference]\\n\\n[outputanmelden]) is a sample drawn from the f-structures for the VP network. The training samples for all the other networks are created in the same way.\\n\\nThe accuracy of our system with information-theoretical network lexical selection is lower than the one with back-propagation networks (around 84% versus around 92%) for the training data. However, the generalization performance on the unseen inputs is better (around 70% versus around 62%). The information-theoretical networks do not over-learn as the back-propagation networks. This is partially due to the reduced number of free parameters in the information-theoretical networks.\\n\\nSummary\\n\\nThe lexical selection approach discussed here has two advantages. First, it is learnable. Little human effort on knowledge engineering is required. Secondly, it is psycholinguistically well-founded in that the approach adopts a local activation processing model instead of relies upon symbol passing, as symbolic systems usually do.\\n\\nBibliography\\n\\nP. F. Brown and et al. A statistical approach to machine translation. Computational Linguistics, 16(2):73-85, 1990.\\n\\nA. M. de Groot and G. L. Nas. Lexical representation of cognates and noncognates in compound bilinguals. Journal of Memory and Language, 30(1), 1991.\\n\\nB. J. Dorr. Conceptual basis of the lexicon in machine translation. Technical Report A.I. Memo No. 1166, Artificial Intelligence Laboratory, MIT, August, 1989.\\n\\nA. L. Gorin and S. E. Levinson. Adaptive acquisition of language. Technical report, Speech Research Department, ATT Bell Laboratories, Murray Hill, 1989.\\n\\nA. N. Jain. Parsec: A connectionist learning architecture for parsing spoken language. Technical Report CMU-CS-91-208, Carnegie Mellon University, 1991.\\n\\nW. E. Lambert, J. Havelka and C. Crosby. The influence of language acquisition contexts on bilingualism. Journal of Abnormal and Social Psychology, 56, 1958.\\n\\nS. Nirenberg, V. Raskin and A. B. Tucker. The structure of interlingua in translator. In S. Nirenburg, editor, Machine Translation: Theoretical and Methodological Issues. Cambridge University Press, Cambridge, England, 1987.\\n\\nL. Osterholtz and et al. Janus: a multi-lingual speech to speech translation system. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, volume 1, pages 209-212. IEEE, 1992.\\n\\nA. Paivio. Mental Representations: A Dual Coding Approach. Oxford University Press, New York, 1986.\\n\\nJ. Pustejovsky and S. Nirenburg. Lexical selection in the process of language generation. In Proceedings of the 25th Annual Conference of the Association for Computational Linguistics, pages 201-206, Standford University, Standford, CA, 1987.\\n\\nA. Robinson. Practical network design and implementation. In Cambridge Neural Network Summer School, 1992.\\n\\nY. Wang and A. Waibel. Connectionist transfer in machine translation. In prepare, 1994.\\n\\nFootnotes\\n\\nThis work was partly supported by ARPA and ATR Interpreting Telephony Research Laboratorie. Where vi means the event that unit i is activated. The f-structures are simplified here for the sake of conciseness.\", metadata={'source': '../data/raw/cmplg-xml/9405035.xml'}),\n",
       " Document(page_content=\"Lexical Acquisition via Constraint Solving\\n\\nThis paper describes a method to automatically acquire the syntactic and semantic classifications of unknown words. Our method reduces the search space of the lexical acquisition  problem by utilizing both the left and the right context of the unknown word. Link Grammar provides a convenient framework in which to implement our method.\\n\\nIntroduction\\n\\nA robust Natural Language Processing (NLP) system must be able to process sentences that contain words unknown to its lexicon. The syntactic and semantic properties of unknown words are derived from those of known words in a sentence, assuming that the given sentence is valid.\\n\\nThe underlying linguistic framework plays a critical role in lexical acquisition. Linguistic frameworks can be broadly classified into two groups: those with phrase structure rules and those without. The lexicon of known words and any phrase structure rules that exist determine the size of the search space for the classification of unknown words. In general, the more complex the phrase structure rules, the larger the search space.\\n\\nThis paper explores lexical acquisition in a framework without phrase structure rules. All constraints on the usage of words are integrated into the lexicon. We use a novel lexical representation that explicitly specifies what syntactic and semantic classes of words may appear to the left and to the right of a word in a valid sentence. If all words are known in a sentence, it is valid only if the associated constraints have a solution. Otherwise, constraints are inferred for unknown words that will make the sentence valid.\\n\\nThis paper begins with an introduction to Link Grammar. We describe the process of acquiring the syntax of unknown words and outline the process of semantic acquisition. We close with a discussion of related work and our plans for the future.\\n\\nLink Grammar\\n\\nEach word in the grammar is defined by a syntactic constraint that is expressed in a disjunctive normal form. Each disjunct consists of a pair of ordered lists of the form\\n\\n((l1,...,lm-1,lm)(rn,rn-1,...,r1))where the left hand list is made up of connectors that must link to words to the left of the word in the sentence  and likewise for the right hand list. Each word can have multiple disjuncts, which implies that it can be used in various syntactic contexts.\\n\\nThe following is a simple example of a Link Grammar:\\n\\nbig, yellow: \\t\\t (( ) (A))\\n\\ncar, corn, condor, gasoline, meat: \\t\\t   ((A,Ds,Os) ( )) \\t\\t   ((A,Ds) (Ss))   \\t\\t   ((Ds) (Ss))    \\t\\t   ((Ds,Os) ( ))    \\t\\t   ((Os) ( ))    eats: \\t\\t  ((Ss) (O)) \\t\\t  ((Ss) ( )) the: \\t\\t  (( ) (D))\\n\\nParsing a sentence in Link Grammar consists of choosing one disjunct for each word such that it can be  connected to the surrounding words as specified in that disjunct. For a simple example consider the sequence of words: ``The condor eats the meat'' and the following  choices of disjuncts for each word from the lexicon above: the: \\t\\t    (( ) (D)) condor: \\t\\t  ((Ds) (Ss)) eats: \\t\\t  ((Ss) (O)) the: \\t\\t   (( ) (D)) meat: \\t\\t   ((Ds,Os) ( )) The following diagram (called a linkage) shows the links among the words that justify the validity of the sentence according to Link Grammar.\\n\\nIn general, a sequence of words is a sentence if it is possible to draw links among the words in such a way that the syntactic constraint of every word is satisfied and all the following meta-rules are observed:\\n\\nPlanarity: Links drawn above the sentence do not intersect.\\n\\nConnectivity: There is a path from any word in the sentence to any other word via the links.\\n\\nExclusion: No two links may connect the same pair of words.\\n\\nSyntactic Acquisition\\n\\nSyntactic acquisition is the process of mapping an unknown  word to a finite set of syntactic categories. In Link Grammar syntactic categories are represented by the constraints that are expressed as disjuncts. Our lexical acquisition system is not called upon to create or identify new syntactic categories as we assume that these are already known.\\n\\nGiven a sentence with unknown words the disjuncts of unknown words are determined based upon the syntactic constraints of the known words in the sentence.\\n\\nFor instance suppose that snipe is an unknown word in the sentence: ``The snipe eats meat''. The following lists all the choices for the disjuncts of the known words which come from the lexicon.\\n\\nthe: \\t\\t  (( ) (D)) snipe: \\t\\t  ((?) (?)) eats: \\t\\t  ((Ss) (O)) \\t\\t  ((Ss) ( )) meat: \\t\\t   ((A,Ds,Os) ( )) \\t\\t   ((A,Ds) (Ss))   \\t\\t   ((Ds) (Ss))    \\t\\t   ((Ds,Os) ( ))    \\t\\t   ((Os) ( ))\\n\\nIt must be determined what disjunct associated with `snipe' will allow for the selection of a single disjunct for every known word such that each word can have its disjunct satisfied in accordance with the meta-rules previously discussed. There are 10 distinct disjuncts in the above grammar and any one of those could be the proper syntactic category for `snipe'.\\n\\nWe could attempt to parse by blindly assigning to `snipe' each of these disjuncts and see which led to a valid linkage. However this is impractical since more complicated grammars will have hundreds or even thousands of known disjuncts. In fact, in the current 24,000 word lexicon there are approximately 6,500 different syntactic constraints. A blind approach would assign all of these disjuncts to `snipe' and then attempt to parse. It is possible to greatly reduce the number of candidate disjuncts by analyzing the disjuncts for the known words. Those disjuncts that violate the constraints of the meta-rules are eliminated.\\n\\nThe disjuncts ((A,Ds)(Ss)) and ((Ds)(Ss)) for `meat' are immediately eliminated as they can never be satisfied since there are no words to the right of `meat'.\\n\\nThe disjunct ((A,Ds,Os)( )) for `meat' can also be eliminated. If the A connector is to be satisfied it would have to be satisfied by `snipe'. The ordering meta-rule implies that the Ds connector in `meat' would have to be satisfied by `the' but then the remaining Os connector in `meat' would not be satisfiable since there are no words preceding `the'.\\n\\nThat leaves the disjuncts ((Ds,Os)( )) and ((Os)( )) as the remaining possibilities for `meat'. The disjunct ((Ds,Os)( )) can be eliminated since the only words that can satisfy the Ds connector are `the' or `snipe'. Again the ordering meta-rule makes it impossible to satisfy the Os connector. Thus the only remaining candidate disjunct for `meat' is ((Os)( )).\\n\\nThe next word considered is `eats'. There are two possible disjuncts and neither can be immediately eliminated. The left hand side of each disjunct consists of an Ss connector. This could only be satisfied by `snipe' which therefore must have an Ss connector in its right hand side. Recall that the left hand side of `meat' consists of an Os connector. This could be satisfied either by the ((Ss)(O)) disjunct for `eats' or if the right hand side of `snipe' consists of ((Os,Ss)). The left hand side of 'snipe' need only consist of a D connector in order to satisfy the right hand side of `the'. Thus the disjunct for `snipe' must be either ((D)(Ss)) or ((D)(Os,Ss)) and we have still not eliminated any of the candidate disjuncts for `eats'. Unfortunately the meta-rules do not allow for the further elimination of candidate disjuncts.\\n\\nIn cases such as this the lexicon is used as a knowledge source and will be used to resolve the issue. The disjunct ((D)(Ss)) is selected for the word `snipe' since it appears in the lexicon and is normally associated with simple nouns. Thus the disjunct ((Ss)(O)) is the only possibility for `eats'.\\n\\nThe disjunct ((D)(Os,Ss)) does not appear in the lexicon and in fact implies that the word it is associated with is both a noun and a verb. To eliminate such nonsensical combinations of connectors the lexicon of known words is consulted to see if a theorized disjunct has been used with a known word, and if so it is accepted. The intuition is that even though a word is unknown it is likely to belong to the same syntactic category as that of some known words. This follows from the assumption that the set of syntactic categories is closed and will not be added to by the lexical acquisition system. For efficiency these constraints can be used to avoid the generation of nonsensical disjuncts in the first place.\\n\\nTo summarize, the following assignment of disjuncts satisfies the meta-rules and leads to the linkage shown below.\\n\\nthe: \\t\\t  (( ) (D)) snipe:\\t\\t  ((D) (Ss)) eats: \\t\\t  ((Ss) (O)) meat: \\t\\t   ((Os) ( ))\\n\\nSemantic Acquisition\\n\\nIn order to semantically classify an unknown word the lexical entries of known words must be augmented with semantic information derived from the actual usage of them in a variety of contexts.\\n\\nAs sentences with no unknown words are parsed, each connector in the syntactic constraints of nouns and verbs is tagged with the noun or verb with which it connects to. For instance given the sentence: ``The condor eats meat'', the nouns and verbs are tagged as follows:\\n\\nthe: \\t\\t  (( ) (D)) condor: \\t\\t  ((D) (Sseats)) eats: \\t\\t  ((Sscondor) (Omeat)) meat: \\t\\t   ((Oseats) ( ))\\n\\nWhen a word occurs in related syntactic contexts the semantic tags on the syntactic constraints are merged through generalization using the superclass information contained in the lexicon. Suppose that the following sentences with no unknown words have been processed.\\n\\nS1: The big cow eats yellow corn. S2: The condor eats meat. S3: The car eats gasoline. The corresponding semantic tags for `eats' are: eats: \\t\\t  ((Sscow) (Ocorn)) \\t\\t  ((Sscondor) (Omeat)) \\t\\t  ((Sscar) (Ogasoline))\\n\\nFrom sentences S1 and S2 a more general semantic constraint is learned since `animal' subsumes `cow' and `condor' and `food' subsumes `corn' and `meat'. This knowledge is expressed by:\\n\\nR1: \\t\\t  ((Ssanimal) (Ofood)) \\t\\t  ((Sscar) (Ogasoline))\\n\\nThe semantic tags applied to the connectors serve as semantic constraints on the words that `eats' connects to. The first disjunct in the above entry tells us that `eats' must have a concept that is subsumed by `animal' to its left and a concept that is subsumed by `food' to its right.\\n\\nWhile the lexicon has no information about the unknown word it does have the semantic constraints of the known words in the sentence. These are used to infer what the semantic classification of the unknown word should be if the sentence is valid.\\n\\nNo semantic information has been acquired for `snipe'. If the nouns and verbs in the sentence, ``The snipe eats meat'',  are tagged with the nouns and verbs that they connect to, the following is obtained:.\\n\\nthe: \\t\\t  (( ) (D))\\n\\nsnipe: \\t\\t  ((D) (Sseats))\\n\\neats: \\t\\t  ((Sssnipe) (Omeat))\\n\\nmeat: \\t\\t   ((Oseats) ( ))\\n\\nThe lexicon has no knowledge of `snipe' but it does have knowledge of the verb, 'eats', that links to `snipe'. It must be determined which of the two usages of `eats' described in R1) applies to the usage of `eats' in ``The snipe eats meat''.\\n\\nAccording to the concept hierarchy `meat' is subsumed by `food' whereas `gasoline' is not. This indicates that the usage ((\\n\\nSsanimal)(Ofood)) is more appropriate and that `snipe' must therefore be tentatively classified as an animal. This classification can be refined as other usages of `snipe' are encountered.\\n\\nDiscussion\\n\\nAn implementation is under way to extend the parser of Link Grammar to automatically acquire the syntax and semantics of unknown words. It seems that the disjuncts of each word are a special kind of feature structure. An interesting topic is to integrate feature structures and unification with Link Grammar to allow more expressive handling of semantic information.\\n\\nBibliography\\n\\nL. Asker, B. Gamback, and C. Samuelsson. EBL[2] : An approach to automatic lexical acquisition. In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), pages 1172-1176, Nantes, France, 1992.\\n\\nR. Berwick. Learning word meanings from examples. In Proceedings of the 8th International Joint Conference on Artificial Intelligence (IJCAI-83), volume 1, pages 459-461, Karlsruhe, West Germany, August 1983.\\n\\nK. Church. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143, Austin, TX, 1988.\\n\\nR. Granger. FOUL-UP: A program that figures out meanings of words from context. In Proceedings of the the 5th International Joint Conference on Artificial Intelligence (IJCAI-77), volume 1, pages 172-178, Cambridge, MA, August 1977.\\n\\nP. Hastings. Automatic Acquisition of Word Meaning from Context. PhD thesis, The University of Michigan, 1994.\\n\\nR. Hudson.\\n\\nWord Grammar.\\n\\nBasil Blackwell, 1984.\\n\\nI.A. Melcuk. Dependency Syntax: Theory and Practice. State University of New York Press, 1988.\\n\\nR. Oehrle, E. Bach, and D. Wheeler, editors. Categorial Grammars and Natural Language Structures. Reidel, Dordrecht, The Netherlands, 1988.\\n\\nD. Russell. Language Acquisition in a Unification-Based Grammar Processing System Using a Real-World Knowledge Base. PhD thesis, University of Illinois at Urbana-Champaign, 1993.\\n\\nG. Satta and O. Stock. Bidirectional context-free grammar parsing for natural language processing. Artificial Intelligence, 69:123-164, 1994.\\n\\nD. Sleator and D. Temperley. Parsing English with a Link Grammar. Technical Report CMU-CS-91-196, Carnegie Mellon University, October 1991.\\n\\nR. Weischedel, M. Meteer, R. Schwartz, L. Ramshaw, and J. Palmucci. Coping with ambiguity and unknown words through probabilistic models. Computational Linguistics, 19(2):359-382, June 1993.\\n\\nU. Zernik. Language acquisition: Learning a hierarchy of phrases. In Proceedings of the 10th International Joint Conference on Artificial Intelligence (IJCAI-87), volume 1, pages 125-132, Milan, Italy, August 1987.\", metadata={'source': '../data/raw/cmplg-xml/9502028.xml'}),\n",
       " Document(page_content='Development of a Spanish Version of the Xerox Tagger\\n\\nThis paper describes work performed withing the CRATER (Corpus Resources And Terminology ExtRaction, MLAP-93/20) project, funded by the Commission of the European Communities. In particular, it addresses the issue of adapting the Xerox Tagger to Spanish in order to tag the Spanish version of the ITU (International Telecommunications Union) corpus. The model implemented by this tagger is briefly presented along with some modifications performed on it in order to use some parameters not probabilistically estimated. Initial decisions, like the tagset, the lexicon and the training corpus are also discussed. Finally, results are presented and the benefits of the mixed model justified.\\n\\nIntroduction\\n\\nThe interest on the Xerox Tagger not only comes from the virtues already mentioned, but also benefits from the attraction that stochastic approaches to Natural Language Processing have revived in the researchers on the field. Widely commented examples of this resurgence of probabilistic techniques include the double special issue that Computational Linguistics devoted recently to this venture. Nevertheless, it is more interesting in this debate the possibility to combine (empirical and racionalist) techniques, rather than approaching to statistical models with a ``let\\'s-see-what-it-can-do\\'\\' idea. Although they are capable of ``doing things\\'\\', with relative simplicity in the estimation of parameters and in a robust way, being this quality, as it is known, difficult to find in knowledge-based systems.\\n\\nThe Xerox Tagger\\n\\nThe Xerox Tagger uses a statistical method for text tagging. In these systems, ambiguity of assignment of a tag to a word is performed on the basis of most likely interpretation. A form of Markov model is used that assumes that a word depends probabilistically on just its part-of-speech, which in turn depends, in most systems though not in the Xerox Tagger, solely on the category of the preceding two words.\\n\\nThe Xerox Tagger is based on an HMM. It uses ambiguity classes and a first-order model to reduce the number of parameters to be estimated without significant reduction in accuracy. According to the authors, reasonable results can be produced training on as few as 3,000 sentences. Besides, ``relatively few ambiguity classes are sufficient for wide coverage, so it is unlikely that adding new words to the lexicon requires retraining, as their ambiguity classes are accomodated.\\'\\' Words not found in the lexicon are assigned an ambiguity class according to both context and suffix information.\\n\\nProcedure\\n\\nLets briefly describe the procedure of the tagger. After the tokenizer has converted the input text into a sequence of tokens, these tokens are passed to the lexicon. Tokens are converted into a set of stems, each annotated with a part-of-speech tag. The set of tags identifies an ambiguity class, which is also delivered by the lexicon.\\n\\nThe training module takes long sequences of ambiguity classes as input. It uses the Baum-Welch algorithm to produce a trained HMM, as input to the tagging module. The tagging module buffers sequences of ambiguity classes between sentence boundaries. These sequences are disambiguated by computing the maximal path through the HMM with the Viterbi algorithm.\\n\\nWords not found in the manually-constructed lexicon ``are generally both open class and regularly inflected\\'\\', according to HREF=\"9505035.html#Cutting92\">Cutting et al., 1992]. A language-specific method can be employed to guess ambiguity classes for these unknown words. Hence, the Xerox Tagger provides a function that computes `suffixes\\' together with probabilistic predictions of a word\\'s category ending in each of the suffixes calculated. This function also operates on an untagged training corpus.\\n\\nAs a final stage, words not found in the lexicon and ending in a suffix not recognized are assigned a default ambiguity class (open class).\\n\\nA mixed model\\n\\nAs already mentioned in the previous section, in case a word is unknown to the system, `suffix\\' information can be used in order to approximate its possible ambiguity class. This information can be calculated by means of the LISP function class-guesser:train-guesser-on-files. The authors strongly recommend the use of this function in order to retarget the tagger to new corpora, new tagsets, and new languages [Jan Pedersen, personal communication]. However, we will try to demonstrate that a system using a set of manually-added suffixes performs better, at least for inflectional languages like Spanish.\\n\\nThe above-mentioned function operates on a training text and calculates two parameters:\\n\\nthe suffixes themselves\\n\\nthe ambiguity class assigned to each suffix\\n\\nIn the suffix calculation, the unique parameter that can be controlled is their maximum lenght. It can be done by changing the value of the variable class-guesser::*suffix-limit*.\\n\\nThe ambiguity class to be assigned to each suffix is selected from the set of classes computed during normal training, which is written to a classes file. This file contains (i) every tag observed in the lexicon (which is, obviously, unambiguous), (ii) every set of ambiguously assigned tags for every form in the lexicon, and (iii) the ambiguity class for the open class (a default class).\\n\\nThe above-mentioned function, after computing a suffix, observes words in the lexicon ending in the proposed suffix and the set of tags assigned to them. It then eliminates those tags not included in the ambiguity class for the open class and, afterwards, tries to match the remaining tags with one of the existing ambiguity classes. If it succeeds, this ambiguity class is assigned to the suffix. Conversely, if it fails, the suffix will receive the default ambiguity class.\\n\\nWhile this behaviour may be correct for both non-inflecting languages (as English) and relatively reduced tagsets, it is considered highly inefficient for inflectional languages and more extensive tagsets. We will try to exemplify this point in the following paragraph.\\n\\nThere are many ambiguous forms in the Spanish lexicon. Most cases range over 2 to 4 tags for each form, but there are a few cases with even 5 or 6. If we establish an open class including all nominal, adjectival, and verbal tags, the classes file will contain, along with this open class, the list of individual tags of the tagset, the default ambiguity class, several ambiguity classes formed by 2-tuples, 3-tuples, 4-tuples and a few 5-tuples and 6-tuples. This means that computed suffixes must be accomodated into these latter ambiguity classes in order to maximize accuracy in the assignment of tags (the use of the default ambiguity class in these cases will produce incorrect results in most cases). Assuming that a is one of the suffixes computed by the above-mentioned function, the problem then is trying to match the set of tags observed in the lexicon for words ending in a included in the intersection with the default ambiguity class with one of the previously computed classes. Words ending in a can, usually, be singular feminine adjectives or nouns, subjunctive present first and third person singular verbs, and indicative present third person singular verbs (#(:ADJGFS :NCFS :VLPI3S :VLPS1S :VLPS3S)). Now, if we take wordforms with 5 different tags, we learn that the number of classes generated by these is limited to just four:\\n\\nObviously, there is no possible matching between the former ambiguity class and any of the latter. The former ambiguity class does simply not exist --there must exist at least one ambiguous form (ending in a or in another suffix) validating an ambiguity class in order for it to be selected when observed in words ending in a. The result observed is that the function is forced to assign the open ambiguity class to most of the suffixes computed.\\n\\nMoreover, in inflectional languages, the selection of the training corpus is also crucial to the issue of suffix calculation. A sufficient amount of text containing an as wide as possible range of words should be gathered and used for training purposes. However, this prerequisite alone does not guarantee a proper computation of suffixes, since the function operates not only on word tokens from the training corpus but also on the system\\'s lexicon. The parameter to be considered in this respect is not the actual size of this lexicon (which, nevertheless, is important in order to accurately assign ambiguity classes to word tokens from a corpus), but the set of ambiguity classes represented in that lexicon --and this set would not increase with the addition of new words.\\n\\nLikewise, inflectional languages, like Spanish, present the characteristic of having a clear correspondance between (linguistically motivated) suffixes and morphosyntactic properties of the word(s) they are attached to. Consequently, this a priori knowledge could be exploited in a tagging system like the one described here. Thus, if a word ending in a can represent the following ambiguity class:\\n\\nthe system should be able to use this information without needing to estimate it.\\n\\nOn the other hand, the practice of manual coding of information for unknown words has been used only to a relative extent in probabilistic models of language. Some systems, like the Xerox Tagger, compute probabilistically both the suffixes and the ambiguity classes associated to them; but others, like the one described in HREF=\"9505035.html#Weischedel93\">Weischedel et al., 1993], include a hybrid approach where suffixes are manually added and ambiguity classes are approximated directly from training data.\\n\\nHowever, all probabilistic taggers work with manually coded information as, for instance, a lexicon. Hence, a new approach could include both manually-computed suffix tables and ambiguity classes, specially for inflectional languages where this information can be straightforwardly obtained, thus improving system accuracy. This approach, however, has the drawback that migrating the system to a new tagset bears more resource conversion work, since both the lexicon and the suffix table will have to be mapped onto it.\\n\\nIn the light of this argumentation, a modification to the system has been proposed and successfully implemented. This consists in merging, during normal training, the set of classes observed in the lexicon with those stated by a linguist in the suffix file. The training process will benefit from the reduction in the number of elements of the ambiguity classes to be computed when words not contained in the lexicon are found, thus improving accuracy in the generation of paths.\\n\\nThe benifits of this methodology of work are shown in the following sections.\\n\\nModel tuning\\n\\nParameter estimation is a central issue in probabilistic models of language. A hidden Markov model of language can be tuned in a variety of ways. Thus, several decisions have been taken concerning the tagset, the lexicon, and the biases. These choices are presented and (hopefully) justified below. The selection of the training corpus and the results obtained are also discussed.\\n\\nThe tagset and the lexicon\\n\\nConsequently, in the construction of a tagset to be used by a probabilistic tagger, a trade-off must be found between exhaustivity and accuracy --the more exhaustive the information encoded in the tagset (the greater the tagset), the less accurate the tagging will be (since the resulting model will be more complex and parameter estimations less accurate).\\n\\nNouns:  common/proper distinction, with various subtypes for propers; semantic information considered in the first tagset (temporal, locative, measurement, numeral, and organization) has been now restricted only to measurement, given the large amount of postediting derived from the initial distinction; other common morphosyntactic information (gender, number).\\n\\nAdverbs: degree, wh information, locative (with subtypes), deixis, and polarity.\\n\\nVerbs:  status (main/auxiliaries), person, number, tense, mood, gender, and finiteness (implicit). Given the rich verbal morphology of Spanish, verbal tags account for 59% of the total number of tags.\\n\\nThis tagset has been considered ``too finegrained to be suitable for a probabilistic tagger\\'\\' [Lauri Karttunen, personal communication].\\n\\nThen, a second, reduced tagset, based on the first one, was built. The number of tags has been dramatically cut down in this tagset to 174. Features previously considered for major categories have been restricted to gender, number and person. Minor categories have also seen reduced the morphosyntactic (and sometimes semantic) information considered at first.\\n\\nThe reduced tagset has been built precisely with the idea of testing the improvement of the tagging accuracy when the number of parameters is simpler.\\n\\nAll probabilistic taggers make use of a lexicon of varied coverage. HREF=\"9505035.html#Cutting92\">Cutting et al., 1992], for instance, report on tagging results on even numbered sentences of the Brown corpus using a 50,000 forms lexicon. With this lexicon and the suffix file, no unknown forms were encountered in the training process, thus providing no training data for forms assigned the open class.\\n\\nSince our starting point is not a tagged corpus in which to perform the testing of a given stochastic model, our lexicon is not specially biased towards the corpus we aim at tagging. On the contrary, we would like to build the tagger on as a uniform lexical material as possible. Hence, the whole set of tags for each word has been taken into account during lexicon building.\\n\\nThe lexicon used by the system has been produced by compiling different sources of information, although some coding work has also been performed. This lexicon is being used in the actual tagging of the ITU corpus, since it provides a more accurate model to lexical ambiguity than that provided by suffix information alone.\\n\\nTraining a hidden Markov model\\n\\nTraining on hidden Markov models of language is performed without a tagged corpus. In a tagger under this regime, state transitions (i.e., transitions between categories) are unobservable. Under these circumstances, the training is performed according to a Maximum Likelihood principle, using the Forward-Backward (FB) or Baum-Welch algorithm. This training process can be biased in a number of ways in order to `force\\' somehow the learning process. Two such ways implemented in the Xerox Tagger, concerning ambiguity classes and state transitions, are described below:\\n\\nThe biasing facts on ambiguity classes are called symbol biases. These represent a kind of lexical probabilities for given equivalence classes. This way, ambiguity classes are annotated with favoured tags. Note, however, that this is stated for a given class and not for individual forms in the lexicon (as it is, for instance, in CLAWS    HREF=\"9505035.html#Garside87\">Garside et al., 1987]), resulting in a less efficient mechanism.\\n\\nThe biasing facts on state transitions are called transition biases. These specify that it is likely or unlikely that a tag is followed by some specific tag(s). The biasing can be formulated either as favoured or as disfavoured probabilities. Disfavoured probabilities receive a small constant but are not disallowed; on the contrary, data in the training corpus may modify probabilities.\\n\\nNot including rare readings in the lexicon in order for the tagger not to select them.\\n\\nUsing different values for the number of iterations (the number of times the same block is used in training) and the size of the block of text used for training.\\n\\nThe choice of the training corpus affects the result.\\n\\nThe model has been initially tuned by means of the addition of both transition and symbol biases. These have not been documented yet, but they include favouring clitic-verb, determiner-noun and noun-adjective transitions, and disfavouring adjective-adjective and preposition-finite verb transitions. Nouns are favoured when they can also be adjectives.\\n\\nTraining corpus and results\\n\\nThe system has been trained using both versions of the tagset. Although the decision of tagging the ITU corpus using the full version of the tagset was already adopted and postediting on the corpus so tagged has begun, a parallel development of the tagger with the reduced tagset has been performed. Results obtained with both tagset are presented in this section.\\n\\nThe full 1M word subset of the corpus being postedited has been used as the training corpus, leaving file SP_itu_corpus_000 as the test corpus. This corpus contains 9,366 tagged tokens. The corpus has been used in an incremental way, testing results with each partial model obtained.\\n\\nIn both cases, the system used includes an initial set of transition and symbol biases which is responsible for the good results obtained with the uniform (untrained) model. The biasing facts are the same for each model, as well as the lexicon (in terms of coverage) and the suffix information file.\\n\\nAs it will become clear by looking at the results, there is not a clear learning curve. The system performs relatively well with the set of initial biases, and even its accuracy improves 2.5% with a small amount of text. However, best results are obtained with as less as 50,000 words, being the accuracy from this corpus size on almost the same. Anyway, since results with other models are so close, it could be difficult to prove Merialdo\\'s claim.\\n\\nWith respect to the comparison between both tagsets, the curve is the same in either case, having also obtained the best results with the same amount of training text. Surprisingly, the accuracy is also the same for both tagsets with the best model. However, in general, the reduced tagset shows an insignificant .1% better accuracy than the full tagset.\\n\\nTable 1 shows the behaviour of the system when tagging the test corpus with the full tagset.\\n\\nAs counted by  UNIX command  wc Real time First figures represent absolute number of errors; second figures do not include foreign words\\n\\nTable 2 shows the behaviour of the system when tagging the test corpus with the reduced tagset.\\n\\nAs counted by  UNIX command  wc Real time First figures represent absolute number of errors; second figures do not include foreign words\\n\\nBenefits of a linguistically enriched model\\n\\nApart from the reasons mentioned in previous sections relative to the soundness of a model based on linguistic knowledge in order to treat suffix information, at least for inflectional languages, there is also a kind of pragmatic reason:  tagging should be more accurate using a linguistically enriched model than with the original, only statistical one. In order to prove this statement, a comparison of the performance of both models will be carried out. For the moment, suffix information files can be compared in order to guess which the best model will be.\\n\\nThe whole subset of the corpus to be postedited for alignment within the CRATER project has been considered as the training corpus for the function that computes suffixes. Results obtained both by hand and automatically are presented in table 3:\\n\\nBesides, this file includes 306 suffixes for the recognition of verbs with enclitics and 22 suffixes for foreign words. Besides, this file includes 306 suffixes for the recognition of verbs with enclitics and 22 suffixes for foreign words.\\n\\nNote that the function automatically calculating suffix information can be executed both with a trained and with an untrained model. Results, however, are better with a previously trained model. Nontheless, these results are far from those obtained with the manually included information. Besides, the number of suffixes is smaller and the maximum lenght does not guarantee the recognition of typical unambiguous suffixes:  -mente, which is always an adverb, or -cin, always a feminine singular noun. Other major drawback of the function is that does not take account of case of words, hence producing suffixes in uppercase and/or lowercase with different information in either case.\\n\\nConsequently, the performance of a model using the approach proposed will be better for Spanish than the original strategy of the tagger.\\n\\nOther issues\\n\\nThe Xerox Tagger lacks the adequate mechanisms for the treatment of complex lexical elements. Segmentation of the text into tokens is performed by means of graphic information like space characters and other delimiters. This poses a problem for the identification of both complex orthographic words comprising more than one textword (i.e. clitic forms, since portmanteaux are to be assigned a specific tag) and textwords that span over more than one orthographic word (i.e. continuous invariant multi-word units).\\n\\nThe first issue, still in the implementation phase, includes the segmentation of higher-order complex words for the recognition and further tagging of verbal forms with enclitics. So far, the system assigns a special tag, VCLI, to these forms. Nevertheless, the code is being changed so as to split these tokens and appropriately tag these elements. The complexity of this task stresses somehow one of the limitations of the Xerox Tagger --the system lacks a morphological analyzer. This limitation questions one of the claims of the developers, namely, its language-indepency. The lexical repertoire of fully inflected forms in languages a high inflectional productivity may collapse the system. This is also true for highly agglutinative languages, where productive word-formation rules make impossible the creation of a wide-coverage lexicon.\\n\\nThe second issue is has been solved by means of a pre-processing phase. Space characters separating components of a complex textword are replaced by an underscore character (_), thus normal tokenization may operate on these items. To this end, a program developed by Theo W. Tams, from  EUROTRA-DK    HREF=\"9505035.html#Jensen90\">Jensen et al., 1990], during the third phase of  EUROTRA-I for a tender on Front End Integration has been used. The source code has been adapted to our requirements.\\n\\nAlong with these, other modifications have been performed to the original Xerox Tagger. Thus, the ouput format has been modified, so that instead of being presented in the following line, tags are placed to the right of every word separated from it by means of an underscore character (_), as it is usual in the taggers from England, specially in the works by the University of Lancaster.\\n\\nSentence boundaries are correctly identified by the tokenizer, with the usual limitations inherent to the tasks --as, for instance, the proper distinction between these and dots in abbreviations. This issue is esential to the system behaviour given that the training process is performed on text chunks that are segmented into sentences.\\n\\nHowever, since the full stop is used for the purpose of sentence identification it cannot be properly tagged by the tokenizer itself. An automatic postediting phase, that currently performs also the correction of certain transitions between categories that the system tends to tag incorrectly, carries out the correct tagging of full stops.\\n\\nBesides, special tokenization rules have been implemented in order to recognize two date formats, as observed in the ITU corpus, namely, dd.mm.yy and yyyy-yyyy.\\n\\nConclusions\\n\\nAcknowlegments\\n\\nWe are grateful to Flora Ramrez Bustamante for her cooments and help in the building of the linguistic resources on which this tagger has been developed. Ruthanna Barnett has also provided her grain of sand with her comments.\\n\\nBibliography\\n\\nA. F. Nieto. CRATER: UPM Progress for the Period April-September 1994. CRATER Internal Document. September 1994.\\n\\nD. Cutting, J. Kupiec, J. Pedersen, and P. Sibun. A Practical Part-of-Speech Tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, Trento.\\n\\nJ.-P. Chanod and P. Tapanainen.Tagging French - comparing a statistical and a constraint-based method. In Proceedings of the EACL-95, Dublin.\\n\\nA. M. Derouault and B. Merialdo. Natural Language Modelling for Phoneme-to-Text Transcription. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-8:742-749.\\n\\nH. Feldweg. Implementation and evaluation of a German HMM for POS disambiguation. In Proceedings of the EACL SIGDAT workshop 1995, Dublin.\\n\\nW. N. Francis and H. Kucera. Frequency analysis of English usage. Lexicon and grammar, Houghton Mifflin, Boston.\\n\\nR. Garside, G. Leech and G.  Sampson. The Computational Analysis of English. A Corpus-Based Approach, Longman, London.\\n\\nF. Jelinek. Markov Source Modeling of Text Generation. In J. K. Skwirzinski, editor, Impact of Processing Techniques on Communication, Nijhoff, Dordrecht.\\n\\nN. Jensen, T. Tams, N. Jaeger and V. Pirrelli. Final Report on Front End Integration. EUROTRA Internal Document.\\n\\nF. Karlsson, A. Voutilainen, J. Heikkil and A. Anttila (eds.) Constraint Grammar: a Language-Independent System for Parsing Unrestricted Text. Mouton de Gruyter, Berlin.\\n\\nJ. M. Kupiec. Probabilistic Models of Short and Long Distance Word Dependencies in Running Texts. In Proceedings of the 1989 DARPA Speech and Natural Language Workshop, pages 290-295, Morgan Kaufman, Philadelphia.\\n\\nD. T. Langendoen and E. Fahmy. Feature-structure markup for presentation at Oxford and Brown workshops, Department of Linguistics, University of Arizona, Tucson, AZ 85721 USA, September.\\n\\nG. Leech and A.  Wilson. Draft Sections 4.6 and 4.7 ofthe EAGLES Interim Report:  Annotation Sub-Group, EAGLES, February.\\n\\nM. P. Marcus and B. Santorini. Building very large natural language corpora: the Penn Treebank, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104, January.\\n\\nB. Merialdo. Tagging English Text with a Probabilistic Model. Computational Linguistics, 20(2), 155-171.\\n\\nG. Sampson. Alternative grammatical coding systems. In Garside et al.. The Computational Analysis of English. A Corpus-Based Approach, Longman, London, 165-183.\\n\\nF. Snchez Len. Spanish tagset for the CRATER project, CRATER Internal Document, March. Also available through WWW as http://xxx.lanl.gov/cmp-lg/9406023.\\n\\nG. F. Simons. Feature System Declarations and the Interpretation of Feature Structures, January 1991.\\n\\nP. Tapanainen and A. Voutilainen. Tagging accurately - Don\\'t guess if you know. To appear in Proceedings of the Fourth Conference on Applied Natural Language Processing, Stuttgart.\\n\\nText Encoding Initiative. TEI AI 1W2. List of Common Morphological Features For Inclusion in TEI Starter Set Of Grammatical-Annotation Tags, June.\\n\\nR. Weischedel, M. Meteer, R. Schwartz, L. Ramshaw, and J. Palmucci. Coping with Ambiguity and Unknown Words through Probabilistic Models. Computational Linguistics, 19(2), 359-382.\\n\\nFootnotes\\n\\nThis work has been developed in the context of the RD project CRATER (Corpus Resources And Terminology ExtRaction, MLAP-93/20), funded by the Commission of the European Communities. Other partners involved in the project are University of Lancater (UK), Computers, Communications and Visions, C[2]V (France) and IBM-France. It can be obtained via ftp from parcftp.xerox.com under the directory pub/tagger. The program runs on Common Lisp, and several implementations have been tested under SunOS 4.x and 5.x, besides that for the Macintosh. The term suffix must be understood in this context in a wide sense (set of ending characters in a word) and not strictly linguistic. However, this greatly depends on the size of the lexicon. Since exhaustive lexicons are ``expensive, if not impossible, to produce\\'\\', in authors\\' words, this statement may become false. This parameter has been set to 5 by the authors. Some masculine nouns and adjectives can end in a, though only in a small number of cases. Imperatives can also end in this suffix. However, these can be treated as exceptions and included in the lexicon. The final version currently used is slightly different. It has 466 POS tags. Besides, status of verbs has also been taken into account. Semantic information on nouns has been eliminated, though proper names and names of the days of the week and of the months have specific tags. Nevertheless, this lexicon has to be used carefully.\\n\\nThe sources for lexical information are free from error. In fact, morphosyntactic information has been observed to be wrong in some cases. An overall correction of the lexicon is being carried out. All results reported refer to version 1.2 of the Xerox Tagger. With version 1.1, the tagging produced is always the same irrespective of the training corpus used. Not surprisingly, the HMM file is also the same and the training times are suspiciously short. Thus, version 1.1 seems to learn nothing from the training corpus. The corpus being tagged has been converted to a shallow SGML representation, specially concerning 8-bit characters. Note that the SGML representation of ISO LATIN characters converts the latter suffix into -cioacute;n, resulting, then, impossible its identification with a suffix limit of 5 characters long. The Spanish version of this tagger will be in the public domain in october, 1995.', metadata={'source': '../data/raw/cmplg-xml/9505035.xml'}),\n",
       " Document(page_content=\"Ellipsis and Quantification: A Substitutional Approach Introduction\\n\\nDalrymple, Shieber and Pereira DalShiPer:eahu  (henceforth, DSP) give an equational treatment of ellipsis via higher-order unification which, amongst other things, provides an insightful  analysis of the interactions between ellipsis and quantification. But it suffers a number of drawbacks, especially when viewed from a computational perspective.\\n\\nFirst, the precise order in which quantifiers are scoped and ellipses resolved determines the final interpretation of elliptical sentences. It is hard to see how DSP's analysis could be implemented within a system employing a pipelined architecture that, say, separates quantifier scoping out from other reference resolution operations--this would seem to preclude the generation of some legitimate readings. Yet many systems, for good practical reasons, employ this kind of architecture.\\n\\nThird, though perhaps less importantly, higher-order unification going beyond second-order matching is required for resolving ellipses involving quantification. This increases the computational complexity of the ellipsis resolution task.\\n\\nWhile the treatment of ellipsis is hopefully of some value in its own right, a more general conclusion can be drawn concerning the requirements for a computational theory of semantics. Briefly, the standard view within formal semantics, which DSP inherit, identifies semantic interpretation with composition: interpretation is the process of taking the meanings of various constituents and composing them together to form the meaning of the whole. This makes semantic interpretation a highly order-dependent affair; e.g. the order in which a functor is composed with its arguments can substantially affect the resulting meaning. This is reflected in the order-sensitive interleaving of scope and ellipsis resolution in DSP's account. In addition, composition is only sensitive to the meanings of its components. Typically there is a many-one mapping from compositions onto meanings. So, for example, whether two terms with identical meanings are merely co-referential or are co-indexed is the kind of information that may get lost: the difference amounts to two ways of composing the same meaning.\\n\\nThe alternative proposed here is to view semantic interpretation as a process of building a (possibly partial) description of the intended semantic composition; i.e. (partial) descriptions of what the meanings of various constituents are,  and how they should be composed together. While the order in which composition operations are performed can radically affect the outcome, the order in which descriptions are built up is unimportant. In the case of ellipsis, this extra layer of descriptive indirection permits an equational treatment of ellipsis that (i) is order-independent, (ii) can take account compositional distinctions that do not result in meaning differences, and also (iii) does not require the use of higher-order unification for dealing with quantifiers.\\n\\nEllipsis Substitutions\\n\\nSimple VP Ellipsis\\n\\nEvaluative Substitutions\\n\\nThe meaning of an ellipsis is composed in essentially the same way, and from the same components, as the meaning of its antecedent. However, some changes need to be made in order to accommodate new material introduced by the ellipsis. The substitutions specify what these changes are. In the example discussed above, the meaning of the ellipsis is built up in the same way as for the antecedent, except that whenever you encounter a term corresponding to `John' or something dependent/co-indexed with it, you it is treated as though it were the term for `Mary' or dependent/co-indexed with it.\\n\\nThis means that the substitutions act as directives controlling the way in which QLF expressions within their scope are evaluated. They are not syntactic operations on QLF expressions -- they are part of the QLF object language.\\n\\nThe reason that substitutions are not `applied' immediately upon ellipsis resolution is as follows. At the time of deciding on the ellipsis substitutions, the precise composition of the antecedent may not yet have been determined. (For instance the scopes of quantifiers or the contextual restrictions on pronouns in the antecedent may not have been resolved; this will correspond to the presence of uninstantiated meta-variables in the antecedent QLF.) The ellipsis should follow, modulo the substitutions, the same composition as the antecedent, whatever that composition is eventually determined to be. It makes no sense to apply the substitutions before the antecedent is fully resolved, though it does make sense to decide what the appropriate substitutions should be.\\n\\nIn practical terms what this amounts to is exploiting re-entrancy in QLFs. The elliptical QLF will contain a predicate formed from the antecedent QLF plus substitutions. Any uninstantiated meta-variables in the antecedent are thus re-entrant in the ellipsis. Consequently, any further resolutions to the antecedent are automatically imposed on the ellipsis. This would not be the case if the substitutions were treated as syntactic operations on QLF to be applied immediately: some re-entrant meta-variables would be substituted out of the ellipsis, and those remaining would not be subject to the substitutions (which would have already been applied) when they were eventually instantiated.\\n\\nScope Parallelism\\n\\nBesides illustrating scope parallelism, this is an example where DSP have to resort to higher-order unification beyond second-order matching. But no such increase in complexity is required under the present treatment.\\n\\nStrict and Sloppy Identity\\n\\nIntuitively, the first reading arises from strictly identifying the elliptical book with the antecedent book. The second arises from strictly identifying the pronouns, while sloppily identifying the books. The third from sloppily identifying both the books and the pronouns. In the literature, the first reading would not be viewed as a case of strict identity. But this view emerges naturally from our treatment of substitutions, and is arguably a more natural characterisation of the phenomena.\\n\\nFor parallel terms, we have no choice about the ellipsis substitution. We replace both the term and its index by the corresponding term and index from the ellipsis. But for all non-parallel terms we have a choice between a strict or a sloppy substitution.\\n\\nA sloppy substitution involves substituting a new term index for the old one. This has the effect of reindexing the version of the term occurring in the ellipsis, so that it refers to the same kind of thing as the antecedent term but is not otherwise linked to it.\\n\\nA strict substitution substitutes the term by its index. In this way, the version of the term occurring in the ellipsis is directly linked to antecedent term.\\n\\nThe three readings of book are illustrated below, listing substitutions to be applied to the antecedent and cashing out the results of their application, though omitting scope.\\n\\nDSP's account of the first reading of book is significantly different from their account of the last two readings. The first reading involves scoping the book quantifier before ellipsis resolution. The other two readings only scope the quantifier after resolution, and differ in giving the pronoun a strict or a sloppy interpretation. In our account the choice of strict or sloppy substitutions for secondary terms can constrain permissible quantifier scopings. But the making of these choices does not have to be interleaved in a precise order with the scoping of quantifiers.\\n\\nMoreover, the difference between strict and sloppy readings does not depend on somehow being able to distinguish between primary and secondary occurrences of terms with the same meaning. In DSP's representation of the antecedent of book, both NPs `John' and `he' give rise to two occurrences of the same term (a constant, j). The QLF representation is able to distinguish between the primary and the secondary, pronominal, reference to John.\\n\\nOther Phenomena\\n\\nAntecedent Contained Deletion: A sloppy substitution for every person that Simon did in the sentence John greeted every person that Simon did results in re-introducing the ellipsis in its own resolution. This leads to an uninterpretable cyclic QLF in much the same way that DSP obtain a violation of the occurs check on sound unification.\\n\\nCascaded Ellipsis: The number of readings obtained for John revised his paper before the teacher did, and then Simon did was used as a benchmark by DSP. The approach here gets the four readings identified by them as most plausible. With slight modification, it gets a fifth reading of marginal plausibility. The modification is to allow (strict) substitutions on terms not explicitly appearing in the ellipsis antecedent -- i.e. the  implicit his paper in the second ellipsis when resolving the third ellipsis.\\n\\nWe do not get a sixth, implausible reading, provided that in the first clause his is resolved as being coindexed with the  for John; i.e. that John and his do not both independently refer to the same individual. Kehler blocks this reading in a similar manner. DSP block the reading by a more artificial restriction on the depth of embedding of expressions in logical forms; they lack the means for distinguishing between coindexed and merely co-referential expressions.\\n\\nOther Forms of Ellipsis: Other forms of ellipsis, besides VP-ellipsis can be handled substitutionally. For example,  NP-ellipsis (e.g. Who slept? John.) is straightforwardly accommodated. PP-ellipsis (e.g. Who left on Tuesday? And on Wednesday?) requires substitutions for  constructions in QLF (not described here) representing prepositional phrases.\\n\\nComparisons\\n\\nFor Kamp, strict identity involves copying the discourse referent of the antecedent and identifying it with that of the ellided pronoun. Sloppy identity copies the conditions on the antecedent discourse referent, and applies them to the discourse referent of the ellided pronoun.\\n\\nNeither Kamp nor Kehler extend their copying/substitution mechanism to anything besides pronouns, as we have done. In Kehler's case, it is hard to see how his role assignment functions can be extended to deal with non-referential terms in the desired manner. DRT's use of discourse referents to indicate scope suggests that Kamp's treatment may be more readily extended in this manner; lists of discourse referents at the top of DRS boxes are highly reminiscent of the index lists in scope nodes.\\n\\nSemantic Evaluation\\n\\nThe non-deterministic nature of evaluation and the role of substitutions draws us to conclude that ellipsis substitutions operate on (descriptions of) the semantic compositions, not the results of such compositions.\\n\\nParallelism and Inference\\n\\nConclusions: Interpretation as Description\\n\\nThe substitutional treatment of ellipsis presented here has broadly the same coverage as DSP's higher-order unification treatment, but has the computational advantages of (i) not requiring order-sensitive interleaving of different resolution operations, and (ii) not requiring greater than second-order matching for dealing with quantifiers. In addition, it cures a slight overgeneration problem in DSP's account.\\n\\nIt has been claimed that these advantages arise from viewing semantic interpretation as a process of building descriptions of semantic compositions. To conclude, a few further arguments for this view, that are independent of any particular proposals for dealing with ellipsis.\\n\\nOrder-Independence: One of the reasons for the computational success of unification-based syntactic formalisms is the order-independence of parser/generator operations they permit. If one looks at the order-sensitive nature of the operations of semantic compositions, they provide a poor starting point for a treatment of semantics enjoying similar computational success. But semantic interpretation, viewed as building a description of the intended composition, is a better prospect.\\n\\nPreserving Information: Focusing exclusively on the results of semantic composition, i.e. meanings, can ignore differences in how those meanings were derived that can be linguistically significant (e.g. co-referential vs co-indexed terms). If this information is not to be lost, some way of referring to the structure of the compositions, as well as to their results, seems to be required.\\n\\nAcknowledgements\\n\\nThe initial implementation of the work described here was carried out as part of the CLARE project, DTI IED4/1/1165. The writing of this paper was supported in part by the FraCaS project, LRE 62-051. I would especially like to thank Hiyan Alshawi and Steve Pulman for help and advice on topics relating to this paper. I have also benefited from conversations with Claire Grover, Ian Lewin and Massimo Poesio.\\n\\nBibliography\\n\\nHiyan Alshawi and Richard Crouch. 1992. Monotonic semantic interpretation. In Proceedings 30th Annual Meeting of the Association for Computational Linguistics, pages 32-38.\\n\\nHiyan Alshawi, David Carter, Richard Crouch, Stephen Pulman, Manny Rayner, and Arnold Smith. 1992. Clare: A contextual reasoning and cooperative response framework for the core language engine. Technical Report CRC-028, SRI International, Cambridge Computer Science Research Centre. Available by anonymous ftp from ftp.ai.sri.com, pub/sri-cam/reports/crc028.ps.Z; also cmp-lg.\\n\\nBob Carpenter. 1993. Skeptical and credulous default unification with applications to templates and inheritance. In E. Briscoe, V. de Paiva, and A. Copestake, editors,   Inheritance, Defaults and the Lexicon, pages 13-37. Cambridge University Press.\\n\\nRobin Cooper, Richard Crouch, Jan van Eijck, Chris Fox, Josef van Genabith, Jan Jaspars, Hans Kamp, Manfred Pinkal, Massimo Poesio, and Stephen Pulman. 1994a. Describing the approaches. FraCaS deliverable, D8. Available by anonymous ftp from ftp.cogsci.ed.ac.uk, pub/FRACAS/del8.ps.gz.\\n\\nRobin Cooper et al. 1994b. Evaluating the descriptive capabilities of semantic theories. FraCaS deliverable, D9. Available by anonymous ftp from ftp.cogsci.ed.ac.uk, pub/FRACAS/del9.ps.gz.\\n\\nMary Dalrymple, Stuart M. Shieber, and Fernando C. N. Pereira. 1991. Ellipsis and higher-order unification. Linguistics and Philosophy, 14:399-452.\\n\\nClaire Gardent. 1993. A unification-based approach to multiple vp ellipsis resolution. In Proceedings 6th European ACL, pages 139-148.\\n\\nMark Gawron and Stanley Peters. 1990. Anaphora and Quantification in Situation Semantics. Number 19 in CSLI Lecture Notes. CSLI, Stanford, CA.\\n\\nClaire Grover, Chris Brew, Suresh Manandhar, and Marc Moens. 1994. Priority union and generalization in discourse grammars. In Proceedings 32nd Annual Meeting of the Association for Computational Linguistics, pages 17-24.\\n\\nDaniel Hardt. 1992. An algorithm for vp ellipsis. In Proceedings 30th Annual Meeting of the Association for Computational Linguistics, pages 9-14.\\n\\nAndrew Kehler. 1993a. A discourse copying algorithm for ellipsis and anaphora resolution. In Proceedings 6th European ACL, pages 203-212.\\n\\nAndrew Kehler. 1993b. The effect of establishing coherence in ellipsis and anaphora resolution. In Proceedings 31st Annual Meeting of the Association for Computational Linguistics, pages 62-69.\\n\\nJohn Nerbonne. 1991. Constraint-based semantics. In Proceedings 8th Amsterdam Colloquium, pages 425-443.\\n\\nHub Prst, Remko Scha, and Martin van den Berg. 1994. Discourse grammar and verb phrase anaphora. Linguistics and Philosophy, 17:261-327.\\n\\nHub Prst. 1992. On Discourse Structure, VP Anaphora and Gapping. Ph.D. thesis, University of Amsterdam.\\n\\nStephen Pulman. 1994. A computational theory of context dependency. In Proceedings of the International Workshop on Computational Semantics, pages 161-171, Tilburg.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9502014.xml'}),\n",
       " Document(page_content=\"Compilation of HPSG to TAG1\\n\\nWe present an implemented compilation algorithm that translates HPSG into lexicalized feature-based TAG, relating concepts of the two theories. While HPSG has a more elaborated principle-based theory of possible phrase structures, TAG provides the means to represent lexicalized structures more explicitly. Our objectives are met by giving clear definitions that determine the projection of structures from the lexicon, and identify ``maximal'' projections, auxiliary trees and foot nodes.\\n\\nIntroduction\\n\\nHead Driven Phrase Structure Grammar (HPSG) and Tree Adjoining Grammar (TAG) are two frameworks which so far have been largely pursued in parallel, taking little or no account of each other. In this paper we will describe an algorithm which will compile HPSG grammars, obeying certain constraints, into TAGs. However, we are not only interested in mapping one formalism into another, but also in exploring the relationship between concepts employed in the two frameworks.\\n\\nHPSG is a feature-based grammatical framework which is characterized by a modular specification of linguistic generalizations through extensive use of principles and lexicalization of grammatical information. Traditional grammar rules are generalized to schemata providing an abstract definition of grammatical relations, such as head-of, complement-of, subject-of, adjunct-of, etc. Principles, such as the Head-Feature-, Valence-, Non-Local- or Semantics-Principle, determine the projection of information from the lexicon and recursively define the flow of information in a global structure. Through this modular design, grammatical descriptions are broken down into minimal structural units referring to local trees of depth one, jointly constraining the set of well-formed sentences.\\n\\nIn HPSG, based on the concept of ``head-domains'', local relations (such as complement-of, adjunct-of) are defined as those that are realized within the domain defined by the syntactic head. This domain is usually the maximal projection of the head, but it may be further extended in some cases, such as raising constructions. In contrast, filler-gap relations are considered non-local. This local vs. non-local distinction in HPSG cuts across the relations that are localized in TAG via the domains defined by elementary trees. Each elementary tree typically represents all of the arguments that are dependent on a lexical functor. For example, the complement-of and filler-gap relations are localized in TAG, whereas the adjunct-of relation is not.\\n\\nThus, there is a fundamental distinction between the different notions of localization that have been assumed in the two frameworks. If, at first sight, these frameworks seem to involve a radically different organization of grammatical relations, it is natural to question whether it is possible to compile one into the other in a manner faithful to both, and more importantly, why this compilation is being explored at all. We believe that by combining the two approaches both frameworks will profit.\\n\\nFrom a TAG perspective, using concepts employed in the HPSG framework, we provide an explicit method of determining the content of the elementary trees (e.g., what to project from lexical items and when to stop the projection) from an HPSG source specification. This also provides a method for deriving the distinctions between initial and auxiliary trees, including the identification of foot nodes in auxiliary trees. Our answers, while consistent with basic tenets of traditional TAG analyses, are general enough to allow an alternate linguistic theory, such as HPSG, to be used as a basis for deriving a TAG. In this manner, our work also serves to investigate the utility of the TAG framework itself as a means of expressing different linguistic theories and intuitions.\\n\\nIn the following we will first briefly describe the basic constraints we assume for the HPSG input grammar and the resulting form of TAG. Next we describe the essential algorithm that determines the projection of trees from the lexicon, and give formal definitions of auxiliary tree and  foot node. We then show how the computation of ``sub-maximal'' projections can be triggered and carried out in a two-phase compilation.\\n\\nBackground\\n\\nWe do not assume atomic labelling of nodes, unlike traditional TAG, where the root and foot nodes of an auxiliary tree are assumed to be labelled identically. Such trees are said to factor out recursion. However, this identity itself isn't sufficient to identify foot nodes, as more than one frontier node may be labelled the same as the root. Without such atomic labels in HPSG, we are forced to address this issue, and present a solution that is still consistent with the notion of factoring recursion.\\n\\nThe following rule schemata cover the combination of heads with subjects and other complements respectively as well as the adjunct constructions.\\n\\nHead\\n\\n\\n\\nSubj\\n\\n\\n\\nSchema\\n\\nAlgorithm\\n\\nBasic Idea\\n\\nWe assume that a reduction takes place along with selection. Informally, this means that if F is the selector feature for some schema, then the value (or the element(s) in the list-value) of F that selects the non-SD(s) is not contained in the F value of the mother node. In case F is list-valued, we assume that the rest of the elements in the list (those that did not select any daughter) are also contained in the F at the mother node. Thus we say that F has been reduced by the schema in question.\\n\\nThe compilation algorithm assumes that all HPSG schemata will satisfy the condition of simultaneous selection and reduction, and that each schema reduces at least one SF. For the head-complement- and head-subject-schema, these conditions follow from the Valence Principle, and the SFs are COMPS and SUBJ, respectively. For the head-adjunct-schema, the ADJUNCT-DTR is the SD, because it selects the HEAD-DTR by its MOD feature. The MOD feature is reduced, because it is a head feature, whose value is inherited only from the HEAD-DTR and not from the ADJUNCT-DTR. Finally, for the filler-head-schema, the HEAD-DTR is the SD, as it selects the FILLER-DTR by its SLASH value, which is bound off, not inherited by the mother, and therefore reduced.\\n\\nWe now give a general description of the compilation process. Essentially, we begin with a lexical description and project phrases by using the schemata to reduce the selection information specified by the lexical type.\\n\\nBasic Algorithm Take a lexical type L and initialize by creating a node with this type. Add a node n dominating this node.\\n\\nFor any schema S in which specified SFs of n are reduced, try to instantiate S with n corresponding to the SD of S. Add another node m dominating the root node of the instantiated schema. (The domination links are introduced to allow for the possibility of adjoining.) Repeat this step (each time with n as the root node of the tree) until no further reduction is possible.\\n\\nWe will fill in the details below in the following order: what information to raise across domination links (where adjoining may take place), how to determine auxiliary trees (and foot nodes), and when to terminate the projection.\\n\\nWe note that the trees produced have a trunk leading from the lexical anchor (node for the given lexical type) to the root. The nodes that are siblings of nodes on the trunk, the selected daughters, are not elaborated further and serve either as foot nodes or substitution nodes.\\n\\nRaising Features Across Domination Links\\n\\nQuite obviously, we must raise the SFs across domination links, since they determine the applicability of a schema and licence the instantiation of an SD. If no SF were raised, we would lose all information about the saturation status of a functor, and the algorithm would terminate after the first iteration.\\n\\nThere is a danger in raising more than the SFs. For example, the head-subject-schema in German would typically constrain a verbal head to be finite. Raising HEAD features would block its application to non-finite verbs and we would not produce the trees required for raising-verb adjunction. This is again because heads in HPSG are not equivalent to lexical anchors in TAG, and that other local properties of the top and bottom of a domination link could differ. Therefore HEAD features and other LOCAL features cannot, in general, be raised across domination links, and we assume for now that only the SFs are raised.\\n\\nDetecting Auxiliary Trees and Foot Nodes\\n\\nTraditionally, in TAG, auxiliary trees are said to be minimal recursive structures that have a foot node (at the frontier) labelled identical to the root. As such category labels (S, NP etc.) determine where an auxiliary tree can be adjoined, we can informally think of these labels as providing selection information corresponding to the SFs of HPSG. Factoring of recursion can then be viewed as saying that auxiliary trees define a path (called the spine) from the root to the foot where the nodes at extremities have the same selection information. However, a closer look at TAG shows that this is an oversimplification. If we take into account the adjoining constraints (or the top and bottom feature structures), then it appears that the root and foot share only some selection information.\\n\\nAlthough the encoding of selection information by SFs in HPSG is somewhat different than that traditionally employed in TAG, we also adopt the notion that the extremities of the spine in an auxiliary tree share some part (but not necessarily all) of the selection information. Thus, once we have produced a tree, we examine the root and the nodes in its frontier. A tree is an auxiliary tree if the root and some frontier node (which becomes the foot node) have some non-empty SF value in common. Initial trees are those that have no such frontier nodes.\\n\\nTermination\\n\\nReturning to the basic algorithm, we will now consider the issue of termination, i.e., how much do we need to reduce as we project a tree from a lexical item.\\n\\nNormally, we expect a SF with a specified value to be reduced fully to an empty list by a series of applications of rule schemata. However, note that the SLASH value is unspecified at the root of the trees T2 and T3. Of course, such nodes would still unify with the SD of the filler-head-schema (which reduces SLASH), but applying this schema could lead to an infinite recursion. Applying a reduction to an unspecified SF is also linguistically unmotivated as it would imply that a functor could be applied to an argument that it never explicitly selected.\\n\\nHowever, simply blocking the reduction of a SF whenever its value is unspecified isn't sufficient. For example, the root of T2 specifies the SUBJ to be a non-empty list. Intuitively, it would not be appropriate to reduce it further, because the lexical anchor (adverb) doesn't semantically license the SUBJ argument itself. It merely constrains the modified head to have an unsaturated SUBJ.\\n\\nRaising Verb (and Infinitive Marker to)\\n\\nAdditional Phases\\n\\nAbove, we noted that the preservation of some SFs along a path (realized as a path from the root to the foot of an auxiliary tree) does not imply that all SFs need to be preserved along that path. Tree T1 provides such an example, where a lexical item, an equi-verb, triggers the reduction of an SF by taking a complement that is unsaturated for SUBJ but never shares this value with one of its own SF values.\\n\\nTo allow for adjoining of auxiliary trees whose root and foot differ in their SFs, we could produce a number of different trees representing partial projections from each lexical anchor. Each partial projection could be produced by raising some subset of SFs across each domination link, instead of raising all SFs. However, instead of systematically raising all possible subsets of SFs across domination links, we can avoid producing a vast number of these partial projections by using auxiliary trees to provide guidance in determining when we need to raise only a particular subset of the SFs.\\n\\nConsider T1 whose root and foot differ in their SFs. From this we can infer that a SUBJ SF should not always be raised across domination links in the trees compiled from this grammar. However, it is only useful to produce a tree in which the SUBJ value is not raised when the bottom of a domination link has both a one element list as value for SUBJ and an empty COMPS list. Having an empty SUBJ list at the top of the domination link would then allow for adjunction by trees such as T1.\\n\\nThis leads to the following multi-phase compilation algorithm. In the first phase, all SFs are raised. It is determined which trees are auxiliary trees, and then the relationships between the SFs associated with the root and foot in these auxiliary trees are recorded. The second phase begins with lexical types and considers the application of sequences of rule schemata as before. However, immediately after applying a rule schema, the features at the bottom of a domination link are compared with the foot nodes of auxiliary trees that have differing SFs at foot and root. Whenever the features are compatible with such a foot node, the SFs are raised according to the relationship between the root and foot of the auxiliary tree in question. This process may need to be iterated based on any new auxiliary trees produced in the last phase.\\n\\nExample Derivation\\n\\nIn the following we provide a sample derivation for the sentence\\n\\n(I know) what Kim wants to give to Sandy.\\n\\nMost of the relevant HPSG rule schemata and lexical entries necessary to derive this sentence were already given above. For the noun phrases what, Kim and Sandy, and the preposition to no special assumptions are made. We therefore only add the entry for the ditransitive verb give, which we take to subcategorize for a subject and two object complements.\\n\\nDitransitive Verb\\n\\nFrom this lexical entry, we can derive in the first phase a fully saturated initial tree by applying first the lexical slash-termination rule, and then the head-complement-, head-subject and filler-head-rule. Substitution at the nodes on the frontier would yield the string what Kim gives to Sandy.\\n\\nThe derivations for the trees for the matrix verb want and for the infinitival marker to (equivalent to a raising verb) were given above in the examples T1 and T3. Note that the SUBJ feature is only reduced in the former, but not in the latter structure.\\n\\nIn the second phase we derive from the entry for give another initial tree (T5) into which the auxiliary tree T1 for want can be adjoined at the topmost domination link. We also produce a second tree with similar properties for the infinitive marker to (T6).\\n\\nBy first adjoining the tree T6 at the topmost domination link of T5 we obtain a structure T7 corresponding to the substring what ... to give to Sandy. Adjunction involves the identification of the foot node with the bottom of the domination link and identification of the root with top of the domination link. Since the domination link at the root of the adjoined tree mirrors the properties of the adjunction site in the initial tree, the properties of the domination link are preserved.\\n\\nThe final derivation step then involves the adjunction of the tree for the equi verb into this tree, again at the topmost domination link. This has the effect of inserting the substring Kim wants into what ... to give to Sandy.\\n\\nConclusion\\n\\nWe have described how HPSG specifications can be compiled into TAG, in a manner that is faithful to both frameworks. This algorithm has been implemented in Lisp and used to compile a significant fragment of a German HPSG. Work is in progress on compiling an English grammar developed at CSLI.\\n\\nThis compilation strategy illustrates how linguistic theories other than those previously explored within the TAG formalism can be instantiated in TAG, allowing the association of structures with an enlarged domain of locality with lexical items. We have generalized the notion of factoring recursion in TAG, by defining auxiliary trees in a way that is not only adequate for our purposes, but also provides a uniform treatment of extraction from both clausal and non-clausal complements (e.g., VPs) that is not possible in traditional TAG.\\n\\nIt should be noted that the results of our compilation will not always conform to conventional linguistic assumptions often adopted in TAGs, as exemplified by the auxiliary trees produced for equi verbs. Also, as the algorithm does not currently include any downward expansion from complement nodes on the frontier, the resulting trees will sometimes be more fractioned than if they had been specified directly in a TAG.\\n\\nThere are also several techniques that we expect to lead to improved parsing efficiency of the resulting TAG. For instance, it is possible to declare specific non-SFs which can be raised, thereby reducing the number of useless trees produced during the multi-phase compilation. We have also developed a scheme to effectively organize the trees associated with lexical items.\\n\\nBibliography\\n\\nRobert Kasper. On Compiling Head Driven Phrase Structure Grammar into Lexicalized Tree Adjoining Grammar. In Proceedings of the 2[nd] Workshop on TAGs, Philadelphia, 1992.\\n\\nA. K. Joshi, K. Vijay-Shanker and D. Weir. The convergence of mildly context-sensitive grammatical formalisms. In P. Sells, S. Shieber, and T. Wasow, eds., Foundational Issues in Natural Language Processing. MIT Press, 1991.\\n\\nCarl Pollard and Ivan Sag. Head Driven Phrase Structure Grammar. CSLI, Stanford  University of Chicago Press, 1994.\\n\\nO. Rambow. Formal and Computational Aspects of Natural Language Syntax. Ph.D. thesis. Univ. of Philadelphia. Philadelphia, 1994.\\n\\nO. Rambow, K. Vijay\\n\\n\\n\\nShanker and D. Weir.\\n\\nD\\n\\n\\n\\nTree Grammars.\\n\\nIn: ACL\\n\\n\\n\\n95.\\n\\nY. Schabes, A. Abeille, and A. K. Joshi. Parsing Strategies with `Lexicalized' Grammars: Application to Tree Adjoining Grammars. COLING-88, pp. 578-583.\\n\\nY. Schabes, and A. K. Joshi. Parsing with lexicalized tree adjoining grammar. In M. Tomita, ed., Current Issues in Parsing Technologies. Kluwer Academic Publishers, 1990.\\n\\nK. Vijay-Shanker. Using Descriptions of Trees in a TAG. Computational Linguistics, 18(4):481-517, 1992.\\n\\nK. Vijay-Shanker and A. K. Joshi. Feature Structure Based Tree Adjoining Grammars. In: COLING-88.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9505009.xml'}),\n",
       " Document(page_content=\"Unsupervised Learning of Word\\n\\n\\n\\nCategory Guessing Rules\\n\\nWords unknown to the lexicon present a substantial problem to part-of-speech tagging. In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words. Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus: prefix morphological rules, suffix morphological rules and ending-guessing rules. The learning was performed on the Brown Corpus data and rule-sets, with a highly competitive performance, were produced and compared with the state-of-the-art.\\n\\nIntroduction\\n\\nIn this paper we describe a new fully automatic technique for learning part-of-speech guessing rules. This technique does not require specially prepared training data and employs fully unsupervised statistical learning using the lexicon supplied with the tagger and word-frequencies obtained from a raw corpus. The learning is implemented as a two-staged process with feedback. First, setting certain parameters a set of guessing rules is acquired, then it is evaluated and the results of evaluation are used for re-acquisition of a better tuned rule-set.\\n\\nGuessing Rules Acquisition\\n\\nThe acquisition of word- POS guessing rules is a three-step procedure which includes the rule extraction, rule scoring and   rule merging phases. At the rule extraction phase, three sets of word-guessing rules (morphological prefix guessing rules, morphological suffix guessing rules and ending-guessing rules) are extracted from the lexicon and cleaned from coincidental cases. At the scoring phase, each rule is scored in accordance with its accuracy of guessing and the best scored rules are included into the final rule-sets. At the merging phase, rules which have not scored high enough to be included into the final rule-sets are merged into more general rules, then re-scored and depending on their score added to the final rule-sets.\\n\\nRule Extraction Phase Extraction of Morphological  Rules.\\n\\nMorphological word-guessing rules describe how one word can be guessed given that another word is known. For example, the rule:   [un (VBD VBN) (JJ)]  says that prefixing the string ``un'' to a word, which can act as past form of verb (VBD) and participle (VBN), produces an adjective (JJ). For instance, by applying this rule to the word ``undeveloped'', we first segment the prefix ``un'' and if the remaining part ``developed'' is found in the lexicon as   (VBD VBN), we conclude that the word ``undeveloped'' is an adjective (JJ). The first POS-set in a guessing rule is called the initial class (I-class) and the  POS-set of the guessed word is called the   resulting class (R-class). In the example above   (VBD VBN) is the I-class of the rule and   (JJ) is the R-class.\\n\\nIn English, as in many other languages, morphological word formation is realised by affixation: prefixation and suffixation. Although sometimes the affixation is not just a straightforward concatenation of the affix with the stem, the majority of cases clearly obey simple concatenative regularities. So, we decided first to concentrate only on simple concatenative cases. There are two kinds of morphological rules to be learned: suffix rules (A[s]) -- rules which are applied to the tail of a word, and prefix rules (A[p]) -- rules which are applied to the beginning of a word. For example:\\n\\nA[s] :    [ed (NN VB) (JJ VBD VBN)]\\n\\nExtraction of Ending Guessing Rules.\\n\\nUnlike morphological guessing rules, ending-guessing rules do not require the main form of an unknown word to be listed in the lexicon. These rules guess a  POS-class for a word just on the basis of its ending characters and without looking up its stem in the lexicon. Such rules are able to cover more unknown words than morphological guessing rules but their accuracy will not be as high. For example, an ending-guessing rule\\n\\nA[e]:  [ing\\n\\n\\n\\n(JJ NN VBG)]\\n\\nsays that if a word ends with ``ing'' it can be an adjective, a noun or a gerund. Unlike a morphological rule, this rule does not ask to check whether the substring preceeding the ``ing''-ending is a word with a particular  POS-tag. Thus an ending-guessing rule looks exactly like a morphological rule apart from the I-class which is always void.\\n\\nRule Scoring Phase\\n\\nOf course, not all acquired rules are equally good as plausible guesses about word-classes: some rules are more accurate in their guessings and some rules are more frequent in their application. So, for every acquired rule we need to estimate whether it is an effective rule which is worth retaining in the final rule-set. For such estimation we perform a statistical experiment as follows: for every rule we calculate the number of times this rule was applied to a word token from a raw corpus and the number of times it gave the right answer. Note that the task of the rule is not to disambiguate a word's  POS but to provide all and only possible  POSs it can take on. If the rule is correct in the majority of times it was applied it is obviously a good rule. If the rule is wrong most of the times it is a bad rule which should not be included into the final rule-set.\\n\\nAnother important consideration for scoring a word-guessing rule is that the longer the affix or ending of the rule the more confident we are that it is not a coincidental one, even on small samples. For example, if the estimate for the word-ending ``o'' was obtained over a sample of 5 words and the estimate for the word-ending ``fulness'' was also obtained over a sample of 5 words, the later case is more representative even though the sample size is the same. Thus we need to adjust the estimation error in accordance with the length of the affix or ending. A good way to do that is to divide it by a value which increases along with the increase of the length. After several experiments we obtained:\\n\\nRule Merging Phase\\n\\nDirect Evaluation Stage\\n\\nprecision - the percentage of  POSs the  guesser assigned correctly  over the total number of  POSs it assigned to the word;\\n\\ncoverage - the proportion of words which the guesser was able to classify, but not necessarily  correctly;\\n\\nIn our experiments we measured word precision and word recall (micro-average). There were two types of data in use at this stage. First, we evaluated the guessing rules against the actual lexicon: every word from the lexicon, except for closed-class words and words shorter than five characters, was guessed by the different guessing strategies and the results were compared with the information the word had in the lexicon. In the other evaluation experiment we measured the performance of the guessing rules against the training corpus. For every word we computed its metrics exactly as in the previous experiment. Then we multiplied these results by the corpus frequency of this particular word and averaged them. Thus the most frequent words had the greatest influence on the aggreagte measures.\\n\\nAfter obtaining the optimal rule-sets we performed the same experiment on a word-sample which was not included into the training lexicon and corpus. We gathered about three thousand words from the lexicon developed for the Wall Street Journal corpus and collected frequencies of these words in this corpus. At this experiment we obtained similar metrics apart from the coverage which dropped about 0.5% for Ending 75 and Xerox rule-sets and 7% for the Suffix 60 rule-set. This, actually, did not come as a surprise, since many main forms required by the suffix rules were missing in the lexicon.\\n\\nIn the next experiment we evaluated whether the morphological rules add any improvement if they are used in conjunction with the ending-guessing rules. We also evaluated in detail whether a conjunctive application with the Xerox guesser would boost the performance. As in the previous experiment we measured the precision, recall and coverage both on the lexicon and on the corpus. Table 2 demonstrates some results of this experiment. The first part of the table shows that when the Xerox guesser is applied before the E75guesser we measure a drop in the performance. When the Xerox guesser is applied after the E75 guesser no sufficient changes to the performance are noticed. This actually proves that the E75 rule-set fully supercedes the Xerox rule-set. The second part of the table shows that the cascading application of the morphological rule-sets together with the ending-guessing rules increases the overall precision of the guessing by a further 5%. This makes the improvements against the base-line Xerox guesser 28% in precision and 7% in coverage.\\n\\nTagging Unknown Words\\n\\nIn the evaluation of tagging accuracy on unknown words we pay attention to two metrics. First we measure the accuracy of tagging solely on unknown words:\\n\\nThis metric gives us the exact measure of how the tagger has done on unknown words. In this case, however, we do not account for the known words which were mis-tagged because of the guessers. To put a perspective on that aspect we measure the overall tagging performance:\\n\\nSince the Brown Corpus model is a general language model, it, in principle, does not put restrictions on the type of text it can be used for, although its performance might be slightly lower than that of a model specialised for this particular sublanguage. Here we want to stress that our primary task was not to evaluate the taggers themselves but rather their performance with the word-guessing modules. So we did not worry too much about tuning the taggers for the texts and used the Brown Corpus model instead. We tagged several texts of different origins, except from the Brown Corpus. These texts were not seen at the training phase which means that neither the taggers nor the guessers had been trained on these texts and they naturally had words unknown to the lexicon. For each text we performed two tagging experiments. In the first experiment we tagged the text with the Brown Corpus lexicon supplied with the taggers and hence had only those unknown words which naturally occur in this text. In the second experiment we tagged the same text with the lexicon which contained only closed-class and short words. This small lexicon contained only 5,456 entries out of 53,015 entries of the original Brown Corpus lexicon. All other words were considered as unknown and had to be guessed by the guessers.\\n\\nWe obtained quite stable results in these experiments. Here is a typical example of tagging a text of 5970 words. This text was detected to have 347 unknown words. First, we tagged the text by the four different combinations of the taggers with the word-guessers using the full-fledged lexicon. The results of this tagging are summarised in Table 3. When using the Xerox tagger with its original guesser, 63 unknown words were incorrectly tagged and the accuracy on the unknown words was measured at 81.8%. When the Xerox tagger was equipped with our cascading guesser its accuracy on unknown words increased by almost 9% upto 90.5%. The same situation was detected with Brill's tagger which in general was slightly more accurate than the Xerox one. The cascading guesser performed better than Brill's original guesser by about 8% boosting the performance on the unknown words from 84.5% to 92.2%. The accuracy of the taggers on the set of 347 unknown words when they were made known to the lexicon was detected at 98.5% for both taggers.\\n\\nIn the second experiment we tagged the same text in the same way but with the small lexicon. Out of 5,970 words of the text, 2,215 were unknown to the small lexicon. The results of this tagging are summarised in Table 4. The accuracy of the taggers on the 2,215 unknown words when they were made known to the lexicon was much lower than in the previous experiment -- 90.3% for the Xerox tagger and 91.5% for Brill's tagger. Naturally, the performance of the guessers was also lower than in the previous experiment plus the fact that many ``semi-closed'' class adverbs like ``however'', ``instead'', etc., were missing in the small lexicon. The accuracy of the tagging on unknown words dropped by about 5% in general. The best results on unknown words were again obtained on the cascading guesser (86%-87.45%) and Brill's tagger again did better then the Xerox one by 1.5%.\\n\\nTwo types of mis-taggings caused by the guessers occured. The first type is when guessers provided broader  POS-classes for unknown words and the tagger had difficulties with the disambiguation of such broader classes. This is especially the case with the ``ing'' words which, in general, can act as nouns, adjectives and gerunds and only direct lexicalization can restrict the search space, as in the case with the word ``going'' which cannot be an adjective but only a noun and a gerund. The second type of mis-tagging was caused by wrong assignments of  POSs by the guesser. Usually this is the case with irregular words like, for example, ``cattle'' which was wrongly guessed as a singular noun (NN) but in fact is a plural noun (NNS).\\n\\nDiscussion and Conclusion\\n\\nWe presented a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for words unknown to the lexicon. This technique does not require specially prepared training data and uses for training the lexicon and word frequencies collected from a raw corpus. Using these training data three types of guessing rules are learned: prefix morphological rules, suffix morphological rules and ending-guessing rules. To select best performing guessing rule-sets we suggested an evaluation methodology, which is solely dedicated to the performance of part-of-speech guessers.\\n\\nEvaluation of tagging accuracy on unknown words using texts unseen by the guessers and the taggers at the training phase showed that tagging with the automatically induced cascading guesser was consistently more accurate than previously quoted results known to the author (85%). The cascading guesser outperformed the guesser supplied with the Xerox tagger by about 8-9% and the guesser supplied with Brill's tagger by about 6-7%. Tagging accuracy on unknown words using the cascading guesser was detected at 90-92% when tagging with the full-fledged lexicon and 86-88% when tagging with the closed-class and short word lexicon. When the unknown words were made known to the lexicon the accuracy of tagging was detected at 96-98% and 90-92% respectively. This makes the accuracy drop caused by the cascading guesser to be less than 6% in general. Another important conclusion from the evaluation experiments is that the morphological guessing rules do improve the guessing performance. Since they are more accurate than ending-guessing rules they are applied before ending-guessing rules and improve the precision of the guessings by about 5%. This, actually, results in about 2% higher accuracy of tagging on unknown words.\\n\\nthe learning of such rules is done from the lexicon rather than tagged corpus, because the guesser's task is akin to the lexicon lookup;\\n\\nthere is a well-tuned statistical scoring procedure which accounts for rule features and frequency distribution;\\n\\nthere is an empirical way to determine an optimum collection of rules, since acquired rules are subject to rigorous direct evaluation in terms of precision, recall and coverage;\\n\\nrules are applied cascadingly using the most accurate rules first.\\n\\nOne of the most important issues in the induction of guessing rule-sets is the choice right data for training. In our approach, guessing rules are extracted from the lexicon and the actual corpus frequencies of word-usage then allow for discrimination between rules which are no longer productive (but have left their imprint on the basic lexicon) and rules that are productive in real-life texts. Thus the major factor in the learning process is the lexicon. Since guessing rules are meant to capture general language regularities the lexicon should be as general as possible (list all possible  POSs for a word) and as large as possible. The corresponding corpus should include most of the words from the lexicon and be large enough to obtain reliable estimates of word-frequency distribution. Our experiments with the lexicon and word frequencies derived from the Brown Corpus, which can be considered as a general model of English, resulted in guessing rule-sets which proved to be domain and corpus independent, producing similar results on test texts of different origin.\\n\\nAlthough in general the performance of the cascading guesser is only 6% worse than the lookup of a general language lexicon there is room for improvement. First, in the extraction of the morphological rules we did not attempt to model non-concatenative cases. In English, however, since most of letter mutations occur in the last letter of the main word it is possible to account for it. So our next goal is to extract morphological rules with one letter mutations at the end. This would account for cases like ``try - tries'', ``reduce - reducing'', ``advise - advisable''. We expect it to increase the coverage of thesuffix morphological rules and hence contribute to the overall guessing accuracy. Another avenue for improvement is to provide the guessing rules with the probabilities of emission of  POSs from their resulting  POS-classes. This information can be compiled automatically and also might improve the accuracy of tagging unknown words.\\n\\nThe described rule acquisition and evaluation methods are implemented as a modular set of  C++ and  AWK tools, and the guesser is easily extendable to sub-language specific regularities and retrainable to new tag-sets and other languages, provided that these languages have affixational morphology. Both the software and the produced guessing rule-sets are available by contacting the author.\\n\\nAcknowledgements\\n\\nSome of the research reported here was funded as part of  EPSRC project IED4/1/5808 ``Integrated Language Database''. I would also like to thank Chris Brew for helpful discussions on the issues related to this paper.\\n\\nBibliography\\n\\nE. Brill 1994. Some Advances in Transformation-Based Part of Speech Tagging. In Proceedings of the Twelfth  National Conference on Artificialntelligence (AAAAI-94), Seattle, WA.\\n\\nE. Brill 1995. Transformation-based error-driven learning and Natural Language processing: a case study in part-of-speech tagging. In Computational Linguistics 21(4) pp. 543-565.\\n\\nW. Francis and H. Kucera 1982. Frequency Analysis of English Usage. Houghton Mifflin,  Boston 1982.\\n\\nJ. Kupiec 1992. Robust Part-of-Speech Tagging Using a Hidden Markov Model. In Computer Speech and Language\\n\\nM. Marcus, M.A. Marcinkiewicz,  and B. Santorini 1993. Building a Large Annotated Corpus of English: The Penn Treebank. In Computational Linguistics, vol 19/2 pp.313-329\\n\\nH. Schmid 1994. Part of Speech Tagging with Neural Networks. In Proceedings of the International Conference on Computational Linguistics, pp.172-176, Kyoto, Japan.\\n\\nE. Tzoukermann, D.R. Radev, and W.A. Gale 1995. Combining Linguistic Knowledge and Statistical Learning in French Part of Speech Tagging. In EACL SIGDAT Workshop, pp.51-59, Dublin, Ireland\\n\\nA. Voutilainen 1995. A Syntax-Based Part-of-Speech Analyser In Proceedings of the Seventh Conference of European Chapter of the Association for Computational Linguistics (EACL) pp.157-164,  Dublin, Ireland\\n\\nR. Weischedel, M. Meteer, R. Schwartz, L. Ramshaw and J. Palmucci 1993. Coping with ambiguity and unknown words through probabilistic models. In Computational Linguistics, vol 19/2 pp.359-382\\n\\nByoung-Tak Zhang and Yung-Taek Kim 1990. Morphological Analysis and Synthesis by Automated Discovery and Acquisition of Linguistic Rules. In Proceedings of the 13th International Conference on Computational Linguistics, pp.431-435, Helsinki, Finland.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9604022.xml'}),\n",
       " Document(page_content='The Compactness of Construction Grammars\\n\\nWe present an argument for construction grammars based on the minimum description length (MDL) principle (a formal version of the Ockham Razor). The argument consists in using linguistic and computational evidence in setting up a formal model, and then applying the MDL principle to prove its superiority with respect to alternative models. We show that construction-based representations are at least an order of magnitude more compact that the corresponding lexicalized representations of the same linguistic data. The result is significant for our understanding of the relationship between syntax and semantics, and consequently for choosing NLP architectures. For instance, whether the processing should proceed in a pipeline from syntax to semantics to pragmatics, and whether all linguistic information should be combined in a set of constraints. From a broader perspective, this paper does not only argue for a certain model of processing, but also provides a methodology for determining advantages of different approaches to NLP.\\n\\nIntroduction: Motivation and Terminology\\n\\nAfter discussing the terminology, we begin our exposition with a sketch of the argument (Section 1.2). The argument will have two parts, of which the first is empirical and based on the work of other researchers on language structure and models of processing, and where the second part applies the MDL principle. Both will be presented in Section 3. For this second part we need to develop some techniques and intuitions. Thus, in Section 2, we present the details of the application of the MDL principle for a grammar of numbers. Since the domain of numbers is completely unambiguous, this exposition move will make transparent the subsequent application of MDL principle to a grammar of English in Section 3.2. We conclude the paper with a few open problems.\\n\\nTerminology\\n\\nBy a grammar we mean a collection of entities, like data structures or logical formulas, that describe a formal language. Thus grammars are always formal. If the formal language in question resembles English, we call the grammar a grammar of English (ditto for other natural languages).\\n\\nWhile it is possible to classify grammars along many dimensions, in this paper we are interested in two: (1) Whether a grammar is lexicalized, and (2) whether a grammar separates information about form and meaning (syntax and semantics) or mixes the two. Obviously, in each case these are mutually exclusive possibilities; i.e. either all information about language is contained in the lexicon or not, and the same for (2). If a grammar is not lexicalized, then it must refer to units larger than words, and we will call such an item of information a structural rule; the most typical example of which would be a phrase structure rule.\\n\\nA construction grammar is a non-lexicalized grammar in which information about form and meaning is kept together in constructions. A construction is a set of constraints about form and meaning of a word, sequence of words, or a sequence of constructions. (A recursive definition).\\n\\nThis definition captures the idea that forms and meanings should be investigated together. However we want to say more than that. Since meaning very often depends on context, it is only natural to make this connection explicit. Therefore in the formalism we use constructions are triples:\\n\\nThe Form describes the construction as a combination of subconstructions (e.g. a noun phrase as a combination of a numeral and a noun); the Meaning  part specifies how the meanings of the subparts contribute to the meaning of the construction (e.g. specifying the number of of elements). The Context   specifies the parameters that are necessary to construct the meaning, and which are not present in the meanings of the parts; for example, the content of the question is necessary to construct the meaning of an answer, especially if the answer is just a sentence fragment.\\n\\nthe length, in bits, of the description of the theory, and\\n\\nthe length, in bits, of data when encoded with the help of the theory.\\n\\nIn our case, the data is the language we want to describe, and the the encoding theory is its grammar (which includes the lexicon). The MDL principle justifies the intuition that a more compact grammatical description is better. At issue is what is the best encoding. To address it, we will be simply comparing two complementary classes of encodings and showing that one of them is usually more compact. The formal side of the argument will be kept to the minimum: after building the two complementary models of language, the mathematics will be simple -- counting. (But we will discuss some ways of producing more refined models).\\n\\nThe line of the argument\\n\\nThe paper is about how to best represent information about language, i.e. about data structures. As we know, data structures are determined by both the types of data and their structure. Therefore we have to argue for particular types and particular structures. In our particular case, to argue for grammars of constructions, we will show that the data types should contain information about both form and meaning. Secondly, we will show that their structure should contain something resembling phrase structure rules; we do it by presenting an MDL-based argument against lexicalized representation of forms and meanings.\\n\\nThe optimality argument goes as follows: we have to prove that for NLP it is preferable not to separate syntactic, semantic and pragmatic information -- which is an argument for having data structures that combine them. But we could imagine say lexicalized grammars in which such information is combined. Hence,  as the second step, we have to show that grammars with \"phrase structure rules\" are better than lexicalized grammars. These two arguments show the superiority of a construction-based approach with respect to alternative grammatical formalism.\\n\\nUsing MDL with grammars of numbers\\n\\nTo make the presentation clear we first discuss a grammar of numbers -- for numbers our whole argument is completely formal and transparent. In the next section we use the same argument for NL grammars.\\n\\nA lexicalized grammar of numbers\\n\\nAny grammar of numbers must somehow express the fact that the value of a digit D depends on its position. I.e.\\n\\nwhere pos(D) is the position of D (counting from the right, beginning with 0). Notice that a lexicalized grammar expresses it directly, and must repeat it for every digit. (We use the same symbols for the digits and their values).\\n\\nBased on that, the value of a number is  the sum of values of its digits:\\n\\nNote that the grammar specifies the formula for value of the type, and the value of the token is given by its instantiation. E.g. in computing\\n\\n, notice the different values of the digit 1. Also, note that given a different set of functions, e.g. head, tail, log, *, +, the\\n\\nfunction would be slightly different, but the lexicon must look essentially the same, because there is no other place to put the data about how the forms determine the meanings.\\n\\nA construction grammar of numbers\\n\\nThe lexical part of a construction grammar can now be much simpler. (And it could be simplified even further by assuming that the value of a token is the token, unless specified otherwise).\\n\\nIn contrast to the lexicalized grammar, in a grammar of constructions we can write a structural rule\\n\\nThis rule defines a production saying that a new structure is obtained by adding a digit D to a previously defined structure DS1. The equality associates the meaning of a new structure\\n\\nwith the meaning of its components\\n\\nand\\n\\n. As a consequence of choosing this kind of representation, the rule about how to compute the meaning of digits has to be stated only once.\\n\\nAs we can see, describing the same language with a grammar of constructions results in a more compact grammar. We saved 11 symbols per non-0 lexical entry, i.e. 99 symbols altogether. Although, we added a structural rule, its size is comparable with the above\\n\\nrule for computing the value of a sequence of digits in the lexicalized grammar. Also note that the latter must additionally refer to the function pos and exponentiation.\\n\\nWhile savings 99 symbols is not much, for larger lexicons the saving would be much bigger. Larger lexicons are obtained by increasing the Base. In this case the grammar production (phrase structure rule) reads:\\n\\nIt can be easily checked that the resulting construction grammar is always an order of magnitude more compact than its lexicalized counterpart.\\n\\nA grammar of constructions is even more compact if additional conditions are placed on sequences of digits, e.g. that only ascending sequences of digits are acceptable. That is so, because typically any such a condition would have to be included in all lexical entries, and the more complicated the condition, the less compact is the lexicalized grammar.\\n\\nThe superiority of constructions for NLU grammars\\n\\nIn this section we present the argument that (a) it makes sense to encode syntactic, semantic and pragmatic information together, and (b) that construction-based grammar are more compact that lexicalized grammar that encode the same semantic information. At the end of the section we discuss some possible extensions of the model to cover the case of \"lexicalized grammars with a few constructions\".\\n\\nData types: FORMS, MEANINGS,  and CONTEXTS\\n\\nThere are three arguments supporting the encoding of linguistic information in data structures that combine syntactic, semantic and pragmatic information. Namely, the linguistic theory, the practice of computational linguists who encode it that way, and experimental evidence from analyzing parsing mechanisms.\\n\\nThe structure of data:  Why the argument for numbers works also for NL\\n\\nTo show that the argument of Section 2 works for a grammar of English we should show that the use of structural rules can result in a more compact grammar.\\n\\nWe need a simple model, so let us consider the problem  of determining whether a clause followed by a set of PPs is nonsensical. For example, we meet at 12 with bob at 6 avenue and 44 street vs. the dow closed at 2200 with bob at 6 avenue and 44 street. In general, this problem cannot be solved without access to a large body of background knowledge; so, let us simplify it further. Assume that sentences that repeat the same type of information are nonsensical, e.g. we meet at 12 pm with bob from 5 to 6 pm To set up the model we have to define a formal language resembling English. We do it in two steps. Let our first formal language L1consist of all sequences of SV (subject-verb), SVO (subject-verb-object), SVOO (subject-verb-object-indirect object), of English taken from some very large corpus. The language LPPwe are interested in consists of sentences of L1 followed by any number of PPs (prepositional phrases) that contain the nouns from L1. (Hence LPP is infinite). Using this model we can discuss differences between a construction grammar and a lexicalized grammar.\\n\\nConstruction grammar\\n\\nTo define a construction grammar for LPP we define the lexicon and a set of productions (phrase structure rules). Let the lexical entries be given by the matrix:\\n\\ni.e. the meaning of a word is given by its linguistic category, a word sense, and the semantic type of the word sense. The meaning is given as a set, because words often belong to different categories and may have multiple word senses, e.g.\\n\\nRemark. We ignore the context to make the argument more general. Also, we could write\\n\\n, and associate one word with multiple lexicon entries (slightly abusing the notation).\\n\\nHaving defined the lexicon, we have to cover the SV, SVO, and SVOO constructions; for instance we could write the SVO-action  construction as\\n\\nHowever for our purposes it is irrelevant how the meanings of those constructions are encoded; the only thing we will need is the existence of the\\n\\nfunction.\\n\\nThe next step is to cover the PPs. We represent them as\\n\\nI.e. we assume that prepositional phrases have types, and each type is a function of the preposition and the noun type. E.g.\\n\\nThus \"at\" followed by the second sense of 2200 (as hour, not number), would be classified as an \"event_time\". (We are conveniently assuming that numerals are nouns).\\n\\nFinally, we define the construction of adding an adjunct to a clause (with + standing for the simple append or a more complex procedure):\\n\\nThus defined construction grammar encodes our formal language LPP.\\n\\nLexicalized grammar\\n\\nFor a lexicalized grammar, as in Section 2, we observe that the meaning of a clause such as the one above must be encoded with each noun. Thus, for each noun we have to encode the meaning of its adjuncts, i.e. repeat the formula: \"if I am combined with preposition X1, then our type will be T=T1; \"if I am combined with preposition X2, then our type will be T=T2; ... and if T does not appear in the meaning E of everything to the left of the preposition immediately to the left of me, then I make sense; and our joint meaning of is the sum of T and E.\" For instance, for each number that can denote an hour we would have:\\n\\n(In addition the formulas might encode constraints on event_time, beginning_time, and other types). As before, the reason for repeating this piece of information is that these formulas must somehow be encoded. Since the reference to anything larger than a word is forbidden, the formula must be repeated for every word separately.\\n\\nWhy adding a few structure rules won\\'t help\\n\\nWith any mathematical model of language one should ask the question how closely it approximates the linguistic reality, and what happens if we introduce some minor changes to it. For example, what if a grammar is 99.99% lexicalized and contains only a few phrase structure rules (e.g. the rule about the meanings of clauses with adjuncts). The answer is that the same argument would apply, provided we can find another productive grammatical phenomenon that behaves similarly to digits in our grammar of numbers or to PPs in the case of LPP. Is there a reason to believe that such productive phenomena are common? Yes.\\n\\nOpen problems and Conclusions\\n\\nSince this paper is about counting, the most natural open problem is the number of constructions beyond the word, and then beyond the clause. Of course, it is not clear how to count them. Still, we know approximately the size of lexicons for spellchecking; similarly, we could ask about the size of a construction grammar for NLU (e.g. at the level of achieving a decent score on a SAT exam) or for summarizing of New York Times stories.\\n\\nFinally, there are many questions that are not directly related to the topic of this paper, e.g. the role of ontologies in describing constructions, the interaction between structural and functional descriptions etc.\\n\\nConclusions: We used a combination of empirical data and the minimum description length principle to show that construction grammars are better representation of linguistic data than their lexicalized counterparts. This hybrid --  linguistic, computational, and mathematical -- argument for a construction-based approach to language understanding is the main contribution of this paper. However, it is likely similar arguments could be used in other circumstances where one has to choose between competing representations.\\n\\nBibliography\\n\\nJ. Allen. Natural Language Understanding. Benjamin/Cummings, Menlo Park, CA, 1987.\\n\\nJ. Dowding, R.Moore, F.Andry, and D.Moran. Interleaving syntax and semantics in an efficien bottom-up parser. In Proc. of 32nd Annual Meeting of the Association for Computational Linguisitcs, ACL. ACL, 1994.\\n\\nJ.E.Fenstad, P-K. Halvorsen, T. Langholm, and J. van Benthem. Situations, Language and Logic. D. Reidel, Dordrecht, Holland, 1987.\\n\\nC.J. Fillmore, P. Kay, and M.C. O\\'Connor. Regularity and idiomaticity in grammatical constructions. Language, 64(3):501-538, 1988.\\n\\nA.E. Goldberg. Constructions: a construction grammar approach to argument structure. The University of Chicago Press, Chicago, IL, 1994.\\n\\nE. A. Hinkelman and J.F. Allen. Two constraints on speech acts ambiguity. In Proc. of 27th Annual Meeting of the Association for Computational Linguisitcs, ACL, pages 212-219, Vancouver, BC, 1989. ACL.\\n\\nG. Hirst. Semantic interpretation and the resolution of ambiguity. Cambridge University Press, Cambridge, Great Britain, 1987.\\n\\nJ. R. Hobbs, M.E. Stickel, D.E. Appelt, and P. Martin. Interpretation as abduction. Artificial Intelligence, 63(1-2):69-142, 1993.\\n\\nK. Jensen. Parsing strategies in a broad-coverage grammar of english. Technical Report RC 12147, IBM T.J. Watson Research Center, Yorktown Heights, New York, 1986.\\n\\nK. Jensen, G.Heidorn, and S.D.Richardson, editors. Natural Language Processing: The PLNLP Approach. Kluwer, Boston, 1993.\\n\\nK. Jensen and J.-L. Binot. Disambiguating prepositional phrase attachments by using on-line dictionary definitions. Computational Linguistics, 13(3-4):251-260, 1988.\\n\\nD. Jurafsky. An On-line Computational Model of Sentence Interpretation. PhD thesis, University of California, Berkeley, 1992. Report No. UCB/CSD 92/676.\\n\\nE.L. Keenan. Relative clauses. In T. Shopen, editor, Language Typology and syntactic description Vol.II., pages 140-170. Cambridge University Press, New York, NY, 1985.\\n\\nS.L. Lytinen.\\n\\nSemantics\\n\\n\\n\\nfirst natural language processing.\\n\\nIn Proceedings AAAI\\n\\n\\n\\n91, pages 111\\n\\n\\n\\n116, Anaheim, CA, 1991.\\n\\nJ.D. McCawley. The Syntactic Phenomena of English. University of Chicago Press, Chicago, IL, 1988.\\n\\nM.Li. and P.Vitanyi, editors. An Introduction to Kolmogorv Complexity and Its Applications. Springer, New York, 1993.\\n\\nM. Noonan. Complementation. In T. Shopen, editor, Language Typology and syntactic description Vol.II., pages 42-140. Cambridge University Press, New York, NY, 1985.\\n\\nY. Ravin. A Decompositional Approach to Predicates Denoting Events. PhD thesis, CUNY, New York, 1987.\\n\\nJ. Rissanen. A universal prior for integers and estimation by minimum description length. Annals of Statistics, 11:416-431, 1982.\\n\\nR.Quirk, S.Greenbaum, G.Leech, and J.Svartvik. A Comprehensive Grammar of the English Language. Longman, New York, NY, 1985.\\n\\nP. Tapainen and A. Voutilainen. Tagging accurately - don\\'t guess if you know. In Proc. Fourth Conference on Applied Natural Language Processing, pages 47-52, Stuttgart, Germany, 1994.\\n\\nN. Ward. A parallel approach to syntax for generation. Artificial Intelligence, 57(2-3):183-225, 1992.\\n\\nW. Zadrozny. From utterances to situations: Parsing with constructions in small domains. In Language, Logic and Computation: The 1994 Moraga Proceedings. CSLI, Stanford, 1994.\\n\\nW. Zadrozny. Parsing with constructions and ontology. In Proc. AAAI Fall Symposium on Knowledge Representation for Natural Language Processing in Implemented Systems. AAAI, New Orleans, Louisiana, 1994.\\n\\nW. Zadrozny and K. Jensen. Semantics of paragraphs. Computational Linguistics, 17(2):171-210, 1991.\\n\\nW. Zadrozny and A. Manaster-Ramer. The significance of constructions. Technical Report RC 20002 (88492), IBM Research, T.J. Watson Research Center, 1995. (a 1993 manuscript).\\n\\nW. Zadrozny, M. Szummer, S. Jarecki, D.E. Johnson, and L. Morgenstern. NL understanding with a grammar of constructions. Proc. Coling\\'94, 1994.\\n\\nFootnotes\\n\\nIBM Research Report RC 20003 (88493)', metadata={'source': '../data/raw/cmplg-xml/9505031.xml'}),\n",
       " Document(page_content=\"Implementation and evaluation of a German HMM for POS disambiguation\\n\\nA German language model for the Xerox HMM tagger is presented. This model's performance is compared with two other German taggers with partial parameter re-estimation and full adaption of parameters from pre-tagged corpora. The ambiguity types resolved by this model are analysed and compared to ambiguity types of English and French. Finally, the model's error types are described. I argue that although the overall performance of these models for German is comparable to results for English and French, a more exact analysis demonstrates important differences in the types of disambiguation involved for German.\\n\\nBackground\\n\\nWithin a current project on adapting bilingual dictionaries for online comprehension assistance (COMPASS, LRE 62-080), the need arose for a POS-disambiguator to facilitate a context sensitive dictionary look-up system. As the COMPASS project makes ample use of Xerox technology for its core look-up engine and for POS disambiguation for languages other than German, the obvious thing to do was to develop a German language model for the Xerox tagger.\\n\\nImplementation of the German language model\\n\\n1. a definition of the tag set to be used by the HMM, 2. a lexicon listing word forms with their equivalence classes, that is the list of POS tags that can be assigned to the word form, 3. a class guesser that assigns equivalence classes to words not covered by the lexicon, 4. a set of initial transition biases, 5. a set of initial symbol biases, 6. a sufficiently large text for training the HMM, 7. a reference text with correctly assigned POS tags, 8. a tokenizer that recognizes words in free text.\\n\\nThe following paragraphs describe these components in more detail.\\n\\n(3)        Class guessers for the Xerox tagger assign potential POS tags to unknown words according to a surface analysis of the word form. In addition to the common practice of mapping POS tags according to the words' suffixes, this implementation makes use of the case of the initial letter of a word -- which is highly significant for POS assignment in German. The class guesser also takes care of POS-assignment for abbreviations, special symbol sequences and language external material in the text. The class guesser, like the lexicon, is a  finite state transducer.\\n\\n(4) The model is trained using a set of initial transition biases, including both positive and negative constraints on tag sequences. Although the model can be trained without initial biases, the performance of the resulting model increases significantly if appropriate initial biases are used.\\n\\nThe biases in the model consisted for the most part in specifications of the most plausible successor tags for each tag in the tag set. They were constructed manually and refined in a series of subsequent training and evaluation runs.\\n\\n(5) Initial symbol biases are an additional set of biases used to define preferences for tag assignment given a particular equivalence class. Only a very few symbol biases were defined before evaluation of the first training runs, mainly to reflect biases towards equivalence classes used in the class guesser. The majority of symbol biases were added to correct misguided biases chosen during the training processes.\\n\\n(7) The reference texts were also taken from the Frankfurter Rundschau, but do not overlap with the training texts. The reference texts amount to a total of approximately 20,000 running words, which were manually tagged and checked.\\n\\n(8) The current version of the implementation uses a straightforward tokenizer accepting one line per token. Training texts are pretokenized using an external tokenizer written in lex.\\n\\nPerformance Comparison with other German models\\n\\nThe main advantage of the Xerox tagger when compared with earlier implementations of HMM taggers is that it can be trained using untagged text. However, the performance of the resulting HMM is very poor if no initial biases are used to help the training process find suitable parameters.\\n\\nChoosing initial biases to help train a model is a subtle task in that it not only requires sound knowledge of the tag set used and the target language the model is aiming at, but it also requires a ``feel'' for how the initial biases may be modified during a given number of training iterations. It is also sometimes frustrating that the linguistic knowledge used to create the initial biases gets ``optimized'' or ``trained'' away in subsequent iterations of training.\\n\\nTo overcome these disadvantages, hybrid technologies have been developed that combine free text training methods with parameter estimation from pre-tagged texts. In such a setting, initial transition and symbol biases are replaced by frequencies of tag sequences and tag instantiation from a relatively small pre-tagged corpus. The counted frequencies are taken as an approximation to the model's probabilities and get smoothed in a small number of training iterations.\\n\\nAssessment of ambiguity types\\n\\nIn the preceding sections the evaluation of the model was purely quantitative. Performance was measured as the percent of mismatches between the output generated by the HMM and the tags assigned by manual tagging. Although this error rate is an appropriate measure for the performance of an HMM given a particular reference text, it says little about the amount of disambiguation done by the tagger, and nothing about the ambiguity types that were involved in the disambiguation process.\\n\\nf(ec) = relative frequency of equivalence class\\n\\nIn an effort to compare the German model with what is reported for other languages, English and French tagged texts were analysed. Both texts contained approximately 10,000 words and were tagged using an English resp. French language model for the Xerox tagger.\\n\\nIf the table is viewed in terms of the major word classes of noun, verb, adjective, adverb, and closed-class forms, the following predominant ambiguity classes for English can be distinguished:\\n\\nnoun vs. verb (share, offer, plan),\\n\\nadjective vs. noun (public, million, high),\\n\\nclosed\\n\\n\\n\\nclass vs. noun (a),\\n\\nadjective vs. noun vs. verb (return, field),\\n\\nclosed-class vs. adverb (by, about, below),\\n\\nand closed-class vs. adjective vs. adverb (round, next),\\n\\nnoun vs. verb (affaire, bout, place),\\n\\nadjective vs. noun (demi, moyen, responsable,\\n\\nadjective vs. verb (appliqu, devenu, fabriqu),\\n\\nclosed-class vs. adjective (numeral un).\\n\\nclosed-class vs. adjective (numeral einen, einer),\\n\\nand verb vs. adjective (fehlgeschlagen, bekannt),\\n\\nThe comparison of the most frequent ambiguity types shows a significant difference between the German model on the one hand and the English and French models on the other. In German most of the effort is going into subclassification within major word classes, while in English and French a good deal of disambiguation work is devoted to separate major word classes.\\n\\nAssessment of error types\\n\\nThe differences in ambiguity types of the models also have effects on the types of errors produced by the German model. Again, errors mainly affect the assignment of words to subclasses within one major word class.\\n\\nThe second most frequent error type involves confusion of infinitives and 1st and 3rd pers. pl. finite present tense forms. These are homographs in German that are notoriously hard to disambiguate within a narrow context.\\n\\nThe difficulty of distinguishing between non-attributive, adjectival usage of participles (i.e. er ist geladen) and participles proper (i.e. er hat den Wagen geladen) was mentioned in the preceding section. In addition a number of these forms may also be used as finite verbs (i.e. erhalten, gehrt), and this is a further source of errors.\\n\\nAlmost all of the remaining errors are misassignments within closed classes, including well-known errors due to long distance phenomena, such as those resulting from the confusion of relative pronouns, demonstrative pronouns and articles in sentences like: die einmal fr die Buchproduktion erfaten Texte or: doch der wollte nicht, das falle auf.\\n\\nConclusion\\n\\nIt is important to notice that the types of disambiguation carried out by the tagger for German are significantly different from the disambiguation work for English and French. While in English and French a fair number of disambiguations involve separating major POS classes such as verb, noun, and adjective, most of the work performed in the German model involves disambiguation between subclasses of one main category, such as finite vs. infinitive verb, noun vs. proper noun, different sub-categories of pronouns, etc.\\n\\nThis finding has consequences for the COMPASS project, where POS disambiguation is employed as one means of disambiguating word senses to facilitate precise dictionary look-up. While this technique helps to confine word senses for English and French, it is of little help for word sense disambiguation in German.\\n\\nHowever, the German model was useful for the project because a tagged reference corpus was required for lexicographic work in order to adapt existing bilingual dictionaries. The tagger was used to annotate all of the 50 million word German corpora contained on the ECI Multilingual Corpus 1 CD-ROM.\\n\\nAcknowledgements\\n\\nI would like to thank Helmut Schmid of the University of Stuttgart for providing extensions of parameter initialization for the Xerox Tagger and Jean-Pierre Chanod and Lauri Karttunen of the Rank Xerox Research Laboratory Grenoble for making available the English and French tagged texts and lexicons. I would also like to acknowledge valuable advice from Tracy Holloway King and Steven Abney, who commented on earlier versions of this paper.\\n\\nThis work has been supported in part by the Ministry for Science and Research of the Land Baden-Wrttemberg under the project ``Corpus Based Development of Lexical Knowledge Bases (ELWIS)'' and by the Commission of the European Community under the LRE project ``Adapting Bilingual Dictionaries for Online Assistance (COMPASS, LRE 62-080)''.\\n\\nBibliography\\n\\nTed Briscoe, Greg Grefenstette, Llus Padr, and Iskander Serail. 1994. Hybrid techniques for training HMM part-of-speech taggers. Acquilex II working paper 45.\\n\\nCELEX. 1993. The CELEX Lexical Database. Dutch, English, German. Max-Planck-Institute for Psycholinguistics, Centre for Lexical Information, Nijmegen. CD-ROM.\\n\\nJean-Pierre Chanod and Pasi Tapanainen. 1994. Statistical and constraint-based taggers for French. Technical Report MLTT - 016, Rank Xerox Research Centre, Grenoble Laboratory, Grenoble.\\n\\nDoung Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, Trento.\\n\\nECI. 1994. Multilingual Corpus 1. Association for Computational Linguistics, European Corpus Intitiative. CD-ROM.\\n\\nDavid Elworthy. 1994. Does Baum-Welch re-estimation help taggers? In Proceedings of the Fourth Conference on Applied Natural Language Processing, Stuttgart, pages 53-58.\\n\\nHelmut Feldweg. 1993. Stochastische Wortartendisambiguierung fr das Deutsche: Untersuchungen mit dem robusten System LIKELY. Technical report, Universitt Tbingen, Seminar fr Sprachwissenschaft.\\n\\nHelmut Feldweg. 1995. Stochastische Wortartendisambiguierung des Deutschen. In Lexikon  Text, pages 241-254, Tbingen. Max Niemeyer. forthcoming.\\n\\nJulian Kupiec and Mike Wilkens. 1994. The DDS tagger guide version 1.1. Xerox Palo Alto Research Center, unpublished manuscript.\\n\\nBernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155-171.\\n\\nHelmut Schmid and Andr Kempe. 1995. Tagging von Korpora mit HMM, Entscheidungsbumen und neuronalen Netzen. In Helmut Feldweg and Erhard Hinrichs, editors, Lexikon Text, Tbingen. Max Niemeyer. forthcoming.\\n\\nChristine Thielen and Manfred Sailer. 1994. Ein Tagset frs Deutsche. Richtlinien fr die manuelle Wortarten-Annotierung von Textkorpora. Seminar fr Sprachwissenschaft, Universitt Tbingen, unpublished Manuscript.\\n\\nChristine Thielen and Anne Schiller. 1995. Ein kleines und erweitertes Tagset frs Deutsche. In Helmut Feldweg and Erhard Hinrichs, editors, Lexikon Text, pages 215-226. Max Niemeyer, Tbingen. forthcoming.\\n\\nChristine Thielen. 1994. Ein Tagset fr die Wortartenklassifizierung des Deutschen. In Harald Trost, editor, KONVENS '94. sterreichische Gesellschaft fr Artificial Intelligence, Wien.\\n\\nKlaus Wothke, Ilona Weck-Ulm, Johannes Heinecke, Oliver Mertineit, and Thomas Pachunke. 1993. Statistically based automatic tagging of German text corpora with parts-of-speech -- some experiments. Technical report, IBM Germany, Heidelberg Scientific Center, Heidelberg.\", metadata={'source': '../data/raw/cmplg-xml/9502038.xml'}),\n",
       " Document(page_content='Combining Multiple Knowledge Sources for Discourse Segmentation\\n\\nWe predict discourse segment boundaries from linguistic features of utterances, using a corpus of spoken narratives as data. We present two methods for developing segmentation algorithms from training data: hand tuning and machine learning. When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning).\\n\\nIntroduction\\n\\nDiscourse Segmentation\\n\\nRelated Work\\n\\nOur Previous Results\\n\\nMethodology\\n\\nBoundary Classification\\n\\nCoding of Linguistic Features\\n\\nEvaluation\\n\\nHand Tuning\\n\\nMachine Learning\\n\\nConclusion\\n\\nWe have presented two methods for developing segmentation hypotheses using multiple linguistic features. The first method hand tunes features and algorithms based on analysis of training errors. The second method, machine learning, automatically induces decision trees from coded corpora. Both methods rely on an enriched set of input features compared to our previous work. With each method, we have achieved marked improvements in performance compared to our previous work and are approaching human performance. Note that quantitatively, the machine learning results are slightly better than the hand tuning results. The main difference on average performance is the higher precision of the automated algorithm. Furthermore, note that the machine learning algorithm used the changes to the coding features that resulted from the tuning methods. This suggests that hand tuning is a useful method for understanding how to best code the data, while machine learning provides an effective (and automatic) way to produce an algorithm given a good feature representation.\\n\\nWe plan to continue our experiments by further merging the automated and analytic techniques, and evaluating new algorithms on our final test corpus. Because we have already used cross-validation, we do not anticipate significant degradation on new test narratives. An important area for future research is to develop principled methods for identifying distinct speaker strategies pertaining to how they signal segments. Performance of individual speakers varies widely as shown by the high standard deviations in our tables. The original NP, hand tuned, and machine learning algorithms all do relatively poorly on narrative 16 and relatively well on 11 (both in the test set) under all conditions. This lends support to the hypothesis that there may be consistent differences among speakers regarding strategies for signaling shifts in global discourse structure.\\n\\nBibliography\\n\\nLeo Breiman, Jerome Friedman, Richard Olshen, and C. Stone. 1984. Classification and Regression Trees. Wadsworth and Brooks, Monterey, CA.\\n\\nWallace L. Chafe. 1980. The Pear Stories. Ablex Publishing Corporation, Norwood, NJ.\\n\\nWilliam Gale, Ken W. Church, and David Yarowsky. 1992. Estimating upper and lower bounds on the performance of word-sense disambiguation programs. In Proc. of the 30th ACL, pages 249-256.\\n\\nBarbara Grosz and Julia Hirschberg. 1992. Some intonational characteristics of discourse structure. In Proc. of the International Conference on Spoken Language Processing.\\n\\nBarbara Grosz and Candace Sidner. 1986. Attention, intentions and the structure of discourse. Computational Linguistics, 12:175-204.\\n\\nBarbara J. Grosz, Aaravind K. Joshi, and Scott Weinstein. 1983. Providing a unified account of definite noun phrases in discourse. In Proc. of the 21st ACL, pages 44-50.\\n\\nMarti A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proc. of the 32nd ACL.\\n\\nJulia Hirschberg and Barbara Grosz. 1992. Intonational features of local and global discourse structure. In Proc. of the Darpa Workshop on Spoken Language.\\n\\nJulia Hirschberg and Diane Litman. 1993. Empirical studies on the disambiguation of cue phrases. Computational Linguistics, 19(3):501-530.\\n\\nJulia Hirschberg and Janet Pierrehumbert. 1986. The intonational structuring of discourse. In Proc. of the 24th ACL.\\n\\nJerry R. Hobbs.\\n\\n1979.\\n\\nCoherence and coreference.\\n\\nCognitive Science, 3(1):67\\n\\n\\n\\n90.\\n\\nAmy Isard and Jean Carletta. 1995. Replicability of transaction and action coding in the map task corpus. In AAAI 1995 Spring Symposium Series: Empirical Methods in Discourse Interpretation and Generation, pages 60-66.\\n\\nMegumi Kameyama. 1986. A property-sharing constraint in centering. In Proc. of the 24th ACL, pages 200-206.\\n\\nH. Kozima. 1993. Text segmentation based on similarity between words. In Proc. of the 31st ACL (Student Session), pages 286-288.\\n\\nAlex Lascarides and Jon Oberlander. 1992. Temporal coherence and defeasible knowledge. Theoretical Linguistics.\\n\\nCharlotte Linde. 1979. Focus of attention and the choice of pronouns in discourse. In Talmy Givon, editor, Syntax and Semantics: Discourse and Syntax, pages 337-354. Academic Press, New York.\\n\\nDiane J. Litman and Rebecca J. Passonneau. 1995. Developing algorithms for discourse segmentation. In AAAI 1995 Spring Symposium Series: Empirical Methods in Discourse Interpretation and Generation, pages 85-91.\\n\\nDiane J. Litman. 1994. Classifying cue phrases in text and speech using machine learning. In Proc. of the 12th AAAI, pages 806-813.\\n\\nWilliam C. Mann and Sandra Thompson. 1988. Rhetorical structure theory. TEXT, pages 243-281.\\n\\nJohanna D. Moore and Cecile Paris. 1993. Planning text for advisory dialogues: Capturing intentional and rhetorical information. Computational Linguistics, 19:652-694.\\n\\nJohanna D. Moore and Martha E. Pollack. 1992. A problem for RST: The need for multi-level discourse analysis. Computational Linguistics, 18:537-544.\\n\\nJane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17:21-48.\\n\\nMegan Moser and Julia D. Moore. 1995. Using discourse analysis and automatic text generation to study discourse cue usage. In AAAI 1995 Spring Symposium Series: Empirical Methods in Discourse Interpretation and Generation, pages 92-98.\\n\\nChristine H. Nakatani, Julia Hirschberg, and Barbara J. Grosz. 1995. Discourse structure in spoken language: Studies on speech corpora. In AAAI 1995 Spring Symposium Series: Empirical Methods in Discourse Interpretation and Generation, pages 106-112.\\n\\nRebecca J. Passonneau and Diane J. Litman. 1993. Intention-based segmentation: Human reliability and correlation with linguistic cues. In Proc. of the 31st ACL, pages 148-155.\\n\\nRebecca J. Passonneau and D. Litman. to appear. Empirical analysis of three dimensions of spoken discourse. In E. Hovy and D. Scott, editors, Interdisciplinary Perspectives on Discourse. Springer Verlag, Berlin.\\n\\nRebecca J. Passonneau. 1994. Protocol for coding discourse referential noun phrases and their antecedents. Technical report, Columbia University.\\n\\nRebecca J. Passonneau. to appear. Interaction of the segmental structure of discourse with explicitness of discourse anaphora. In E. Prince, A. Joshi, and M. Walker, editors, Proc. of the Workshop on Centering Theory in Naturally Occurring Discourse. Oxford University Press.\\n\\nLivya Polanyi. 1988. A formal model of discourse structure. Journal of Pragmatics, pages 601-638.\\n\\nJohn R. Quinlan. 1993. C4.5 : Programs for Machine Learning. Morgan Kaufmann Publishers, San Mateo, Calif.\\n\\nRachel Reichman. 1985. Getting Computers to Talk Like You and Me: Discourse Context, Focus, and Semantics. Bradford. MIT, Cambridge.\\n\\nJ. C. Reynar. 1994. An automatic method of finding topic boundaries. In Proc. of the 32nd ACL (Student Session), pages 331-333.\\n\\nLisa J. Stifleman. 1995. A discourse analysis approach to structured speech. In AAAI 1995 Spring Symposium Series: Empirical Methods in Discourse Interpretation and Generation, pages 162-167.\\n\\nBonnie L. Webber. 1991. Structure and ostension in the interpretation of discourse deixis. Language and Cognitive Processes, pages 107-135.\\n\\nSholom M. Weiss and Casimir Kulikowski. 1991. Computer systems that learn: classification and prediction methods from statistics, neural nets, machine learning, and expert systems. Morgan Kaufmann.\\n\\nFootnotes', metadata={'source': '../data/raw/cmplg-xml/9505025.xml'}),\n",
       " Document(page_content=\"Bottom\\n\\n\\n\\nUp Earley Deduction\\n\\nWe propose a bottom-up variant of Earley deduction. Bottom-up deduction is preferable to top-down deduction because it allows incremental processing (even for head-driven grammars), it is data-driven, no subsumption check is needed, and preference values attached to lexical items can be used to guide best-first search. We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps.\\n\\nIntroduction\\n\\nEarley deduction is a very attractive framwork for natural language processing because it has the following properties and applications.\\n\\nMemoization and reuse of partial results\\n\\nIncremental processing by addition of new items\\n\\nHypothetical reasoning by keeping track of dependencies between items\\n\\nBest-first search by means of an agenda\\n\\nLike Earley's algorithm, all of these approaches operate top-down (backward chaining). The interest has naturally focussed on top-down methods because they are at least to a certain degree goal-directed.\\n\\nIn this paper, we present a bottom-up variant of Earley deduction, which we find advantageous for the following reasons:\\n\\nIncrementality: Portions of an input string can be analysed as soon as they are produced (or generated as soon as the what-to-say component has decided to verbalize them), even for grammars where one cannot assume that the left-corner has been predicted before it is scanned. Data-Driven Processing: Top-down algorithms are not well suited for processing grammatical theories like Categorial Grammar or  HPSG that would only allow very general predictions because they make use of general schemata instead of construction-specific rules. For these grammars data-driven bottom-up processing is more appropriate. The same is true for large-coverage rule-based grammars which lead to the creation of very many predictions. Subsumption Checking: Since the bottom-up algorithm does not have a prediction step, there is no need for the costly operation of subsumption checking. Search Strategy: In the case where lexical entries have been associated with preference information, this information can be exploited to guide the heuristic search.\\n\\nBottom\\n\\n\\n\\nup Earley Deduction\\n\\nScanning\\n\\nThe purpose of the scanning step, which corresponds to lexical lookup in chart parsers, is to look up base cases of recursive definitions to serve as a starting point for bottom-up processing. The scanning step selects clauses that can appear as leaves in the proof tree for a given goal G.\\n\\nConsider the following simple definition of an  HPSG, with the recursive definition of the predicate sign/1.\\n\\nsign(X) [\\n\\n\\n\\nphrasal_sign(X).\\n\\nsign(X) [\\n\\n\\n\\nlexical_sign(X).\\n\\nphrasal_sign(X  dtrs:(head_dtr:HD\\n\\ncomp_dtr:CD)  ) [\\n\\n\\n\\nsign(HD),\\n\\nsign(CD),\\n\\nprinciples(X,HD,CD).\\n\\nprinciples(X,HD,CD) [\\n\\n\\n\\nconstituent_order_principle(X,HD,CD),\\n\\nhead_feature_principle(X,HD),\\n\\n...\\n\\nconstituent_order_principle(phon:X_Ph,\\n\\nphon:HD_Ph,\\n\\nphon:CD_Ph) [\\n\\n\\n\\nsequence_union(CD_Ph,HD_Ph,X_Ph).\\n\\nThe predicate sign/1 is defined recursively, and the base case is the predicate lexical_sign/1. But, clearly it is not restrictive enough to find only the predicate name of the base case for a given goal. The base cases must also be instantiated in order to find those that are useful for proving a given goal. In the case of parsing, the lookup of base cases (lexical items) will depend on the words that are present in the input string. This is implied by the first goal of the predicate principles/3, the constituent order principle, which determines how the  PHON value of a constituent is constructed from the  PHON values of its daughters. In general, we assume that the constituent order principle makes use of a linear and non-erasing operation for combining strings. If this is the case, then all the words contained in the PHON value of the goal can have their lexical items selected as unit clauses to start bottom-up processing.\\n\\nBase case lookup must be defined specifically for different grammatical theories and directions of processing by the predicate lookup/2, whose first argument is the goal and whose second argument is the selected base case. The following clause defines the lookup relation for parsing with  HPSG.\\n\\n% lookup(+Goal,\\n\\n\\n\\nBaseCase)\\n\\nlookup(sign(phon:PhonList),\\n\\nlexical_sign(phon:[Word]  synsem:X)\\n\\n) [\\n\\n\\n\\nmember(Word,PhonList),\\n\\nlexicon(Word,X).\\n\\nNote that the base case clauses can become further instantiated in this step. If concatenation (of difference lists) is used as the operation on strings, then each base case clause can be instantiated with the string that follows it. This avoids combination of items that are not adjacent in the input string.\\n\\nlookup(sign(phon:PhonList),\\n\\nlexical_sign(phon:[Word|Suf]\\n\\n\\n\\nSuf\\n\\nsynsem: Synsem)\\n\\n) [\\n\\n\\n\\nappend(_,[Word|Suf],PhonList),\\n\\nlexicon(Word,Synsem).\\n\\nIn bottom-up Earley deduction, the first step towards proving a goal is perform lookup for the goal, and to add all the resulting (unit) clauses to the chart. Also, all non-unit clauses of the program, which can appear as internal nodes in the proof tree of the goal, are added to the chart.\\n\\nThe scanning step achieves a certain degree of goal-directedness for bottom-up algorithms because only those clauses which can appear as leaves in the proof tree of the goal are added to the chart.\\n\\nIndexing\\n\\nAn item in normal context-free chart parsing can be regarded as a pair consisting of a dotted rule R and the substring S that the item covers (a pair of starting and ending position). The fundamental rule of chart parsing makes use of these string positions to ensure that only adjacent substrings are combined and that the result is the concatenation of the substrings.\\n\\nMoreover, the use of string positions known from chart parsing is too inflexible because it allows only concatenation of adjacent contiguous substrings. In linguistic theory, the interest has shifted from phrase structure rules that combine adjacent and contiguous constituents to\\n\\nprinciple-based approaches to grammar that state general well-formedness conditions instead of describing particular constructions (e.g. HPSG)\\n\\nHowever, the string positions are useful as an indexing of the items so that it can be easily detected whether their combination can contribute to a proof of the goal. This is especially important for a bottom-up algorithm which is not goal-directed like top-down processing. Without indexing, there are too many combinations of items which are useless for a proof of the goal, in fact there may be infinitely many items so that termination problems can arise.\\n\\nWe generalize the indexing scheme from chart parsing in order to allow different operations for the combination of strings. Indexing improves efficiency by detecting combinations that would fail anyway and by avoiding combinations of items that are useless for a proof of the goal.\\n\\nBelow, we give some examples of possible indexing schemes. Other indexing schemes can be used if they are needed.\\n\\nWith the use of indices, the lookup relation becomes a relation between goals and items. The following specification of the lookup relation provides indexing according to string positions as in a chart parser (usable for combination schemes 2, 3, and 4).\\n\\nlookup(sign(phon:PhonList), item(lexical_sign(phon:[Word] synsem:X), Begin-End) ) [- nth_member(Word,Begin,End,PhonList), lexicon(Word,X). nth_member(X,0,1,[X|_]). nth_member(X,N1,N2,[_|R]) [- nth_member(X,N0,N1,R), N2 is N1 + 1.\\n\\nGoal Types\\n\\nIn constraint-based grammars there are some predicates that are not adequately dealt with by bottom-up Earley deduction, for example the Head Feature Principle and the Subcategorization Principle of  HPSG. The Head Feature Principle just unifies two variables, so that it can be executed at compile time and need not be called as a goal at runtime. The Subcategorization Principle involves an operation on lists (append/3 or delete/3 in different formalizations) that does not need bottom-up processing, but can better be evaluated by top-down resolution if its arguments are sufficiently instantiated. Creating and managing items for these proofs is too much of a computational overhead, and, moreover, a proof may not terminate in the bottom-up case because infinitely many consequences may be derived from the base case of a recursively defined relation.\\n\\nIn order to deal with such goals, we associate the goals in the body of a clause with goal types. The goals that are relevant for bottom-up Earley deduction are called waiting goals because they wait until they are activated by a unit clause that unifies with the goal. Whenever a unit clause is combined with a non-unit clause all goals up to the first waiting goal of the resulting clause are proved according to their goal type, and then a new clause is added whose selected goal is the first waiting goal.\\n\\nCorrectness and Completeness\\n\\nIn order to show the correctness of the system, we must show that the scanning step only adds consequences of the program to the chart, and that any items derived by the inference rule are consequences of the program clauses. The former is easy to show because all clauses added by the scanning step are instances of program clauses, and the inference rule performs a resolution step whose correctness is well-known in logic programming. The other goal types are also proved by resolution.\\n\\nThere are two potential sources of incompleteness in the algorithm. One is that the scanning step may not add all the program clauses to the chart that are needed for proving a goal, and the other is that the indexing may prevent the derivation of a clause that is needed to prove the goal.\\n\\nIn order to avoid incompleteness, the scanning step must add all program clauses that are needed for a proof of the goal to the chart, and the combination of indices may only fail for inference steps which are useless for a proof of the goal. That the lookup relation and the indexing scheme satisfy this property must be shown for particular grammar formalisms.\\n\\nIn order to keep the search space small (and finite to ensure termination) the scanning step should (ideally) add only those items that are needed for proving the goal to the chart, and the indexing should be chosen in such a way that it excludes derived items that are useless for a proof of the goal.\\n\\nBest\\n\\n\\n\\nFirst Search\\n\\nFor practical NL applications, it is desirable to have a best-first search strategy, which follows the most promising paths in the search space first, and finds preferred solutions before the less preferred ones.\\n\\nThere are often situations where the criteria to guide the search are available only for the base cases, for example\\n\\nweighted word hypotheses from a speech recognizer\\n\\nGoals and clauses are associated with preference values that are intended to model the degree of confidence that a particular solution is the `correct' one. Unit clauses are associated with a numerical preference value, and non-unit clauses with a formula that determines how its preference value is computed from the preference values of the goals in the body of the clause. Preference values can (but need not) be interpreted as probabilities.\\n\\nThe preference values are the basis for giving priorities to items. For unit clauses, the priority is identified with the preference value. For non-unit clauses, where the preference formula may contain uninstantiated variables, the priority is the value of the formula with the free variables instantiated to the highest possible preference value (in case of an interpretation as probabilities: 1), so that the priority is equal to the maximal possible preference value for the clause.\\n\\nThe algorithm is parametrized with respect to the relation lookup/2 and the choice of the indexing scheme, which are specific for different grammatical theories and directions of processing.\\n\\nImplementation\\n\\nThe bottom-up Earley deduction algorithm described here has been implemented in Quintus Prolog as part of the GeLD system. GeLD (Generalized Linguistic Deduction) is an extension of Prolog which provides typed feature descriptions and preference values as additions to the expressivity of the language, and partial evaluation, top-down, head-driven, and bottom-up Earley deduction as processing strategies. Tests of the system with small grammars have shown promising results, and a medium-scale  HPSG for German is presently being implemented in GeLD. The lookup relation and the choice of an indexing scheme must be specified by the user of the system.\\n\\nConclusion and Future Work\\n\\nWe have proposed bottom-up Earley deduction as a useful alternative to the top-down methods which require subsumption checking and restriction to avoid prediction loops.\\n\\nThe proposed method should be improved in two directions. The first is that the lookup predicate should not have to be specified by the user, but automatically inferred from the program.\\n\\nBibliography\\n\\nGosse Bouma and Gertjan van Noord. Head-driven parsing for lexicalist grammars: Experimental results. In EACL93, pages 71 - 80, Utrecht, NL, 1993.\\n\\nChris Brew. Adding preferences to CUF. In Jochen Drre, editor, DYANA-2 Deliverable R1.2.A: Computational Aspects of Constraint-Based Linguistic Description I, pages 57 - 69. Esprit Basic Research Project 6852, 1993.\\n\\nJochen Drre. Generalizing Earley deduction for constraint-based grammars. In Jochen Drre, editor, DYANA-2 Deliverable R1.2.A: Computational Aspects of Constraint-Based Linguistic Description I, pages 23 - 41. Esprit Basic Research Project 6852, 1993.\\n\\nGregor Erbach. Using preference values in typed feature structures to exploit non-absolute constraints for disambiguation. In Harald Trost, editor, Feature Formalisms and Linguistic Ambiguity. Ellis-Horwood, 1993.\\n\\nGregor Erbach. Towards a theory of degrees of grammaticality. In Carlos Martn-Vide, editor, Current Issues in Mathematical Linguistics. North-Holland, Amsterdam, to appear. Also published as CLAUS Report 34, Universitt des Saarlandes, 1993.\\n\\nDale Douglas Gerdemann. Parsing and Generation of Unification Grammars. PhD thesis, University of Illinois at Urbana-Champaign, 1991. Cognitive Science technical report CS-91-06 (Language Series).\\n\\nMark Johnson. Memoization in constraint logic programming. Department of Cognitive Science, Brown University. Presented at the 1st International Conference on Constraint Programming, Newport, Rhode Island; to appear in the proceedings, 1993.\\n\\nMartin Kay. Algorithm schemata and data structures in syntactic processing. Technical Report CSL-80-12, XEROX PARC, Palo Alto, CA, 1980.\\n\\nGnter Neumann. A Uniform Tabular Algorithm for Natural Language Parsing and Generation and its Use within Performance-based Methods. PhD thesis, University Saarbrcken. forthcoming.\\n\\nFernando C.N. Pereira and David H.D. Warren. Parsing as deduction. In ACL Proceedings, 21st Annual Meeting, pages 137-144, 1983.\\n\\nCarl Pollard. Generalized Context-Free Grammars, Head Grammars, and Natural Language. PhD thesis, Stanford, 1984.\\n\\nMike Reape. A theory of word order and discontinuous constituency in West Germanic. In E. Engdahl and M. Reape, editors, Parametric Variation in Germanic and Romance: Preliminary Investigations, pages 25-40. ESPRIT Basic Research Action 3175  DYANA, Deliverable R1.1.A, 1990.\\n\\nStuart M. Shieber. A uniform architecture for parsing and generation. In Proceedings of the 12th International Conference on Computational Linguistics (COLING), Budapest, 1988.\\n\\nGertjan van Noord. Reversibility in Natural Language Processing. PhD thesis, Rijksuniversiteit Utrecht, NL, 1993.\\n\\nK. Vijay-Shanker, David J. Weir, and Aravind K. Joshi. Characterizing structural descriptions produced by various grammatical formalisms. In 25th Annual Meeting, pages 104-111, Stanford, CA, 1987. Association for Computational Linguistics.\\n\\nDavid J. Weir. Characterizing Mildly Context-Sensitive Grammar Formalisms. PhD thesis, Department of Computer and Information Science, University of Pennsylvania, 1988.\\n\\nMats Wirn. A comparison of rule-invocation strategies in context-free chart parsing. In ACL Proceedings, Third European Conference, pages 226-235, 1987.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9502004.xml'}),\n",
       " Document(page_content='Measuring Semantic Complexity\\n\\nWe define semantic complexity using a new concept of meaning automata. We measure the semantic complexity of understanding of prepositional phrases, of an \"in depth understanding system\", and of a natural language interface to an on-line calendar. We argue that it is possible to measure some semantic complexities of natural language processing systems before building them, and that systems that exhibit relatively  complex behavior can be built from semantically simple components.\\n\\nIntroduction\\n\\nThe problem\\n\\nWe want to account for the difference between the following kinds of dialogs:\\n\\nDialog 1:\\n\\nDialog 2:\\n\\nMore practically, we would like to be able to measure the process of understanding natural language, and in particular, to estimate the difficulty of a NLU task before building a system for doing that task.\\n\\nPractical advantages of a small domain:  MINCAL The number of semantic/ontological categories is small The second advantage of a limited domain lies in the relatively small number of semantic categories. For example, for the domain of calendars the number of concepts is less than 100; for room scheduling it is about 20. Even for a relatively complex office application, say, WordPerfect Office 4.0, the number of semantic categories is between 200 and 500 (the number depends what counts as a category, and what is merely a feature). Why this is important? Because not only do we need a set of semantic categories, but also we have to encode background knowledge about them. For instance, given the concept of \"range\" with its \"beginning\", \"end\" and \"measure\" (e.g. hours) smaller than the value of \"end\". We should know that two different meetings cannot occupy the same room in overlapping periods of time, we should know the number of days in any month, and that meetings are typically scheduled after the current date, etc.\\n\\nBut what is a \"small domain\"? Semantic complexity: from intuitions to meaning automata\\n\\nWe are now ready to introduce the concept of semantic complexity for sets of sentences and natural language understanding tasks, i.e. numbers measuring how complicated they are. To factor in the \"degree of understanding\", those numbers will be computed relative to some semantic types. Then, for example, if we examine the semantic complexity of two sets of 24 sentences, one consisting of very simple time expressions, and the other of a set of idioms, it turns out - surprisingly - that from a certain perspective they have identical complexities, but from another perspective they do not.\\n\\nTwo sets of 24 sentences and their intuitive complexity\\n\\nLet us consider the meanings of the following two constructions:\\n\\npp\\n\\nat X pm/am\\n\\npp\\n\\nat noun(bare) For each construction we will consider 24 cases. For the first construction these are the numbers 1-12 followed by am or pm; for the second construction these are expressions such as at work, at lunch, at school, .... Of course the construction\\n\\nat noun(bare) is open ended, but for the sake of comparison, we will choose 24 examples. For simplicity, we will consider the two constructions simply as sets of sentences. We have then two 24-element sets of sentences: The set T contains sentences\\n\\nThe meeting is at X PM_or_AM where X ranges from 1 to 12, and PM_or_AM is either am or pm. The set S contains 24 sentences of the type\\n\\nIntuitively, accounting for the semantics of the latter is more complicated, because in order to explain the meaning of the expression John is at work we have to have as the minimum the concept of working, of the place of work being a different place than the current discourse location, and of a habitual activity. In other words, a whole database of facts must be associated with it. Furthermore, as the bare noun changes, e.g. into John is at liberty, this database of facts has to change, too. This is not the case for at 7 am, and 8 pm. Here, we simply map the expression X pm into\\n\\nhour(X+12) (ignoring everything else).\\n\\nMeaning automata and their complexity\\n\\nIn order to prove or disprove the intuitions described in the preceding few paragraphs we need some tools. One of the tools for measuring complexity widely used in theoretical computer science is Kolmogorov complexity.\\n\\nWe could define semantic complexity of a set of sentences S as its Kolmogorov complexity, i.e. as the size (measured by the number of states) of the simplest machine M, such that for any sentence s in S its semantics is given by M(s). However this definition is problematic, because it assumes that there is one correct semantics for any sentence, and we believe that this is not so. It is also problematic because the function K assigning its Kolmogorov complexity to a string is not computable.\\n\\nThus, instead, we will define  Q-complexity of a set of sentences S as the size of the simplest model scheme M=MS, such that any sentence s in S its semantics is given by M(s), and M(s) correctly answers all questions about s contained in Q.\\n\\nThe words \"model scheme\" can stand for either \"Turing machine\", or \"Prolog program\", or \"description\", or a related notion. In this paper we think of M as a Turing machine that computes the semantics of the sentences in S, and measure its size by the number of states. Of course, there can be more than one measure of the size of the simplest model scheme M; and in practice we will deal not with the simplest model scheme, but with the simplest we are able to construct. And to take care of the possible non-computability of the function computing Q-complexity of a set of sentences, we can put some restriction on the Turing machine, e.g. requiring it to be finite state or a stack automaton.\\n\\nWe can now define the concept of meaning automaton (M-automaton) as follows. Let Q be a set of questions. Formally, we treat each question as a (partial) function from sentences to a set of answers A:\\n\\nIntuitively, each question examines a sentence for a piece of relevant information. Under this assumption the semantics of a sentence (i.e. a formal string) is not given by its truth conditions or denotation but by a set of answers:\\n\\nNow, given a set of sentences S and a set of questions Q, their meaning automaton is a function\\n\\nwhich satisfies the constraint\\n\\nM (s,q) =  q(s)\\n\\ni.e. a function which gives a correct answer to every question. We call it a meaning automaton because for any sentence s\\n\\nFinally, the Q-complexity of the set S is the size of the smallest such M.\\n\\nNote that the idea of a meaning automaton as a question answer map allows us to bypass all subtle semantics questions without doing violence to them. And it has some hope of being a computationally tractable approach.\\n\\nMeasuring semantic complexity Semantic complexity classes yes/no, \"what-is\" and other complexities yes-no complexities of T and S are the same\\n\\nWe now can measure the yes-no-complexity of both T and S. Let\\n\\nbe the mapping from\\n\\n,\\n\\nwhere\\n\\nand\\n\\n, if X=Y, and no otherwise. (\\n\\n, and we identify the time expressions with numbers for the sake of simplicity). Clearly, under this mapping all the questions can be correctly answered (remember that question q13 returns yes for\\n\\n,\\n\\nand no otherwise).\\n\\nis a similar mapping: we choose arbitrary 24 tokens, and map the sentences of S into them in a 1-1 fashion. As before, for each s in S,\\n\\nis well defined, and each question of the type Is John at breakfast/.../at age? can be truthfully answered.\\n\\nIf we measure the semantic complexity by the number of pairs in the\\n\\nfunctions, the yes-no complexities of both sets are the same and equal 24[2]. If we measure it by the number of states of their respective Turing machines, because the two problems are isomorphic, their yes-no complexity will again be identical. For example, we can build a two state, 4-tape Turing machine. It would scan symbols on two input tapes, and print no on the output tape if the two input symbols are not equal. The third input tape would contain five 1\\'s and be used as a counter (the binary string twxyz represents the number\\n\\n1t+2w+4x+8y+8z+1). The machine moves always to the right, scanning the symbols. If it terminates with accept and the empty output tape, it means yes; if it terminates with accept and the no on the output tape, it means no. This machine can be described as a\\n\\ntable, hence we can assign the complexity of 30 to it.\\n\\nWe arrive at a surprising conclusion that a set of idiomatic expressions with complicated meanings and a trivial construction about time can have the same semantic complexity. (From the perspective of answering yes/no questions).\\n\\n\"what is?\"\\n\\n\\n\\ncomplexity\\n\\nLet U be a finite set of tokens. Consider the following semantic machine MU: For any token u in U, if the input is \"what is u\" the output is a definition of u. For simplicity, assume that the output is one token, i.e. can be written in one move; let assume also that the input also consists only of one token, namely u, i.e. the question is implicit. Then, the size of MU is the measure of \"what is\"-complexity of U. Now, consider T and S as sets of tokens. For T we get the \"what is\" complexity measure of 12+4=16, as we can ask about every number, the meeting, the word \"is\", and the tokens \"am\" and \"pm\". (We assume \"the meeting\" to be a single word). For S we get 24+2=26, as we can ask about every X in \"at X\", about \"is\", and about \"John\".\\n\\nThus, the semantic \"what is\"-complexity of S is greater than the \"what is\"-complexity of T. But, interestingly, the \"what is\"-complexity of T is smaller than its yes/no-complexity.\\n\\nComplexity of NL interfaces as Q-complexity\\n\\nWe note that the definition of Q-complexity makes sense not only for declarative sentences but also for commands. Consider, e.g.,  a NL interface to a calendar. The set Q consists of questions about parameters of calendar events: event_time?, event_name?, alarm_on?, event_topic?, event_participants?. In general, in the context of a set of commands, we can identify Q with the set of queries about the required and optional parameters of actions described by those commands.\\n\\nIterated \"what is? \"-complexity Semantical simplicity of  MINCAL and  BORIS\\n\\nIn the previous subsection we have introduced some natural Q-complexity measures, such as yes/no-complexity with\\n\\nand\\n\\n,\\n\\nor \"what\\n\\n\\n\\nis\"\\n\\n\\n\\ncomplexity with\\n\\n, and the answers perhaps given by some reference works:\\n\\n. We have shown how these two kinds of complexity measures distinguish between the two sets of sentences with \"at\". We have also argued that semantic complexities of NL interfaces can be measured in a similar fashion. For instance, for a calendar interface we could use\\n\\nand\\n\\nand for  ELIZA\\n\\n\\n\\ntype programs:\\n\\n,\\n\\nand\\n\\n.\\n\\nHowever we have not yet explained the difference in the apparent semantic complexities of  BORIS and  MINCAL. We will do it now. First, as we noticed in Section 1.3, their vocabulary sizes and the sizes of their respective knowledge bases are almost identical. Thus, their \"what-is\"-complexities are roughly the same.\\n\\nBut now our theory can give an explanation of why the sentence The meeting is at 5 seems simpler than Sarah cheated on Paul. Namely, for the last sentence we assume not only the ability to derive and discuss the immediate consequences of that fact such as \"broken obligation \" or \"is Paul aware of it? \", but also such related topics as \"Sarah\\'s emotional life\" , \"sexually transmitted diseases\", \"antibiotics\", \"germs\", \"flu\", and \"death of grandmother\". In other words, the real complexity of discussing a narrative is at least the complexity of \"iterated-what-is\" combined with \"iterated-why\" (and might as well include alternative questions). By the arguments of the preceding section this would require really extensive background knowledge, and the Q-complexity would range between 10[5] and 10[7]. In contrast, the Q-complexity of  MINCAL is less than 10[4].\\n\\nNow, obviously, one can argue that this analysis is immaterial, because both programs only fake understanding, and that real understanding of the concept of a meeting with a VIP would include e.g. accompanying emotions or its possible consequences for a project. This is a valid point, but the analysis stands, because changing topics, discussing whys and whats is typical for discussing a story, but does not fit into the \"conversation for action\" paradigm.\\n\\nHow to build a complex system from semantically  simple components?\\n\\nWhat is the significance of the numbers we computed in the previous sections? It is an argument showing that it is possible to analyze some cases of semantic complexity of some natural language understanding task before building systems for doing them (e.g. yes/no and what-is complexities). Now, we want to argue that systems that exhibit (or can be attributed) complex behavior can be built from semantically simple components, where semantic simplicity is measured by Q-complexity.\\n\\n\"What is\"-complexity: A natural language understanding system has to deal with a set of basic objects. For our domains of interest, these are actions (typically, given by VPs), objects of actions (given by NPs), and its parameters (described by PPs). These basic objects combine into possibly quite complex entities to describe properties of situations (e.g. parameters of a meeting).\\n\\nIt can be argued that \"what is\"-complexity is a reasonable measure of how complex is the set of those basic objects. Namely, \"what is\"-complexity and \"twice-iterated-what-is\" -complexity measures the size of the database of background knowledge facts. Intuitively, this is a reasonable measure of their semantic complexity.\\n\\nComplexity of grammatical constructions: In many cases the complexity of a new construction is not much greater than the complexity of the subconstructions they are built from. This is the case of the simple imperative construction S(imp)\\n\\nVP NP. In this case, and in general, there is a trade-off between letting the grammar overgeneralize, e.g. allowing \"schedule a cafeteria\", and increasing the complexity of the grammar, e.g. by increasing the number of noun categories np(event), np(place) etc.\\n\\nSimilarly, as new constructions introduce more complexities, for example, S(imp)\\n\\nVP NP PP, we can increase the number of constructions. In S(imp)\\n\\nVP NP PP, PP can modify either the NP or the VP, and the complexity of deciding the meaning of the sentence is a product of all possible combinations of meanings of VPs and NPs. To reduce the number of combinations we split S(imp)\\n\\nVP NP PP into\\n\\nS(imp)\\n\\nVP NP(event) PP(at, time),\\n\\nS(imp)\\n\\nVP NP(event) PP(at, place), and use defaults and filters to exclude less plausible combinations (such as places modifying actions in the calendar context). Thus, roughly, the complexity of the grammar can be estimated by the number of grammatical constructions, defaults and filters.\\n\\n, e.g. abbab, where abbabdescribes the enumeration of the elements of A under which a is assigned to an element in A-B and b is assigned to an element in\\n\\n; the stack is used to store the elements; an element is removed from the stack if the next element is different; the automaton accepts a sequence if at the end only b\\'s are left on the stack. Notice that the meanings of A and B is ignored here; hence from the point of view of semantic complexity, the semantics of most A are B would be very simple (5 states is enough).\\n\\nConclusions\\n\\nWhat are the contributions of this paper? 1. We have defined semantic complexity by connecting the concept of Kolmogorov complexity with the types of questions that can apply to a sentence (a string). We have introduced the concept of a meaning automaton i.e. an abstract machine for answering questions of interest. 2. We have analyzed semantic complexities of simple examples involving prepositional phrases and of larger NLU programs. 3. We have introduced a new concept of meaning of a string, identifying it with the set of values for a fixed set of questions. 4. We have presented some arguments to the effect that intuitively complex NLU tasks can be done by combining simple semantic automata.\\n\\n(2) Can we estimate semantic complexities by statistical means? This is possible for some cases of \"what-is\"-complexity, e.g. by estimating the number of technical terms in a corpus.\\n\\n(3) Can we express semantic complexity of a NLU task as a function of the complexity of an automaton partially solving the task and the description (or a corpus) of the whole task. This would be a most welcome result. It would mean that given e.g. a corpus of phrases and a prototype that successfully assigns semantics to 22% of them we could say that a complete system would be, say, two orders of magnitude more complex.\\n\\nOf course, we are aware of the fact that without some constraints on the type of the corpus/description and the type of automata this kind of problem is undecidable, but the point is to find appropriate constraints. For instance, for \"what is\"-complexity such a result is trivially holds: the size of the corpus determines the size of the explanation table.\\n\\n(4) It would be interesting to see under what circumstances the iteration of \"what is\" questions would result in fixed points, e.g. for sets T and S, and what would these fixpoints be (excluding \"everything\"). Similarly iterations of why questions might eventually result in a fix point. But when?\\n\\n(5) If we measure the semantic complexity by the number of pairs in the\\n\\nAcknowledgments. I\\'d like to thank D. Kanevsky for our discussions of semantic complexity, and W. Savitch for comments on an earlier draft.\\n\\nBibliography\\n\\nE. Bilange and J-Y. Magadur. A robust approach for handling oral dialogues. Proc. Coling\\'92, pages 799-805, 1992.\\n\\nH. Bunt.\\n\\nContext and dialogue control.\\n\\nThink, 3(May):19\\n\\n\\n\\n31, 1994.\\n\\nE.J. Crothers. Paragraph Structure Inference. Ablex Publishing Corp., Norwood, New Jersey, 1979.\\n\\nK. Devlin. Logic and Information. Cambridge University Press, Cambridge, 1991.\\n\\nM.G. Dyer. In-Depth Understanding. MIT Press, Cambridge, MA, 1983.\\n\\nA.C. Graesser. Prose Comprehension Beyond the Word. Springer, New York, NY, 1981.\\n\\nW. Lehnert, M.G.Dyer, P.N.Johnson, C.J.Yang, and S. Harley. Boris - an experiment in in-depth understanding of narratives. Artificial Intelligence, 20(1):15-62, 1983.\\n\\nM. Li and P.M.B.Vitanyi. Inductive reasoning and kolmogorov complexity. Journal of Commputer and System Sciences, 44(2):343-384, 1992.\\n\\nJ. Rissanen. A universal prior for integers and estimation by minimum description length. Annals of Statistics, 11:416-431, 1982.\\n\\nR.Quirk and S.Greenbaum. A Concise Grammar of Contemporary English. Harcourt Brace Jovanovich, Inc., New York, NY, 1973.\\n\\nW. J. Savitch. Why it might pay to assume that languages are infinite. Annals of Mathematics and Artificial Intelligence, 8(1,2):17-26, 1993.\\n\\nR. C. Schank, editor. Conceptual Information Processing. Americal Elsevier, New York, NY, 1975.\\n\\nJ.A. Simpson and E.S.C. Weiner, editors. The Oxford English Dictionary. Clarendon Press, Oxford, England, 1989.\\n\\nJ. Sinclair, editor.\\n\\nCollins\\n\\n\\n\\nCobuild English Language Dictionary.\\n\\nCollins ELT, London, 1987.\\n\\nJ. van Benthem. Towards a computational semantics. In Peter Gardenfors, editor, Generalized Quantifiers, pages   31-71. D.Reidel, Dordrecht, Holland, 1987.\\n\\nJ. Weizenbaum. Eliza. Communications of the ACM, 9(1):36-45, 1966.\\n\\nR. Wilensky, D.N. Chin, M. Luria, J. Martin, J. Mayfield, and D. Wu. The Berkeley Unix consultant project. Computational Linguistics, 14(4):35-84, 1988.\\n\\nT. Winograd and F. Flores. Understanding Computers and Cognition. Ablex, Norwood, NJ, 1986.\\n\\nW. Zadrozny.\\n\\nOn compositional semantics.\\n\\nProc. Coling\\'92, pages 260\\n\\n\\n\\n266, 1992.\\n\\nW. Zadrozny. Reasoning with background knowledge - a three-level theory. Computational Intelligence 10, 2 (1994).\\n\\nW. Zadrozny. From compositional to systematic semantics. Linguistic and Philosophy, 17(4) (1994).\\n\\nW. Zadrozny. From utterances to situations: Parsing prepositional phrases in a small domain. Proc. 4th Conference on Situation Theory and its Applications, 1994.\\n\\nW. Zadrozny and K. Jensen. Semantics of paragraphs. Computational Linguistics, 17(2):171-210, 1991.\\n\\nW. Zadrozny and A. Manaster-Ramer. The significance of constructions. (Manuscript from 1993) IBM Research Technical Report  RC 20002(88492), 1995.\\n\\nW. Zadrozny, M. Szummer, S. Jarecki, D. E. Johnson, and L. Morgenstern. NL understanding with a grammar of constructions. Proc. Coling\\'94, 1994.\\n\\nFootnotes\\n\\nto appear in Proc. BISFAI\\'95, The Fourth Bar-Ilan Symposium on Foundations of Artificial Intelligence, June 20-22, 1995, Ramat-Gan and Jerusalem, Israel', metadata={'source': '../data/raw/cmplg-xml/9505019.xml'}),\n",
       " Document(page_content=\"Possessive pronouns as determiners in Japanese-to-English machine translation\\n\\nPossessive pronouns are used as determiners in English when no equivalent would be used in a Japanese sentence with the same meaning. This paper proposes a heuristic method of generating such possessive pronouns even when there is no equivalent in the Japanese. The method uses information about the use of possessive pronouns in English treated as a lexical property of nouns, in addition to contextual information about noun phrase referentiality and the subject and main verb of the sentence that the noun phrase appears in. The proposed method has been implemented in NTT Communication Science Laboratories' Japanese-to-English machine translation system ALT-J/E. In a test set of 6,200 sentences, the proposed method increased the number of noun phrases with appropriate possessive pronouns generated, by 263 to 609, at the cost of generating 83 noun phrases with inappropriate possessive pronouns.\\n\\nIntroduction\\n\\nPossessive pronouns are often used as determiners in English when no equivalent would be used in a Japanese sentence with the same meaning. For example, when referring to specific family members in English, it is normal to specify whose relations they are. In Japanese these are only specified if they are not obvious from the context. For a machine translation system to generate appropriate English when translating from Japanese, it is necessary to determine which pronouns should be used and when.\\n\\nDifferences in the use of possessive pronouns in Japanese and English\\n\\nThe English translations of the test set contain 657 noun phrases with possessive pronouns. The sentences containing these noun phrases were examined in order to determine how the possessive pronouns could be generated by a machine translation system. The noun phrases were divided into three groups, according to whether the possessive pronoun had an equivalent in the original Japanese, or could be predicted as an obligatory part of an English expression or if neither of the above conditions held.\\n\\nThe third and final group (III) consists of 359 noun phrases (54%) where the original Japanese had neither a possessive construction, as in group I, nor arose in an English expression in which it was obligatory, as in group II. These noun phrases were those where English conventionally uses a possessive pronoun to indicate a relationship such as ownership, as in my wallet, or a family relationship, my father, but Japanese does not. The use of possessive pronouns with the nouns which head the noun phrases in group III seems to be tied to the particular words. In particular, words which denote  BODY PARTS, WORK, PERSONAL POSSESSIONS, ATTRIBUTES and relational nouns such as  KIN and  PEOPLE DEFINED BY THEIR RELATION TO ANOTHER PERSON (e.g. assailant, partner, subordinate) were commonly translated with possessive pronouns. The semantic hierarchy of 2,800 categories used in   ALT-J/E was not fine-grained enough to identify the words by their denotation alone. We therefore identified the nouns manually and marked them with a special flag in the lexicon. These nouns will be referred to as `trigger-nouns' as they trigger the use of possessive pronouns when they are used in English. We are investigating automating the identification process using a parsed bilingual corpus aligned at the noun phrase level.\\n\\nExisting translation algorithms\\n\\nThis section describes the overall process of translation in   ALT-J/E, and in particular how the possessive pronouns in noun phrases from groups (I) and (II) are translated.\\n\\nThe overall process of translation can be divided into seven parts. First, ALT-J/E splits the Japanese text into morphemes. Second, it analyses the sentence syntactically, often giving multiple possible interpretations. Third, it rewrites complicated Japanese expressions into simpler ones. Fourth, ALT-J/E semantically evaluates the various interpretations. Fifth, syntactic and semantic criteria are used to select the best interpretation. Sixth, the selected interpretation is transferred into English. Finally, the English sentence is adjusted to give the correct inflectional forms.\\n\\nWhen the Japanese analysis stage has parsed the sentence correctly and an appropriate pattern has been chosen in the transfer stage then the correct possessive pronoun will be generated.\\n\\nFor the 105 sentences of group II where the translator uses an idiom containing a possessive pronoun, the machine translation system does not always choose the same idiom as the human translator. In the cases where the machine generates an idiom that does use a possessive pronoun it is generated correctly.\\n\\nGenerating possessive pronouns in noun phrases headed by trigger-nouns\\n\\nThis section describes the proposed method for appropriately generating possessive pronouns for noun phrases headed by trigger nouns. The discussion will be illustrated with examples of translations from two versions of ALT-J/E. The original version (hereafter the '93 version) does not use the proposed method for generating possessive pronouns. The version that uses the proposed method will be referred to as the '94 version.\\n\\nThe generation of possessive pronouns in noun phrases headed by trigger-nouns occurs at the end of the transfer phrase. We shall call the pronouns generated for these noun phrases `default possessive pronouns' because they are generated as a default, not as a result of being explicitly indicated in the Japanese or in the translation pattern.\\n\\nEffects of noun phrase referentiality\\n\\nFilling the determiner slot\\n\\nRestrictions determined from the meanings of verbs\\n\\nExamining the test set showed that the meanings of verbs can be used to determine whether a possessive pronoun should be generated or not for noun phrases headed by trigger-nouns. Noun phrases which are the direct objects of verbs that express possession, such as own, have or possess and noun phrases that are the object of verbs that express that the object has just been acquired, for example, the direct object of buy, acquire or steal are translated with an indefinite article rather than a possessive pronoun even when headed by trigger-nouns.\\n\\nBoth these cases can be explained by considering the verb's meaning. In the first case the verb itself shows that the subject is the possessor of the object, so a possessive pronoun is not needed to show the meaning. If a possessive pronoun is used, it especially emphasises the fact that the subject's referent possesses the referent of the object. In the second case, in which the subject `acquires' the object, the object is not `possessed' by the subject until after the action described by the verb is completed, so a possessive pronoun is not used.\\n\\nIf a noun phrase headed by a trigger-noun is the direct object of a verb of  POSSESSION or  ACQUISITION then do not generate a possessive pronoun.\\n\\nKIN and  BODY PARTS\\n\\nIn the test set, noun phrases denoting  KIN or  BODY PARTS are modified by possessive pronouns used deictically when they are the subject of the sentence. Therefore, the pronoun is determined according to the modality of the sentence: e.g. for declarative sentences the pronoun is first person singular (giving my), whereas for imperative or interrogative sentences it is the second person (giving your).\\n\\nTwo special cases were identified. Nouns which explicitly denote PARENTS or  CHILDREN are only translated with possessive pronouns if they appear together in the same sentence. In this case, they are translated as though they are related to each other but not to the speaker. Therefore the following special rule has been implemented: Only generate a possessive pronoun for trigger nouns which explicitly denote  PARENTS or  CHILDREN if a sentence contains one of each category, in which case the first to appear is the antecedent of the second to appear.\\n\\nThe second special case was for sentences with compound subjects that include nouns that denote  KIN. For example, if the subject is me and my spouse and the person in the noun phrase in question is a member of the family other than `our' children (or grandchildren) then they will normally be either related to me or to my spouse, but not both, therefore they will be modified by my rather than our: e.g. My wife and I gave my sister a book but My wife and I gave our child a book. Similarly siblings will not normally have children or grandchildren in common so   my will be used for children and grandchildren: My sister and I gave our mother a book but My sister and I gave my child a book. These rules have not yet been implemented.\\n\\nResults\\n\\nThe evaluation was conducted by comparing the machine generated translation of the noun phrases headed by trigger-nouns with the human translations. A machine generated possessive pronoun is judged to be appropriate if it also appears in one or more of the human translations. If a pronoun is generated that does not appear in the human translations, it is judged to be not appropriate. 429 (57%) of the noun phrases headed by trigger-nouns do not require a possessive pronoun to be generated by the proposed method. For example the noun phrase phrase is non-referential, or the determiner slot is already filled, or the noun phrases was dominated by a verb of POSSESSION or  ACQUISITION. These noun phrases are all translated correctly by the `93 version as it has no special processing for generating possessive pronouns. It fails, however, to generate possessive pronouns when they are judged as necessary in the remaining 323 noun phrases (43%). Thus the accuracy of the '93 version (the number judged correct over the total number) is only 57% (429/752).\\n\\nIn the '94 version, using the proposed method, noun phrases are generated when wanted 80% of the time (the number of noun phrases with appropriate possessive pronouns generated (263) over the number of noun phrases where a possessive pronoun was judged appropriate (323)). The errors caused by not generating the appropriate pronoun are mainly due to errors in the parse selected in the analysis stage and conflicts with other rules. We estimate that overall improvements in the parsing and transfer stages can solve these problems for 30 of the noun phrases considered. Thus the estimated potential success rate is 91% (293/323).\\n\\nThe proposed method, however, introduces a new source of errors, over-generation of possessive pronouns. Possessive pronouns are inappropriately generated for 83 noun phrases headed by trigger-nouns (11% of the total number). Two solutions are proposed. First, to improve the processing that determines the noun phrase referentiality and definiteness, this would block possessive pronouns from being generated by filling the determiner slot with a more appropriate determiner. Second, to introduce explicit semantic constraints (e.g. : only generate a possessive pronoun for trigger-nouns that denote CLOTHING in the object position if the subject denotes a HUMAN), these would stop pronouns from being generated unnecessarily. We estimate that a combination of these solutions can reduce the over-generation to around 45 noun phrases (6%). To reduce the errors beyond this, we would require a discourse analysis capable of actually determining explicit possessive relationships within a local world model. Until such an analysis becomes feasible, some over-generation is inevitable with the proposed method. We make it easier to correct for this during post-editing by tagging possessive pronouns generated by the proposed method as being less reliable than possessive pronouns generated from directly from possessive expressions in the source text or transfer patterns.\\n\\nConclusion\\n\\nIn order to examine when possessive pronouns should be generated when translating between Japanese and English 6,200 Japanese sentences with English translations were examined. 657 examples of noun phrases containing possessive pronouns were found in the human translations. The existing algorithms used by the Japanese-to-English machine translation system ALT-J/E were sufficient for 46% of the noun phrases. A heuristic method for appropriately generating possessive pronouns for the remaining 54% was proposed. The method uses cue words we call trigger-nouns, along with contextual information about noun phrase referentiality and the subject and main verb of the sentence that the noun phrase appears in. The proposed method was implemented in ALT-J/E. It increased the number of noun phrases with appropriate possessive pronouns generated by 263 to 609, but at the cost of generating 83 noun phrases with inappropriate possessive pronouns. We intend to increase the number of appropriate possessive pronouns generated by resolving rule conflicts and to reduce the number of inappropriate possessive pronouns generated by adding more semantic constraints.\\n\\nAcknowledgments\\n\\nWe would like to thank Tsuneko Nakazawa for her comprehensive criticism and advice; the reviewer, Graham, Monique and Mitsuyo Bond for their comments and suggestions; and Toshiaki Nebashi, Kazuya Fukamachi and Yoshitake Ichii for their invaluable help in implementing the processing described here.\\n\\nBibliography\\n\\nBOND, FRANCIS,  KENTARO OGURA,   SATORU IKEHARA. 1994. Countability and number in Japanese-to-English machine translation. In Proceedings of the 15th International Conference on Computational Linguistics (COLING '94), 32-38. (cmp-lg/9511001).\\n\\nBOND, FRANCIS,  KENTARO OGURA,   TSUKASA KAWAOKA. 1995. Noun phrase reference in Japanese-to-English machine translation. In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI '95), 1-14. (cmp-lg/9601008).\\n\\nCORNISH, TIM,  KIMIKAZU FUJITA,   RYOCHI SUGIMURA. 1994. Towards machine translation using contextual information. In Proceedings of the 15th International Conference on Computational Linguistics (COLING '94), 51-56.\\n\\nIKEHARA, SATORU,  SATOSHI SHIRAI,   KENTARO OGURA. 1994. Criteria for evaluating the linguistic quality of Japanese to English machine translations. Journal of Japanese Society for Artificial Intelligence 9. (in Japanese).\\n\\nIKEHARA, SATORU,  SATOSHI SHIRAI,  AKIO YOKOO, HIROMI NAKAIWA. 1991. Toward an MT system without pre-editing - effects of new methods in ALT-J/E-. In Proceedings of MT Summit III, 101-106. (cmp-lg/9510008).\\n\\nMURATA, MASAKI,   MAKOTO NAGAO. 1993. Determination of referential property and number of nouns in Japanese sentences for machine translation into English. In Proceedings of the Fifth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI '93), 218-25.\\n\\nNAKAIWA, HIROMI,   SATORU IKEHARA. 1992. Zero pronoun resolution in a Japanese to English machine translation system using verbal semantic attributes. In Proceedings of the 3rd Conference on Applied Natural Language Processing (ANLP '92), 201-208.\\n\\nNAKAIWA, HIROMI,  AKIO YOKOO,   SATORU IKEHARA. 1994. A system of verbal semantic attributes focused on the syntactic correspondence between Japanese and English. In Proceedings of the 15th International Conference on Computational Linguistics (COLING '94), 672-678.\\n\\nOGURA, KENTARO,  AKIO YOKOO,  SATOSHI SHIRAI, SATORU IKEHARA. 1993. Japanese to English machine translation and dictionaries. In Proceedings of the 44th Congress of the International Astronautical Federation, Graz, Austria.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9601006.xml'}),\n",
       " Document(page_content=\"Memory-Based Lexical Acquisition and Processing\\n\\nCurrent approaches to computational lexicology in language technology are knowledge-based (competence-oriented) and try to abstract away from specific formalisms, domains, and applications. This results in severe complexity, acquisition and reusability bottlenecks. As an alternative, we propose a particular performance-oriented approach to Natural Language Processing based on automatic memory-based learning of linguistic (lexical) tasks. The consequences of the approach for computational lexicology are discussed, and the application of the approach on a number of lexical acquisition and disambiguation tasks in phonology, morphology and syntax is described.\\n\\nIntroduction\\n\\nIn computational lexicology, three basic questions guide current research: (1) which knowledge should be in the lexicon, (2) how should this knowledge be represented (e.g., to cope with the problems of lexical gaps), and (3) how can this knowledge be acquired. Current lexical research in language technology is eminently knowledge-based in this respect. It is also generally acknowledged that there exists a natural order of dependencies between these three research questions: acquisition techniques depend on the type of knowledge representation used and the type of knowledge that should be acquired, and the type of knowledge representation used depends on what should be represented.\\n\\nIn this paper, we propose an alternative approach in which a performance-oriented (behaviour-based) perspective is taken instead of a competence-oriented (knowledge-based) one. We try to automatically learn the language processing task on the basis of examples. The effect of this is that the priorities between the three goals discussed earlier are changed: the representation of the acquired knowledge depends on the acquisition technique used, and the knowledge acquired depends on what the learning algorithm has induced as being relevant in solving the task. This shift in focus introduces a new type of reusability: reusability of acquisition method rather than reusability of acquired knowledge. It also has as a consequence that it is no longer a priori evident that there should be different components for lexical and non-lexical knowledge in the internal representation of an NLP system solving a task, except when the task learned is specifically lexical.\\n\\nThe structure of the paper will be as follows. In Section 2 we will explain the differences between the knowledge-based and the behaviour-based approach to Natural Language Processing (NLP). Section 3 introduces lazy learning, the symbolic machine learning paradigm which we have used in experiments in lexical acquisition. In Section 4, we show how virtually all linguistic tasks can be redefined as a classification task, which can in principle be solved by lazy learning algorithms. Section 5 gives an overview of research results in applying lazy learning to the acquisition of lexical knowledge, and Section 6 concludes with a discussion of advantages and limitations of the approach.\\n\\nKnowledge\\n\\n\\n\\nBased versus Behaviour\\n\\n\\n\\nBased\\n\\nOne of the central intuitions in current knowledge-based NLP research is that in solving a linguistic task (like text-to-speech conversion, parsing, or translation), the more linguistic knowledge is explicitly modeled in terms of rules and knowledge bases, the better the performance.\\n\\nAs far as lexical knowledge is concerned, this knowledge is represented in a lexical knowledge base, introduced either by hand or semi-automatically using machine-readable dictionaries. The problem of reusability is dealt with by imposing standards on the representation of the knowledge, or by applying filters or translators to the lexical knowledge. Not only is there a huge and costly linguistic engineering effort involved in the building of a knowledge-based lexicon in the first place, the effort is duplicated for every translation module between two different formats of the lexical knowledge. In practice, most NLP projects therefore start lexicon construction from scratch, and end up with unrealistically few lexical items.\\n\\nIn this paper, we will claim that regardless of the state of theory-formation about some linguistic task, simple data-driven learning techniques, containing very little a priori linguistic knowledge, can lead to performance systems solving the task with an accuracy higher than state-of-the art knowledge-based systems. We will defend the view that all linguistic tasks can be formulated as a classification task, and that simple memory-based learning techniques based on a consistency heuristic can learn these classifications tasks.\\n\\nIn this approach, reusability resides in the acquisition method. The same, simple, machine learning method may be used to induce linguistic mappings whenever a suitable number of examples (a corpus) is available, and can be reused for any number of training sets representing different domains, sublanguages, languages, theoretical formalisms, and applications. In this approach, emphasis shifts from knowledge representation (competence) to induction of systems exposing useful behaviour (performance), and from knowledge engineering to the simpler process of data collection. Fig. 1 illustrates the difference between the two approaches.\\n\\nSupervised Machine Learning of Linguistic Tasks\\n\\nIn supervised Machine Learning, a learner is presented with a number of examples describing a mapping to be learned, and the learner should extract the necessary regularities from the examples and apply them to new, previously unseen input. It is useful in Machine Learning to make a distinction between a learning component and a performance component. The performance component produces an output (e.g., a syntactic category) when presented with an input (e.g., a word and its context) using some kind of representation (decision trees, classification hierarchies, rules, exemplars, ...). The learning component implements a learning method. It is presented with a number of examples of the required input-output mapping, and as a result modifies the representation used by the performance system to achieve this mapping for new, previously unseen inputs. There are several ways in which domain bias (a priori knowledge about the task to be learned) can be used to optimize learning. In the experiments to be described we will not make use of this possibility.\\n\\nThere are several ways we can measure the success of a learning method. The most straightforward way is to measure accuracy. We randomly split a representative set of examples into a training set and a test set, train the system on the training set, and compute the success rate (accuracy) of the system on the test set, i.e., the number of times the output of the system was equal to the desired output. Other evaluation criteria include learning and performance speed, memory requirements, clarity of learned representations, etc.\\n\\nLazy Learning\\n\\nVariants of Lazy Learning\\n\\nExamples are represented as a vector of feature values with an associated category label. Features define a pattern space, in which similar examples occupy regions that are associated with the same category (note that with symbolic, unordered feature values, this geometric interpretation doesn't make sense).\\n\\nDuring training, a set of examples (the training set) is presented in an incremental fashion to the classifier, and added to memory. During testing, a set of previously unseen feature-value patterns (the test set) is presented to the system. For each test pattern, its distance to all examples in memory is computed, and the category of the least distant instance is used as the predicted category for the test pattern.\\n\\nIn lazy learning, performance crucially depends on the distance metric used. The most straightforward distance metric would be the one in equation (1), where X and Y are the patterns to be compared, and\\n\\nis the distance between the values of the i-th feature in a pattern with n features.\\n\\nDistance between two values is measured using (2) for numeric features (using scaling to make the effect of numeric features with different lower and upper bounds comparable), and (3), an overlap metric, for symbolic features.\\n\\nFeature weighting\\n\\nThe main idea of information gain weighting is to interpret the training set as an information source capable of generating a number of messages (the different category labels) with a certain probability. The information entropy of such an information source can be compared in turn for each feature to the average information entropy of the information source when the value of that feature is known. Those features that reduce entropy most are most informative.\\n\\nDatabase information entropy is equal to the number of bits of information needed to know the category given a pattern. It is computed by (4), where pi (the probability of category i) is estimated by its relative frequency in the training set.\\n\\nFor each feature, it is now computed what the information gain is of knowing its value. To do this, we compute the average information entropy for this feature and subtract it from the information entropy of the database. To compute the average information entropy for a feature (5), we take the average information entropy of the database restricted to each possible value for the feature. The expression D[f=v] refers to those patterns in the database that have value v for feature f. V is the set of possible values for feature f. Finally, |D| is the number of patterns in a (sub)database.\\n\\nInformation gain is then obtained by (6), and scaled to be used as a weight for the feature during distance computation.\\n\\nFinally, the distance metric in (1) is modified to take into account the information gain weight associated with each feature.\\n\\nEven in itself, information gain may be a useful measure to discover which features are important to solve a linguistic task. Fig. 2 shows the information gain pattern for the prediction of the diminutive suffix of nouns in Dutch. In this task, features are an encoding of the two last syllables of the noun the diminutive suffix of which has to be predicted (there are five forms of this suffix in Dutch). Each part (onset, nucleus, coda) of each of the two syllables (if present) is a separate feature. For each syllable, the presence or absence of stress is coded as well. The feature information gain pattern clearly shows that most relevant information for predicting the suffix is in the rime (nucleus and coda) of the last syllable, and that stress is not very informative for this task (which conforms to recent linguistic theory about diminutive formation in Dutch).\\n\\nAdditional Extensions\\n\\nIn addition, the exemplars themselves can be weighted, based on typicality (how typical is a memory item for its category) or performance (how well is an exemplar doing in predicting the category of test patterns), storage can be minimized by keeping only a selection of examples, etc.\\n\\nLazy Learning of Linguistic Tasks\\n\\nLinguistic tasks (including lexical tasks) are context-sensitive mappings from one representation to another (e.g., from text to speech, from spelling to parse tree, from parse tree to logical form, from source language to target language etc.). These mappings tend to be many-to-many and complex because they can often only be described by conflicting regularities, sub-regularities, and exceptions.\\n\\nTo illustrate the difference between the traditional knowledge-based approach with the lazy learning approach, consider Fig. 3. Suppose a problem can be described by referring to only two features (a typical problem would need tens or hundreds of features). In a knowledge-based approach, the computational linguist looks for dimensions (features) to describe the solution space, and formulates rules which in their condition part define areas in this space and in their action part the category or solution associated with this area. Areas may overlap, which makes necessary some form of rule ordering or ``elsewhere condition'' principle.\\n\\nFor example, the two dimensions might be case and number of adjectives in some language, and the three categories might be different suffixes associated with different combinations of values for the case and number features.\\n\\nIn a lazy learning approach, on the other hand, knowledge acquisition is automatic. We start from a number of examples, which can be represented as points in feature space. This initial set of examples may contain noise, misclassifications, etc. Information-theoretic metrics like information gain basically modify this feature space automatically by assigning more or less weight to particular features (dimensions). In constructive induction, completely new feature dimensions may be introduced for separating the different category areas better in feature space. Exemplar weighting and memory compression schemes modify feature space further by removing points (exemplars) and by increasing or decreasing the ``attraction area'' of exemplars, i.e., the size of the neighbourhood of an exemplar in which this exemplar is counted as the nearest neighbour. We are finally left with a reorganized feature space that optimally separates the different categories, and provides good generalization to unseen inputs. In this process, no linguistic engineering and no handcrafting were involved.\\n\\nLinguistic Tasks as Classification\\n\\nLazy Learning is fundamentally a classification paradigm. Given a description in terms of feature-value pairs of an input, a category label is produced. This category should normally be taken from a finite inventory of possibilities, known beforehand. It is our hypothesis that all useful linguistic tasks can be redefined this way. All linguistic problems can be described as context-sensitive mappings. These mappings can be of two kinds: identification and segmentation (identification of boundaries).\\n\\nIdentification. Given a set of possibilities (categories) and a relevant context in terms of attribute values, determine the correct possibility for this context. Instances of this include part of speech tagging, grapheme-to-phoneme conversion, lexical selection in generation, morphological synthesis, word sense disambiguation, term translation, stress assignment, etc.\\n\\nSegmentation. Given a target and a context, determine whether and which boundary is associated with this target. Examples include syllabification, morphological analysis, syntactic analysis (in combination with tagging), etc.\\n\\nExamples\\n\\nThe approach proposed in this paper is fairly recent, and experiments have focused on phonological and morphological tasks rather than on tasks like term disambiguation. However, we hope to have made clear that the approach is applicable to all classification problems in NLP. In this section we briefly describe some of the experiments and hope the reader will refer to the cited literature for a more detailed description.\\n\\nSyllable Boundary Prediction\\n\\nHere the task to be solved is to decide where syllable boundaries should be placed given a word form in its spelling or pronunciation representation (the target language was Dutch). In a knowledge-based solution, we would implement well-known phonological principles like the maximal onset principle and the sonority hierarchy, as well as a morphological parser to decide on the position of morphological boundaries, some of which overrule the phonological principles. This parser requires at least lexical knowledge about existing stems and affixes and the way they can be combined.\\n\\nGrapheme\\n\\n\\n\\nto\\n\\n\\n\\nPhoneme Conversion\\n\\nGrapheme-to-phoneme conversion is a central module in text-to-speech systems. The task here is to produce a phonetic transcription given the spelling of a word. Again, in the knowledge-based approach, the lexical requirements for such a system are extensive. In a typical knowledge-based system solving the problem, morphological analysis (with lexicon), phonotactic knowledge, and syllable structure determination modules are designed and implemented.\\n\\nWord Stress Assignment\\n\\nAlthough this research was primarily intended to show that an empiricist learning method with little a priori knowledge performed better than a learning approach in the context of the ``Principles and Parameters'' framework as applied to metrical phonology, the results also showed that even in the presence of a large amount of noise (from the point of view of the learning algorithm), the algorithm succeeded in automatically extracting the major generalizations that govern stress assignment in Dutch, with no linguistic a priori knowledge except syllable structure.\\n\\nPart of Speech Tagging\\n\\nIn this as yet unpublished research, a slightly more complex learning procedure was applied to the problem of part of speech tagging (an identification problem). First, a lexicon was derived from the training set. The training set consists of a number of texts in which each word is assigned the correct part of speech tag (its category). To derive a lexicon, we find for each word how many times it was associated with which categories. We can then make an inventory of ambiguous categories, e.g., a word like man would belong to the ambiguous category noun-or-verb. The next step consists of retagging the training corpus with these ambiguous categories. Advantages of this extra step are (i) that ambiguity is restricted to what actually occurs in the training corpus (making as much use as possible of sublanguage characteristics), and (ii) that we have a much more refined measure of similarity in lazy learning: whereas non-ambiguous categories can only be equal or not, ambiguous categories can be more or less equal. For the actual tagging problem, a moving window approach was again used, using patterns of ambiguous categories (a target and a left and right context). Results are only preliminary here, but suggest a performance comparable to hidden markov modeling approaches.\\n\\nConclusion\\n\\nThere are both theoretical and practical aspects to the work described in this paper. First, as far as linguistic engineering is concerned, a new approach to the reusability problem was proposed. Instead of concentrating on linguistic engineering of theory-neutral, poly-theoretic, multi-applicable lexical representations combined with semi-automatic migration of lexical knowledge between different formats, we propose an approach in which a single inductive learning method is reused on different corpora representing useful linguistic mappings, acquiring the necessary lexical information automatically and implicitly.\\n\\nSecondly, the theoretical claim underlying this proposal is that language acquisition and use (and a fortiori lexical knowledge acquisition and use) are behaviour-based processes rather than knowledge-based processes. We sketched a memory-based lexicon with the following properties:\\n\\nThe lexicon is not a static data structure but a set of lexical processes of identification and segmentation. These processes implement lexical performance.\\n\\nEach lexical process is represented by a set of exemplars (solved cases) in memory, which act as models to new input.\\n\\nNew instances of a lexical process are solved through either memory lookup or similarity-based reasoning.\\n\\nThere is no representational difference between regularities, subregularities, and exceptions.\\n\\nRule-like behaviour is a side-effect of the operation of the similarity matching process and the contents of memory.\\n\\nThe contents of memory (the lexical exemplars) can be approximated as a set of rules for convenience.\\n\\nThere are also some limitations to the method. The most important of these is the sparse data problem. In problems with a large search space (e.g., thousands of features relevant to the task), a large amount of training patterns is necessary in order to cover the search space sufficiently. In general, this is not a problem in NLP, where for most problems large corpora are available or can be collected. Also, information gain or other feature weighting techniques can be used to automatically reduce the dimensionality of the problem, sometimes effectively solving the sparse data problem.\\n\\nAnother problem concerns long-distance dependencies, especially in syntax. The methods described often make use of a moving window approach in which only a local part of an input representation is used. Whenever important factors determining a category decision are outside the scope of a pattern, the category assignment cannot be learned. A possible solution for this problem is the cascading of different lazy learning systems, one working on the output of the other. For example, a learning system for part of speech tagging could be combined with a learning system taking patterns of disambiguated tags as input, and producing constituent types as output. Taking patterns of constituent types as input, a third learning system should have no problem assigning ``long-distance'' dependencies: given the right representation, all dependencies are local.\\n\\nBibliography\\n\\nAha, D.: A study of Instance-Based Algorithms for Supervised Learning Tasks. University of California at Irvine technical report 90-42, 1990.\\n\\nAha, D., Kibler, D. and Albert, M.: Instance-Based Learning Algorithms. Machine Learning 6, (1991) 37-66.\\n\\nVan den Bosch, A. and Daelemans, W.: `Data-oriented methods for grapheme-to-phoneme conversion.' Proceedings of the Sixth conference of the European chapter of the ACL, ACL, (1993) 45-53.\\n\\nBriscoe, T., de Paiva, V. and Copestake, A.: Inheritance, Defaults and the Lexicon. Cambridge: Cambridge University Press, 1993.\\n\\nCost, S. and Salzberg, S.: A weighted nearest neighbour algorithm for learning with symbolic features. Machine Learning 10, (1993) 57-78.\\n\\nDaelemans, W. and Gazdar, G.: (guest eds.) Special Issue Computational Linguistics on Inheritance in Natural Language Processing, 18 (2) and 18 (3), 1992.\\n\\nDaelemans, W. and van den Bosch, A.: Generalization Performance of Backpropagation Learning on a Syllabification Task. In: M.F.J. Drossaers and A. Nijholt (eds.) Connectionism and Natural Language Processing. Proceedings Third Twente Workshop on Language Technology, (1992) 27-38.\\n\\nDaelemans, W. and van den Bosch, A.: `A Neural Network for Hyphenation.' In: I. Aleksander and J. Taylor (eds.) Artificial Neural Networks II: Proceedings of the International Conference on Artificial Neural Networks. Elsevier Science Publishers, (1992) 1647-1650.\\n\\nDaelemans, W. and van den Bosch, A.: `TABTALK: Reusability in Data-oriented grapheme-to-phoneme conversion.' Proceedings of Eurospeech, Berlin, (1993) 1459-1466.\\n\\nDaelemans, W., Gillis, S., Durieux, G., van den Bosch, A.: Learnability and Markedness in Data-Driven Acquisition of Stress. In: T. Mark Ellison and James M. Scobbie (eds) Computational Phonology. Edinburgh Working Papers in Cognitive Science 8, (1993) 157-178.\\n\\nDaelemans, W., Gillis, S., and Durieux, G.: `The Acquisition of Stress, a data-oriented approach.' Computational Linguistics 20 (3), (1994) forthcoming.\\n\\nDerwing, B. L. and Skousen, R.: Real Time Morphology: Symbolic Rules or Analogical Networks. Berkeley Linguistic Society 15: (1989) 48-62.\\n\\nFriedman, J., Bentley, J., and Finkel, R., an algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software, (1977) 3 (3).\\n\\nGillis, S., Daelemans, W., Durieux, G. and van den Bosch, A.: `Learnability and Markedness: Dutch Stress Assignment.' In: Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, Boulder Colorado, USA, Hillsdale: Lawrence Erlbaum Associates, (1993) 452-457.\\n\\nKira, K. and Rendell, L.: A practical approach to feature selection. Proceedings International Conference on Machine Learning, 1992.\\n\\nKitano, H.: Challenges of massive parallelism. Proceedings IJCAI 1993, 813-834.\\n\\nKolodner, J.: Case\\n\\n\\n\\nBased Reasoning. San\\n\\n\\n\\nMateo: Morgan\\n\\n\\n\\nKaufmann. 1993.\\n\\nLing, C.: Learning the past tense of English verbs: The symbolic Pattern Associator vs. Connectionist Models. Journal of Artificial Intelligence Research 1, (1994) 209-229.\\n\\nPustejovsky, J.: Dictionary/Lexicon. In: Stuart C. Shapiro (ed. ), Encyclopedia of artificial intelligence, New York: Wiley, 1992, 341-365.\\n\\nQuinlan, J. R.: Induction Of Decision Trees. Machine Learning 1, (1986) 81-106.\\n\\nSalzberg, S.: A nearest hyperrectangle learning method. Machine Learning 6, (1990) 251-276.\\n\\nSejnowski, T. and Rosenberg, C.: NETtalk: a parallel network that learns to read aloud. Complex Systems 1, (1986) 145-168.\\n\\nSimmons, R. and Yu, Y.: The acquisition and use of context-dependent grammars for English. Computational Linguistics 18 (3) (1992), 391-418.\\n\\nSmith, E. and Medin, D.: Categories and Concepts. Cambridge, MA: Harvard University Press, 1981.\\n\\nSkousen, R.: Analogical Modeling of Language. Dordrecht: Kluwer, 1989.\\n\\nStanfill, C. and Waltz, D.L. : Toward Memory-based Reasoning. Communications of the ACM (1986) 29: 1213-1228.\\n\\nWeiss, S. and Kulikowski, C.: Computer systems that learn. San-Mateo: Morgan Kaufmann, 1991.\\n\\nWinston, P.: Artificial Intelligence. Reading Mass. : Addison-Wesley, 1992.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9405018.xml'}),\n",
       " Document(page_content=\"Off\\n\\n\\n\\nline Optimization for Earley\\n\\n\\n\\nstyle HPSG Processing\\n\\nA novel  approach to   HPSG based natural language processing  is described  that  uses an  off-line compiler to  automatically  prime a declarative grammar  for generation or parsing, and inputs  the primed grammar to an advanced Earley-style processor. This way we provide an elegant  solution to  the  problems  with  empty  heads and  efficient bidirectional processing which is illustrated for the special  case of HPSG generation. Extensive  testing  with  a large   HPSG grammar  revealed  some  important  constraints  on  the  form  of the grammar.\\n\\nIntroduction\\n\\nShieber  (1988)  gave   the  first  use  of  Earley's  algorithm   for generation,  but this algorithm  does not use  the  prediction step to restrict feature  instantiations on the  predicted  phrases, and  thus lacks goal-directedness. Though Gerdemann (1991) showed how to modify the  restriction  function to make top-down  information available for the  bottom-up  completion   step,  Earley  generation  with  top-down prediction still has  a problem in that generating  the  subparts of a construction in the  wrong order  might lead to massive nondeterminacy or even nontermination. Gerdemann (1991) partly overcame this problem by  incorporating a  head-driven  strategy  into  Earley's  algorithm. However, evaluating the head of a construction  prior to its dependent subparts still suffers  from  efficiency problems when the  head of  a construction  is  either   missing,   displaced   or   underspecified. Furthermore,  Martinovic  and  Strzalkowski  (1992)  and  others  have observed that a simple head-first reordering of  the grammar rules may still  make   insufficient   restricting   information  available  for generation unless  the  form of the  grammar is restricted to unary or binary rules.\\n\\nStrzalkowski's Essential  Arguments Approach ( EAA; 1993b)  is  a top-down approach  to generation and  parsing with logic grammars that uses  off-line  compilation  to automatically  invert  parser-oriented logic  grammars. The inversion process consists of both the automatic static  reordering of nodes in the  grammar, and  the interchanging of arguments in rules with recursively defined heads. It is based on the notion  of  essential  arguments,  arguments   which   must  be instantiated to ensure the efficient  and terminating execution  of  a node. Minnen  et   al. (1995)  observe  that  the     EAA  is computationally infeasible, because  it  demands  the investigation of almost  all  possible  permutations  of  a  grammar. Moreover,   the interchanging of arguments  in  recursive  procedures as  proposed  by Strzalkowski  fails  to guarantee that input  and output grammars  are semantically equivalent. The Direct Inversion Approach ( DIA) of Minnen  et  al. (1995)  overcomes  these  problems   by  making  the reordering process more  goal-directed and developing a  reformulation technique that allows the successful treatment  of rules which exhibit head-recursion. Both  the  EAA and the   DIA were presented as approaches  to  the  inversion  of  parser-oriented  grammars  into grammars  suitable for generation.\\n\\nHowever, both approaches  can just as well take a declarative grammar specification  as  input to produce generator  and/or  parser-oriented  grammars  as  in Dymetman  et  al. (1990). In  this  paper  we  adopt  the  latter  theoretically  more interesting perspective.\\n\\nAdvanced Earley Generation\\n\\nOptimizations\\n\\nWe further improved a  typed extension of Gerdemann's Earley generator with  a number of  techniques that reduce the number  of edges created during generation. Three optimizations were especially helpful. The first  supplies each edge  in  the chart  with  two  indices,  a  backward  index pointing to the state in the chart  that the edge  is predicted from, and a forward index pointing to the states  that are  predicted  from  the  edge. By matching  forward  and  backward indices, the edges that must be combined for completion can be located faster. This indexing technique, as illustrated below, improves  upon the more complex indices in Gerdemann (1991)  and  is  closely related  to OLDT-resolution (Tamaki and Sato, 1986).\\n\\nActive edge 2 resulted  from active edge  1 through  prediction. The backward index of  edge 2 is  therefore  identified  with the  forward index of edge 1. Completion of  an active edge results in an edge with identical  backward  index. In the case of our example, this would be the steps from edge 2 to edge 3 and edge 3 to edge 4. As nothing gets predicted from a passive edge (4),  it does not have a forward  index. In order to use passive  edge 4  for completion of an  active edge, we only need to consider those edges which have a forward index identical to the backward index of 4.\\n\\nThe second optimization  creates a table of  the categories which have been used to make predictions from. As discussed in Gerdemann (1991), such a table can be used to avoid redundant predictions without a full and expensive subsumption  test. The third  indexes  lexical entries which is necessary to obtain constant-time lexical access.\\n\\nThe optimizations of our Earley-generator lead to significant gains in efficiency. However,  despite  these   heuristic  improvements,  the problem of goal-directedness is not solved.\\n\\nEmpty Heads\\n\\nEmpty  or  displaced  heads  present  the principal  goal-directedness problem for any head-driven generation approach (Shieber et al., 1990; Knig, 1994; Gerdemann and  Hinrichs, in  press),  where empty  head refers not  just  to a  construction in which the  head  has  an empty phonology,  but to any  construction in  which the head  is  partially unspecified. Since  phonology   does  not   guide  generation,  the phonological realization of  the head of a construction plays no  part in  the generation  of that construction. To  better  illustrate  the problem that underspecified heads pose, consider the sentence:\\n\\nHat Karl Marie gekt?\\n\\nHas Karl Marie kissed?\\n\\n``Did Karl kiss Mary?''\\n\\nfor  which  we  adopt the  argument composition  analysis presented  in Hinrichs  and Nakazawa  (1989): the  subcat list of  the auxiliary verb is  partially  instantiated  in  the  lexicon  and only becomes  fully  instantiated  upon  its  combination  with  its verbal complement,  the main verb. The phrase structure rule that  describes  this construction is\\n\\nThough a  head-driven generator must generate first  the  head of  the rule, nothing prescribes the order of generation of the complements of the  head. If the generator generates second the main verb  then the subcat list of the main verb instantiates the subcat list of the head, and generation  becomes a deterministic procedure in which complements are generated in sequence. However, if the generator generates second some complement  other than the main verb, then the subcat list of the head  contains  no  restricting  information  to  guide  deterministic generation, and  generation  becomes a  generate-and-test procedure in which complements  are generated at random, only to  be  eliminated by further unifications. Clearly then,  the order of  evaluation of  the complements  in  a rule  can  profoundly  influence the efficiency  of generation, and an  efficient  head-driven  generator  must  order the evaluation of the complements in a rule accordingly.\\n\\nOff\\n\\n\\n\\nline versus On\\n\\n\\n\\nline\\n\\nDynamic, on-line reordering  can solve  the ordering problem discussed in the previous  subsection, but is rather unattractive:  interpreting grammar  rules  at  run  time   creates  much  overhead,  and  locally determining  the  optimal   evaluation   order  is  often  impossible. Goal-freezing can also  overcome the ordering  problem, but is equally unappealing: goal-freezing is  computationally expensive,  it  demands the  procedural  annotation   of  an   otherwise  declarative  grammar specification,  and  it presupposes  that  a  grammar writer possesses substantial  computational processing expertise. We chose  instead to deal  with the  ordering problem  by  using  off-line  compilation  to automatically  optimize  a  grammar  such  that it  can  be  used  for generation,  without  additional   provision  for  dealing   with  the evaluation order, by our Earley generator.\\n\\nOff\\n\\n\\n\\nline Grammar Optimization\\n\\nOur off-line grammar optimization is based on  a generalization of the dataflow analysis employed in the  DIA to a dataflow analysis for typed  feature  structure  grammars. This dataflow analysis  takes as input a  specification of the  paths  of the  start category that  are considered fully instantiated. In case of generation, this means that the user  annotates the path specifying the logical  form,  i.e.,  the path cont (or some of its subpaths), as  bound. We use  the type  hierarchy and  an  extension  of the unification and generalization operations such that path annotations are preserved, to determine the flow of (semantic) information between the  rules and the  lexical entries in  a grammar. Structure sharing  determines the dataflow within the rules of the grammar.\\n\\nThe  dataflow analysis is used to determine the relative efficiency of a particular evaluation order  of the right-hand side categories  in a phrase structure rule by  computing  the  maximal  degree  of nondeterminacy introduced by the evaluation of each of these categories. The maximal degree of nondeterminacy introduced by a right-hand side category equals the maximal number  of  rules  and/or  lexical  entries  with  which  this category   unifies   given  its  binding  annotations. The  optimal evaluation order of the right-hand side categories is found by comparing the maximal degree of nondeterminacy introduced  by the evaluation  of the individual categories with the degree of nondeterminacy the grammar is allowed  to introduce: if the  degree of nondeterminacy introduced  by the evaluation  of one  of  the right-hand side categories in a rule exceeds the  admissible  degree  of  nondeterminacy the  ordering  at hand  is rejected. The  degree  of  nondeterminacy the  grammar is allowed  to introduce is originally set to one and consecutively incremented until the optimal evaluation order for all rules in the grammar is found.\\n\\nExample\\n\\nProcessing Head\\n\\nConstraints on Grammar\\n\\nComplement Displacement\\n\\nGeneralization\\n\\nConcluding Remarks\\n\\nAn innovative approach to  HPSG processing is described that uses an off-line compiler to automatically prime  a declarative grammar for generation  or parsing,  and inputs the  primed grammar to an advanced Earley  processor. Our  off-line  compiler  extends  the  techniques developed in the context of the   DIA  in that it  compiles typed feature structure  grammars, rather than simple  logic grammars. The approach  allows  efficient  bidirectional   processing  with  similar generation  and  parsing times. It is shown  that  combining  off-line techniques with an advanced Earley-style generator provides an elegant solution to the general problem that empty or displaced heads pose for conventional head-driven generation.\\n\\nThe developed off-line compilation  techniques make crucial use of the fundamental properties of the  HPSG formalism. The monostratal, uniform treatment of syntax, semantics and phonology supports dataflow analysis, which is used extensively to provide  the  information  upon which  off-line compilation  is  based. Our  compiler  uses  the type hierarchy to determine paths with a  value of  a minimal type  without appropriate features  as bound. However, the equivalent of this kind of minimal types in  untyped feature structure grammars  are constants which can be used in a similar fashion for off-line optimization.\\n\\nBibliography\\n\\nAppelt, Douglas. 1987. Bidirectional Grammars  and  the Design of Natural Language Generation Systems. In  Proceedings of  TINLAP-3, Las Cruces, New Mexico,  USA.\\n\\nDymetman, Marc; Pierre Isabelle and Franois Perrault. 1990. A Symmetrical Approach to Parsing and Generation. In Proceedings of  COLING-90, Helsinki, Finland.\\n\\nGerdemann, Dale. 1991. Parsing and Generation of Unification Grammars. Doctoral  dissertation. University of Illinois. Published as  Beckman Institute Cognitive  Science technical report,  number  CS-91-06. Urbana-Champaign, Illinois,  USA.\\n\\nGerdemann, Dale and Erhard  Hinrichs. in  press. Some Open Problems in Head-driven Generation. In  Linguistics  and  Computation. CSLI  Lecture Notes. Stanford, California,  USA.\\n\\nGerdemann, Dale and   Paul King. 1994. The   Correct   and   Efficient   Implementation   of  Appropriateness Specifications for Typed Feature Structures. In Proceedings of  COLING-94, Kyoto, Japan.\\n\\nHinrichs, Erhard and Tsuneko Nakazawa. 1989. Subcategorization and  VP Structure in German. Paper  presented to the '3rd  Symposium  on  Germanic  Linguistics' at Purdue University. Published  as   SFB  340  technical report. Tbingen, Germany.\\n\\nHinrichs, Erhard; Detmar Meurers and Tsuneko  Nakazawa. 1994. Partial  VP  and  Split  NP  Topicalization  in  German:  An HPSG Analysis and its Implementation. SFB 340 technical report nr. 58. Tbingen, Germany.\\n\\nKnig, Esther. 1994. Syntactic-Head-Driven  Generation. In Proceedings of  COLING-94. Kyoto, Japan\\n\\nMartinovic, Miroslav and Tomek Strzalkowski. 1992. Comparing Two Grammar-based Generation Algorithms: A Case Study. In  Proceedings of    ACL-92, Newark, Delaware,  USA.\\n\\nMellish, Chris. 1981. The Automatic Generation of Mode Declarations for Prolog Programs. In  Proceedings   of  the  Workshop  on  Logic  Programming  and Intelligent Systems, Los Angeles, California,  USA.\\n\\nMinnen, Guido; Dale  Gerdemann  and Erhard   Hinrichs. 1995. Direct Automated Inversion of Logic Grammars. In  New  Generation  Computing, volume 13, number 2.\\n\\nPereira, Fernando and  Stuart Shieber. 1987. Prolog and Natural Language Analysis. CSLI Lecture Notes, number 10. Stanford, California,  USA.\\n\\nPollard, Carl and Ivan Sag. 1994. Head-driven Phrase Structure Grammar. University of Chicago Press, Chicago, Illinois,  USA.\\n\\nShieber, Stuart. 1988. A Uniform Architecture for Parsing and Generation. In Proceedings of  COLING-88. Budapest, Hungary.\\n\\nShieber, Stuart; Gertjan van  Noord; Fernando Pereira and Robert Moore. 1990. Semantic Head-Driven Generation. In Computational Linguistics, volume 16, number 1.\\n\\nStrzalkowski, Tomek (editor). 1993a. Reversible  Grammar  in  Natural Language  Processing. Kluwer Academic Publishers, Dordrecht, The Netherlands.\\n\\nStrzalkowski, Tomek. 1993b. A General Computational Method for Grammar Inversion. In  Strzalkowski 1993a.\\n\\nTamaki, Hisao and Taisuke Sato 1986. OLD Resolution with Tabulation. In  Proceedings of the Third International  Conference on  Logic Programming. Berlin, Germany\\n\\nFootnotes\\n\\nThe  presented   research  was  sponsored  by  Teilprojekt     B4 ``Constraints   on   Grammar   for   Efficient   Generation''  of  the Sonderforschungsbereich 340  ``Sprachtheoretische Grundlagen fr die Computerlinguistik''  of  the  Deutsche  Forschungsgemeinschaft. The authors wish to thank Paul King, Detmar Meurers and Shuly Wintner  for valuable  comments  and   discussion. Of  course,   the  authors  are responsible for all remaining errors. For expository reasons, we refrain from a division between the subject and the other complements of a verb as in chapter 9 of Pollard and Sag (1994). The  test-grammar  does  make   this  division   and  always guarantees the correct  order  of the complements  on the  comps list with respect to the  obliqueness  hierarchy. Furthermore, we use abbreviations    of   paths,   such   as   cont   for    synsem|loc|cont,  and assume  that  the  semantics  principle  is encoded in the phrase structure rule.\", metadata={'source': '../data/raw/cmplg-xml/9502005.xml'}),\n",
       " Document(page_content=\"EXPLORING THE ROLE OF PUNCTUATION IN PARSING NATURAL TEXT\\n\\nFew, if any, current NLP systems make any significant use of punctuation. Intuitively, a treatment of punctuation seems necessary to the analysis and production of text. Whilst this has been suggested in the fields of discourse structure, it is still unclear whether punctuation can help in the syntactic field. This investigation attempts to answer this question by parsing some corpus-based material with two similar grammars -- one including rules for punctuation, the other ignoring it. The punctuated grammar significantly out-performs the unpunctuated one, and so the conclusion is that punctuation can play a useful role in syntactic processing.\\n\\nINTRODUCTION\\n\\nThere are no current text based natural language analysis or generation systems that make full use of punctuation, and while there are some that make limited use, like the Editor's Assistant [Dale 1990], they tend to be the exception rather than the rule. Instead, punctuation is usually stripped out of the text before processing, and is not included in generated text.\\n\\nIntuitively, this seems very wrong. Punctuation is such an integral part of written language that it is difficult to imagine naturally producing any significant body of unpunctuated text, or being able to easily understand any such body of text.\\n\\nHowever, this is what has been done in the computational linguistics field. The reason that it has always been too difficult to incorporate any coherent account of punctuation into any system is because no such coherent account exists.\\n\\nPunctuation has long been considered to be intimately related to intonation: that is that different punctuation marks simply give the reader cues as to the possible prosodic and pausal characteristics of the text [Markwardt, 1942]. This claim is questioned by Nunberg [1990], since such a transcriptional view of punctuation is theoretically uninteresting, and also correlates rather badly with intonation in any case.\\n\\nHowever, even if we recognise that punctuation fulfils a linguistic role of its own, it is by no means clear how this role is defined. Since there is still no concise linguistic account of the function of punctuation, we have to rely mainly on personal intuitions. This in turn introduces new problems, since there is a great deal of idiosyncrasy associated with the use of punctuation marks. Whilst most people may agree on core situations in which use of a given punctuation mark is desirable, or even necessary, there are still many situations where their use is less clear.\\n\\nIn his recent review, Humphreys [1993] suggests that accounts of punctuation fall into three categories: ``The first ...is selflessly dedicated to the task of bringing Punctuation to the Peasantry ...The second sort is the Style Guide, written by editors and printers for the private pleasure of fellow professionals ...The third, on the linguistics of the punctuation system, is much the rarest of all.''\\n\\nThus whilst we do not really want to rely on publishers' style guides, since the accounts of punctuation they contain are rather too proscriptive and concentrate on the use of punctuation rather than its meaning, the academic accounts of punctuation are far from numerous. In the work of Dale [1991], the potential of punctuation in the field of discourse and natural language generation is explored. However, little mention is made anywhere of the role of punctuation within a syntactic framework. Therefore the current investigation tries to determine whether taking consideration of punctuation can further the goals of syntactic analysis of natural language.\\n\\nPUNCTUATION\\n\\nPunctuation, as we consider it, can be defined as the central part of the range of non-lexical orthography. Although arguments could be made for including the sub-lexical marks (e.g. hyphens, apostrophes) and structural marks (e.g. bullets in itemisations), they are excluded since they tend to be lexicalised or rather difficult to represent, respectively. Indeed, it is difficult to imagine the representation of structural punctuation, other than through the use of some special structural description language such as SGML.\\n\\nWithin our definition of punctuation then, we find broadly three types of mark: delimiting, separating and disambiguating, as described by Nunberg [1990]. Some marks, the comma especially, fall into multiple categories since they can have different roles, and the categories each perform distinct linguistic functions.\\n\\nDelimiters (e.g. comma, dash, parenthesis) occur to either side of a particular lexical expression to remove that expression from the immediate syntactic context of the surrounding sentence (1). The delimited phrase acts as a modifier to the adjacent phrase instead.\\n\\nSeparating marks come between similar grammatical items and indicate that the items form a list (2). They are therefore similar to conjunctions in their behaviour, and can sometimes replace conjunctions in a list.\\n\\nDisambiguating marks, usually commas, occur where an unintentional ambiguity could result if the marks were not there (3), and so perhaps illustrate best why the use of punctuation within NL systems could be beneficial.\\n\\nIn addition to the nature of different punctuation marks, there are several phenomena described by Nunberg [1990] which it is useful to consider before implementing any treatment of punctuation: sep 0mm\\n\\nPoint absorption: strong point symbols (comma, dash, semicolon, etc.) absorb weaker adjacent ones (4). Commas are least powerful, and periods most powerful;\\n\\nBracket absorption: commas and dashes are removed if they occur directly before an end quote or parenthesis (5);\\n\\nQuote transposition: punctuation directly to the right of an end quote is moved to the left of that character (6). This phenomenon occurs chiefly in American English, but can occur generally;\\n\\nGraphic absorption: orthographically, but not linguistically, similar coincident symbols are absorbed (7). Thus the dot marking an abbreviation will absorb an adjacent period whereas it would not absorb an adjacent comma.\\n\\nIn addition to the phenomena associated with the interaction of punctuation, there are also distinct phenomena observable in the interaction of punctuation and lexical expressions. Thus delimited phrases cannot immediately contain delimited phrases of the same type (the sole exception may be with parentheticals, though many people object to nested parentheses) and adjuncts such as the colon-expansion cannot contain further similar adjuncts. Therefore, in the context of colon and semicolon scoping, (8) is ambiguous, but (9) is not.\\n\\nTHE GRAMMAR\\n\\nRecognition of punctuational phenomena does not imply that they can be successfully encoded into a NL grammar, or whether the use of such a punctuated grammar will result in any analytical advantages.\\n\\nNunberg [1990] advocates two separate grammars, operating at different levels. A lexical grammar is proposed for the lexical expressions occurring between punctuation marks, and a text grammar is proposed for the structure of the punctuation, and the relation of those marks to the lexical expressions they separate. The text grammar has within it distinct levels, such as phrasal and clausal, at which distinct punctuational phenomena can occur.\\n\\nThis should, in theory, make for a very neat system: the lexical syntactic processes being kept separate from those that handle punctuation. However, in practice, this system seems unlikely to succeed since in order to work, the lexical expressions that occur between punctuation marks must carry additional information about the syntactic categories occurring at their edges so that the text grammar can constrain the function of the punctuation marks.\\n\\nFor example, if a sentence includes an itemised noun phrase (10), the lexical expression before the comma must be marked as ending with a noun phrase, and the lexical expression after the comma must be marked as starting with a noun phrase. A rule in the text grammar could then process the separating comma as it clearly comes between two similar syntactic elements.\\n\\nHowever, as (11) shows, the separating comma concept could require information about the categories at arbitrarily deep levels occurring at the ends of lexical expressions surrounding punctuation marks.\\n\\nEven with the above edge-category information, the parsing process is not necessarily made any easier (since often the full partial parses of all the separate expressions have to be held and joined). Therefore we seem to be at no advantage if we use this approach. In addition, it is difficult to imagine what linguistic or psychological motivation such a separation of punctuation from lexical text could hold, since it seems rather unlikely that people process punctuation at a separate level to the text it surrounds.\\n\\nHence it seems more sensible to use an integrated grammar, which handles both words and punctuation. This lets us describe the interaction of punctuation and lexical expressions far more logically and concisely than if the two were separated. Good examples of this are disambiguating commas; in a unified grammar we can simply write rules with an optional comma among the daughters (12).\\n\\nA feature-based tag grammar was written for this investigation (based loosely on one written by Briscoe and Waegner [1992]), and used in conjunction with the parser included in the Alvey Tools' Grammar Development Environment (GDE) [Carroll et al, 1991], which allows for rapid prototyping and easy analysis of parses. It should be stressed that this grammar is solely one of tags, and so is not very detailed syntactically.\\n\\nIn order to handle the additional complications of punctuation, the notion of stoppedness of a category has been introduced. Thus every category in the grammar has a stop feature which describes the punctuational character following it (13), and defaults to [st -] (unstopped) if there is no such character.\\n\\nSince the rules of the grammar further dictate that the mother category inherits the stop value of its rightmost daughter, only rules to specifically add punctuation for categories which could be lexicalised are necessary. Thus a rule for the additional of a punctuation mark after a lexicalised noun would be as in (14). (The calligraphic letters represent unification variables.)\\n\\nWe can then specify that top level categories must be [st f] (period), that items in a list should be [st c] (comma), etc. In rules where we want to force a particular punctuation mark to the right of a category, that mark can be included in the rule, with the preceding category unstopped: (15) illustrates the addition of a comma-delimited noun phrase to a noun phrase. Specifically mentioning the punctuation mark prevents the delimited phrase from being unstopped, resulting in an unstopped mother category. Note that the phenomenon of point absorption has been captured by unifying the value of the st feature of the mother and the identity of the final punctuation mark. Thus the possible values of st are all the possible values of punc in addition to [st -].\\n\\nThus the stop feature seems sufficient to cope with the punctuational phenomena introduced above. In order to incorporate the phenomena of interaction between punctuation and lexical expressions (e.g. preventing immediate nesting of similar delimited phrases), we need to introduce a small number of additional features into the grammar. If, for example, we make a comma-delimited noun phrase [cm +], we can then stipulate that any noun phrase that includes a comma-delimited phrase has the feature [cm -], so that the two cannot unify (16). Note that the unification of mother and right-most daughter stop values is omitted for clarity of presentation.\\n\\nWe can incorporate the relative scoping of colons and semicolons, as discussed previously, into the grammar very easily too. The semicolon rule (17) accepts any value of co in its arguments, but the colon rule (18) only accepts [co -]. The mother category of the colon rule bears the feature [co +] to prevent inclusion into further colon-bearing sentences. Note that there are more versions of the colon rule, which deal with different constituents to either side of the colon, and also that, since the GDE does not permit the disjunction of feature values, the semicolon rule is merely an abbreviation of the multiple rules required in the grammar. Stop unification is again omitted.\\n\\nHence the inclusion of a few simple extra features in a normal grammar has achieved an acceptable treatment of punctuational phenomena. Since this work only represents the initial steps of providing a full and proper account of the role of punctuation, no claims are made for the theoretical validity or completeness of this approach!\\n\\nTHE CORPUS\\n\\nFor the current investigation it was necessary to use a corpus sufficiently rich in punctuation to illustrate the possible advantages or disadvantages of utilising punctuation within the parsing process. Obviously a sentence which includes no punctuation will be equally difficult to parse with both punctuated and unpunctuated grammars. Similarly, for sentences including only one or two marks of punctuation, the use of punctuation is likely to be rather procedural, and hence not necessarily very revealing.\\n\\nTherefore the tagged Spoken English Corpus was chosen [Taylor Knowles, 1988]. This features some very long sentences, and includes rich and varied punctuation. Since the corpus has been punctuated manually, by several different people, some idiosyncrasy occurs in the punctuational style, but there is little punctuation which would be deemed inappropriate to the position it occurs in.\\n\\nA subset of 50 sentences was chosen from the whole corpus. Between them these sentences include material taken from news broadcasts, poetry readings, weather forecasts and programme reviews, so a wide variety of language is covered.\\n\\nThe lengths of the sentences varied from 3 words to 63 words, the average being 31 words; and the punctuational complexity of the sentences varied from one mark (just a period) to 16 marks, the average being 4 punctuation marks. A sample tagged sentence is shown in (19), where fs denotes a period.\\n\\nThe punctuated grammar, developed with this subset of the corpus, was used to parse the corpus subset, and then an unpunctuated version of the same grammar was used to parse the same subset. The reason that testing was performed on the training corpus was that, in the absence of a complete treatment of punctuation, the punctuational phenomena in the training corpus were the only ones the grammar could work with, and although they included almost all of the core phenomena mentioned, slightly different instances of the same phenomena could cause a parse failure. For reference, a small set of novel sentences were also parsed with the grammars, to determine their coverage outside the closed test.\\n\\nThe unpunctuated version of the grammar was prepared by removing all the features relating to specifically punctuational phenomena, and also removing explicit mention of punctuation marks from the rules. This, of course, left behind certain rules that were functionally identical, and so duplicate rules were removed from the grammar. Similarly for rules which performed the same function at different levels in the grammar (e.g. attachment of prepositions to the end of a sentence with a comma was also catered for by rules allowing prepositions to be attached to noun and verb phrases without a comma).\\n\\nRESULTS\\n\\nResults of parsing with the punctuated grammar were very good, yielding, on average, a surprisingly small number of parses. The number of parses ranged from 1 to 520, with an average of 38. This average is unrepresentatively high, however, since only 4 sentences had over 50 parses. These were, in general, those with high numbers of punctuation marks, all containing at least 5, as in (20). Ignoring the four smallest and four largest results then, the average number of parses is reduced to just 15. Example (21) is more representative of parsing. On examination, a great number of the ambiguities seem to be due to inaccuracies or over-generality in the lexical tags assigned to words in the corpus. The word more, for example, is triple ambiguous as determiner, adjective and noun, irrespective of where it occurs in a sentence.\\n\\nBesides the ambiguity of corpus tags, a problem arose with words that had been completely mistagged. If these caused the parse to fail completely, the tag was changed in the development phase of the grammar, but even so, the number of complete mistags was rather small in the sub-corpus used: around 10 words in the 50 sentences used.\\n\\nOn examination of the grammar and the corpus, it is possible to understand why this has happened. The punctuated grammar had to allow for sentences including comma-delimited noun phrases adjacent to undelimited noun phrases, as illustrated by the rules (15) and (16). These are relatively easy to mark and recognise when the punctuation is available. However, without punctuational clues, and with the under-specific tagging system, any compound noun could appear as a set of delimited noun phrases with the unpunctuated grammar.\\n\\nTherefore the unpunctuated grammar was further trimmed, to such an extent that parses no longer accurately reflected the linguistic structure of the sentences, since, for example, comma delimited noun phrases and compound nouns became indistinguishable. Some manual preparation of the sentences was also carried out to prevent the reoccurrance of simple, but costly, misparses.\\n\\nThe results of the parse now became much more tractable. For basic sentences, as predicted, there was little difference in the performance of punctuated and unpunctuated grammars. Results were within an order of magnitude, showing that no significant advantage was gained through the use of punctuation. The sentences in (23) and (24) received 1 and 11 parses respectively with the unpunctuated grammar.\\n\\nFor the most complex sentences, however, the number of parses with the unpunctuated grammar was typically more than two orders of magnitude higher than with the punctuated grammar. The sentence in (25) had 12,096 unpunctuated parses.\\n\\nParsing a set of ten previously unseen punctuationally complex sentences with the punctuated grammar resulted in seven of the ten being unparsable. The other three parsed successfully, with the number of parses falling within the range of the results of the first part of the investigation. The parse failures, on examination, were due to novel punctuational constructions occurring in the sentences which the grammar had not been designed to handle. Parsing the unseen sentences with the unpunctuated grammar resulted in one parse failure, with the results for the other 9 sentences reflecting the previous results for complex sentences.\\n\\nDISCUSSION\\n\\nThis investigation seems to support the original premise -- that inclusion and use of punctuational phenomena within natural language syntax can assist the general aims of natural language processing.\\n\\nWe have seen that for the simplest sentences, use of punctuation gives us little or no advantage over the more simple grammar, but, conversely, does no harm and can reflect the actual linguistic construction a little more accurately.\\n\\nFor the longer sentences of real language, however, a grammar which makes use of punctuation massively outperforms an otherwise similar grammar that ignores it. Indeed, it is difficult to see how any grammar that takes no notice of punctuation could ever become successful at analysing such sentences unless some huge amount of semantic and pragmatic knowledge is used to disambiguate the analysis.\\n\\nHowever, as was shown by the attempt at parsing the novel sentences, knowledge of the role of punctuation is still severely limited. The grammar only performed reliably on those punctuational phenomena it had been designed with. Unexpected constructs caused it to fail totally. Therefore, following the recognition that punctuation can play a crucial role in natural language syntax, what is needed is a thorough investigation into the theory of punctuation. Then theoretically based analyses of punctuation can play a full and important part in the analysis of language.\\n\\nACKNOWLEDGEMENTS\\n\\nThis work was carried out under Esprit Acquilex-II, BRA 7315, and an ESRC Research Studentship, R00429334171. Thanks for instructive and helpful comments to Ted Briscoe, John Carroll, Robert Dale, Henry Thompson and anonymous CoLing reviewers.\\n\\nFootnotes\\n\\nThroughout this paper I shall refer to sentence-final dots as periods rather than full-stops, to avoid confusion.\", metadata={'source': '../data/raw/cmplg-xml/9505024.xml'}),\n",
       " Document(page_content='Encoding Lexicalized Tree Adjoining Grammars with a Nonmonotonic Inheritance Hierarchy\\n\\nThis paper shows how DATR, a widely used formal language for lexical knowledge representation, can be used to define an LTAG lexicon as an inheritance hierarchy with internal lexical rules. A bottom-up featural encoding is used for LTAG trees and this allows lexical rules to be implemented as covariation constraints within feature structures. Such an approach eliminates the considerable redundancy otherwise associated with an LTAG lexicon.\\n\\nIntroduction\\n\\nThere are also several further benefits to be gained from using an established general purpose  LKRL such as DATR. First, it makes it easier to compare the resulting LTAG lexicon with those associated with other types of lexical syntax: there are existing DATR lexicon fragments for HPSG, PATR and Word Grammar, among others. Second, DATR is not restricted to syntactic description, so one can take advantage of existing analyses of other levels of lexical description, such as phonology, prosody, morphology, compositional semantics and lexical semantics. Third, one can exploit existing formal  and implementation work on the language.\\n\\nRepresenting LTAG trees\\n\\nOnce we adopt this representational strategy, writing an LTAG lexicon in DATR becomes similar to writing any other type of lexicalist grammar\\'s lexicon in an inheritance-based  LKRL. In HPSG, for example, the subcategorisation frames are coded as lists of categories, whilst in LTAG they are coded as trees. But, in both cases, the problem is one of concisely describing feature structures associated with lexical entries and relationships between lexical entries. The same kinds of generalization arise and the same techniques are applicable. Of course, the presence of complete trees and the fully lexicalized approach provide scope for capturing generalizations lexically that are not available to approaches that only identify parent and sibling nodes, say, in the lexical entries.\\n\\nEncoding lexical entries\\n\\nThis basic organisational structure can be expressed as the following DATR fragment: VERB: [] == TREENODE [cat] == v [type] == anchor [parent] == VPTREE:[]. VERB+NP: [] == VERB [right] == NPCOMP:[]. VERB+NP+PP: [] == VERB+NP [right right] == PTREE:[] [right right root] == to. VERB+NP+NP: [] == VERB+NP [right right] == NPCOMP:[]. Die: [] == VERB [root] == die. Eat: [] == VERB+NP [root] == eat. Give: [] == VERB+NP+PP [root] == give. Spare: [] == VERB+NP+NP [root] == spare. Ignoring for the moment the references to TREENODE, VPTREE, NPCOMP and PTREE (which we shall define shortly), we see that VERB defines basic features for all verb entries (and can be used directly for intransitives such as Die), VERB+NP inherits from VERB but adds an  NP complement to the right of the verb (for transitives), VERB+NP+PP inherits from VERB+NP but adds a further PP complement and so on. Entries for regular verb lexemes are then minimal - syntactically they just inherit everything from the abstract definitions.\\n\\nThis DATR fragment is incomplete, because it neglects to define the internal structure of the TREENODE and the various subtree nodes in the lexical hierarchy. Each such node is a description of an LTAG tree at some degree of abstraction. The following DATR statements complete the fragment, by providing definitions for this internal structure: TREENODE: [] == undef [type] == internal. STREE: [] == TREENODE [cat] == s. VPTREE: [] == TREENODE [cat] == vp [parent] == STREE:[] [left] == NPCOMP:[]. NPCOMP: [] == TREENODE [cat] == np [type] == substitution. PPTREE: [] == TREENODE [cat] == pp. PTREE: [] == TREENODE [cat] ==  p [type] == anchor [parent] == PPTREE:[] Here, TREENODE represents an abstract node in an LTAG tree and provides a (default) type of internal. Notice that VERB is itself a TREENODE (but with the nondefault type anchor), and the other definitions here define the remaining tree nodes that arise in our small lexicon: VPTREE is the node for VERB\\'s parent, STREE for VERB\\'s grandparent, NPCOMP defines the structure needed for  NP complement substitution nodes, etc.\\n\\nTaken together, these definitions provide a specification for Give just as we had it before, but with the addition of type and root features. They also support some other verbs too, and it should be clear that the basic technique extends readily to a wide range of other verbs and other parts of speech. Also, although the trees we have described are all initial trees (in LTAG terminology), we can describe auxiliary trees, which include a leaf node of type foot just as easily. A simple example is provided by the following definition for auxiliary verbs: AUXVERB: [] == TREENODE [cat] == v [type] == anchor [parent cat] == vp [right cat] == vp [right type] == foot.\\n\\nLexical rules\\n\\nHaving established a basic structure for our LTAG lexicon, we now turn our attention towards capturing other kinds of relationship among trees. We noted above that lexical entries are actually associated with tree families, and that these group together trees that are related to each other. Thus in the same family as a standard ditransitive verb, we might find the full passive, the agentless passive, the dative alternation, the various relative clauses, and so forth. It is clear that these families correspond closely to the outputs of transformations or metarules in other frameworks, but the XTAG system currently has no formal component for describing the relationships among families nor mechanisms for generating them. And so far we have said nothing about them either - we have only characterized single trees.\\n\\nHowever, LTAG\\'s large domain of locality means that all such relationships can be viewed as directly lexical, and thus expressible by lexical rules. In fact we can go further than this: because we have embedded the domain of these lexical rules, namely the LTAG tree structures, within the feature structures, we can view such lexical rules as covariation constraints within feature structures, in much the same way that the covariation of, say, syntactic and morphological form is treated. In particular, we can use the mechanisms that DATR already provides for feature covariation, rather than having to invoke in addition some special purpose lexical rule machinery.\\n\\nWe consider six construction types found in the XTAG grammar: passive, dative, subject-auxiliary inversion, wh-questions, relative clauses and topicalisation. Our basic approach to each of these is the same. Lexical rules are specified by defining a derived output tree structure in terms of an input tree structure, where each of these structures is a set of feature specifications of the sort defined above. Each lexical rule has a name, and the input and output tree structures for rule foo are referenced by prefixing feature paths of the sort given above with [input foo ..] or [output foo ..]. So for example, the category of the parent tree node of the output of the passive rule might be referenced as [output passive parent cat]. We define a very general default, stating that the output is the same as the input, so that lexical relationships need only concern themselves with components they modify. This approach to formulating lexical rules in DATR is quite general and in no way restricted to LTAG: it can be readily adapted for application in the context of any feature-based lexicalist grammar formalism.\\n\\nUsing this approach, the dative lexical rule can be given a minimalist implementation by the addition of the following single line to VERB+NP+PP, defined above. VERB+NP+PP: [output dative right right] == NPCOMP:[]. This causes the second complement to a ditransitive verb in the dative alternation to be an  NP, rather than a  PP as in the unmodified case. Subject-auxiliary inversion can be achieved similarly by just specifying the output tree structure without reference to the input structure (note the addition here of a form feature specifying verb form): AUXVERB: [output auxinv form] == finite-inv [output auxinv parent cat] == s [output auxinv right cat] == s.\\n\\nPassive is slightly more complex, in that it has to modify the given input tree structure rather than simply overwriting part of it. The definitions for passive occur at the VERB+NP node, since by default, any transitive or subclass of transitive has a passive form. Individual transitive verbs, or whole subclasses, can override this default, leaving their passive tree structure undefined if required. For agentless passives, the necessary additions to the VERB+NP node are as follows: VERB+NP: [output passive form] == passive [output passive right] == \"[input passive right right]\". Here, the first line stipulates the form of the verb in the output tree to be passive, while the second line redefines the complement structure: the output of passive has as its first complement the second complement of its input, thereby discarding the first complement of its input. Since complements are daisy-chained, all the others move up too.\\n\\nWh-questions, relative clauses and topicalisation are slightly different, in that the application of the lexical rule causes structure to be added to the top of the tree (above the  S node). Although these constructions involve unbounded dependencies, the unboundedness is taken care of by the LTAG adjunction mechanism: for lexical purposes the dependency is local. Since the relevant lexical rules can apply to sentences that contain any kind of verb, they need to be stated at the VERB node. Thus, for example, topicalisation and wh-questions can be defined as follows: VERB: [output topic parent parent parent cat] == s [output topic parent parent left cat] == np [output topic parent parent left form] == normal [output whq] == \"[output topic]\" [output whq parent parent left form] == wh. Here an additional  NP and  S are attached above the original  S node to create a topicalised structure. The wh-rule inherits from the topicalisation rule, changing just one thing: the form of the new NP is marked as wh, rather than as normal. In the full fragment, the  NP added by these rules is also syntactically cross-referenced to a specific  NP marked as null in the input tree. However, space does not permit presentation or discussion of the DATR code that achieves this here.\\n\\nApplying lexical rules\\n\\nHowever, in our full fragment, additional support is provided to achieve and constrain this rule chaining. Word definitions include boolean features indicating which rules to apply, and the presence of these features trigger inheritance between appropriate input and output paths and the base and surface specifications at the ends of the chain. For example, Word1 is an alternative way of specifying the dative alternant of Give, but results in inheritance linking equivalent to that found in Give-dat above: Word1: [] == Give [alt dative] == true. More interestingly, Word2 properly describes a wh-question based on the agentless passive of the dative of Give. Word2: [] == Give [alt whq] == true [alt dative] == true [alt passive] == true. [parent left form] == null Notice here the final line of Word2 which specifies the location of the `extracted\\'  NP (the subject, in this case), by marking it as null. As noted above, the full version of the whq lexical rule uses this to specify a cross-reference relationship between the wh- NP and the null  NP.\\n\\nWe can, if we wish, encode constraints on the applicability of rules in the mapping from boolean flags to actual inheritance specifications. Thus, for example, whq, rel, and topic are mutually exclusive. If such constraints are violated, then no value for surface gets defined. Thus Word3 improperly attempts topicalisation in addition to wh-question formation, and, as a result, will fail to define a surface tree structure at all: Word3: [] == Give [alt whq] == true [alt topic] == true [alt dative] == true [alt passive] == true [parent left form] == null.\\n\\nThis approach to lexical rules allows them to be specified at the appropriate point in the lexical hierarchy, but overridden or modified in subclasses or lexemes as appropriate. It also allows default generalisation over the lexical rules themselves, and control over their application. The last section showed how the whq lexical rule could be built by a single minor addition to that for topicalisation. However, it is worth noting that, in common with other DATR specifications, the lexical rules presented here are rule instances which can only be applied once to any given lexeme - multiple application could be supported, by making multiple instances inherit from some common rule specification, but in our current treatment such instances would require different rule names.\\n\\nComparison with related work\\n\\nThis differs from our approach in a number of ways. First, our use of nonmonotonic inheritance allows us to manipulate total instead of partial descriptions of trees. The abstract verb class in the Vijay-Shanker Schabes account subsumes both intransitive and transitive verb classes but is not identical to either - a minimal-satisfying-model step is required to map partial tree descriptions into actual trees. In our analysis, VERB is the intransitive verb class, with complements specifically marked as undefined: thus VERB:[right] == undef is inherited from TREENODE and VERB+NP just overrides this complement specification to add an  NP complement. Second, we describe trees using only local tree relations (between adjacent nodes in the tree), while Vijay-Shanker  Schabes also use a nonlocal dominance relation.\\n\\nBoth these properties are crucial to our embedding of the tree structure in the feature structure. We want the category information at each tree node to be partial in the conventional sense, so that in actual use such categories can be extended (by unification or whatever). So the feature structures that we associate with lexical entries must be viewed as partial. But we do not want the tree structure to be extendible in the same way: we do not want an intransitive verb to be applicable in a transitive context, by unifying in a complement  NP. So the tree structures we define must be total descriptions. And of course, our use of only local relations allows a direct mapping from tree structure to feature path, which would not be possible at all if nonlocal relations were present.\\n\\nSo while these differences may seem small, they allow us to take this significant representational step - significant because it is the tree structure embedding that allows us to view lexical rules as feature covariation constraints. The result is that while Vijay-Shanker Schabes use a tree description language, a category description language and a further formalism for lexical rules, we can capture everything in one framework all of whose components (nonmonotonicity, covariation constraint handling, etc.) have already been independently motivated for other aspects of lexical description.\\n\\nBecker\\'s sharp distinction between his metarules and his hierarchy gives rise to some problems that our approach avoids. Firstly, he notes that his metarules are subject to lexical exceptions and proposes to deal with these by stating ``for each entry in the (syntactic) lexicon .. which metarules are applicable for this entry\\'\\' (1993,126). We have no need to carry over this use of (meta)rule features since, in our account, lexical rules are not distinct from any other kind of property in the inheritance hierarchy. They can be stated at the most inclusive relevant node and can then be overridden at the exceptional descendant nodes. Nothing specific needs to be said about the nonexceptional nodes.\\n\\nSecondly, his metarules may themselves be more or less similar to each other and he suggests (1994,11) that these similarities could be captured if the metarules were also to be organized in a hierarchy. However, our approach allows us to deal with any such similarities in the main lexical hierarchy itself rather than by setting up a separate hierarchical component just for metarules (which appears to be what Becker has in mind).\\n\\nThirdly, as he himself notes (1993,128), because his metarules map from elementary trees that are in the inheritance hierarchy to elementary trees that are outside it, most of the elementary trees actually used are not directly connected to the hierarchy (although their derived status with respect to it can be reconstructed). Our approach keeps all elementary trees, whether or not they have been partly defined by a lexical rule, entirely within the lexical hierarchy.\\n\\nIn fact, Becker himself considers the possibility of capturing all the significant generalizations by using just one of the two mechanisms that he proposes: ``one might want to reconsider the usage of one mechanism for phenomena in both dimensions\\'\\' (1993,135). But, as he goes on to point out, his existing type of inheritance network is not up to taking on the task performed by his metarules because the former is monotonic whilst his metarules are not. However, he does suggest a way in which the hierarchy could be completely replaced by metarules but argues against adopting it (1993,136).\\n\\nAs will be apparent from the earlier sections of this paper, we believe that Becker\\'s insights about the organization of an LTAG lexicon can be better expressed if the metarule component is replaced by an encoding of (largely equivalent) lexical rules that are an integral part of a nonmonotonic inheritance hierarchy that stands as a description of all the elementary trees.\\n\\nAcknowledgements\\n\\nA precursor of this paper was presented at the September 1994 TAG+ Workshop in Paris. We thank the referees for that event and the ACL-95 referees for a number of helpful comments. We are also grateful to Aravind Joshi, Bill Keller, Owen Rambow K. Vijay-Shanker and The XTAG Group. This research was partly supported by grants to Evans from SERC/EPSRC (UK) and to Gazdar from ESRC (UK).\\n\\nBibliography\\n\\nAnne Abeill, Kathleen Bishop, Sharon Cote,  Yves Schabes. 1990. A lexicalized tree adjoining grammar for English. Technical Report MS-CIS-90-24, Department of Computer  Information Science, Univ. of Pennsylvania.\\n\\nFrancois Andry, Norman Fraser, Scott McGlashan, Simon Thornton,  Nick Youd. 1992. Making DATR work for speech: lexicon compilation in SUNDIAL. Comput. Ling., 18(3):245-267.\\n\\nPetra Barg. 1994. Automatic acquisition of DATR theories from observations. Theories des Lexicons: Arbeiten des Sonderforschungsbereichs 282, Heinrich-Heine Univ. of Dsseldorf, Dsseldorf.\\n\\nTilman Becker. 1993. HyTAG: A new type of tree adjoining grammar for hybrid syntactic representation of free word order languages. Ph.D. thesis, Univ. des Saarlandes.\\n\\nTilman Becker. 1994. Patterns in metarules. In Proceedings of the Third International Workshop on Tree Adjoining Grammars, 9-11.\\n\\nDoris Bleiching. 1992. Prosodisches Wissen in Lexicon. In G. Grz, ed., KONVENS-92, 59-68. Springer-Verlag.\\n\\nDoris Bleiching. 1994. Integration von Morphophonologie und Prosodie in ein hierarchisches Lexicon. In H. Trost, ed., Proceedings of KONVENS-94, 32-41.\\n\\nTed Briscoe, Valeria de Paiva,  Ann Copestake. 1993. Inheritance, Defaults,  the Lexicon. CUP.\\n\\nDunstan Brown  Andrew Hippisley. 1994. Conflict in Russian genitive plural assignment: A solution represented in DATR. J. of Slavic Linguistics, 2(1):48-76.\\n\\nLynne Cahill  Roger Evans. 1990. An application of DATR: the TIC lexicon. In ECAI-90, 120-125.\\n\\nLynne Cahill.\\n\\n1990.\\n\\nSyllable\\n\\n\\n\\nbased morphology.\\n\\nIn COLING\\n\\n\\n\\n90, volume 3, 48\\n\\n\\n\\n53.\\n\\nLynne Cahill.\\n\\n1993.\\n\\nMorphonology in the lexicon.\\n\\nIn EACL\\n\\n\\n\\n93, 87\\n\\n\\n\\n96.\\n\\nGreville Corbett  Norman Fraser. 1993. Network morphology: a DATR account of Russian nominal inflection. J. of Linguistics, 29:113-142.\\n\\nWalter Daelemans  Gerald Gazdar, eds. 1992. Special issues on inheritance. Comput. Ling., 18(2  3).\\n\\nChristy Doran, Dania Egedi, Beth Ann Hockey,  B. Srinivas. 1994a. Status of the XTAG system. In Proceedings of the Third International Workshop on Tree Adjoining Grammars, 20-23.\\n\\nChristy Doran, Dania Egedi, Beth Ann Hockey, B. Srinivas,  Martin Zaidel. 1994b. XTAG system -- a wide coverage grammar for English. In COLING-94, 922-928.\\n\\nMarkus Duda  Gunter Gebhardi. 1994. DUTR - a DATR-PATR interface formalism. In H. Trost, ed., Proceedings of KONVENS-94, 411-414.\\n\\nRoger Evans  Gerald Gazdar. 1989a. Inference in DATR. In EACL-89, 66-71.\\n\\nRoger Evans  Gerald Gazdar. 1989b. The semantics of DATR. In AISB-89, 79-87.\\n\\nDaniel P. Flickinger. 1987. Lexical Rules in the Hierarchical Lexicon. Ph.D. thesis, Stanford Univ.\\n\\nNorman Fraser  Greville Corbett. in press. Gender, animacy,  declensional class assignment: a unified account for Russian. In Geert Booij  Jaap van Marle, ed., Yearbook of Morphology 1994. Kluwer, Dordrecht.\\n\\nDafydd Gibbon. 1992. ILEX: a linguistic approach to computational lexica. In Ursula Klenk, ed., Computatio Linguae: Aufstze zur algorithmischen und quantitativen Analyse der Sprache (Zeitschrift fr Dialektologie und Linguistik, Beiheft 73), 32-53. Franz Steiner Verlag, Stuttgart.\\n\\nA. K. Joshi, L. S. Levy,  M. Takahashi. 1975. Tree adjunct grammars. J. Comput. Syst. Sci., 10(1):136-163.\\n\\nJames Kilbury, Petra [Barg] Nrger,  Ingrid Renz. 1991. DATR as a lexical component for PATR. In EACL-91, 137-142.\\n\\nJames Kilbury, Petra Barg,  Ingrid Renz. 1994. Simulation lexicalischen Erwerbs. In Christopher Habel  Gert Rickheit Sascha W. Felix, ed,   Kognitive Linguistik: Reprsentation und Prozesse, 251-271. Westdeutscher Verlag, Opladen.\\n\\nJames Kilbury. 1990. Encoding constituent structure in feature structures. Unpublished manuscript, Univ. of Dsseldorf, Dsseldorf.\\n\\nAdam Kilgarriff  Gerald Gazdar. 1995. Polysemous relations. In Frank Palmer, ed., Grammar  Meaning: essays in honour of Sir John Lyons, 1-25. CUP.\\n\\nAdam Kilgarriff.\\n\\n1993.\\n\\nInheriting verb alternations.\\n\\nIn EACL\\n\\n\\n\\n93, 213\\n\\n\\n\\n221.\\n\\nHagen Langer.\\n\\n1994.\\n\\nReverse queries in DATR.\\n\\nIn COLING\\n\\n\\n\\n94, 1089\\n\\n\\n\\n1095.\\n\\nMarc Light, Sabine Reinhard,  Marie Boyle-Hinrichs. 1993. INSYST: an automatic inserter system for hierarchical lexica. In EACL-93, page 471.\\n\\nMarc Light. 1994. Classification in feature-based default inheritance hierarchies. In H. Trost, ed., Proceedings of KONVENS-94, 220-229.\\n\\nSabine Reinhard  Dafydd Gibbon. 1991. Prosodic inheritance  morphological generalisations. In EACL-91, 131-136.\\n\\nJames Rogers  K. Vijay-Shanker. 1992. Reasoning with descriptions of trees. In ACL-92, 72-80.\\n\\nK. Vijay-Shanker  Yves Schabes. 1992. Structure sharing in lexicalized tree-adjoining grammar. In COLING-92, 205-211.\\n\\nThe XTAG Research Group. 1995. A lexicalized tree adjoining grammar for English. Technical Report IRCS Report 95-03, The Institute for Research in Cognitive Science, Univ. of Pennsylvania.\\n\\nFootnotes', metadata={'source': '../data/raw/cmplg-xml/9505030.xml'}),\n",
       " Document(page_content='Multilingual Sentence Categorization according to Language 1\\n\\nIssues in sentence categorization according to language is fundamental for NLP, especially in document processing. In fact, with the growing amount of multilingual text corpus data becoming available, sentence categorization, leading to multilingual text structure, opens a wide range of applications in multilingual text analysis such as information retrieval or preprocessing of multilingual syntactic parser. The major difficulties in sentence categorization are convergence and textual errors. Convergence since dealing with short entries involve discarding languages from few clues. Textual errors since documents coming from different electronic ways may contain spelling and grammatical errors as well as character recognition errors generated by OCR. We describe here an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency. The implementation is fast, small, robust and textual errors tolerant. Tested for french, english, spanish and german discrimination, the system gives very interesting results, achieving in one test 99.4% correct assignments on real sentences. The resolution power is based on grammatical words (not the most common words) and alphabet. Having the grammatical words and the alphabet of each language at its disposal, the system computes for each of them its likelihood to be selected.\\n\\nThe name of the language having the optimum likelihood will tag the sentence -- but non resolved ambiguities will be maintained. We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system\\'s classification performance. Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions.\\n\\nCategorization according to Language\\n\\nFrom Text Categorization ...\\n\\nEmergence of text categorization according to language came with the need of processing texts coming from all over the world. The goal of text categorization is to tag texts with the name of the language in which they are written. Information retrieval is the main application field.\\n\\nWhile some text categorization systems give very good results, the major problem is that their quality is entirely based on the training set. Profiles require a lot of data to converge and building a large representative training set is a real problem. Moreover, this method assume that texts are monolingual and results will be affected when dealing with multilingual texts. It does not care about natural language properties : it only considers texts as streams of characters. There is no linguistic justification.\\n\\n...to Multilingual Sentence Categorization\\n\\nToday, the problem is quiet different. Texts are more and more multilingual (especially due to citations) and we don\\'t have enough tools to process them efficiently. Tagging sentences with the name of their language solves this problem by switching each application in function of the language. This affects the whole NLP, Information retrieval is not the only field to be concerned: syntactic analysis and every applications based on it are concerned, making study about one particular language in multilingual texts without parasitic noise is also possible.\\n\\nUsing the previous method is not possible because the sentence is a too small unit to converge. The analysis method must be more precise to reveal each possible change of language.\\n\\nWe remark that a change of language in a text could appear at each change of sentence (more often paragraph) or in each included segment via quotes, parenthesis, dashes or colons. We will call sentence the traditionnal sentence but also each segment included in it.\\n\\nMultilingual Sentence Categorization\\n\\nStudying quantities of texts, we try to understand as well as possible ways to discriminate languages. We present in this section the results of our research which has been implemented and in the next section, other directions which seems obviously promising.\\n\\nGrammatical Words as Discriminant\\n\\nIn this section, we are going to motivate the reasons which lead us to choose grammatical words as discriminant.\\n\\nGrammatical words are proper to each language and are in a whole different from one language to another. Moreover, they are short, not numerous and we can easily build an exhaustive list. So, these words can be use as discriminant of language. But can we use them as discriminant of sentences?\\n\\nGrammatical words in sentences represent on average about 50% of words. They can\\'t be omitted because they structure sentences and make them understandable. Furthermore, relying on grammatical words allows textual errors tolerance and foreign words import from other languages (usual in scientific texts). It\\'s also important to note that foreign words import concerns nouns, verbs, adjectives but never grammatical words.\\n\\nThese rules will allow us to categorize sentences which have enough grammatical words but in short sentences (less than 10 words), there are few grammatical words, and by the way, few clues. We must introduce new knowledges to improve short sentences categorization.\\n\\nUsing the Alphabet Notes We can also remark that using grammatical words is different from using most common words. In fact, most common words require training set dependency and it is well known that a representative training set is very difficult to get. The number of words to hold is quiet subjective. Moreover, frequency is relative to texts, not to sentences.\\n\\nImproving Categorization\\n\\nThere are two levels to improve sentences categorization: a level below using words morphology and a level above using text structure. These improvements haven\\'t been implemented yet and will be the object of further works.\\n\\nKnowledge upon Words Morphology Syllabation: the idea is to check the good syllabation of words in a language. It requires to distinguish first, middles and last syllabs. (Using only endings seems to be a possible way)\\n\\nSequences of voyells or consonants: the idea is that these sequences are proper to each language.\\n\\nUsing Text Structure\\n\\nWhen dealing with texts, we can also use heuristical knowledge about text structure:\\n\\nIn a same paragraph, contiguous sentences are written in the same language\\n\\nTitles of a paragraph are written in the same language as their body\\n\\nIncluded blocks in a sentence (via parenthesis, ...) are written in the same language as the sentence.\\n\\nImplementation\\n\\nThe implementation of this research can be divided in two parts: sentence tokenization and language classification.\\n\\nSentence tokenization\\n\\nSentence tokenization is a problem in itsef because documents may come through different electronic ways. Also a sentence doesn\\'t always start with a capitalized letter and finish with a full stop (especially in emails). Texts are not formated and miscellaneous characters can be found everywhere.\\n\\nAcronyms, abbreviations, full names and numbers increase the problem by inserting points and/or spaces everywhere without following any rule. But, no rule can ever exist in free style texts.\\n\\nWe wrote a robust sentence parser which solves the majority of these cases, allowing us to categorize in good conditions multilingual sentences.\\n\\nLanguage classification\\n\\nThe realization simply implements the previous ideas.\\n\\nThe classification principle is the following:\\n\\nChecked whether the word belongs to the grammatical words list of some languages.\\n\\nIf so, incremented their likelihood to be selected.\\n\\nChecked whether the word morphology lets think it belongs to some languages.\\n\\nIf so, incremented their likelihood to be selected.\\n\\nTag the sentence with the names of the languages which have the same and highest likelihood.\\n\\nThis algorithm has a linear complexity in time.\\n\\nEvaluation\\n\\nThe Test\\n\\n\\n\\nBed\\n\\nResults\\n\\nThe results we obtained were expected. They express the fact that a sentence is usually written with grammatical words and that grammatical words are totally discriminant for sentences of more than 8 words.\\n\\nFrom 1 to 3 words, there are mainly total undeterminations. In fact, the corpus shows that we are processing included segments (via quotes and parenthesis) and there are no grammatical words and few clues to rely on. Deductions really start between 4 and 6 words. Here, sentences and grammatical words appear but in few quantities to allow a perfect deduction.\\n\\nErrors\\n\\nIsolating a single language does not mean exactly isolating the right language. The error rate is about 0.01% and concerns very short sentences (\"e mail\" where \"e\" is analysed as Spanish), a change of language without quotes in a sentence or an unexpected language (the Latin \"Orbi et Urbi\").\\n\\nConclusion\\n\\nThis classification method is based on texts observation and understanding of their natural properties. It does not depend on training sets and converges fast enough to achieve very good results on sentences.\\n\\nThis tool is now a switch of Jacques Vergne\\'s multilingual syntactic parser (for french, english and spanish).\\n\\nThe aim of this paper is also to point that the more the linguistic properties of the object are used, the best the results are.\\n\\nBibliography\\n\\nWilliam B. Cavnar and John M. Trenkle. 1994. N-gram-based text categorization. In Symposium On Document Analysis and Information Retrieval, pages 161-176, University of Nevada, Las Vegas.\\n\\nNadine Lucas, Nishina Kikuko, Akiba Tomoyoshi, and Surech K.G. 1993. Discourse analysis of scientific textbooks in japanese : a tool for producing automatic summaries. Technical Report 93TR-0004, Department of Computer Science, Tokyo Institute of Technology, Meguro-ku Ookayama 2-12-1, Tokyo 152, Japan, March.\\n\\nNadine Lucas. 1992. Syntaxe du paragraphe dans les textes scientifiques en japonais et en français. In Colloque international : Parcours linguistiques de discours spécialisé, Université Paris III, Septembre.\\n\\nJacques Vergne. 1993. Syntactic properties of natural languages and application to automatic parsing. In SEPLN 93 congress, Granada, Spain, August. Sociedad Espaola para el Procesamiento del Lenguaje Natural.\\n\\nJacques Vergne. 1994. A non recursive sentence segmentation, applied to parsing of linear complexity in time. In New Methods in Language Processing, pages 234-241, June.\\n\\nFootnotes\\n\\nThis Paper is published in the Proceedings of the European Chapter of the Association for Computational Linguistics SIGDAT Workshop ``From text to tags : Issues in Multilingual Language Analysis\\'\\' held March 95 in Dublin.', metadata={'source': '../data/raw/cmplg-xml/9502039.xml'}),\n",
       " Document(page_content=\"Estimating Performance of Pipelined Spoken Language Translation Systems\\n\\nMost spoken language translation systems developed to date rely on a pipelined architecture, in which the main stages are speech recognition, linguistic analysis, transfer, generation and speech synthesis. When making projections of error rates for systems of this kind, it is natural to assume that the error rates for the individual components are independent, making the system accuracy the product of the component accuracies. The paper reports experiments carried out using the SRI-SICS-Telia Research Spoken Language Translator and a 1000-utterance sample of unseen data. The results suggest that the naive performance model leads to serious overestimates of system error rates, since there are in fact strong dependencies between the components. Predicting the system error rate on the independence assumption by simple multiplication resulted in a 16% proportional overestimate for all utterances, and a 19% overestimate when only utterances of length 1-10 words were considered.\\n\\nINTRODUCTION\\n\\nMost spoken language translation systems rely on a pipelined architecture, including speech recognition, linguistic analysis, transfer, generation and speech synthesis. A major advantage is that components can be developed and tested independently. This is particularly important for spoken language translation, since expertise in multiple languages is not often found in the same location. An obvious disadvantage is brittleness: if one model fails to produce or pass on a correct interpretation, the whole translation process fails. To obtain modularity as well as robustness our system consists of modules with multiple outputs and mechanisms for using additional knowledge sources to reorder multiple inputs. We have used several statistical and other automatic methods to model knowledge sources within the modules.\\n\\nWhen making projections of error rates for systems of this kind, it is natural to assume that the error rates for the individual components are independent, making the system accuracy the product of the component accuracies. Here, we will produce experimental evidence suggesting that this simple model leads to serious overestimates of system error rates, since there are in fact strong dependencies between the components. For example, if an utterance fails recognition then, had it been recognized, it would have had a higher than average chance of failing linguistic analysis; similarly, utterances which fail linguistic analysis due to incorrect choice in the face of ambiguity are more likely to fail during the transfer and generation phases if the correct choice is substituted. Intuitively, utterances which are hard to hear are also hard to understand and translate.\\n\\nThe results showed that dependencies, in some instances quite striking, existed between the performances of most pairs of phases. For example, the error rates for the linguistic analysis phase, applied to correctly and incorrectly recognized utterances respectively, differed by a factor of about 3.5; a chi-squared test indicated that this was significant at the P=0.0005 level. The dependencies existed at all utterance lengths, and were even stronger when evaluation was limited to the portion of the corpus consisting of utterances of length 1-10 words. Predicting the system error rate on the independence assumption by simple multiplication resulted in a 16% proportional overestimate for all utterances, and a 19% overestimate for the 1-10 word utterances.\\n\\nTHE SPOKEN LANGUAGE TRANSLATOR\\n\\nEXPERIMENTS\\n\\nMany researchers working in the field of automatic spoken language understanding have made the informal observation that utterances hard for one module in an integrated system have a greater than average chance of being hard for other modules; this effect is sometimes referred to as ``synergy''. Quantitative studies are hard to come by, however, which motivated the experiments described here. The test corpus used was the 1001-utterance set of ATIS data provided for the December 1993 ARPA Spoken Language Systems evaluations. This corpus was unseen data for the present purposes.\\n\\nWe focussed our investigations on four conceptual functionalities in the system: speech recognition, source language analysis, grammar specialization, and transfer-and-generation. This breakdown was motivated partially by the expense and tedium of judging intermediate results by hand; ideally, we would have preferred a more fine-grained division, for example splitting transfer-and-generation into two phases. The results seem however adequate to illustrate our basic point. The error rate for each functionality was defined as follows: Speech recognition\\n\\nProportion of utterances for which the preferred N-best hypothesis is not an acceptable variant of the transcribed utterance. ``Acceptable variant'' was judged strictly: thus for example substitution of ``a'' by ``the'' or vice versa was normally judged unacceptable, but ``all the'' instead of ``all of the'' would normally be acceptable.\\n\\nSource language analysis\\n\\nGrammar specialization\\n\\nProportion of input utterance hypotheses receiving an analysis with the normal grammar that receive no analysis with the specialized grammar.\\n\\nTransfer\\n\\n\\n\\nand\\n\\n\\n\\ngeneration\\n\\nProportion of input utterance hypotheses receiving an analysis with the normal grammar that do not produce an acceptable translation.\\n\\nThe basic method for establishing correlations among processing functionalities was to contrast results between two sets of inputs, corresponding to i) correct upstream processing and ii) incorrect but correctable upstream processing respectively. In the second case, the input was substituted by input in which the upstream errors had been corrected. The expectation was that in cases where an upstream error had occurred the chance of failure in a given component would be higher even if the upstream error were corrected; this indeed proved to be the case.\\n\\nThe simplest example is provided by the linguistic processing phase. Of the 1001 utterances, 789 were recognized acceptably, and 212 unacceptably. 706 of the utterance in the first group received a QLF (89.5%); when the 212 misrecognized utterances were replaced by the correctly transcribed reference versions, only 135 (63.7%) received a QLF. Thus one can conclude that utterances failing recognition would anyway be 3.5 times as likely to fail linguistic processing as well. According to a standard chi-squared test, this result is significant at the P=0.0005 level.\\n\\nMoving on to the grammar specialization phase, there are two possible types of upstream error for a given utterance: recognition can fail, or the utterance can be out of coverage for the general (unspecialized) grammar. Only the first type of error is correctable. So the meaningful population of examples is the set of 706 + 135 = 841 utterances for which a QLF is produced assuming correct recognition. Of the 706 correctly recognized examples, 653 (92.5%) still produced a QLF when the specialized grammar was used instead of the general one. Of the 135 incorrectly recognized example, only 101 (74.8%) passed grammar specialization. The ratio of error rates, 3.4, is similar to the one for linguistic analysis, and is also significant at the P=0.0005 level.\\n\\nFor the transfer-and-generation phase, the population of meaningful examples is again 841, but this time there are two types of correctable upstream error: either recognition or grammar specialization can fail. Of the 653 examples with no upstream error, 539 (82.5%) produced a good translation; of the 841 - 653 = 188 examples with a correctable upstream error, 119 (63.3%) produce a good translation. The ratio of error rates, 2.1, is lower than for the linguistic analysis and grammar specialization phases, but is still significant at the P=0.0005 level.\\n\\nIf we calculate error rates for each phase over the whole population of meaningful examples (correct upstream processing + correctable upstream errors), we get the following figures. Recognition 1001 examples; 789 successes; error rate = 21.2%. Linguistic analysis 1001 examples; 706 + 135 = 841 successes; error rate = 15.9%. Grammar specialization 841 examples; 653 + 101 = 754 successes; error rate = 10.3%. Transfer and generation 841 examples; 539 + 119 successes; error rate = 21.8%. On the naive model, the error rate for the whole system should be (1 - (1 - 0.212)(1 - 0.159)(1 - 0.103)(1 - 0.218)) = 0.535. In actual fact, however, the error rate is (1 - 539/1001) = 0.462. Thus the naive model overestimates the error rate by a factor of 0.535/0.462 = 1.16.\\n\\nIt is not immediately clear why these strong correlations exist. One likely hypothesis which we felt needed investigation is that they are a simple consequence of the known fact that accuracy in general correlates strongly with utterance length, with long utterances being difficult for all processing stages. If this were so, one would expect the effect to be less pronounced if the long utterances were removed. Interestingly, this does not turn out to be true. We repeated the experiments using only utterances of 1 to 10 words in length (688 utterances of the original 1001): the new results, in summary, were as follows. All of them were significant at the P=0.0005 level.\\n\\nSpeech recognition\\n\\n577 utterances (83.9%) were acceptably recognized.\\n\\nLinguistic analysis\\n\\n531 of the 577 acceptably recognized utterances (92.0%) received a QLF; 75 of the 111 unacceptably recognized utterances (67.6%) received a QLF. The ratio of error rates is 4.1.\\n\\nGrammar specialization\\n\\n497 of the 531 correctly recognized utterances receiving a QLF (93.6%) passed grammar specialization; 54 of the 75 relevant incorrectly recognized utterances did so (72.0%). The ratio of error rates is 4.4.\\n\\nTransfer and generation\\n\\n428 of the 497 utterances with no upstream error received a good translation (86.1%); 67 of the 109 utterances with a correctable upstream error did so (61.5%). The ratio of error rates is 2.8.\\n\\nThe naive model predicts a combined error rate of 45.1%; the real error rate is 37.8%. Thus the naive model overestimates the error rate by a factor of 1.19, an even larger difference than for the entire set.\\n\\nA more plausible explanation for the correlations is that they arise from the fact that all the components of the system are trained on, and therefore biased towards, rather similar data. This training may be automatic, or it may arise from system developers devoting their efforts to more frequently occurring phenomena (a strategy followed deliberately in adapting the Core Language Engine to the ATIS domain). Even if training and test sentences formally outside the domain are excluded from consideration, some sentences will still be more ``typical'' than others in that they employ more frequently occurring words, word sequences, constructions and concepts. It is quite probable that typicality at one level - say, that of word N-grams, making correct recognition more likely - is strongly correlated with typicality at others - say, source language grammar coverage, especially when specialized.\\n\\nSUMMARY AND CONCLUSIONS\\n\\nThere are several interesting conclusions to be drawn from the results presented above. Most obviously, pipelined systems are clearly doing rather better than the naive model predicts. More interestingly, the experiments clearly show that the whole concept of evaluating individual components of a pipelined system in isolation is more complex than one at first imagines. Since all the components tend to find the same utterances difficult, the upstream components act as a filter which separate out the hard examples and pass on the easy ones. Thus a test which measures the performance of a component in an ideal situation, assuming no upstream errors, will in practice give a more or less misleading picture of how it will behave in the context of the full system. In general, downstream components will always have a lower error rate than a test of this type suggests.\\n\\nIn particular, the performance of the language processing component of a pipelined speech-understanding system is not something that can meaningfully be measured in isolation. A clear understanding of this fact allows development effort to be focussed more productively on work that improves system performance as a whole.\\n\\nBibliography\\n\\nAgns, M-S., Alshawi, H., Bretan, I., Carter, D.M. Ceder, K., Collins, M., Crouch, R., Digalakis, V., Ekholm, B., Gambck, B., Kaja, J., Karlgren, J., Lyberg, B., Price, P., Pulman, S., Rayner, M., Samuelsson, C. and Svensson, T., Spoken Language Translator: First Year Report, joint SRI/SICS technical report, 1994.\\n\\nAlshawi, H., The Core Language Engine, Cambridge, Massachusetts: The MIT Press, 1992.\\n\\nAlshawi, H. and Carter, D.M., Training and Scaling Preference Functions for Disambiguation, To appear in Computational Linguistics, 1995. Also available as SRI technical report.\\n\\nAlshawi, H., Carter, D., Rayner, M. and Gambck, B., ``Transfer through Quasi Logical Form'', Proc. 29th ACL, Berkeley, 1991.\\n\\nCeder, K. and Lyberg, B., ``Yet Another Rule Compiler for Text-to-Speech Conversion? '', Proc. ICSLP, Banff, 1993.\\n\\nDigalakis, V. and Murveit, H., ``Genones: Optimizing the Degree of Tying in a Large Vocabulary HMM Speech Recognizer'', Proc. of the Inter. Conf. on Acoust., Speech and Signal Proc., 1994.\\n\\nGambck, B. and Rayner, M., ``The Swedish Core Language Engine'', Proc. 3rd NOTEX, Linkping, 1992.\\n\\nMurveit, H., Butzberger, J., Digalakis, V. and Weintraub, M., ``Large Vocabulary Dictation using SRI's DECIPHER(TM) Speech Recognition System: Progressive Search Techniques'', Proc. of the Inter. Conf. on Acoust., Speech and Signal Proc., Minneapolis, Minnesota, April 1993.\\n\\nRayner, M., Alshawi, H., Bretan, I., Carter, D.M., Digalakis, V., Gambck, B., Kaja, J., Karlgren, J., Lyberg, B., Price, P., Pulman, S. and Samuelsson, C., ``A Speech to Speech Translation System Built From Standard Components''. Proc. ARPA workshop on Human Language Technology, 1993\\n\\nRayner, M., D. Carter, V. Digalakis and P. Price, ``Combining Knowledge Sources to Reorder N-Best Speech Hypothesis Lists''. To appear in Proc. ARPA workshop on Human Language Technology, 1994\\n\\nRayner, M., Bretan, I., Carter, D., Collins, M., Digalakis, V., Gambck, B., Kaja, J., Karlgren, J., Lyberg, B., Price, P., Pulman S. and Samuelsson, C., ``Spoken Language Translation with Mid-90's Technology: A Case Study''. Proceedings of Eurospeech '93, Berlin, 1993.\\n\\nSamuelsson, C. and Rayner, M., ``Quantitative Evaluation of Explanation-Based Learning as a Tuning Tool for a Large-Scale Natural Language System''. Proc. 12th International Joint Conference on Artificial Intelligence. Sydney, Australia, 1991.\\n\\nSamuelsson, C., Fast Natural Language Parsing Using Explanation-Based Learning, PhD thesis, Royal Institute of Technology, Stockholm, Sweden, 1994.\\n\\nShieber, S. M., van Noord, G., Pereira, F.C.N and Moore, R.C., ``Semantic-Head-Driven Generation'', Computational Linguistics, 16:30-43, 1990.\", metadata={'source': '../data/raw/cmplg-xml/9407009.xml'}),\n",
       " Document(page_content='A Model-Theoretic Framework for Theories of Syntax Introduction\\n\\nGenerative grammar and formal language theory share a common origin in a procedural notion of grammars: the grammar formalism provides a general mechanism for recognizing or generating languages while the grammar itself specializes that mechanism for a specific language. At least initially there was hope that this relationship would be informative for linguistics, that by characterizing the natural languages in terms of language-theoretic complexity one would gain insight into the structural regularities of those languages. Moreover, the fact that language-theoretic complexity classes have dual automata-theoretic characterizations offered the prospect that such results might provide abstract models of the human language faculty, thereby  not just identifying these regularities, but actually accounting for them.\\n\\nOver time, the two disciplines have gradually become estranged, principally due to a realization that the structural properties of languages that characterize natural languages may well not be those that can be distinguished by existing language-theoretic complexity classes. Thus the insights offered by formal language theory might actually be misleading in guiding theories of syntax. As a result, the emphasis in generative grammar has turned from formalisms with restricted generative capacity to those that support more natural expression of the observed regularities of languages. While a variety of distinct approaches have developed, most of them can be characterized as constraint based--the formalism (or formal framework) provides a class of structures and a means of precisely stating constraints on their form, the linguistic theory is then expressed as a system of constraints (or principles) that characterize the class of well-formed analyses of the strings in the language.\\n\\nBut the accompanying loss of language-theoretic complexity results is unfortunate. While such results may not be useful in guiding syntactic theory, they are not irrelevant. The nature of language-theoretic complexity hierarchies is to classify languages on the basis of their structural properties. The languages in a class, for instance, will typically exhibit certain closure properties (e.g., pumping lemmas) and the classes themselves admit normal forms (e.g., representation theorems). While the linguistic significance of individual results of this sort is open to debate, they at least loosely parallel typical linguistic concerns:  closure properties state regularities that are exhibited by the languages in a class, normal forms express generalizations about their structure. So while these may not be the right results, they are not entirely the wrong kind of results. Moreover, since these classifications are based on structural properties and the structural properties of natural language can be studied more or less directly, there is a reasonable expectation of finding empirical evidence falsifying a hypothesis about language-theoretic complexity of natural languages if such evidence exists.\\n\\nFinally, the fact that these complexity classes have automata-theoretic characterizations means that results concerning the complexity of natural languages will have implications for the nature of the human language faculty. These automata-theoretic characterizations determine, along one axis, the types of resources required to generate or recognize the languages in a class. The regular languages, for instance, can be characterized by finite-state (string) automata--these languages can be processed using a fixed amount of memory. The context-sensitive languages, on the other had, can be characterized by linear-bounded automata--they can be processed using an amount of memory proportional to the length of the input. The context-free languages are probably best characterized by finite-state tree automata--these correspond to recognition by a collection of processes, each with a fixed amount of memory,  where the number of processes is linear in the length of the input and all communication between processes is completed at the time they are spawned. As a result, while these results do not necessarily offer abstract models of the human language faculty (since the complexity results do not claim to characterize the human languages, just to classify them), they do offer lower bounds on certain abstract properties of that faculty.\\n\\nThe Monadic Second-Order Language of  Trees Feature Specification Defaults in GPSG\\n\\nWe now turn to our first application--the definition of Feature Specification Defaults (FSDs) in GPSG. Since GPSG is presumed to license (roughly) context-free languages, we are not concerned here with establishing language-theoretic complexity but rather with clarifying the linguistic theory expressed by GPSG. FSDs specify conditions on feature values that must hold at a node in a licensed tree unless they are overridden by some other component of the grammar; in particular, unless they are incompatible with either a feature specified by the ID rule licensing the node (inherited features) or a feature required by one of the agreement principles--the Foot Feature Principle (FFP), Head Feature Convention (HFC), or Control Agreement Principle (CAP). It is the fact that the default holds just in case it is incompatible with these other components that gives FSDs their dynamic flavor. Note, though, in contrast to typical applications of default logics, a GPSG grammar is not an evolving theory. The exceptions to the defaults are fully determined when the grammar is written. If we ignore for the moment the effect of the agreement principles, the defaults are roughly the converse of the ID rules: a non-default feature occurs iff it is licensed by an ID rule.\\n\\ncan be expressed:\\n\\nThe agreement principles require pairs of nodes occurring in certain configurations in local trees to agree on certain classes of features. Thus these principles do not introduce features into the trees, but rather propagate features from one node to another, possibly in many steps. Consequently, these principles cannot override FSDs by themselves; rather every violation of a default must be licensed by an inherited feature somewhere in the tree. In order to account for this propagation of features, the definition of FSDs in GKPS is based on identifying pairs of nodes that co-vary wrt the relevant features in all possible extensions of the given tree. As a result, although the treatment in GKPS is actually declarative, this fact is far from obvious.\\n\\nThe key thing to note about this treatment of FSDs is its simplicity relative to the treatment of GKPS. The second-order quantification allows us to reason directly in terms of the sequence of nodes extending from the privileged node to the local tree that actually licenses the privilege. The immediate benefit is the fact that it is clear that the property of satisfying a set of FSDs is a static property of labeled trees and does not depend on the particular strategy employed in checking the tree for compliance.\\n\\nChains in GB A Comparison and a Contrast\\n\\nOne finds a strong contrast, on the other hand, in the way in which GB and GPSG encode language universals. In GB it is presumed that all principles are universal with the theory being specialized to specific languages by a small set of finitely varying parameters. These principles are simply properties of trees. In terms of models, one can understand GB to define a universal language--the set of all analyses that can occur in human languages. The principles then distinguish particular sub-languages--the head-final or the pro-drop languages, for instance. Each realized human language is just the intersection of the languages selected by the settings of its parameters. In GPSG, in contrast, many universals are, in essence, closure properties that must be exhibited by human languages--if the language includes trees in which a particular configuration occurs then it includes variants of those trees in which certain related configurations occur. Both the ECPO principle and the metarules can be understood in this way. Thus while universals in GB are properties of trees, in GPSG they tend to be properties of sets of trees. This makes a significant difference in capturing these theories model-theoretically; in the GB case one is defining sets of models, in the GPSG case one is defining sets of sets of models. It is not at all clear what the linguistic significance of this distinction is; one particularly interesting question is whether it has empirical consequences.\\n\\nConclusion\\n\\nBibliography\\n\\nBlackburn, Patrick, Claire Gardent, and Wilfried Meyer-Viol. 1993. Talking about trees. In EACL 93, pages 21-29. European Association for Computational Linguistics.\\n\\nBlackburn, Patrick and Wilfried Meyer-Viol. 1994. Linguistics, logic, and finite trees. Bulletin of the IGPL, 2(1):3-29, March.\\n\\nBchi, J. R. 1960. Weak second-order arithmetic and finite automata. Zeitschrift fr mathematische Logik und Grundlagen der Mathematik, 6:66-92.\\n\\nCarpenter, Bob. 1992. The Logic of Typed Feature Structures; with Applications to Unification Grammars, Logic Programs and Constraint Resolution. Number 32 in Cambridge Tracts in Theoretical Computer Science. Cambridge University Press.\\n\\nCornell, Thomas Longacre. 1992. Description Theory, Licensing Theory, and Principle-Based Grammars and Parsers. Ph.D. thesis, University of California Los Angeles.\\n\\nDawar, Anuj and K. Vijay-Shanker. 1990. An interpretation of negation in feature structure descriptions. Computational Linguistics, 16(1):11-21.\\n\\nDoner, John. 1970. Tree acceptors and some of their applications. Journal of Computer and System Sciences, 4:406-451.\\n\\nGazdar, Gerald, Ewan Klein, Geoffrey Pullum, and Ivan Sag. 1985. Generalized Phrase Structure Grammar. Harvard University Press.\\n\\nGazdar, Gerald, Geoffrey Pullum, Robert Carpenter, Ewan Klein, T. E. Hukari, and R. D. Levine. 1988. Category structures. Computational Linguistics, 14:1-19.\\n\\nGorn, Saul. 1967. Explicit definitions and linguistic dominoes. In John F. Hart and Satoru Takasu, editors, Systems and Computer Science, Proceedings of the Conference held at Univ. of Western Ontario, 1965. Univ. of Toronto Press.\\n\\nGurevich, Yuri. 1988. Logic and the challenge of computer science. In E. Brger, editor, Current Trends in Theoretical Computer Science. Computer Science Press, chapter 1, pages 1-57.\\n\\nImmerman, Neil. 1989. Descriptive and computational complexity. In Proceedings of Symposia in Applied Mathematics, pages 75-91. American Mathematical Society.\\n\\nJohnson, David E. and Paul M. Postal. 1980. Arc Pair Grammar. Princeton University Press, Princeton, New Jersey.\\n\\nJohnson, Mark. 1988. Attribute-Value Logic and the Theory of Grammar. Number 16 in CSLI Lecture Notes. Center for the Study of Language and Information, Stanford, CA.\\n\\nJohnson, Mark. 1989. The use of knowledge of language. Journal of Psycholinguistic Research, 18(1):105-128.\\n\\nKasper, Robert T. and William C. Rounds. 1986. A logical semantics for feature structures. In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics.\\n\\nKasper, Robert T. and William C. Rounds. 1990. The logic of unification in grammar. Linguistics and Philosophy, 13:35-58.\\n\\nKeller, Bill. 1993. Feature Logics, Infinitary Descriptions and Grammar. Number 44 in CSLI Lecture Notes. Center for the Study of Language and Information.\\n\\nKracht, Marcus. 1995. Syntactic codes and grammar refinement. Journal of Logic, Language, and Information, 4:41-60.\\n\\nManzini, Maria Rita. 1992. Locality: A Theory and Some of Its Empirical Consequences. MIT Press, Cambridge, Ma.\\n\\nMoshier, M. Drew and William C. Rounds. 1987. A logic for partially specified data structures. In ACM Symposium on the Principles of Programming Languages.\\n\\nRizzi, Luigi.\\n\\n1990.\\n\\nRelativized Minimality.\\n\\nMIT Press.\\n\\nRogers, James. 1994. Studies in the Logic of Trees with Applications to Grammar Formalisms. Ph.D. dissertation, Univ. of Delaware.\\n\\nRogers, James. 1995. On descriptive complexity, language complexity, and GB. In Patrick Blackburn and Maarten de Rijke, editors, Specifying Syntactic Structures. In Press. Also available as IRCS Technical Report 95-14. cmp-lg/9505041.\\n\\nRogers, James. 1996a. A Descriptive Approach to Language-Theoretic Complexity. Studies in Logic, Language, and Information. CSLI Publications. To appear.\\n\\nRogers, James. 1996b. The descriptive complexity of local, recognizable, and generalized recognizable sets. Technical report, IRCS, Univ. of Pennsylvania. In Preparation.\\n\\nRogers, James.\\n\\n1996c.\\n\\nGrammarless phrase\\n\\n\\n\\nstructure grammar.\\n\\nUnder Review.\\n\\nRogers, James and K. Vijay-Shanker. 1994. Obtaining trees from their descriptions: An application to tree-adjoining grammars. Computational Intelligence, 10:401-421.\\n\\nSmolka, Gert. 1989. A feature logic with subsorts. LILOG Report 33, IBM Germany, Stuttgart.\\n\\nStabler, Jr., Edward P. 1992. The Logical Approach to Syntax. Bradford.\\n\\nFootnotes', metadata={'source': '../data/raw/cmplg-xml/9604023.xml'}),\n",
       " Document(page_content=\"DEVELOPING AND EVALUATING A PROBABILISTIC LR PARSER OF PART-OF-SPEECH AND PUNCTUATION LABELS\\n\\nWe describe an approach to robust domain-independent syntactic parsing of unrestricted naturally-occurring (English) input. The technique involves parsing sequences of part-of-speech and punctuation labels using a unification-based grammar coupled with a probabilistic LR parser. We describe the coverage of several corpora using this grammar and report the results of a parsing experiment using probabilities derived from bracketed training data. We report the first substantial experiments to assess the contribution of punctuation to deriving an accurate syntactic analysis, by parsing identical texts both with and without naturally-occurring punctuation marks.\\n\\nIntroduction\\n\\nThis work is part of an effort to develop a robust, domain-independent syntactic parser capable of yielding the one correct analysis for unrestricted naturally-occurring input. Our goal is to develop a system with performance comparable to extant part-of-speech taggers, returning a syntactic analysis from which predicate-argument structure can be recovered, and which can support semantic interpretation. The requirement for a domain-independent analyser favours statistical techniques to resolve ambiguities, whilst the latter goal favours a more sophisticated grammatical formalism than is typical in statistical approaches to robust analysis of corpus material.\\n\\nBriscoe and Carroll (1993) describe a probablistic parser using a wide-coverage unification-based grammar of English written in the Alvey Natural Language Tools (ANLT) metagrammatical formalism (Briscoe et al., 1987), generating around 800 rules in a syntactic variant of the Definite Clause Grammar formalism (DCG, Pereira Warren, 1980) extended with iterative (Kleene) operators. The ANLT grammar is linked to a lexicon containing about 64K entries for 40K lexemes, including detailed subcategorisation information appropriate for the grammar, built semi-automatically from a learners' dictionary (Carroll Grover, 1989). The resulting parser is efficient, capable of constructing a parse forest in what seems to be roughly quadratic time, and efficiently returning the ranked n-most likely analyses (Carroll, 1993, 1994). The probabilistic model is a refinement of probabilistic context-free grammar (PCFG) conditioning CF `backbone' rule application on LR state and lookahead item. Unification of the `residue' of features not incorporated into the backbone is performed at parse time in conjunction with reduce operations. Unification failure results in the associated derivation being assigned a probability of zero. Probabilities are assigned to transitions in the LALR(1) action table via a process of supervised training based on computing the frequency with which transitions are traversed in a corpus of parse histories.\\n\\nExperiments with this system revealed three major problems which our current research is addressing. Firstly, although the system is able to rank parses with a 75% chance that the correct analysis will be the most highly ranked, further improvement will require a `lexicalised' system in which (minimally) probabilities are associated with alternative subcategorisation possibilities of individual lexical items. Currently, the relative frequency of subcategorisation possibilities for individual lexical items is not recorded in wide-coverage lexicons, such as ANLT or COMLEX (Grishman et al., 1994). Secondly, removal of punctuation from the input (after segmentation into text sentences) worsens performance as punctuation both reduces syntactic ambiguity (Jones, 1994) and signals non-syntactic (discourse) relations between text units (Nunberg, 1990). Thirdly, the largest source of error on unseen input is the omission of appropriate subcategorisation values for lexical items (mostly verbs), preventing the system from finding the correct analysis. The current coverage of this system on a general corpus (e.g. Brown or LOB) is estimated to be around 20% by Briscoe (1994). We have developed a variant probabilistic LR parser which does not rely on subcategorisation and uses punctuation to reduce ambiguity. The analyses produced by this parser could be utilised for phrase-finding applications, recovery of subcategorisation frames, and other `intermediate' level parsing problems.\\n\\nPart\\n\\n\\n\\nof\\n\\n\\n\\nspeech Tag Sequence Grammar\\n\\nSeveral robust parsing systems exploit the comparative success of part-of-speech (PoS) taggers, such as Fidditch (Hindle, 1989) or MITFP (de Marcken, 1990), by reducing the input to a determinate sequence of extended PoS labels of the type which can be practically disambiguated in context using a (H)MM PoS tagger (e.g. Church, 1988). Such approaches, by definition, cannot exploit subcategorisation, and probably achieve some of their robustness as a result. However, such parsers typically also employ heuristic rules, such as `low' attachment of PPs to produce unique `canonical' analyses. This latter step complicates the recovery of predicate-argument structure and does not integrate with a probabilistic approach to parsing.\\n\\nWe utilised the ANLT metagrammatical formalism to develop a feature-based, declarative description of PoS label sequences for English. This grammar compiles into a DCG-like grammar of approximately 400 rules. It has been designed to enumerate possible valencies for predicates (verbs, adjectives and nouns) by including separate rules for each pattern of possible complementation in English. The distinction between arguments and adjuncts is expressed, following X-bar theory (e.g. Jackendoff, 1977), by Chomsky-adjunction of adjuncts to maximal projections (XP\\n\\nXP\\n\\nAdjunct\\n\\nText Grammar and Punctuation\\n\\nNunberg (1990) develops a partial `text' grammar for English which incorporates many constraints that (ultimately) restrict syntactic and semantic interpretation. For example, textual adjunct clauses introduced by colons scope over following punctuation, as (1a) illustrates; whilst textual adjuncts introduced by dashes cannot intervene between a bracketed adjunct and the textual unit to which it attaches, as in (2b).\\n\\nWe have developed a declarative grammar in the ANLT metagrammatical formalism, based on Nunberg's procedural description. This grammar captures the bulk of the text-sentential constraints described by Nunberg with a grammar which compiles into 26 DCG-like rules. Text grammar analyses are useful because they demarcate some of the syntactic boundaries in the text sentence and thus reduce ambiguity, and because they identify the units for which a syntactic analysis should, in principle, be found; for example, in (3), the absence of dashes would mislead a parser into seeking a syntactic relationship between three and the following names, whilst in fact there is only a discourse relation of elaboration between this text adjunct and pronominal three.\\n\\nThe rules of the text grammar divide into three groups: those introducing text-sentences, those defining text adjunct introduction and those defining text adjuncts (Nunberg, 1990). An example of each type of rule is given in (4a-c).\\n\\nThese rules are phrase structure schemata employing iterative operators, optionality and disjunction, preceded by a mnemonic name. Non-terminal categories are text sentences, units or adjuncts which carry features mostly representing the punctuation marks which occur as daughters in the rules (e.g. +sc represents presence of a semi-colon marker), whilst terminal punctuation is represented as +pxx (e.g. +pda, dash). (4a) states that a text sentence can contain zero or more text units with a semi-colon at their right boundary followed by a text unit optionally followed by a question or exclamation mark. (4b) states that a text unit not containing a semi-colon can consist of a text unit or adjunct not containing dashes, colons or semi-colons followed by a text adjunct introduced by a dash. This type of `unbalanced' adjunct can only be expanded by (4c) which states that it consists of a single opening dash followed by a text unit which does not itself contain dashes or semi-colons. The features on the first daughter of (4b) force dash adjuncts to have lower precedence and narrower scope than colons or semi-colons, blocking interpretations of multiple dashes as sequences of `unbalanced' adjuncts.\\n\\nNunberg (1990) invokes rules of (point) absorption which delete punctuation marks (inserted according to a simple context-free text grammar) when adjacent to other `stronger' punctuation marks. For instance, he treats all dash interpolated text adjuncts as underlyingly balanced, but allows a rule of point absorption to convert (5a) into (6b).\\n\\nThe various rules of absorption introduce procedurality into the grammatical framework and require the positing of underlying forms which are not attested in text. For this reason, `absorption' effects are captured through propagation of featural constraints in parse trees. For instance, (6a) is blocked by including distinct rules for the introduction of balanced and unbalanced text adjuncts and only licensing the latter text sentence finally.\\n\\nThe text grammar has been tested on Susanne and covers 99.8% of sentences. (The failures are mostly text segmentation problems). The number of analyses varies from one (71%) to the thousands (0.1%). Just over 50% of Susanne sentences contain some punctuation, so around 20% of the singleton parses are punctuated. The major source of ambiguity in the analysis of punctuation concerns the function of commas and their relative scope as a result of a decision to distinguish delimiters and separators (Nunberg 1990:36). Therefore, a text sentence containing eight commas (and no other punctuation) will have 3170 analyses. The multiple uses of commas cannot be resolved without access to (at least) the syntactic context of occurrence.\\n\\nThe Integrated Grammar\\n\\nDespite Nunberg's observation that text grammar is distinct from syntax, text grammatical ambiguity favours interleaved application of text grammatical and syntactic constraints. The integration of text and PoS sequence grammars is straightforward and remains modular, in that the text grammar is `folded into' the PoS sequence grammar, by treating text and syntactic categories as overlapping and dealing with the properties of each using disjoint sets of features, principles of feature propagation, and so forth. The text grammar rules are represented as left or right branching rules of `Chomsky-adjunction' to lexical or phrasal constituents. For example, the simplified rule for combining NP appositional or parenthetical text adjuncts is N2[+ta]\\n\\nH2 Ta[+bal]\\n\\nParsing the Susanne and SEC Corpora\\n\\nThe integrated grammar has been used to parse Susanne and the quite distinct SEC Corpus (Taylor  Knowles, 1988), a 50K word treebanked corpus of transcribed British radio programmes punctuated by the corpus compilers. Both corpora were retagged with determinate punctuation and PoS labelling using the Acquilex HMM tagger (Elworthy, 1993, 1994) trained on text tagged with a slightly modified version of CLAWS-II labels (Garside et al., 1987).\\n\\nCoverage and Average Ambiguity\\n\\nTo examine the efficiency and coverage of the grammar we applied it to our retagged versions of Susanne and SEC. We used the ANLT chart parser (Carroll, 1993), but modified just to count the number of possible parses in the parse forests (Billot  Lang, 1989) rather than actually unpacking them. We also imposed a per-sentence time-out of 30 seconds CPU time, running in Franz Allegro Common Lisp 4.2 on an HP PA-RISC 715/100 workstation with 96 Mbytes of physical memory.\\n\\nWe define the `coverage' of the grammar to be the inverse of the proportion of sentences for which no analysis was found--a weak measure since discovery of one or more global analyses does not entail that the correct analysis is recovered. For both corpora, the majority of sentences analysed successfully received under 100 parses, although there is a long tail in the distribution. Monitoring this distribution is helpful during grammar development to ensure that coverage is increasing but the ambiguity rate is not. A more succinct though less intuitive measure of ambiguity rate for a given corpus is what we call the average parse base (APB), defined as the geometric mean over all sentences in the corpus of\\n\\nAs the grammar was developed solely with reference to Susanne, coverage of SEC is quite robust. The two corpora differ considerably since the former is drawn from American written text whilst the latter represents British transcribed spoken material. The corpora overall contain material drawn from widely disparate genres / registers, and are more complex than those used in DARPA ATIS tests and more diverse than those used in MUC. The APBs for Susanne and SEC of 1.256 and 1.239 respectively indicate that sentences of average length in each corpus could be expected to be assigned of the order of 97 and 126 analyses (i.e.\\n\\n1.256[20.1] and\\n\\n1.239[22.6]). Black et al. (1993:156) quote a parse base of 1.35 for the IBM grammar for computer manuals applied to sentences 1-17 words long. Although, as mentioned above, Black's measure may not be exactly the same as our APB measure, it is probable that the IBM grammar assigns more analyses than ours for sentences of the same length. Black achieves a coverage of around 95%, as opposed to our coverage rate of 67-74% on much more heterogeneous data and longer sentences.\\n\\nThe parser throughput on these tests, for sentences successfully analysed, is around 45 words per CPU second on an HP PA-RISC 715/100. Sentences of up to 30 tokens (words plus punctuation) are parsed in an average under 0.6 seconds each, whilst those around 60 tokens take on average 4.5 seconds. Nevertheless, the relationship between sentence length and processing time is fitted well by a quadratic function, supporting the findings of Carroll (1994) that in practice NL grammars do not evince worst-case parsing complexity.\\n\\nCoverage, Ambiguity and Punctuation\\n\\nWe have also run experiments to evaluate the degree to which punctuation is contributing useful information. Intuitively, we would expect the exploitation of text grammatical constraints to both reduce ambiguity and extend coverage (where punctuation cues discourse rather than syntactic relations between constituents). Jones (1994) reports a preliminary experiment evaluating reduction of ambiguity by punctuation. However, the grammar he uses was developed only to cover the test sentences, drawn entirely from the SEC corpus which was punctuated post hoc by the corpus developers (Taylor and Knowles, 1988).\\n\\nWe took all in-coverage sentences from Susanne of length 8-40 words inclusive containing internal punctuation; a total of 2449 sentences. The APB for this set was 1.273, mean length 22.5 words, giving an expected number of analyses for an average sentence of 225. We then removed all sentence-internal punctuation from this set and re-parsed it. Around 8% of sentences now failed to receive an analysis. For those that did (mean length 20.7 words), the APB was now 1.320, so an average sentence would be assigned 310 analyses, 38% more than before. On closer inspection, the increase in ambiguity is due to two factors: a) a significant proportion of sentences that previously received 1-9 analyses now receive more, and b) there is a much more substantial tail in the distribution of sentence length vs. number of parses, due to some longer sentences being assigned many more parses. Manual examination of 100 depunctuated examples revealed that in around a third of cases, although the system returned global analyses, the correct one was not in this set (Briscoe Carroll, 1994). With a more constrained (subcategorised) syntactic grammar, many of these examples would not have received any global syntactic analysis.\\n\\nParse Selection\\n\\nA probabilistic LR parser was trained with the integrated grammar by exploiting the Susanne treebank bracketing. An LR parser (Briscoe and Carroll, 1993) was applied to unlabelled bracketed sentences from the Susanne treebank, and a new treebank of 1758 correct and complete analyses with respect to the integrated grammar was constructed semi-automatically by manually resolving the remaining ambiguities. 250 sentences from the new treebank were kept back for testing. The remainder, together with a further set of analyses from 2285 treebank sentences that were not checked manually, were used to train a probabilistic version of the LR parser, using Good-Turing smoothing to estimate the probability of unseen transitions in the LALR(1) table (Briscoe and Carroll, 1993; Carroll, 1993). The probabilistic parser can then return a ranking of all possible analyses for a sentence, or efficiently return just the n-most probable (Carroll, 1993).\\n\\nThe table also gives an indication of the best and worst possible performance of the disambiguation component of the system, showing the results obtained when parse selection is replaced by a simple random choice, and the results of evaluating the manually-created treebank against the corresponding Susanne bracketings. In this latter figure, the mean number of crossings is greater than zero mainly because of compound noun bracketing ambiguity which our grammar does not attempt to resolve, always returning a right-branching binary analysis.\\n\\nBlack (1993:7) uses the crossing brackets measure to define a notion of structural consistency, where the structural consistency rate for the grammar is defined as the proportion of sentences for which at least one analysis contains no crossing brackets, and reports a rate of around 95% for the IBM grammar tested on the computer manual corpus. The problem with the GEIG scheme and with structural consistency is that both are still weak measures (designed to avoid problems of parser/treebank representational compatibility) which lead to unintuitive numbers whose significance still depends heavily on details of the relationship between the representations compared (c.f. the compound noun issue mentioned above).\\n\\nSchabes et al. (1993) and Magerman (1995) report results using the GEIG evaluation scheme which are numerically superior to ours. However, their experiments are not strictly compatible because they both utilise more homogeneous and probably simpler corpora. In addition, Schabes et al. do not recover tree labelling, whilst Magerman has developed a parser designed to produce identical analyses to those used in the Penn Treebank, removing the problem of spurious errors due to grammatical incompatibility. Both these approaches achieve better coverage by constructing the grammar fully automatically. No one has yet shown that any robust parser is practical and useful for some NLP task. However, it seems likely that say rule-to-rule semantic interpretation will be easier with hand-constructed grammars with an explicit, determinate ruleset. A more meaningful comparison will require application of different parsers to an identical and extended test suite and utilisation of a more stringent standard evaluation procedure sensitive to node labellings.\\n\\nParse Selection and Punctuation\\n\\nConclusions\\n\\nBriscoe and Carroll (1993) and Carroll (1993) showed that the LR model, combined with a grammar exploiting subcategorisation constraints, could achieve good parse selection accuracy but at the expense of poor coverage of free text. The results reported here suggest that improved coverage of heterogeneous text can be achieved by exploiting textual and grammatical constraints on PoS and punctuation sequences. The experiments show that grammatical coverage can be greatly increased by relaxing subcategorisation constraints, and that text grammatical or punctuation-cued constraints can reduce ambiguity and increase coverage during parsing.\\n\\nTo our knowledge these are the first experiments which objectively demonstrate the utility of punctuation for resolving syntactic ambiguity and improving parser coverage. They extend work by Jones (1994) and Briscoe and Carroll (1994) by applying a wide-coverage text grammar to substantial quantities of naturally-punctuated text and by quantifying the contribution of punctuation to ambiguity resolution in a well-defined probabilistic parse selection model.\\n\\nAccurate enough parse selection for practical applications will require a more lexicalised system. Magerman's (1995) parser is an extension of the history-based parsing approach developed at IBM (e.g. Black, 1993) in which rules are conditioned on lexical and other (essentially arbitrary) information available in the parse history. In future work, we intend to explore a more restricted and semantically-driven version of this approach in which, firstly, probabilities are associated with different subcategorisation possibilities, and secondly, alternative predicate-argument structures derived from the grammar are ranked probabilistically. However, the massively increased coverage obtained here by relaxing subcategorisation constraints underlines the need to acquire accurate and complete subcategorisation frames in a corpus-driven fashion, before such constraints can be exploited robustly and effectively with free text.\\n\\nReferences Billot, S. and Lang, B. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the 27th Meeting of Association for Computational Linguistics, 143-151. Vancouver, Canada.\\n\\nBlack, E., Garside, R. and Leech, G. (eds.) 1993. Statistically-Driven Computer Grammars of English: The IBM/ Lancaster Approach. Rodopi, Amsterdam.\\n\\nBriscoe, E. 1994. Prospects for practical parsing of unrestricted text: robust statistical parsing techniques. In Oostdijk, N  de Haan, P. eds. Corpus-based Research into Language. Rodopi, Amsterdam: 97-120.\\n\\nBriscoe, E. and Carroll, J. 1993. Generalised probabilistic LR parsing for unification-based grammars. Computational Linguistics 19.1: 25-60.\\n\\nBriscoe, E. and Carroll, J. 1994. Parsing (with) Punctuation. Rank Xerox Research Centre, Grenoble, MLTT-TR-007.\\n\\nBriscoe, E., Grover, C., Boguraev, B. and Carroll, J. 1987. A formalism and environment for the development of a large grammar of English. In Proceedings of the 10th International Joint Conference on Artificial Intelligence, 703-708. Milan, Italy.\\n\\nCarroll, J. 1993. Practical unification-based parsing of natural language. Cambridge University, Computer Laboratory, TR-314.\\n\\nCarroll, J. 1994. Relating complexity to practical performance in parsing with wide-coverage unification grammars. In Proceedings of the 32nd Meeting of Association for Computational Linguistics, 287-294. Las Cruces, NM.\\n\\nCarroll, J. and Grover, C. 1989. The derivation of a large computational lexicon for English from LDOCE. In Boguraev, B. and Briscoe, E. eds. Computational Lexicography for Natural Language Processing. Longman, London: 117-134.\\n\\nChurch, K. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the 2nd Conference on Applied Natural Language Processing, 136-143. Austin, Texas.\\n\\nElworthy, D. 1993. Part-of-speech tagging and phrasal tagging. Acquilex-II Working Paper 10, Cambridge University Computer Laboratory (can be obtained from cide@cup.cam.ac.uk).\\n\\nElworthy, D. 1994. Does Baum-Welch re-estimation help taggers?. In Proceedings of the 4th Conf. Applied NLP. Stuttgart, Germany.\\n\\nGarside, R., Leech, G. and Sampson, G. 1987. Computational analysis of English. Longman, London.\\n\\nGrishman, R., Macleod, C. and Meyers, A. 1994. Comlex syntax: building a computational lexicon. In Proceedings of the International Conference on Computational Linguistics, COLING-94, 268-272. Kyoto, Japan.\\n\\nGrover, C., Carroll, J. and Briscoe, E. 1993. The Alvey Natural Language Tools Grammar (4th Release). Cambridge University Computer Laboratory, TR-284.\\n\\nHarrison, P., Abney, S., Black, E., Flickenger, D., Gdaniec, C., Grishman, R., Hindle, D., Ingria, B., Marcus, M., Santorini, B. and Strzalkowski, T. 1991. Evaluating syntax performance of parser/grammars of English. In Proceedings of the Workshop on Evaluating Natural Language Processing Systems. ACL.\\n\\nHindle, D. 1989. Acquiring disambiguation rules from text. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, 118-25. Vancouver, Canada.\\n\\nJackendoff, R 1977. X-bar Syntax. MIT Press; Cambridge, MA..\\n\\nJones, B 1994. Can punctuation help parsing?. In Proceedings of the Coling94. Kyoto, Japan.\\n\\nMagerman, D. 1995. Statistical decision-tree models for parsing. In Proceedings of the 33rd annul Meeting of the Association for Computational Linguistics. Boston, MA.\\n\\nde Marcken, C. 1990. Parsing the LOB corpus. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, 243-251. New York.\\n\\nNunberg, G. 1990. The linguistics of punctuation. CSLI Lecture Notes 18, Stanford, CA.\\n\\nPereira, F. and Warren, D. 1980. Definite clause grammars for language analysis - a survey of the formalism and a comparison with augmented transition networks. Artificial Intelligence 13.3: 231-278.\\n\\nSampson, G. 1994. Susanne: a Doomsday book of English grammar. In Oostdijk, N  de Haan, P. eds. Corpus-based Research into Language. Rodopi, Amsterdam: 169-188.\\n\\nSampson, G., Haigh, R., and Atwell, E. 1989. Natural language analysis by stochastic optimization: a progress report on Project APRIL. Journal of Experimental and Theoretical Artificial Intelligence 1: 271-287.\\n\\nSchabes, Y., Roth, M. and Osborne, R. 1993. Parsing of the Wall Street Journal with the inside-outside algorithm. In Proceedings of the Meeting of European Association for Computational Linguistics. Utrecht, The Netherlands.\\n\\nTaylor, L. and Knowles, G. 1988. Manual of information to accompany the SEC corpus: the machine-readable corpus of spoken English. University of Lancaster, UK, Ms..\\n\\nFootnotes\\n\\nBlack et al. (1993:13) define an apparently similar measure, parse base, as the ``geometric mean of the number of parses per word for the entire corpus'', but in the immediately following sentence talk about raising it to the power of the number of words in a sentence, which is inappropriate for a simple ratio. This is a strong measure, since it not only accounts for structural identity between trees, but also correct rule application at every node.\", metadata={'source': '../data/raw/cmplg-xml/9510005.xml'}),\n",
       " Document(page_content=\"A CENTERING APPROACH TO PRONOUNS 25th Annual Meeting of the Association of Computational Linguistics, 1987 Introduction\\n\\nCONSTRAINTS 1. There is precisely one Cb. 2. Every element of Cf(Un) must be realized in Un. 3. Cb(Un) is the highest-ranked element of\\n\\nCf(Un\\n\\n\\n\\n1) that\\n\\nis realized in Un.\\n\\nRULES\\n\\n1.\\n\\nIf some element of\\n\\nCf(Un-1) is realized as a pronoun in Un, then so is Cb(Un). 2. Continuing is preferred over retaining which is preferred over shifting.\\n\\nWe are aware that this ranking usually coincides with surface constituent order in English. It would be of interest to examine data from languages with relatively freer constituent order (e.g. German) to determine the influence of constituent order upon centering when the grammatical functions are held constant. In addition, languages that provide an identifiable topic function (e.g. Japanese) suggest that topic takes precedence over subject.\\n\\nExtension\\n\\nBrennan drives an Alfa Romeo. She drives too fast. Friedman races her on weekends. She often beats her.\\n\\nAlgorithm for centering and pronoun binding\\n\\nThere are three basic phases to this algorithm. First the proposed anchors are constructed, then they are filtered, and finally, they are classified and ranked. The proposed anchors represent all the co-specification relationships available for this utterance.\\n\\nDiscussion\\n\\nDiscussion of the algorithm\\n\\nThe goal of the current algorithm design was conceptual clarity rather than efficiency. The hope is that the structure provided will allow easy addition of further constraints and preferences. It would be simple to change the control structure of the algorithm so that it first proposed all the continuing or retaining anchors and then the shifting ones, thus avoiding a precomputation of all possible anchors.\\n\\nA computational system for generation would try to plan a retention as a signal of an impending shift, so that after a retention, a shift would be preferred rather than a continuation.\\n\\nFuture Research\\n\\nOf course the local approach described here does not provide all the necessary information for interpreting pronouns; constraints are also imposed by world knowledge, pragmatics, semantics and phonology.\\n\\nThere are other interesting questions concerning the centering algorithm. How should the centering algorithm interact with an inferencing mechanism? Should it make choices when there is more than one proposed anchor with the same ranking? In a database query system, how should answers be incorporated into the discourse model? How does centering interact with a treatment of definite/indefinite NP's and quantifiers?\\n\\nWe are exploring ideas for these and other extensions to the centering approach for modeling reference in local discourse.\\n\\nAcknowledgements\\n\\nWe would like to thank the following people for their help and insight: Hewlett Packard Lab's Natural Language group, CSLI's DIA group, Candy Sidner, Dan Flickinger, Mark Gawron, John Nerbonne, Tom Wasow, Barry Arons, Martha Pollack, Aravind Joshi, two anonymous referees, and especially Barbara Grosz.\\n\\nBibliography\\n\\nEmmon Bach and Barbara H. Partee. Anaphora and semantic structure. In J. Kreiman and A. Ojeda, editors, Papers from the Parasession on Pronouns and Anaphora, pages 1-28. CLS, Chicago, IL, 1980.\\n\\nNoam Chomsky.\\n\\nOn binding.\\n\\nLinguistic Inquiry, 11:1\\n\\n\\n\\n46, 1980.\\n\\nBarbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. Providing a unified account of definite noun phrases in discourse. In Proc. 21st Annual Meeting of the ACL, Association of Computational Linguistics, pages 44-50, 1983.\\n\\nBarbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. Towards a computational theory of discourse interpretation. Unpublished Manuscript, 1986.\\n\\nBarbara J. Grosz and Candace L. Sidner. Attentions, intentions and the structure of discourse. Computational Linguistics, 12:175-204, 1986.\\n\\nRaymonde Guindon. Anaphora resolution: Short term memory and focusing. In Proc. 23st Annual Meeting of the ACL, Association of Computational Linguistics, pages 218-227, Chicago, IL, 1985.\\n\\nAravind K. Joshi and Scott Weinstein. Control of inference: Role of some aspects of discourse structure - centering. In Proc. International Joint Conference on Artificial Intelligence, pages 385-387, 1981.\\n\\nMegumi Kameyama. A property-sharing constraint in centering. In Proc. 24th Annual Meeting of the ACL, Association of Computational Linguistics, pages 200-206, New York, NY, 1986.\\n\\nPeter Sells. Coreference and bound anaphora: A restatement of the facts. In Choe Berman and McDonough, editors, Proceedings of NELS 16, University of Massachusetts, 1985. GLSA.\\n\\nIvan Sag and Jorge Hankamer. Towards a theory of anaphoric processing. Linguistics and Philosophy, 7:325-345, 1984.\\n\\nCandace L. Sidner. Focusing for interpretation of pronouns. American Journal of Computational Linguistics, 7(4):217-231, 1981.\\n\\nCandace L. Sidner. Focusing in the comprehension of definite anaphora. In M. Brady and R.C. Berwick, editors, Computational Models of Discourse. MIT Press, 1983.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9410005.xml'}),\n",
       " Document(page_content='Automatic Identification of Support Verbs: A Step Towards a Definition of Semantic Weight\\n\\nCurrent measures of the readability of texts are very simplistic, typically based on counts of words or syllables per sentence. A more sophisticated analysis needs to take account of the fact that the particular distributions of meanings across wordings chosen by the writer, and the consequent variations in syntactic structure, have a significant effect on readability. A step towards the required sophistication is provided by the notion of  LEXICAL DENSITY (Halliday, 1985), which suggests that different words carry different amounts of semantic weight; this idea of semantic weight is also used implicitly in areas such as information retrieval and authorship attribution. Current definitions of these notions of lexical density and semantic weight are based on the division of words into closed and open classes, and on intuition. This paper develops a computationally tractable definition of semantic weight, concentrating on what it means for a word to be semantically light; the definition involves looking at the frequency of a word in particular syntactic constructions which are indicative of lightness. Verbs such as make and take, when they function as support verbs, are often considered to be semantically light.\\n\\nTo test our definition, we carried out an experiment based on that of Grefenstette and Teufel (1995), where we automatically identify light instances of these words in a corpus; this was done by incorporating our frequency-related definition of semantic weight into a statistical approach similar to that of Grefenstette and Teufel. The results show that this is a plausible definition of semantic lightness for verbs, which can possibly be extended to defining semantic lightness for other classes of words.\\n\\nIntroduction\\n\\nThere are a number of ways of measuring properties of text, and from there proceeding to make stylistic judgments; they can be found in style guides, and include calculating readability indices, counting the number of passive constructions, and so on. One attribute of text that is rarely mentioned explicitly in these style guides, but which underpins many of the pieces of advice, is that of SEMANTIC DENSITY (see Dras and Dale, 1995). Consider the following pair of sentences, taken from Halliday (1985):\\n\\nIt is apparent that the first of the pair is `denser\\' than the second: both express the same (propositional) meaning, but the first does so in a more compact way. Halliday terms this  LEXICAL DENSITY, `the density with which information is presented\\' (p68), and measures it by looking at the proportion of  CONTENT WORDS. Halliday adopts a fairly standard conception of content words as those which belong to the open word classes:  nouns, verbs, adjectives and so on. Non-content words are then those that belong to the closed classes, such as prepositions, auxiliaries and so on. The non-content, closed class words are viewed by Halliday as lacking in informational content.\\n\\nHe does, however, note that there are some words on the borderline between content and non-content words which are lexical items but in many cases do little more than perform a grammatical function. These include the noun thing, as in That\\'s a thing I could do without (which could be rewritten as I could do without that), and the verb make, as in Christophe made a decision to come to the Drag Day (possibly rewritten as Christophe decided to come to the Drag Day).\\n\\nIn this paper I look at a possible definitional extension of non-content words, which incorporates the intuition expressed by Halliday that words like make often contribute little, if any, propositional meaning to the text. This new definition is tested by an experiment modelled on that of Grefenstette and Teufel (1995), which tries to find the support verb that particular nominalisations will take--why decision, for example, takes make, and not have, do, eat or perambulate.\\n\\nLightness of Words\\n\\nCurrent views of lightness\\n\\nThe fact that a word contributes little if any content to a text is used in a number of areas of linguistics and computational linguistics. In information retrieval, non-content words are discarded, as they cannot help to identify the topic of a text. Mosteller and Wallace (1984), on the other hand, retain them and discard the content words when attempting to statistically determine the authorship of the disputed Federalist papers, reasoning that while content words may vary across topic, for a given author non-content words will not. Halliday (1985), as mentioned above, uses them to define the informational density of a text, in order to compare spoken and written text.\\n\\nWhat comprises the class of non-content words is neither uniform nor clearly defined. Halliday defines it to be the set of those words which are part of a closed class system; information retrieval commonly uses a combination of high-frequency and known function words; Mosteller and Wallace use a list which was derived from sources such as the King James Bible.\\n\\nHalliday proposes that relative frequency of a word can be used to indicate the amount of information it contributes. If this is true, the choice of closed class words to represent non-content words is plausible, since a given grammatical item (the, and, it) is more likely to have a higher frequency of occurrence than a given lexical item (dog, run, verisimilitude). It would also include make and thing, which are high frequency lexical items.\\n\\nHowever, this idea needs to be further refined. A quick inspection of a corpus will show that there are a number of words with definite propositional content which rank above non-content words in frequency. In the 8 million word Grolier\\'s Encyclopedia the verb include (which definitely conveys information, so it can\\'t be a light constituent) occurs transitively 4284 times, as against make\\'s 2697 times.\\n\\nA different view\\n\\nMake does, however, occur more frequently in constructions which I will call  LIGHT CONSTRUCTIONS, such as make a decision; they are mentioned under one name or another by linguists and style guide authors, and what characterises them is that the light constituent can be deleted (with some rewriting of the remaining text to retain grammaticality). For example, make a decision can be rewritten as decide, the light element being make. Jespersen (1954) is one of the earliest to note these, commenting on the  LIGHT VERBS in expressions such as take a walk. Style guide writers like Kane (1983) mention `deadwood\\' which can be eliminated from phrases such as It is important for teachers to have a knowledge of their students (a possible rewriting being It is important for teachers to know their students). There are quite a few of these constructions, such as light verbs with noun phrase complements, light verbs with adjectival complements, and light nouns with post-modifiers (Dras and Dale, 1995), but this paper only looks at one construction, the light verbs with NP complements. By definition, these constructions will characteristically contain light verbs; this paper therefore proposes that a modified definition be used for indicating whether a word can be considered a non-content one: that the word has a high relative frequency in these light constructions.\\n\\nIt has been suggested that semantic factors are what determine the relationship between a syntactic construction and its associated light verb. Wierzbicka (1982) proposes a set of semantic rules for determining the light verb that corresponds to a particular noun object--an explanation of why one can have a drink but not *have an eat. However, defining these rules by hand for all nouns would be too time-consuming to be practical. Grefenstette and Teufel (1995) take a statistical approach to finding what is termed the  SUPPORT VERB for a particular noun. They look at several nouns, including appeal,  proposal, and demand. In a corpus of newspaper articles, they look for occurrences of the noun and corresponding verb to find the most likely candidate for the support verb. They find that the most likely support verb for appeal is make, which accords with intuition, but for proposal, their system also finds reject as an equally likely candidate; and for demand, the most likely candidate is meet. In this paper, I conduct a similar experiment, finding support verbs for given nouns, to test the definition proposed above: that a word\\'s status regarding content-freeness is related to its frequency of occurrence in light constructions.\\n\\nExperiment\\n\\nThe aim of the experiment is to show that there is a relationship between relative frequency of verbs in particular constructions and the content-freeness of these verbs. A consequence of this is to be able to choose the light verb that corresponds to a nominalisation in a light verb-NP complement construction--the nominalisation\\'s support verb (SV). Deverbal nominalisations are chosen as they are the kinds of grammatical entities which enter into the SV-NP complement construction.\\n\\nExperiment design\\n\\nA way of extracting light verbs from a corpus is to simply take all verb-object pairs where the object is a deverbal nominalisation. Grefenstette and Teufel use only  LOCAL INFORMATION, information that is specific to a particular nominal. Counting all occurrences of each noun in verb-object pairs yields a local relative frequency for each verb with respect to that noun. So, to determine the support verb for proposal they look only at verbs which co-occur with the noun proposal. While it seems intuitively obvious to native English speakers that make is a more likely candidate for support verb than reject, the local frequency evidence does not indicate this. Speakers also use the fact that make is the support verb for other nominalisations such as judgment and decision. I have termed this knowledge  GLOBAL INFORMATION. Counting all occurrences of each verb, regardless of their objects, yields a global relative frequency for that verb. In this experiment the local information is combined with the global information to produce a modified likelihood of being a support verb.\\n\\nDeriving local and global information\\n\\nTo gather local and global information, the 1992 version of Grolier\\'s encyclopedia, tagged by the part-of-speech tagger developed by Brill (1993), was used. A heuristic for producing the local information involved searching the corpus for the nominal, determining the verb (if any) for which the nominal was the direct object, and measuring the relative frequency of these verbs.\\n\\nThe theoretical global information is a measure of how productive a given support verb is: that is, how many different instances of the SV-NP construction it enters into. The more productive verbs (like make) rank higher on the list than less productive verbs (like bear); this is combined with the local information so that the more productive verbs, for a particular nominal, are subsequently ranked more highly than by the local information alone. This weighting technique is similar to that used by Yarowsky (1992) in the context of sense disambiguation. In his work he uses counts of words in a window around a key word to determine the salience of this key word to a particular sense. These word counts are weighted so that more common words contribute less; that is, the less common words are accorded more importance. We, on the other hand, want to give more importance to the more common words, given our assumption that it is high relative frequency in particular constructions that helps define semantic lightness.\\n\\nGrefenstette and Teufel note that a confounding factor in the local information, when picking out nominals and their associated verbs, is that the nominal may have become  CONCRETISED. Generally, nominals represent an abstract concept, being essentially events represented in noun form; but it is possible for the nominal to represent a physical embodiment of that concept. For example:\\n\\nThe abstract and concretised versions will tend to have different associated verbs. However, if the relative frequency hypothesis is true, and the global information is an accurate reflection of the innate lightness of a verb, this will elevate the light verb over the `heavier\\' ones, which will be more likely to be associated with the concretised forms.\\n\\nIn practice, the global information is calculated from the aggregate of the local data. This means that there is a lot of noise--all of the incorrect candidates for support verb are included in the global information--but again, if the relative frequency hypothesis is true, the relative frequency of the support verb in the local information will be high (although not necessarily the highest), while this is not true for non-support verbs. So aggregating all of these should reinforce the support verbs and not the others.\\n\\nGenerating nominalisations\\n\\nTo construct the global information, a comprehensive list of deverbal nominalisations is needed, together with the associated support verbs, determined from the local information. To generate this list in a partially automated manner, Longman\\'s Dictionary of Contemporary English (LDOCE) was used, including both built-in information and a heuristic: a nominal is an event represented in noun form, so the procedure used here for deriving a list of them involved looking for nouns with associated  STEM VERBS.\\n\\nSome verbs have this information encoded in their entries: for example, adjust lists adjustment as its nominalisation; there were 257 verbs in this category. For others, an automatic orthographic heuristic that matched nouns with verbs was manually filtered to produce 1414 more deverbal nominalisations. A system to identify support verbs for nominalisations was implemented by tabulating all the verbs for which these nominals were the direct object.\\n\\nThe list of nominals did not cover some of the nominals from the test set (listed in the table below). The local information was generated for each of the excluded test set nominals and aggregated into the global information. Candidates for support verb were ranked in order of the product of local and global relative frequency of each candidate verb.\\n\\nThe test set and results\\n\\nThe light verb constructions and their constituent nominals used for testing were taken from a range of sources, so that they would not be biased to one particular genre.\\n\\nthe source text;\\n\\nthe corresponding verb, which the source can be rewritten as;\\n\\nthe reference for the source text;\\n\\nthe system\\'s first choice candidate for support verb for the source text\\'s constituent nominalisation;\\n\\nthe system\\'s second choice; and\\n\\nthe ratio of the adjusted frequency, which is defined as the product of local and global relative frequencies, for the first and second choices.\\n\\nDiscussion\\n\\nOf the 18 examples, 13 of the choices for support verb match the corresponding one from the source text. Of the five incorrect ones, three were incorrect because of lack of data: there are no occurrences of snooze or shove as direct objects of verbs in Grolier\\'s, most probably because they belong to a more informal register than that used in encyclopedias. Similarly, have a drink is an informal phrase that would not normally be found in an encyclopedia.\\n\\nAnother of the incorrect cases, harm, had cause as the proposed alternative. This is an equally valid support verb, and in any case, do was the second choice by only a small margin. This is true for a number of cases: where there is an alternative support verb to the one used in the source text, the second alternative represents another plausible choice, and the frequency ratio margin is small (for example, for change and resemblance).\\n\\nSo if only the cases where enough data exists are considered, and if alternative support verbs are allowed, the success rate becomes 14 of 15.\\n\\nConclusion\\n\\nIt is apparent that what constitutes a valid light verb construction depends on the genre and register of the text. More accurate results could no doubt be obtained by using a corpus that was more representative of general English. Also, where shortcuts were taken (for example, by not removing concretised nominalisations), more precision could be obtained. Notwithstanding these considerations, using the relative frequency of a verb in light constructions seems to be a fairly good indicator of a verb\\'s content-freeness, providing plausible choices for support verbs for nominalisations.\\n\\nFurther work will involve extending this definition to other light constructions--light verbs with adjectival complements, and light nouns with post-modifiers--and fitting closed-class words, which traditionally comprise the class of non-content words, into the framework. This framework can then be used to give a more accurate indication of lexical density; to more accurately choose words to leave out, or include in, in the fields of information retrieval and authorship attribution; and to fine-tune stylistic judgments.\\n\\nAcknowledgements\\n\\nThis work has benefited from the kind assistance of Robert Dale and Mark Lauer. I\\'d also like to thank Mike Johnson for many fruitful discussions. Financial support is gratefully acknowledged from the Australian Government and the Microsoft Institute.\\n\\nBibliography\\n\\n1 Brill, Eric. 1993. A Corpus-based Approach to Language Learning. PhD Thesis, University of Pennsylvania, PA.\\n\\n2 Dras, Mark and Robert Dale. 1995. Style and Semantic Density. Microsoft Institute Research Report no. 95-04.\\n\\n3 Grefenstette, Greg and Simone Teufel. 1995. \"Corpus-based Method for Automatic Identification of Support Verbs for Nominalizations\". To appear in Proc. of EACL\\'95.\\n\\n4\\n\\nHalliday, Michael A. K.\\n\\n1985.\\n\\nSpoken and Written Language.\\n\\nOxford University Press, Oxford.\\n\\n5 Harris, Zellig. 1957. \"Co-occurrence and transformation in linguistic structure\". Language, 33, 293-340.\\n\\n6 Huddleston, Rodney. 1968. Sentence and Clause in Scientific English. Communication Research Centre, University College, London.\\n\\n7 Jespersen, Otto. 1942. A Modern English Grammar on Historical Principles, Vol VI. Allen and Unwin, London.\\n\\n8 Kane, Thomas S. 1983. The Oxford Guide to Writing. Oxford University Press. New York, NY.\\n\\n9 Mosteller, Frederick and David Wallace. 1984. Applied Bayesian and Classical Inference: the Case of the Federalist Papers. Springer-Verlag. New York, NY.\\n\\n10 Wierzbicka, Anna. 1982. \"Why Can You Have a Drink When You Can\\'t *Have an Eat?\". Language, 58(4), 753-799.\\n\\n11 Yarowsky, D. 1992. \"Word sense disambiguation using statistical models of Roget\\'s categories trained on large corpora\". Proceedings of the 14th International Conference on Computational Linguistics, 454-460.\\n\\nFootnotes\\n\\nReprinted with kind permission from: \"Automatic Identification of Support Verbs: A Step Towards a Definition of Semantic Weight\" in Proceedings of the Eighth Australian Joint Conference on Artificial Intelligence (World Scientific, Singapore, 1995) pp 451 - 458. Copyright by World Scientific Publishing Co. Pte, 1995.', metadata={'source': '../data/raw/cmplg-xml/9510007.xml'}),\n",
       " Document(page_content=\"Tagging the Teleman Corpus\\n\\nExperiments were carried out comparing the Swedish Teleman and the English Susanne corpora using an HMM-based and a novel reductionistic statistical part-of-speech tagger. They indicate that tagging the Teleman corpus is the more difficult task, and that the performance of the two different taggers is comparable.\\n\\nIntroduction\\n\\nThe performed experiments do not only serve to evaluate the two taggers, but also shed some new light on the Teleman corpus as an evaluation domain for part-of-speech taggers compared to other, English, corpora.\\n\\nThe Teleman Corpus\\n\\nFor the experiments, we used two different tagsets. First, we used the original tagset, consisting of 258 tags. Each of the 14,191 word types can have between one and 15 of the 258 tags (the highly ambiguous word ``fr'' (for, stern, lead, too, ...) has the maximum number of tags). We then used a reduced tagset, consisting of 19 tags, which represent  common syntactic categories and punctuation. This tagset is identical  to that used in the publications mentioned above. Each of the word types then has between one and 7 tags (``fr'' and ``i'' have the maximum number of tags).\\n\\nComparison with an English Corpus\\n\\nTags in the Susanne corpus with indices are counted as separate tags.\\n\\nUnknown words are words that occur only in the test set, but not in the training set.\\n\\nThe remaining 9,823 words of the Susanne corpus were not used in the experiments.\\n\\nThe HMM Approach\\n\\n, with Ti tags and Wi words, which is impossible in all practical cases, one calculates and maximizes\\n\\nto find the best sequence of tags for a given sequence of words.\\n\\nThe Reductionistic Statistical Approach\\n\\nAlthough not yet fully realized, the basic philosophy behind the reductionistic statistical approach is to give it the same expressive power as the Constraint Grammar system.\\n\\nConstraint Grammar\\n\\nThe Constraint Grammar system works as follows: First, the input string is assigned all possible tags from the lexicon, or rather, from the morphological analyzer. Then, tags are removed iteratively by repeatedly applying a set of rules, or constraints, to the tagged string. When no more tags are removed by the last iteration, the process terminates, and morphological disambiguation is concluded. Then a set of syntactic tags are assigned to the tagged input string and a similar process is performed for syntactic disambiguation. This method is often referred to as reductionistic tagging.\\n\\nThe rules are sort-of formulated as finite state automata [Tapanainen, personal communication], which allows very fast processing.\\n\\nEach rule applies to a current word with a set of candidate tags. The structure of a rule is typically: ``In the following context, discard the following tags.'' or ``In the following context, commit to the following tag.'' We will call discarding or committing to tags the rule action. A typical rule context is: ``There is a word to the left that is unambiguously tagged with the following tag, and there are no intervening words tagged with such and such tags.''\\n\\nThe New Approach\\n\\nThe structure of the Constraint Grammar rules readily allows their contexts to be viewed as the conditionings of conditional probabilities, and the actions have an obvious interpretation as the corresponding probabilities.\\n\\nEach context type can be seen as a separate information source, and we will combine information sources\\n\\nby multiplying the scaled\\n\\nprobabilities:\\n\\nThis formula can be established by Bayesian inversion, then performing the independence assumptions, and renewed  Bayesian inversion:\\n\\nIn standard statistical part-of-speech tagging there are only two information sources -- the lexical probabilities and the tags assigned to neighbouring words. We thus have:\\n\\nThe context will in general not be fully disambiguated. Rather than employing dynamic programming over the lattice of remaining candidate tags, the new approach uses the weighted average over the remaining candidate tags  to estimate the probabilities:\\n\\nIt is assumed that\\n\\nconstitutes a partition of the context C, i.e., that\\n\\nand that\\n\\nfor\\n\\n. In particular, trigram probabilities are combined as follows:\\n\\nHere T denotes a candidate tag of the current word, Tl denotes a candidate tag of the immediate left neighbour, and Tr denotes a candidate tag of the  immediate right neighbour. C is the set of ordered pairs (Tl,Tr) drawn from the set  of candidate tags of the immediate neighbours.\\n\\nis the symmetric trigram\\n\\nprobability.\\n\\nThe tagger is reductionistic since it repeatedly removes low-probability candidate tags. The probabilities are then recalculated, and the process terminates when  the probabilities have stabilized and no more tags can be removed without  jeopardizing the recall; candidate tags are only removed if their probabilities are below some threshold value.\\n\\nSparse Data\\n\\nHandling sparse data consists of two different tasks: 1. Estimating the probabilities of events that do not occur in the training data. 2. Improving the estimates of conditional probabilities where the number of observations under this conditioning is small. Coping with unknown words, i.e., words not encountered in the training set, is an archetypical example of the former task. Estimating probability distributions conditional on small contexts is an example of the latter task. We will examine several approaches to these tasks.\\n\\n, where Nf denotes the frequency of frequency f.  Alternatively, one can use linear interpolation of the probabilities obtained by MLE,\\n\\nSuccessive Abstraction\\n\\nAssume that we want to estimate the probability\\n\\nof the event  E given a context C from the number of times NE it occurs in N = |C| trials, but that this data is sparse. Assume further that there is abundant  data in a more general context\\n\\nthat we want to use to  get a better estimate of\\n\\n.\\n\\nIf there is an obvious linear order\\n\\nof the various  generalizations Ck of C, we can build the estimates of\\n\\non the relative frequency\\n\\nof event E in context Ck and the previous estimates of\\n\\n. We call this method linear successive abstraction. A simple example is estimating the probability\\n\\nof a tag T given\\n\\n, the last j+1 letters of the word. In this case, the estimate will be based on the relative frequencies\\n\\n, the square root of the size of context C, which is the active ingredient of the standard deviation of r.\\n\\nIf there is only a partial order of the various generalizations, the scheme is  still viable. For example, consider generalizing symmetric trigram statistics, i.e., statistics  of the form\\n\\n. Here, both Tl and Tr are one-step generalizations of the context Tl,Tr, and both have in turn the common  generalization\\n\\nand\\n\\nWe call this partial successive abstraction.\\n\\nExperiments\\n\\nFor the experiments, both corpora were divided into three sets, one large set and two small sets. We used three different divisions into training and testing sets. First, all three sets were used for both training and testing. In the second and third case, training and test sets were disjoint, the large set and one of the small sets were used for training, the remaining small set was used for testing. As a baseline to indicate what is gained by taking the context into account, we performed an additional set of experiments that used lexical probabilities only, and ignored the context.\\n\\nHMM Approach\\n\\nAs opposed to trigram tagging, lexical tagging ignores context probabilities and is based solely on lexical probabilities. Each word is assigned its most frequent tag from the training corpus. Unknown words were assigned the most frequent tag of words that occurred exactly once in the training corpus. The most frequent tags for single occurrence words are for the Teleman corpus NNSS (indefinite noun-noun compound) and noun (large and small tagset, resp. ), for the Susanne corpus NN2 (plural common noun) and NN (common noun; again large and small tagset resp. ).\\n\\nTagging speed was generally between 1000 and 2000 words per second on a SparcServer 1000; most of this variation was due to variations in the number of unknown words.\\n\\nAnother interesting result is that accuracy increases when the size of the tagset increases for the cases where known text is tagged and context probabilities are taken into account. This means that the additional information about the context in the larger tagset is very helpful for disambiguation, but only when disambiguating known text. This could arise from the fact that a large number (\\n\\n) of the trigrams that occur in the training text occur exactly once. And most of the possible trigrams do not occur at all (generally more than 90%, depending on the size of the tagset). Now, the trigram approach has a distinct bias to those trigrams that occurred once over those that never occurred. These happen to be the right ones for known text but not necessarily for new text, thus the positive effect of a larger tagset vanishes for fresh text.\\n\\nThe results for the Susanne corpus are similar to those reported in other publications for (other) English corpora.\\n\\nReductionistic Approach The employed treatment of unknown words is quite effective.\\n\\nUsing contextual information, i.e., trigrams, improves tagging accuracy.\\n\\nThe performance is on pair with the HMM tagger and comparable to state-of-the-art statistical part-of-speech taggers.\\n\\nTeleman is a considerably tougher nut to crack than Susanne.\\n\\nThe tagging speed was typically a couple of hundred words per second on a SparcServer 1000, but varied with the size of the tagset and the amount of remaining ambiguity.\\n\\nConclusions\\n\\nThe experiments with the HMM approach show that it is much harder to process the Swedish than the English corpus. Although the two corpora are not fully comparable because of the differences in size and tagsets used, they reveal a strong tendency. The difficulty in processing is mostly due to the rather large number of unknown words in the Swedish corpus and the higher degree of ambiguity despite having smaller tagsets. These effects mainly arise from the higher morphological variation of Swedish which calls for additional strategies to be applied. These could be the use of a large corpus-independent lexicon and a separate morphological analysis.\\n\\nIt is reassuring to see that the reductionistic tagger performs as well as the HMM tagger, indicating that the new framework is as powerful as the conventional one when using strictly conventional information sources. The new framework also enables using the same sort of information as the highly successful Constraint Grammar approach, and the hope is that the addition of further information sources can advance state-of-the-art performance of statistical taggers.\\n\\nViewed as an extension of the Constraint Grammar approach, the new scheme allows making decisions on the basis of not fully disambiguated portions of the input string. The absolute value of the probability of each tag can be used as a quantitative measure of when to remove a particular candidate tag and when to leave in the ambiguity. This provides a tool to control the tradeoff between recall (accuracy) and precision (remaining ambiguity).\\n\\nAcknowledgements\\n\\nWe wish to thank Bjrn Gambck for providing information on previous work with the Teleman corpus.\\n\\nBibliography\\n\\nL. E. Baum. ``An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes'', Inequalities III, pp. 1-8, 1972.\\n\\nP. F. Brown, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer and P. S. Roossin. ``Class-based n-gram models of natural language'', Computational Linguistics 18(4) pp. 467-479, 1992.\\n\\nDouglass Cutting. ``A Practical Part-of-Speech Tagger'', in Procs. 9th Scandinavian Conference on Computational Linguistics, pp. 65-70, Stockholm University, 1994.\\n\\nDouglass R. Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun. ``A Practical Part-of-Speech Tagger''. in Procs. 3rd Conference on Applied Natural Language Processing, pp. 133-140, ACL, 1992.\\n\\nMartin Eineborg and Bjrn Gambck. ``Tagging Experiments Using Neural Networks'', in Procs. 9th Scandinavian Conference on Computational Linguistics, pp. 71-82, Stockholm University, 1994.\\n\\nN. W. Francis and H. Kucera. Frequency Analysis of English Usage, Houghton Mifflin, Boston, 1982.\\n\\nW. A. Gale and K. W. Church. ``Poor Estimates of Context are Worse than None'', in Proc. of the Speech and Natural Language Workshop, pp. 283-287, Morgan Kaufmann, 1990.\\n\\nI. J. Good. ``The population frequencies of species and the estimation of population parameters'', Biometrika 40, pp. 237-264, 1953.\\n\\nFred Karlsson, Atro Voutilainen, Juha Heikkil and Arto Anttila (eds). Constraint Grammar. A Language-Independent System for Parsing Unrestricted Text, Mouton de Gruyter, Berlin / New York, 1995.\\n\\nCarl G. de Marcken. ``Parsing the LOB Corpus'', in Procs. 28th Annual Meeting of the Association for Computational Linguistics, pp. 243-251, ACL 1990.\\n\\nL. R. Rabiner. ``A tutorial on hidden Markov models and selected applications in speech recognition'', in Proceedings of the IEEE 77(2), pp. 257-285, 1989.\\n\\nGeoffrey Sampson. English for the Computer, Oxford University Press, Oxford, 1995.\\n\\nChrister Samuelsson. ``Morphological Tagging Based Entirely on Bayesian Inference'', in Procs. 9th Scandinavian Conference on Computational Linguistics, pp. 225-238, Stockholm University, 1994.\\n\\nUlf Teleman. Manual fr grammatisk beskrivning av talad och skriven svenska, (in Swedish), Studentlitteratur, Lund, Sweden 1974.\\n\\nA. Viterbi. ``Error bounds for convolutional codes and an asymptotically optimum decoding algorithm'', in IEEE Transactions on Information Theory, pp. 260-269, 1967.\\n\\nAtro Voutilainen and Juha Heikkil. ``An English constraint grammar (ENGCG): a surface-syntactic parser of English'', in Procs. 14th International Conference on English Language Research on Computerized Corpora, pp. 189-199, Zrich, 1994.\", metadata={'source': '../data/raw/cmplg-xml/9505026.xml'}),\n",
       " Document(page_content='Context and ontology in understanding of dialogs Introduction:  arguments for linking forms, meanings and contexts\\n\\nWe present a new model of natural language based on the concept of construction, consisting of a set of features of form, a set of semantic and pragmatic conditions describing its application context, and a description of its meaning. The model gives us a better handle on phenomena of real language than the standard approaches, such as syntax + semantics a la Montague. It is also an alternative to the standard design based on the pipeline syntax-semantics-pragmatics (we have little to say about morphology at this point). Since this work has been implemented, there is also a computational argument in favor of this approach.\\n\\nWe claim that a linking of form, meaning and context is needed to accurately describe NL constructions, both \"standard\" and \"non-standard\". However, we are not arguing for the unsuitability of syntax for describing a \"core\" of language or a universal grammar. We believe, that such a language core is small, and that the syntactic descriptions of this core are naturally paired with their meanings, which produces a construction-based universal grammar. Furthermore, new methods are needed to handle phenomena of real languages. In this paper, our aim is not to come with linguistic generalizations about universal structural properties of sentences (although, of course, we have nothing against them), but to come with an effective method for natural language understanding, which in addition to computational effectiveness would also have some linguistic and psychological plausibility.\\n\\nThe second group of arguments is strictly linguistic. Many linguistic phenomena can be naturally described by the pairing of their syntactic form with their semantic features. Such patterns of interdependence of syntax and semantics are common both for standard constructions, like NPs, VPs, or Ss, and for more exotic ones, such as sentences with \"let alone\". Attempts to account for those phenomena in systems without such interdependence of syntax and semantics lead to drastic overgeneralizations.\\n\\nFinally, we note that context, especially dialog context, changes the range of applicable constructions. Everybody knows that Kissinger thinks bananas makes sense as an answer to the question What is (was) Nixon\\'s favorite fruit. And, in spoken discourse context, \"fragments\" behave like open idioms; e.g. \"afternoon\" might mean \"in the afternoon\", \"two\" the second of the choices (e.g. for a meeting time), and \"checking\" can stand for \"from (the) checking (account)\", i.e. semantically be a \"source\".\\n\\nThe paper is organized as follows: In Section 2, we present our NLU system and an example that illustrates the need for contextual information in understanding dialogs. In Section 3 we describe the formalism of constructions; Section 4 presents examples of construction (from words to discourse). Section 5 is about the role of  ontologies in the grammar. Section 6 contains an example of parsing with a construction grammar. In Section 7, we argue that construction-based approach captures the intuition of compositional semantics, and that it is an effective and reusable encoding of linguistic knowledge.\\n\\nGrammar, context and ontology in a dialog system\\n\\nIn  MINCAL, the grammar consists of about a hundred constructions and a few hundred lexical entries, and includes both sentential constructions and discourse constructions. (Similar numbers for other applications). The system also contains an encoding of elementary facts about time and parameters of meetings; this background knowledge is encoded in about a hundred Prolog clauses.\\n\\nWe can make the  following observations. (a) From the point of view of the operation of the system, we do not care about the structure of the sentences (provided the parser can handle them); we care only about their meanings, and these meanings depend on context. (b) The issue of ontology is not trivial. Namely, we have to distinguish not between three languages (representations) and corresponding ontologies: the language/ontology for parsing, the language/ontology for representing information about the domain, e.g. the calendar functions, and the language/ontology for representing information about the particular application, e.g. xdiary. A natural question arising is the role of all three representations in parsing.\\n\\nWe will talk about the interaction of form and meaning in the next section. Now we want to make a few observations about the context-dependence of interpreting dialogs. In the first sentence of the above dialog, the context is used to prevent another reading in which with Bob modifies schedule, as in Dance a tango with Bob!. That is, we use a contextual rule saying that for the calendar application people do not modify actions, nor places. By setting up a set of such domain- and application-specific filters, we can remove ambiguities of PP attachment in most cases, e.g. in Set up a lunch in the cafeteria with my boss Postpone the interview at 10 to Monday.\\n\\nIn general, taking the context into account during parsing allows the parser to focus on certain constructions rather than others, as well as to compute certain meanings more accurately. Using context to restrict the constructions that are triggered during parsing greatly reduces the number of edges our parser has to consider, resulting in the increase of speed. The dependence of meaning on context is well known, e.g. deixis or deciding the reference of pronouns. But there are very natural examples not connected to deixis: in our dialog, the expression 8 is interpreted only as a time expression (and not a place or something else), because of the context of the preceding question (asking about the time of the meeting). Similarly,  for computing the meaning of other time expressions: 4 to 6 is more likely to mean \"beginning at 4 and ending at 6\" than \"5:56\" in the context of a question about the time of a meeting. But by using the context the application, we can completely eliminate the second reading, since all meetings are scheduled in 5 minute increments.\\n\\nConstructions: integrating forms, meanings and contexts Constructions: The concept\\n\\nA grammar is a collection of constructions. Each construction is given by the matrix:\\n\\nFor example, we can analyze expressions No, but I\\'ll do it now/send it tomorrow/..., typically given as an answer to a question whether something has been done, as a discourse construction. Its description uses such relations as previous_utterance/p_utter, previous_sentence/p_sent (i.e. the propositional content of the previous utterance), the message/meaning of S, m(S).\\n\\nAs we can see, the construction applies only in the context of a previously asked question, and its message says that the answer to the question is negative, after which it elaborates the answer with a sentence S.\\n\\nThere is no agreement on what is context, or, even more important, on what is not. But, again, from the point of view of an abstract computing device, a context is a collection of relations that are interpreted as constraints. A partial list of such relations can be found in the books on pragmatics and it includes such relations as speaker, hearer, topic, presupposed, assertion, question, command, declaration, ...     . To this list we can add previous_utterance, current_domain, current_application, current_question, default_value, etc.\\n\\nTypes and properties of constructions\\n\\nTo end this general exposition, we list four basic differences between construction grammars and other grammars. (1) Construction grammars are not lexicalized; (2) They are not head driven (since in real dialogs and texts it is often impossible to find the head of a clause); (3) Parsing produces only flat, two level, \"semantic\" structures - which is important from the point of view of efficiency; (4) Ontology plays an important role in organizing the grammar and in guiding the parsing.\\n\\nExamples of constructions\\n\\nSentences and phrases\\n\\nThe set of core constructions describes an agent performing an action; the NP is as an agent and a verb as an action. The English constructions SV, SVO, SVOO, SVOC (subject-verb-object-complement), ... could all be described in a similar fashion. But we show only the SVO construction.\\n\\nThe construction specifies that the action is given by the meaning of the VP (which we assume would consist of a V and adverbials); the agent and object are given by the meanings of the two NPs.\\n\\nWords as constructions\\n\\nWe view languages as collections of constructions which range from words to discourse. We have seen how the same representation scheme can be used for different constructions. In this subsection we apply it to lexical items. For instance, the verbs \"see\" and \"hit\" can be represented as follows:\\n\\nWe end showing how one can write the matrices of the constructions pronoun(him) and determiner(the).\\n\\nEmbedding ontology into grammar\\n\\nIn our construction grammar the representation of constructions is closely coupled with a semantic taxonomy. Thus, for instance, not only do we have an np construction, but also such constructions as np(place), np(duration), np(time), np(time(hour)) etc. In other words, the semantic hierarchy is reflected in the set of linguistic categories. (Notice that categories are not the same as features). Hence we do not have a list, but a tree of grammatical categories.\\n\\nTo be more specific, let us consider temporal categories. Time is divided into the following categories minute, hour, weekday, month,  relative, day_part, week_part, phase, and more categories could be added if needed, e.g. century. Some of these categories are further subdivided: day_part into afternoon, morning, ..., or relative(_) (e.g. in two hours) into hour, minute, day_part, week_part, weekday.\\n\\nNote that (a) different constructions can describe the same object of a given category, for example hour can be given by a numeral or a numeral  followed by the word \"am\" or \"pm\"; (b) there is a continuum of both categories and constructions describing objects of a given category, for instance, we have the following sequence of types of constructions\\n\\nnp ] np(time) ] np(time(month)) ]\\n\\n]  np(time(month(january))) = january\\n\\ncorresponding to the sequence of categories\\n\\nentity ] time ] month = time(month) ]\\n\\n]  january =  time(month(january))\\n\\nIn the first case january is a word (words are constructions, too), in the second case it is a concept.\\n\\nWe end this sections with two notes. First, we can ask: what about verbs and their hierarchies? -- At present we have no hierarchy of actions, and no hierarchy of verbs; perhaps when related actions can appear as parameters of a plan such a need would arise. However np hierarchies help describe arguments of verbs.\\n\\nParsing with constructions\\n\\n\\n\\nexamples\\n\\nParsing with constructions differs a bit from syntactic parsing. First, the collection of features that are used to drive parsing is richer, because it contains terms with semantic and pragmatic interpretation. Second, semantic and pragmatic information is used during parsing. Third, descriptions assigned to strings by the parser are different, namely, the structural information is lost; instead the meanings/messages are produced.\\n\\nThe meaning of the pp (0, 4)  is a simple transformation of the meaning of the np in  (1, 4). But notice that this happens because we have a specific construction which says that a combination of on with an np(time(_)) produces event_time. Altogether, we have encoded a few dozens various pp constructions, but in a given application only a fraction of them are used.\\n\\nIn the same context, please note that, because of the close relationship between the domain ontology and the hierarchy of constructions, we can also postulate a close relationship between the type of meaning a construction expresses and its category (i.e. name), for example, the type of meaning of np(time(hour)) is [type  event_time].\\n\\nAt the end, we obtain two parses of (0, 6), but they have the same messages. The difference lies in the category assigned to pp_list; and in this particular case the choice of that category is not important.\\n\\nKnowledge and meaning\\n\\nCompositionality\\n\\nGiven that, note that a grammar of constructions can account for the ability of the language user to understand the meaning of novel sentences without separating the language into syntax, semantics and pragmatics. Namely, the meaning of a larger construction is a combination of the meanings of its parts (and the way they are put together). The message of the larger construction specifies how its meaning depends on the meanings of the parts.\\n\\nOntology and knowledge bases\\n\\nWe view construction grammars as representations of domain-independent linguistic knowledge. That is why we need a domain-dependent semantic module to map linguistic representations into concepts of the domain. The fact that we have been able to use the same sets of constructions for various domains is an argument for feasibility of this approach.\\n\\nObviously NLU is impossible without access to vast bodies of background knowledge. The fact that meanings of NL utterances are classified and described with the help of general ontologies suggests that linking linguistic and non-linguistic knowledge for the purpose of reasoning about dialogs and texts might be possible without the help of a translator program. Furthermore, it might be possible to modularize those sources of knowledge.\\n\\nConclusions\\n\\nThe innovations we have proposed -- the close coupling of semantic hierarchies with linguistic categories, the use of context in representing linguistic knowledge, and the representation of the grammar as a dictionary of constructions -- not only facilitate the development of the grammar, but also its interaction with the inference engine and the application.\\n\\nObviously, the usefulness of this approach is not limited to natural language interfaces. We have used parts of the grammar of constructions for some information retrieval tasks; and we can easily imagine it being applied to text skimming and to machine translation in limited domains.\\n\\nThe approach to language understanding we are advocating has several advantages: it agrees with the facts of language; it is not restricted to a \"core\" of language, but applies to standard and more exotic constructions, including language fragments; and it is computationally feasible, as it has been implemented in a complete working system.\\n\\nSummarizing, the most important innovations implemented in the system are the close coupling of semantic hierarchies with the set of linguistic categories; the use of context in representing linguistic knowledge, esp. for discourse constructions; and a non-lexicalist encoding of the grammar in a dictionary of constructions. We have obtained a new language model in which forms cannot be separated from meanings. We can talk about meaning in a systematic way, but we do not have compositionality described as a homomorphism from syntax to semantics. (This is theoretically interesting, also because it is closer to intuitions than current models of semantics). We have validated this model by building prototypes of natural language interfaces.\\n\\nBibliography\\n\\nT. Copeck, S. Delisle, and S.Szpakowicz. Parsing and case analysis in tanka. 1992.\\n\\nR.M.W. Dixon. A New Approach to English Grammar on Semantic Principles. Clarendon Press, Oxford, 1991.\\n\\nG. Fauconnier. Mental Spaces. MIT Press, Cambridge, Massachusetts, 1985.\\n\\nC.J. Fillmore, P.Kay, and M.C. O\\'Connor. Regularity and idiomaticity in grammatical constructions. Language, 64(3):501-538, 1988.\\n\\nA.E. Goldberg. Constructions: a construction grammar approach to argument structure. The University of Chicago Press, Chicago, IL, 1994.\\n\\nK. Jensen. Parsing strategies in a broad-coverage grammar of english. Technical Report RC 12147, IBM T.J. Watson Research Center, Yorktown Heights, New York, 1986.\\n\\nD. Jurafsky. An On-line Computational Model of Sentence Interpretation. PhD thesis, University of California, Berkeley, 1992. Report No. UCB/CSD 92/676.\\n\\nP. Kay. Anaphoric binding in construction grammar. In BLS 20, Berkeley, CA, 1994.\\n\\nR.W. Langacker. Foundations of Cognitive Grammar II. Stanford U. Press, Stanford, CA, 1991.\\n\\nS.L. Lytinen.\\n\\nSemantics\\n\\n\\n\\nfirst natural language processing.\\n\\nIn Proceedings AAAI\\n\\n\\n\\n91, pages 111\\n\\n\\n\\n116, Anaheim, CA, 1991.\\n\\nJ.D. McCawley. The Syntactic Phenomena of English. University of Chicago Press, Chicago, IL, 1988.\\n\\nM. McCord, A. Bernth, S. Lappin, and W. Zadrozny. Natural language processing technology based on slot grammar. International Journal on Artificial Intelligence Tools, 1(2):229-297, 1992.\\n\\nI. Melcuk and A. Polguere. A formal lexicon in meaning-text theory (or how to do lexica with words). Computational Linguistics, 13(3-4):261-275, 1987.\\n\\nI. Melcuk. Dependency Syntax: Theory and Practice. State University of New York Press, Albany, NY, 1988.\\n\\nJ. Nerbonne. Computational semantics -- linguistics and processing. In S. Lappin, editor, The Handbook of Contemporary Semantic Theory. Blackwell, Oxford, 1995.\\n\\nP. Sells. Lectures on Contemporary Syntactic Theories. CSLI Lecture Notes (3), Stanford, CA, 1985.\\n\\nL. Talmy. Lexicalization patterns: semantic structure in lexical forms. In T. Shopen, editor, Language Typology and syntactic description Vol.III, pages 57-149. 1985.\\n\\nW. Zadrozny, M. Szummer, S. Jarecki, D.E. Johnson, and L. Morgenstern. NL understanding with a grammar of constructions. Proc. Coling\\'94, 1994.\\n\\nW. Zadrozny. From compositional to systematic semantics. Linguistic and Philosophy, (4):329-342, 1994.\\n\\nA.M. Zwicky. Dealing out meaning: Fundamentals of syntactic constructions. In BLS 20, Berkeley, CA, 1994.\\n\\nFootnotes', metadata={'source': '../data/raw/cmplg-xml/9505032.xml'}),\n",
       " Document(page_content=\"T\\n\\nWe present an unsupervised learning algorithm that acquires a natural-language lexicon from raw speech. The algorithm is based on the optimal encoding of symbol sequences in an MDL framework, and uses a hierarchical representation of language that overcomes many of the problems that have stymied previous grammar-induction procedures. The forward mapping from symbol sequences to the speech stream is modeled using features based on articulatory gestures. We present results on the acquisition of lexicons and language models from raw speech, text, and phonetic transcripts, and demonstrate that our algorithm compares very favorably to other reported results with respect to segmentation performance and statistical efficiency.\\n\\nIntroduction\\n\\nInternally, a sentence is a sequence of discrete elements drawn from a finite vocabulary. Spoken, it becomes a continuous signal- a series of rapid pressure changes in the local atmosphere with few obvious divisions. How can a pre-linguistic child, or a computer, acquire the skills necessary to reconstruct the original sentence? Specifically, how can it learn the vocabulary of its language given access only to highly variable, continuous speech signals? We answer this question, describing an algorithm that produces a linguistically meaningful lexicon from a raw speech stream. Of course, it is not the first answer to how an utterance can be segmented and classified given a fixed vocabulary, but in this work we are specifically concerned with the unsupervised acquisition of a lexicon, given no prior language-specific knowledge.\\n\\nIn contrast to several prior proposals, our algorithm makes no assumptions about the presence of facilitative side information, or of cleanly spoken and segmented speech, or about the distribution of sounds within words. It is instead based on optimal coding in a minimum description length (MDL) framework. Speech is encoded as a sequence of articulatory feature bundles, and compressed using a hierarchical dictionary-based coding scheme. The optimal dictionary is the one that produces the shortest description of both the speech stream and the dictionary itself. Thus, the principal motivation for discovering words and other facts about language is that this knowledge can be used to improve compression, or equivalently, prediction.\\n\\nThe success of our method is due both to the representation of language we adopt, and to our search strategy. In our hierarchical encoding scheme, all linguistic knowledge is represented in terms of other linguistic knowledge. This provides an incentive to learn as much about the general structure of language as possible, and results in a prior that serves to discriminate against words and phrases with unnatural structure. The search and parsing strategies, on the other hand, deliberately avoid examining the internal representation of knowledge, and are therefore not tied to the history of the search process. Consequently, the algorithm is relatively free to restructure its own knowledge, and does not suffer from the local-minima problems that have plagued other grammar-induction schemes.\\n\\nThe Problem\\n\\nBroadly, the task we are interested in is this: a listener is presented with a lengthy but finite sequence of utterances. Each utterance is an acoustic signal, sensed by an ear or microphone, and may be paired with information perceived by other senses. From these signals, the listener must acquire the necessary expertise to map a novel utterance into a representation suitable for higher analysis, which we will take to be a sequence of words drawn from a known lexicon.\\n\\n[[[ the][ [[govern][ment]]]][[ of][[ the][[ united][[ state]s]]]]].\\n\\nHere, each unit enclosed in brackets (henceforth these will simply be called words) has an entry in the lexicon. There is a word   united states that might be assigned a meaning independently of   united or states. Similarly, if the pronunciation of   government is not quite the concatenation of govern and ment (as wanna is not the concatenation of want and to) then there is a level of representation where this is naturally captured. We submit that this hierarchical representation is considerably more useful than one that treats the government of the united states as an atom, or that provides no structure beyond that obvious from the placement of spaces.\\n\\nIf we accept this sort of representation as an intelligent goal, then why is it hard to achieve? First of all, notice that even given a known vocabulary, continuous speech recognition is very difficult. Pauses are rare, as anybody who has listened to a conversation in an unknown language can attest. What is more, during speech production sounds blend across word boundaries, and words undergo tremendous phonological and acoustic variation: what are you doing is often pronounced /wc/. Thus, before reaching the language learner, unknown sounds from an unknown number of words drawn from an unknown distribution are smeared across each other and otherwise corrupted by various noisy channels. From this, the learner must deduce the parameters of the generating process. This may seem like an impossible task- after all, every utterance the listener hears could be a new word. But if the listener is a human being, he or she is endowed with a tremendous knowledge about language, be it in the form of a learning algorithm or a universal grammar, and this constrains and directs the learning process; some part of this knowledge pushes the learner to establish equivalence classes over sounds. The performance of any machine learning algorithm on this problem is largely dependent on how well it mimics that behavior.\\n\\nThe Learning Framework\\n\\nWe adopt this MDL framework. It is well-defined, has a foundation in information complexity, and (as we will see) leads directly to a convenient lexical representation. For our purposes, we choose the class of grammars in such a way that each grammar is essentially a lexicon. It is our premise that within this class, the grammar with the best predictive properties (the shortest description length) is the lexicon of the source. Additionally, the competition to compress the input provides a noble incentive to learn more about the source language than just the lexicon, and to make use of all cross-linguistic invariants.\\n\\nWe prefer to leave open the question of whether children make use of supervisory information, and attack the question of whether such information is necessary for language acquisition. Gold's proof, for example, does not hold for suitably constrained classes of languages or for grammars interpreted in a probabilistic framework. Furthermore, both the acquisition and use of prosodic and intonational clues for segmentation falls out naturally given the correct unsupervised learning framework, since they are generalizations that enable speech to be better predicted. For these reasons, and also because there are many important engineering problems where labeled training data is unavailable or expensive, we prefer to investigate unsupervised methods. A working unsupervised algorithm would both dispel many of the learnability-argument myths surrounding child language acquisition, and be a valuable tool to the natural language engineering community.\\n\\nA Model of Speech Production The Class of Grammars\\n\\nAll unsupervised learning techniques rely on finding regularities in data. In language, regularities exist at many different scales, from common sound sequences that are words, to intricate patterns of grammatical categories constrained by syntax, to the distribution of actions and objects unique to a conversation. These all interact to create weaker second-order regularities, such as the high probability of the word   the after of. It can be extremely difficult to separate the regularities tied to ``interesting'' aspects of language from those that naturally arise when many complex processes interact. For example, the 19-character sequence radiopasteurization appears six times in the Brown corpus, far too often to be a freak coincidence. But at the same time, the 19-character sequence scratching her nose also appears exactly six times. Our intuition is that radiopasteurization is some more fundamental concept, but it is not easy to imagine principled schemes of determining this from the text alone. The enormous number of uninteresting coincidences in everyday language is distracting; plainly, a useful algorithm must be capable of extracting fundamental regularities even when such coincidences abound. This and the minimum description length principle are the motivation for our lexical representation (our class of grammars).\\n\\nIn this class of grammars, terminals are drawn from an arbitrary alphabet. For the time being, let us assume they are ascii characters, though in the case of speech processing they are phonemes. Nonterminals are concatenated sequences of terminals. Together, terminals and nonterminals are called ``words''. The purpose of a nonterminal is to capture a statistical pattern that is not otherwise predicted by the grammar. In this work, these patterns are merely unusually common sequences of characters, though given a richer set of linguistic primitives, the framework extends naturally. As a general principle, it is advantageous to add a word to the grammar when its characters appear more often than can be explained given other knowledge, though the cost of actually representing the word acts as a buffer against words that occur only marginally more often than expected, or that have unlikely (long) descriptions.\\n\\nSome of the coincidences in the input data are of interest, and others are not. We assume that the vast majority of the less interesting coincidences (scratching her nose) arise from interactions between more fundamental processes (verbs take noun-phrase arguments; nose is a noun, and so on). This suggests that fundamental processes can be extracted by looking for patterns within the uninteresting coincidences, and implies a recursive learning scheme: extract patterns from the input (creating words), and extract patterns from those words, and so on. These steps are equivalent to compressing not only the input, but also the parameters of the compression algorithm, in a never-ending attempt to identify and eliminate the predictable. They lead us to a class of grammars in which both the input and nonterminals are represented in terms of words.\\n\\nFinding the Optimal Grammar\\n\\nFortunately, the form of our grammar permits the use of a significantly better behaved search algorithm. There are several reasons for this. First, because each word is decomposable into its representation, adding or deleting a word does not drastically alter the character of the grammar. Second, because all of the information about a word necessary for parsing is contained in its surface form and its probability, its representation is free to change abruptly from one iteration to the next, and is not tied to the history of the search process. Finally, because the representation of a word serves as a prior that discriminates against unnatural words, search tends not to get bogged down in linguistically implausible grammars.\\n\\nProbability Estimation\\n\\nThere are two complications that arise in the estimation. The first is quite interesting. For a description to be well-defined, the graph of word representations can not contain cycles: a word can not be defined in terms of itself. So some partial ordering must be imposed on words. Under the concatenative model that has been discussed, this is easy enough, since the representation of a word can only contain shorter words. But there are obvious and useful extensions that we have experimented with, such as applying the phoneme-to-phone model at every level of representation, so that a word like wanna can be represented in terms of want and to. In this case, a chicken-and-egg problem must be solved: given two words, which comes first? It is not easy to find good heuristics for this problem, and computing the description length of all possible orderings is obviously far too expensive.\\n\\nAdding and Deleting Words\\n\\nThe governing motive for changing the dictionary is to reduce the combined description length of U and G, so any improvement a new word brings to the description of an utterance must be weighed against its representation cost. The general strategy for building new words is to look for a set of existing words that occur together more often than independent chance would predict. The addition of a new word with the same surface form as this set will reduce the description length of the utterances it occurs in. If its own cost is less than the reduction, the word is added. Similarly, words are deleted when doing so would reduce the combined description length. This generally occurs as shorter words are rendered irrelevant by longer words that model more of the input.\\n\\nUnfortunately, the addition or deletion of a word from the grammar could have a substantial and complex impact on the probability distribution p(w). Because of this, it is not possible to efficiently gauge the exact effect of such an action on the overall description length, and various approximations are necessary. Rather than devote space to them here, they are described in appendix B, along with other details related to the addition and deletion of words from the dictionary.\\n\\nOne interesting addition needed for processing speech is the ability to merge changes that occur in the phoneme-to-phone mapping into existing words. Often, a word is used to match part of a word with different sounds; for instance doing /du/ may initially be analyzed as do /du/ + in //, because in is much more probable than -ing. This is a common pair that will be joined into a single new word. Since in most uses of this word the /n/ changes to a //, it is to the algorithm's advantage to notice this and create /du/ from /du/. The other possible approach, to build words based on the surface forms found in the input rather than the concatenation of existing words, is less attractive, both because it is computationally more difficult to estimate the effect of adding such words, and because surface forms are so variable.\\n\\nExperiments and Results\\n\\nThere are at least three qualities we hope for in our algorithm. The first is that it captures regularities in the input, using as efficient a model as possible. This is tested by its performance as a text-compression and language modeling device. The second is that it captures regularity using linguistically meaningful representations. This is tested by using it to compress unsegmented phonetic transcriptions and then verifying that its internal representation follows word boundaries. Finally, we wish it to learn given even the most complex of inputs. This is tested by applying the algorithm to a multi-speaker corpus of continuous speech.\\n\\nText Compression and Language Modeling\\n\\nAfter fifteen iterations, the training text is compressed from 43,337,280 to 11,483,361 bits, a ratio of 3.77:1 with a compression rate of 2.12 bits/character; this compares very favorably with the 2.95 bits/character achieved by the LZ77 based gzip program. 9.5% of the description is devoted to the parameters (the words and other overhead), and the rest to the text. The final dictionary contains 30,347 words. The entropy rate of the training text, omitting the dictionary and other overhead, is 1.92 bits/character. The entropy rate of this same language model on the held-out test set (the remaining 10% of the corpus) is 2.04 bits/character. A slight adjustment of the conditions for creating words produces a larger dictionary, of 42,668 words, that has a slightly poorer compression rate of 2.19 bits/character but an entropy rate on the test set of 1.97 bits/character, identical to the base rate Ristad and Thomas achieve using a non-monotonic context model. So far as we are aware, all models that better our entropy figures contain far more parameters and do not fare well on the total description-length (compression rate) criterion. This is impressive, considering that the simple language model used in this work has no access to context, and naively reproduces syntactic and morphological regularities time and time again for words with similar behavior.\\n\\nSegmentation\\n\\nAcquisition from Speech\\n\\nThe experiments we have performed on raw speech are preliminary, and included here principally to demonstrate that our algorithm does learn words even in the very worst of conditions. The conditions of these initial tests are so extreme to make detailed analysis irrelevant, but we believe the final dictionaries are convincing in their own right.\\n\\nWe ran the algorithm on the 1890 `si' sentences from TIMIT, both on the raw speech using the Viterbi analyses and on the cleaner transcriptions. This is a very difficult training corpus: TIMIT was designed to aid in the training of acoustic models for speech recognizers, and as a consequence the source material was selected to maximize phonetic diversity. Not surprisingly, therefore, the source text is very irregular and contains few repetitions. It is also small. As a final complication, the sentences in the corpus were spoken by hundreds of different speakers of both sexes, in many different dialects of English. We hope in the near future to apply the algorithm to a longer corpus of dictated Wall Street Journal articles; this should be a fairer test of performance.\\n\\nExtensions\\n\\nWords are more than just sounds- they have meanings and syntactic roles, that can be learned using very similar techniques to those we have already described. Here we very briefly sketch what such extensions might look like.\\n\\nWord Meanings\\n\\nWe have fleshed out this extension more fully and conducted some initial experiments, and the algorithm seems to learn word meanings quite reliably even given substantial noise and ambiguity. At this time, we have not conducted experiments on learning word meanings with speech, though the possibility of learning a complete dictation device from speech and textual transcripts is not beyond imagination.\\n\\nSurface Syntax\\n\\nThis tree can be represented by the left derivation string\\n\\nAlthough we are just beginning work in this area, this close link between our current representation and context-free grammars gives us great hope that we can learn CFG's that compete with or better the best Markov models for prediction problems, and produce plausible phrase structures.\\n\\nApplications\\n\\nThe algorithm we have described for learning words has several properties that make it a particularly good tool for solving language engineering problems. First of all, it reliably reproduces linguistic structure in its internal representations. This can not be said of most language models, which are context based. Using our algorithm for text compression, for instance, enables the compressed text to be searched or indexed in terms of intuitive units like words. Together with the fact that the algorithm compresses text extremely well (and has a rapid decompression counterpart), this means it should be useful for off-line compression of databases like encyclopedias.\\n\\nSecondly, the algorithm is unsupervised. It can be used to construct dictionaries and extend existing ones without expensive human labeling efforts. This is valuable for machine translation and text indexing applications. Perhaps more importantly, because the algorithm constructs dictionaries from observed data, its entries are optimized for the application at hand; these sorts of dictionaries should be significantly better for speech recognition applications than manually constructed ones that do not necessarily reflect common usage, and do not adapt themselves across word boundaries (i.e. no wanna like words).\\n\\nFinally, the multi-layer lexical representation used in the algorithm is well suited for tasks like machine translation, where idiomatic sequences must be represented independently of the words they are built from, while at the same time the majority of common sequences function quite similarly to the composition of their components.\\n\\nRelated Work\\n\\nThis paper has touched on too many areas of language and induction to present an adequate survey of related work here. Nevertheless, it is important to put this work in context.\\n\\nConclusions\\n\\nWe have presented a general framework for lexical induction based on a form of recursive compression. The power of that framework is demonstrated by the first computer program to acquire a significant dictionary from raw speech, under extremely difficult conditions, with no help or prior language-specific knowledge. This is the first work to present a complete specification of an unsupervised algorithm that learns words from speech, and we hope it will lead researchers to study unsupervised language-learning techniques in greater detail. The fundamental simplicity of our technique makes it easy to extend, and we have hinted at how it can be used to learn word meanings and syntax. The generality of our algorithm makes it a valuable tool for language engineering tasks ranging from the construction of speech recognizers to machine translation.\\n\\nThe success of this work raises the possibility that child language acquisition is not dependent on supervisory clues in the environment. It also shows that linguistic structure can be extracted from data using statistical techniques, if sufficient attention is paid to the nature of the language production process. We hope that our results can be improved further by incorporating more accurate models of morphology and phonology.\\n\\nAcknowledgments\\n\\nThe author would like to thank Marina Meila, Robert Berwick, David Baggett, Morris Halle, Charles Isbell, Gina Levow, Oded Maron, David Pesetsky and Robert Thomas for discussions and contributions related to this work.\\n\\nBibliography\\n\\nJames K. Baker. Trainable grammars for speech recognition. In Proceedings of the 97th Meeting of the Acoustical Society of America, pages 547-550, 1979.\\n\\nLeonard E. Baum, Ted Petrie, George Soules, and Norman Weiss. A maximization technique occuring in the statistical analysis of probabalistic functions in markov chains. Annals of Mathematical Statistics, 41:164-171, 1970.\\n\\nSteven Bird and T. Mark Ellison. One-level phonolgy: Autosegmental representations and rules as finite automata. Computational Linguistics, 20(1):55-90, 1994.\\n\\nMichael Brent. Minimal generative explanations: A middle ground between neurons and triggers. In Proc. of the 15th Annual Meeting of the Cognitive Science Society, pages 28-36, 1993.\\n\\nMichael R. Brent, Andrew Lundberg, and Sreerama Murthy. Discovering morphemic suffixes: A case study in minimum description length induction. 1993.\\n\\nGlenn Carroll and Eugene Charniak. Learning probabalistic dependency grammars from labelled text. In Working Notes, Fall Symposium Series, AAAI, pages 25-31, 1992.\\n\\nTimothy Andrew Cartwright and Michael R. Brent. Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition. In Proc. of the 16th Annual Meeting of the Cognitive Science Society, Hillsdale, New Jersey, 1994.\\n\\nStanley F. Chen. Bayesian grammar induction for language modeling. In Proc. 32nd Annual Meeting of the Association for Computational Linguistics, pages 228-235, Cambridge, Massachusetts, 1995.\\n\\nNoam A. Chomsky. The Logical Structure of Linguistic Theory. Plenum Press, New York, 1955.\\n\\nThomas M. Cover and Joy A. Thomas. Elements of Information Theory. John Wiley  Sons, New York, NY, 1991.\\n\\nAnne Cutler.\\n\\nSegmentation problems, rhythmic solutions.\\n\\nLingua, 92(1\\n\\n\\n\\n4), 1994.\\n\\nCarl de Marcken. The acquisition of a lexicon from paired phoneme sequences and semantic representations. In International Colloquium on Grammatical Inference, pages 66-77, Alicante, Spain, 1994.\\n\\nCarl de Marcken. Lexical heads, phrase structure and the induction of grammar. In Third Workshop on Very Large Corpora, Cambridge, Massachusetts, 1995.\\n\\nSabine Deligne and Frdric Bimbot. Language modeling by variable length sequences: Theoretical formulation and evaluation of multigrams. In IEEE ? ?? ?, pages 169-172, ? ?? ?, 1995.\\n\\nStephen Della Pietra, Vincent Della Pietra, and John Lafferty. Inducing features of random fields. Technical Report CMU-CS-95-144, Carnegie Mellon University, Pittsburgh, Pennsylvania, May 1995.\\n\\nA. P. Dempster, N. M. Liard, and D. B. Rubin. Maximum liklihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B(39):1-38, 1977.\\n\\nT. Mark Ellison. The Machine Learning of Phonological Structure. PhD thesis, University of Western Australia, 1992.\\n\\nW. N. Francis and H. Kucera. Frequency analysis of English usage: lexicon and grammar. Houghton-Mifflin, Boston, 1982.\\n\\nE. Mark Gold. Language identification in the limit. Information and Control, 10:447-474, 1967.\\n\\nMorris Halle. On distinctive features and their articulatory implementation. Natural Language and Linguistic Theory, 1:91-105, 1983.\\n\\nMorris Halle and Alec Marantz. Distributed morphology and the pieces of inflection. In Kenneth Hale and Samuel Jay Keyser, editors, The View from Building 20: Essays in Linguistics in Honor of Sylvain Bromberger. MIT Press, Cambridge, MA, 1993.\\n\\nPeter W. Jusczyk. Discovering sound patterns in the native language. In Proc. of the 15th Annual Meeting of the Cognitive Science Society, pages 49-60, 1993.\\n\\nPeter W. Jusczyk. Infants speech perception and the development of the mental lexicon. In Judith C. Goodman and Howard C. Nusbaum, editors, The Development of Speech Perception. MIT Press, Cambridge, MA, 1994.\\n\\nRonald M. Kaplan and Martin Kay. Regular models of phonological rule systems. Computational Linguistics, 20(3):331-378, 1994.\\n\\nMichael Kenstowicz. Phonology in Generative Grammar. Blackwell Publishers, Cambridge, MA, 1994.\\n\\nB. MacWhinney and C. Snow. The child language data exchange system. Journal of Child Language, 12:271-296, 1985.\\n\\nDonald Cort Olivier. Stochastic Grammars and Language Acquisition Mechanisms. PhD thesis, Harvard University, Cambridge, Massachusetts, 1968.\\n\\nFernando Pereira and Yves Schabes. Inside-outside reestimation from partially bracketed corpora. In Proc. 29th Annual Meeting of the Association for Computational Linguistics, pages 128-135, Berkeley, California, 1992.\\n\\nLawrence Rabiner and Biing-Hwang Juang. Fundamentals of Speech Recognition. Prentice Hall, Englewood Cliffs, NJ, 1993.\\n\\nJorma Rissanen. Modeling by shortest data description. Automatica, 14:465-471, 1978.\\n\\nJorma Rissanen. Stochastic Complexity in Statistical Inquiry. World Scientific, Singapore, 1989.\\n\\nEric Sven Ristad and Robert G. Thomas. New techniques for context modeling. In Proc. 32nd Annual Meeting of the Association for Computational Linguistics, Cambridge, Massachusetts, 1995.\\n\\nJeffrey M. Siskind. Naive physics, event perception, lexical semantics, and language acquisition. PhD thesis TR-1456, MIT Artificial Intelligence Lab., 1992.\\n\\nJeffrey Mark Siskind. Lexical acquisition as constraint satisfaction. Technical Report IRCS-93-41, University of Pennsylvania Institute for Research in Cognitive Science, Philadelphia, Pennsylvania, 1993.\\n\\nJeffrey Mark Siskind. Lexical acquisition in the presence of noise and homonymy. In Proc. of the American Association for Artificial Intelligence, Seattle, Washington, 1994.\\n\\nJeffrey L. Sokolov and Catherine E. Snow. The changing role of negative evidence in theories of language development. In Clare Gallaway and Brian J. Richards, editors, Input and interaction in language acquisition, pages 38-55. Cambridge University Press, New York, NY, 1994.\\n\\nR. J. Solomonoff. The mechanization of linguistic learning. In Proceedings of the 2nd International Conference on Cybernetics, pages 180-193, 1960.\\n\\nAndrew Spencer. Morphological Theory. Blackwell Publishers, Cambridge, MA, 1991.\\n\\nAndreas Stolcke. Bayesian Learning of Probabalistic Language Models. PhD thesis, University of California at Berkeley, Berkeley, CA, 1994.\\n\\nJ. Gerald Wolff. Language acquisition and the discovery of phrase structure. Language and Speech, 23(3):255-269, 1980.\\n\\nJ. Gerald Wolff. Language acquisition, data compression and generalization. Language and Communication, 2(1):57-89, 1982.\\n\\nJ. Ziv and A. Lempel. Compression of individual sequences by variable rate coding. IEEE Transactions on Information Theory, 24:530-536, 1978.\\n\\nPhonetic Model Modeling the Generation of a Phone Adding and Deleting Words\\n\\nTo compute these values, estimates of c'(X) and c'(w|X) must be available (these are not discussed further). These equations give approximate values for probabilities and counts after the change is made. The total change in description length from G to G' is given by\\n\\nIf a word X is deleted from G (creating G') then in all places Xoccurs words from its representation must be used to replace it. This leads to the estimates\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9512002.xml'}),\n",
       " Document(page_content=\"TEXT SEGMENTATION BASED ON SIMILARITY BETWEEN WORDS\\n\\nThis paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis.\\n\\nINTRODUCTION\\n\\nA text is not just a sequence of words, but it has coherent structure. The meaning of each word can not be determined until it is placed in the structure of the text. Recognizing the structure of text is an essential task in text understanding, especially in resolving anaphora and ellipsis.\\n\\nOne of the constituents of the text structure is a text segment. A text segment, whether or not it is explicitly marked, as are sentences and paragraphs, is defined as a sequence of clauses or sentences that display local coherence. It resembles a scene in a movie, which describes the same objects in the same situation.\\n\\nThis paper proposes an indicator, called the lexical cohesion profile (LCP), which locates segment boundaries in a narrative text. LCP is a record of lexical cohesiveness of words in a sequence of text. Lexical cohesiveness is defined as word similarity (Kozima and Furugori, 1993) computed by spreading activation on a semantic network. Hills and valleys of LCP closely correlate with changing of segments.\\n\\nSEGMENTS AND COHERENCE\\n\\nSeveral methods to capture segment boundaries have been proposed in the studies of text structure. For example, cue phrases play an important role in signaling segment changes. (Grosz and Sidner, 1986) However, such clues are not directly based on coherence which forms the clauses or sentences into a segment.\\n\\nYoumans (1991) proposed VMP (vocabulary management profile) as an indicator of segment boundaries. VMP is a record of the number of new vocabulary terms introduced in an interval of text. However, VMP does not work well on a high-density text. The reason is that coherence of a segment should be determined not only by reiteration of words but also by lexical cohesion.\\n\\nMorris and Hirst (1991) used Roget's thesaurus to determine whether or not two words have lexical cohesion. Their method can capture almost all the types of lexical cohesion, e.g. systematic and non-systematic semantic relation. However it does not deal with strength of cohesiveness which suggests the degree of contribution to coherence of the segment.\\n\\nComputing Lexical Cohesion\\n\\nKozima and Furugori (1993) defined lexical cohesiveness as semantic similarity between words, and proposed a method for measuring it. Similarity between words is computed by spreading activation on a semantic network which is systematically constructed from an English dictionary (LDOCE).\\n\\nLEXICAL COHESION PROFILE Cohesiveness of a Word List\\n\\nCorrelation between LCP\\n\\nand text segments.\\n\\nLCP and Its Feature\\n\\nThe LCP, shown in Figure 3, has large hills and valleys, and also meaningless noise. The graph is so complicated that one can not easily determine which valley should be considered as a segment boundary.\\n\\nThe shape of the window, which defines weight of words in it for pattern production, makes LCP smooth. Experiments on several window shapes (e.g. triangle window, etc.) shows that Hanning window is best for clarifying the macroscopic features of LCP.\\n\\nVERIFICATION OF LCP\\n\\nThis section inspects the correlation between LCP and segment boundaries perceived by the human judgments. The curve of Figure 4 shows the LCP of the simplified version of O.Henry's ``Springtime la Carte'' (Thornley, 1960). The solid bars represent the histogram of segment boundaries reported by 16 subjects who read the text without paragraph structure.\\n\\nCONCLUSION\\n\\nThis paper proposed LCP, an indicator of segment changing, which concentrates on lexical cohesion of a text segment. The experiment proved that LCP closely correlate with the segment boundaries captured by the human judgments, and that lexical cohesion plays main role in forming a sequence of words into segments.\\n\\nIn future research, the author needs to examine validity of LCP for other genres -- Hearst (1993) segments expository texts. Incorporating other clues (e.g. cue phrases, tense and aspect, etc.) is also needed to make this segmentation method more robust.\\n\\nACKNOWLEDGMENTS\\n\\nThe author is very grateful to Dr. Teiji Furugori, University of Electro-Communications, for his insightful suggestions and comments on this work.\\n\\nREFERENCES Grosz, Barbara J., and Sidner, Candance L. (1986). ``Attention, intentions, and the structure of discourse.'' Computational Linguistics, 12, 175-204.\\n\\nHalliday, Michael A. K., Hasan, Ruqaiya (1976). Chesion in English. Longman.\\n\\nHearst, Marti, and Plaunt, Christian (1993). ``Subtopic structuring for full-length document access,'' to appear in SIGIR 1993, Pittsburgh, PA.\\n\\nKozima, Hideki, and Furugori, Teiji (1993). ``Similarity between words computed by spreading activation on an English dictionary.'' to appear in Proceedings of EACL-93.\\n\\nMorris, Jane, and Hirst, Graeme (1991). ``Lexical cohesion computed by thesaural relations as an indicator of the structure of text.'' Computational Linguistics, 17, 21-48.\\n\\nThornley, G. C. editor (1960). British and American Short Stories, (Longman Simplified English Series). Longman.\\n\\nWest, Michael (1953). A General Service List of English Words. Longman.\\n\\nYoumans, Gilbert (1991). ``A new tool for discourse analysis: The vocabulary-management profile.'' Language, 67, 763-789.\", metadata={'source': '../data/raw/cmplg-xml/9601005.xml'}),\n",
       " Document(page_content=\"An Extended Clustering Algorithm for Statistical Language Models\\n\\nStatistical language models frequently suffer from a lack of training data. This problem can be alleviated by  clustering, because it reduces the number of free parameters that need to be trained. However, clustered models have the following drawback: if there is ``enough'' data to train an unclustered model, then the clustered variant may perform worse. On currently used language modeling corpora, e.g. the Wall Street Journal corpus, how do the performances of a clustered and an unclustered model compare? While trying to address this question, we develop the following two ideas. First, to get a clustering algorithm with potentially high performance, an existing algorithm is extended to deal with higher order N-grams. Second, to make it possible to cluster large amounts of training data more efficiently,  a heuristic to speed up the algorithm is presented. The resulting clustering algorithm can be used to cluster trigrams on the Wall Street Journal corpus and the language models it produces can compete with existing back-off models. Especially when there is only little training data available, the clustered models clearly outperform the back-off models.\\n\\nIntroduction\\n\\nIt is well known that statistical language models often suffer from a lack of training data. This is true for standard tasks and even more so when one tries to build a language model for a new domain, because a large corpus of texts from that domain is usually not available. One frequently used approach to alleviate this problem is to construct a clustered language model. Because it has fewer parameters, it needs less training data. The main advantage of a clustered model are its robustness, even in the face of little or sparse training data,  and its compactness. Particularly when a language model is used during the acoustic search in a speech recogniser, having a more compact, e.g. less complex model, can be of considerable importance. The main drawback of clustered models is that they may perform worse than an unclustered model, if there is ``enough'' data to train the latter. Do corpora currently used for language modeling, e.g. the Wall Street Journal corpus,  contain enough data in that sense? Or, in other words, how does the performance of a clustered model compare with that of an unclustered model? In this paper, we will attempt to partly answer this question and, along the way, an extended, more efficient clustering algorithm will be developed.\\n\\nBackground and Related Work\\n\\nThe component of the speech recogniser that calculates p(A|W) is called the acoustic model, the component calculating p(W)the language model. With\\n\\nW=w1,...,wn, one can further decompose p(W)using the definition of conditional probabilities as\\n\\nThese models are called (M+1)-gram models and in practice, mostly bigram (M=1) and trigram (M=2) models are used. Even in these cases, the number of parameters that need to be estimated from training data can be quite large. For a speech recogniser with a vocabulary of 20,000words, the bigram needs to estimate roughly\\n\\n20,000[2=4*108] parameters and a trigram\\n\\n20,000[3=8\\n\\n\\n\\n1012].\\n\\nOne way to alleviate this problem is to use class based models. Let\\n\\nbe a function that maps each word w to its class\\n\\nG(w)=gw and let |G| denote the number of classes. We can then model the probability of wi as p(w_{i}|w_{1},...,w_{i-1}) \\\\approx  p_{G}(w_{i}|w_{i-M},...,w_{i-1})\\\\\\\\ =  p_{G}(G(w_{i})|G(w_{i-M}),...,G(w_{i-1}))*p_{G}(w_{i}|G(w_{i})). \\\\end{eqnarray} --> Thus, if |G|=1000 classes are being used, the class-based bigram model has\\n\\n1,000[2+20,000=1.02*106] parameters and the class-based trigram model\\n\\n1,000[3+20,000=1.00002*109]. This constitutes a significant reduction in both cases.\\n\\nIn order to automatically find classification functions  G, the classification problem is first converted into an optimisation problem. Suppose the function F(G) indicates how good the classification G is. One can then reformulate the classification problem as finding the classification G[*]that maximises F:\\n\\nwhere\\n\\ncontains the set of possible classifications which are at our disposal.\\n\\nwhere the product is over all possible pairs\\n\\n(g1, g2). Because N(g1) does not depend on g2 and N(g2) does not depend on g1, this can again be simplified to\\n\\nAfter taking the logarithm, one obtains the equivalent optimisation criterion\\n\\nF['']ML\\n\\nLeaving\\n\\n\\n\\nOne\\n\\n\\n\\nOut Criterion\\n\\nLet Ti denote the data without the pair\\n\\n(wi\\n\\n\\n\\n1, wi) and\\n\\npG,Ti(w|v) the probability estimates based on a given classification G and training corpus Ti. Given a particular Ti, the probability of the ``held-out'' part\\n\\n(wi\\n\\n\\n\\n1, wi) is\\n\\npG,Ti(wi|wi-1). The probability of the complete corpus, where each pair is in turn considered the ``held-out'' part is the leaving-one-out likelihood LLO\\n\\nAs shown before,\\n\\npG,Ti(g2, w)=pG,Ti(w) (if the classification Gis a function) and since\\n\\npTi(w) is actually independent of G, one can drop it out of the maximization and thus need not specify an estimate for it.\\n\\nAs will be shown later, one can guarantee that every class g1 and g2 has been seen at least once in the ``retained'' part and one can thus use relative counts as estimates for class uni-grams: p_{G,T_{i}}(g_{1})  =  \\\\frac{N_{T_{i}}(g_{1})}{N_{T_{i}}}\\\\\\\\ p_{G,T_{i}}(g_{2})  =  \\\\frac{N_{T_{i}}(g_{2})}{N_{T_{i}}}. \\\\end{eqnarray} -->\\n\\nClustering Algorithm\\n\\nWe will now determine the complexity of the algorithm. Let C be the maximal number of clusters for G, let E be the number of elements one tries to cluster (e.g. E=|V|), and let I be the number of iterations. When one moves w from gw to g'w in the inner loop, one needs to change the counts\\n\\nN(gw, g2) and\\n\\nN(g'w, g2) for all g2. The amount by which the counts need to be changed is equal to the number of times w occurred with cluster g2. Since this amount is independent of g'w, one only needs to calculate it once for each w. The amount can then be looked up in constant time within the loop, thus making the inner loop of order C. The inner loop is executed once for every cluster w can be moved to, thus giving a complexity of the order of C[2]. For each w, one needed to calculate the number of times w occurred with all clusters g2. For that one has to sum up all the bigram counts\\n\\nN(w,v):G(v)=g2, which is on the order of E, thus giving a complexity of the order of  E+C[2]. The two outer loops are executed I and E times, thus giving a total complexity of the order of\\n\\nI\\n\\n\\n\\nE\\n\\n\\n\\n(E+C[2]).\\n\\nExtending the Clustering Algorithm to N-grams\\n\\nThe tradeoff between these models is one of accuracy versus complexity. Approach a), which only uses one clustering function G, could produce\\n\\n|G|[|V|]\\n\\ndifferent clusterings (for each word in V, it can choose one of the |G| clusters). Approach b), which uses M+1different clustering functions, can represent\\n\\ndifferent clusterings, including all the clusterings of approach a). Approach c), which uses one clustering function for the tuples\\n\\nwi-M,...,wi-1 and one for wi, can produce\\n\\n|G1|[(|V|M)+|G2||V|]\\n\\nSpeeding up the Algorithm\\n\\nIf one wants to use the clustering algorithm on large corpora, the complexity of the algorithm becomes a crucial issue. As shown in the last two sections, the complexity of the algorithm is\\n\\nO(I*E*(E+C[2])), where C is the maximally allowed number of clusters, I is the number of iterations and E is the number of elements to cluster (|V| in case of bigrams, |V|[M+1] in case of the extended algorithm). C crucially determines the quality of the resulting language model and one would therefore like to chose it as big as possible. Unfortunately, because the algorithm is quadratic in C, this may be very costly. We therefore developed the following heuristic to speed up the algorithm.\\n\\nThe factor C[2] comes from the fact that one tries to move a word w to each of the C possible clusters (O(C)), and for each of these one has to calculate the difference in the optimisation function (O(C) again). If, based on some heuristic, one could select a fixed number t of target clusters, then one could only try moving w to these tclusters, rather than to all possible clusters C. This may of course lead to the situation where one does not move a word to the best possible cluster (because it was not selected by the heuristic), and thus potentially to a decrease in performance. But this decrease in performance depends of course on the heuristic function used and we will come back to this issue when we look at the practical results.\\n\\nThe heuristic used in this work is as follows. For each cluster g1, one keeps track of the h clusters that most frequently co-occur with g1 in the tables\\n\\nN(g1, g2). For example, if g1 is a cluster of G1 (the situation is symmetric for G2), then the h biggest entries in\\n\\nN(g1, g) are the h clusters being stored. When one tries to move a word w, one also constructs a list of the h most frequent clusters that follow w(one can  get this for free as part of the factor E in (E+C[2])). One then simply calculates the number of clusters that are in both lists and takes this as the heuristic score H(g1). The bigger H(g1), the more similar are the distributions of w and g1, and the more likely it is that g1 is a good target cluster to which w should be moved. Because the length of the lists is a constant h calculating the heuristic score is also independent of C. One can thus calculate the heuristic score of all C clusters in O(C). However, once one has decided to move w to a given cluster, one would have to update the lists containing the h most frequent clusters following each cluster g1(the lists might have changed due to the last moving of a word). Since the update is O(C) for a given g1, the update would again be O(C[2]) for all clusters. In order to avoid this, one can make another approximation at this point. One can only update the list for the original  and the new cluster of w. The full update of all the lists  is only performed after a certain number u of words have been moved.\\n\\nTo sum up, we can say that one can select t target clusters using the heuristic in O(C). Following that, one tries  moving w to each of these t clusters, which is again O(C). Moreover, several times per iteration (depending on u), one updates the list of most frequent clusters which is O(C[2]). Thus, the complexity of the heuristic version of the algorithm is\\n\\nO(I*(E*(E+C)+C[2])). The complexity still contains the factor C[2], but this time not within the inner parenthesis. The factor C[2] will thus be smaller than E*(E+C), and is only given for completeness.\\n\\nWe will now present a practical evaluation of the heuristic algorithm. The heuristic itself is parameterised by h, the number of most frequent clusters one uses to calculate the heuristic score, t, the number of best ranked target clusters one tries to move word w to and u, the number indicating after how many words a full update of the list of most frequent clusters is performed. In order to evaluate the heuristic for a given set of parameters, one can simply compare the final value of the approximation function and the resulting perplexity of the heuristic algorithm with that of the full algorithm.\\n\\n). Judging from the time behaviour of the standard algorithm, one would expect it to take around 32 hours to run with 1000 clusters, whereas the heuristic algorithm, as will be shown later, only takes about half an hour  (for t=10).\\n\\nBased on the results of these experiments, we chose h=5, t=10 and u=1000 for future experiments with the heuristic version of the algorithm.\\n\\nResults\\n\\nFrom all the results given here, one can see that the clustered language models can still compete with unclustered models, even when a large corpus, such as the Wall Street Journal corpus, is being used.\\n\\nConclusions\\n\\nIn this paper, an existing clustering algorithm is extended to deal with higher order N-grams. Moreover, a heuristic version of the algorithm is introduced, which leads to a very significant speed up (up to a factor of 32), with only a slight loss in performance (5%). This makes it possible to apply the resulting algorithm to the clustering of bigrams and trigrams on the Wall Street Journal corpus. The results are shown to be comparable to standard back-off bigram models. Moreover, in the absence of many million words of training data, the clustered model is more robust and clearly outperforms the non-clustered models. This is an important point, because for many real world speech recognition applications, the amount of training data available for a certain task or domain is in general unlikely to exceed several million words. In those cases, the clustered models seem like a good alternative to back-off models and certainly one that deserves close investigation.\\n\\nThe main advantage of the clustering models, its robustness in the face of little training data, can also be seen from the results and in these situations, the clustered algorithm is preferable to the standard back-off models.\\n\\nAppendix A: Deriving the Optimisation Function\\n\\nwhere the product is over all possible pairs\\n\\n(g1, g2). Because N(g1) does not depend on g2 and N(g2) does not depend on g1, one can simplify this again to\\n\\nTaking the logarithm, one obtains the equivalent optimisation criterion\\n\\nLet Ti denote the data without the pair\\n\\n(wi\\n\\n\\n\\nM,...,wi\\n\\n\\n\\n1, wi) and\\n\\npG,Ti(w|vM,...,v1) the probability estimates based on a given classification G and training corpus Ti. Given a particular Ti, the probability of the ``held-out'' part\\n\\n(wi\\n\\n\\n\\nM,...,wi\\n\\n\\n\\n1, wi) is\\n\\npG,Ti(wi|wi-M,...,wi-1). The probability of the complete corpus, where each pair is in turn considered the ``held-out'' part is the leaving-one-out likelihood LLO\\n\\nAs before,\\n\\npG,Ti can be dropped from the optimisation criterion and relative frequencies can be used as estimators for the class unigrams: p_{G,T_{i}}(g_{1})  =  \\\\frac{N_{T_{i}}(g_{1})}{N_{T_{i}}}\\\\\\\\ p_{G,T_{i}}(g_{2})  =  \\\\frac{N_{T_{i}}(g_{2})}{N_{T_{i}}}. \\\\end{eqnarray} -->\\n\\nIn the case of the class bi-gram, one can again use the absolute discounting method for smoothing. Let\\n\\nn0,Ti be the number of unseen pairs\\n\\n(g1, g2) and\\n\\nn+,Tithe number of\\n\\nseen pairs\\n\\n(g1, g2), leading to the following smoothed estimate\\n\\nAgain, the empirically determined constant value b=0.75 is used during clustering. The probability distribution\\n\\npG,Ti(g1, g2) will always be evaluated on the ``held-out'' part\\n\\n(wi\\n\\n\\n\\nM,...,wi\\n\\n\\n\\n1, wi) and with\\n\\ng1,i=G1(wi\\n\\n\\n\\nM,...,wi\\n\\n\\n\\n1) and\\n\\ng2,i=G2(wi) one obtains \\\\lefteqn{ p_{G,T_{i}}(g_{1,i}, g_{2,i})} \\\\nonumber \\\\\\\\ =  \\\\left\\\\{ \\\\begin{array}{ll} \\\\frac{N_{T_{i}}(g_{1,i}, g_{2,i}) - b}{N_{T_{i}}}  \\\\mbox{if $N_{T_{i}}(g_{1,i}, g_{2,i})>0$ }\\\\\\\\ \\\\frac{n_{+, T_{i}}*b}{n_{0,T_{i}}*N_{T_{i}}}  \\\\mbox{if $N_{T_{i}}(g_{1,i}, g_{2,i})=0$ }\\\\end{array} \\\\right. \\\\end{eqnarray} -->\\n\\nBibliography\\n\\nX. Aubert, C. Dugast, H. Ney, and V. Steinbiss. Large vocabulary continuous speech recognition of wall street journal data. In Proceedings of International Conference on Acoustics, Speech and Signal Processing, 1994.\\n\\nDavid Carter. Improving language models by clustering training sentences. In to appear in ANLP 94, Stuttgart Germany, 1994.\\n\\nR. O. Duda and P.E. Hart. Pattern Classification and Scene Analysis. Wiley, New York, 1973.\\n\\nJohn E. Freund. Modern Elementary Statistics. Prentice-Hall, Englewood Cliffs, New Jersey, 7th Edition, 1988.\\n\\nMichele Jardino and Gilles Adda. Automatic word classification using simulated annealing. In Proceedings of International Conference on Acoustics, Speech and Signal Processing, pages 41-43. Minneapolis, MN, 1993.\\n\\nFred Jelinek. Self-organized language modeling for speech recognition. In Alex Waibel and Kai-Fu Lee, editors, Readings in Speech Recognition, pages 450-506. Morgan Kaufmann, San Mateo, CA, 1990.\\n\\nS. Katz. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing, 35:400-401, 1987.\\n\\nReinhard Kneser and Hermann Ney. Improved clustering techniques for class-based statistical language modelling. In European Conference on Speech Communication and Technology, pages 973-976. Berlin, Germany, September 1993.\\n\\nHermann Ney and Ute Essen. Estimating `small' probabilities by leaving-one-out. In European Conference on Speech Communication and Technology, pages 2239-2242. Berlin, Germany, 1993.\\n\\nHermann Ney, Ute Essen, and Reinhard Kneser. On structuring probabilistic dependencies in stochastic language modelling. Computer, Speech and Language, 8:1:1-38, 1994.\\n\\nDouglas B. Paul. Experience with a stack decoder-based HMM CSR and back-off n-gram language models. In Proceedings of DARPA Speech and Natural Language Workshop, pages 284-288, 1991.\\n\\nFernando Pereira, Naftali Tishby, and Lillian Lee. Distributional clustering of English words. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 183-190. Columbus, OH, 1993.\\n\\nRonald Rosenfeld.\\n\\npersonal communication.\\n\\n1994.\\n\\nJoerg P. Ueberla. Analysing a simple language model - some general conclusions for language models for speech recognition. Computer, Speech and Language, 8:153-176, 1994.\\n\\nJoerg P. Ueberla. On using selectional restriction in language models for speech recognition. Technical report, School of Computing Science, Simon Fraser University, Canada, CMPT TR 94-03, 1994.\\n\\nFootnotes\\n\\nThis model is also sometimes referred to as bi-pos model, where pos stands for Parts Of Speech. If\\n\\n(wi-1, wi) occurs only once in the complete corpus, then\\n\\npG,Ti(wi|wi-1) will have to be calculated based on the corpus Ti, which does not contain any occurrences of\\n\\n(wi-1, wi). Only the 500,000 most frequent bigrams were clustered using G1.\", metadata={'source': '../data/raw/cmplg-xml/9412003.xml'}),\n",
       " Document(page_content=\"Limited Attention and Discourse Structure\\n\\nThis squib examines the role of limited attention in a theory of discourse structure and proposes a model of attentional state that relates current hierarchical theories of discourse structure to empirical evidence about human discourse processing capabilities. First, I present examples that are not predicted by Grosz and Sidner's stack model of attentional state. Then I consider an alternative model of attentional state, the cache model, which accounts for the examples, and which makes particular processing predictions. Finally I suggest a number of ways that future research could distinguish the predictions of the cache model and the stack model.\\n\\nHierarchical versus Linear Recency\\n\\nIn computational theories of discourse, there are at least three processes presumed to operate under a  LIMITED ATTENTION CONSTRAINT of some type: (1) ellipsis interpretation; (2) pronominal anaphora interpretation; and (3) inference of discourse relations between representations A and B of utterances in a discourse, e.g. B MOTIVATES A. In each case, the interpretation of the current element B of a discourse depends on the accessibility of another earlier element A. According to the LIMITED ATTENTION CONSTRAINT only a limited number of candidates need to be considered in the processing of B, e.g. only a limited number of entities in the discourse model are potential cospecifiers for a pronoun.\\n\\nEvidence for Limited Attention from Anaphoric Processing\\n\\nHowever, in dialogue B, utterance 8a is more difficult, if not impossible, to interpret. This is surprising because utterance B-4 is hierarchically recent for B-8a, just as it is in dialogue A. The interruption in dialogue B is but a slightly longer version of that in dialogue A. Inasmuch as the stack model is a precise formulation of hierarchical recency, it does not predict the infelicity of dialogue B. The problem arises partly because the stack model includes no constraints related to the length, depth, or the amount of processing required for an embedded segment. Thus, these types of extended embedded segments suggest that the limited attention constraint must be sensitive to some aspect of linear recency.\\n\\nEvidence for Limited Attention from Informational Redundancy\\n\\n[C] [] ( 3) E: And I was wondering - should I continue on with the certificates or ( 4) H: Well it's difficult to tell because we're so far away from any of them - but I would suggest this - if all of these are 6 month certificates and I presume they are ( 5) E: Yes ( 6) H: Then I would like to see you start spreading some of that money around ( 7) E: uh huh ( 8) H: Now in addition, how old are you? . (discussion and advice about retirement investments) . (21) E: uh huh and (22a) H: But as far as the certificates are concerned, (22b) I'D LIKE THEM SPREAD OUT A LITTLE BIT  - (22c) THEY'RE ALL 6 MONTH CERTIFICATES (23) E: Yes (24) H: And I don't like putting all my eggs in one basket...\\n\\nThe Cache Model of Attentional State\\n\\nThe notion of a cache in combination with main memory, as is standard in computational architectures, is a good basis for a computational model of human attentional capacity in processing discourse. All conversants in a dialogue have their own cache and some conversational processes are devoted to keeping these caches synchronized.\\n\\nThere are three operations involving the cache and main memory. Items in the cache can be preferentially  RETAINED and items in main memory can be  RETRIEVED to the cache. Items in the cache can also be  STORED to main memory.\\n\\nWhen new items are retrieved from main memory to the cache, or enter the cache directly due to events in the world, other items may be DISPLACED, because the cache has limited capacity. Displaced items are stored in main memory. The determination of which items to displace is handled by a cache replacement policy. The specification of the cache replacement policy is left open, however, replacing items that haven't been recently used, with the exception of those items that are preferentially retained, is a good working assumption, as shown by previous work on linear recency.\\n\\nTwo factors determine when cache operations are applied: (1) the speaker's intentions and the hearer's recognition of intention; (2) expectations about what will be discussed.\\n\\nThe cache model maintains the distinction between intentional structure and attentional state first proposed by Grosz and Sidner (1986). This distinction is critical. Just as a cache can be used for processing the references and operations of a hierarchically structured program, so can a cache be used to model attentional state when discourse intentions are hierarchically structured. The intentions of a conversant and the recognition of the other's intentions determine what is  RETRIEVED from main memory and what is preferentially  RETAINED in the cache.\\n\\nWhen conversants start working towards the achievement of a new intention, that intention may utilize information that was already in the cache. If so, that information will be preferentially retained in the cache because it is being used. Whenever the new intention requires information that is not currently in the cache, that information must be retrieved from main memory. Thus the process of initiating the achievement of the new intention has the result that some, and perhaps all, of the items currently in the cache are replaced with items having to do with the new intention.\\n\\nWhen conversants return to a prior intention, information relevant to that intention must be retrieved from main memory if it has not been retained in the cache.\\n\\nEvaluating Critical Evidence: comparing the cache with the stack\\n\\nIn this section, I wish to examine evidence for the cache model, look at further predictions of the model,and then discuss evidence relevant to both stack and cache models in order to draw direct comparisons between them. First, I contrast the mechanisms of the models with respect to certain discourse processes.\\n\\nNew intention subordinate to current intention: (1) Stack pushes new focus space; (2) Cache retrieves entities related to new intention\\n\\nIntention completed: (1) Stack pops focus space for intention from stack, entities in focus space are no longer accessible; (2) Cache doesn't retain entities for completed intention, but they remain accessible until displaced\\n\\nNew intention subordinate to prior intention: (1)  Stack pops focus spaces for intervening segments, focus space for prior intention accessible after pop; (2) Cache retrieves entities related to prior intention from main memory to cache, unless retained in the cache\\n\\nInformationally redundant utterances: (1) Stack predicts no role for IRUs when they are represented in focus space on top of stack, because information should be immediately available; (2) Cache predicts that IRUs reinstantiate or refresh known information in the cache\\n\\nReturning from interruption: (1)  In the stack model, the length and depth of the interruption and the processing required is irrelevant; (2) In the cache model, the length of the interruption or the processing required predicts retrievals from main memory\\n\\nNext, consider the differences between the models with respect to the function of IRUs. In dialogue C, a version of the dialogue without the IRUs is possible but is harder to interpret. Consider dialogue C without 22b, 22c and 23, i.e. replace 22a to 24 with But as far as the certificates are concerned, I don't like all my eggs in one basket. Interpreting this alternate version requires the same inference, namely that having all your investments in six month certificates constitutes the negatively evaluated condition of having all your eggs in one basket. However the inference requires more effort to process.\\n\\nThe stack model doesn't predict a function for the IRUs. However, according to the cache model, IRUs make information accessible that is not accessible by virtue of hierarchical recency, so that processes of content-based inferences, inference of discourse relations, and interpretation of anaphors can take place with less effort. Thus, one prediction of the cache model is that a natural way to make the anaphoric forms in dialogue B more easily interpretable is to re-realize the relevant proposition with an IRU, as in 8a':My problem is that my daughter is working, as well as her uh husband.\\n\\nThe IRU may function this way since: (1) the IRU reinstantiates the necessary information in the cache; or (2) the IRU is a retrieval cue for retrieval of information to the cache. Here reinstantiation is certainly sufficient, but in general these cases cannot be distinguished from corpus analysis. It should be possible to test psychologically using reaction time methods, whether and under what conditions IRUs function to simply reinstantiate an entity in the cache, and when they serve as retrieval cues.\\n\\nIt should also be possible to test whether entities that are in the focus spaces on the stack, according to the stack model, are more accessible than entities that have been popped off the stack. In the cache model, the entities in these focus spaces would not have a privileged attentional status, unless of course they had been refreshed in the cache by being realized implicitly or explicitly in the intervening discussion.\\n\\nThe second hypothesis is that the content of the return utterance indicates what information to retrieve from main memory to the cache. The occurrence of IRUs as in dialogue C is one way of doing this. IRUs at the locus of a return can: (1) reinstantiate required information in the cache so that no retrieval is necessary; (2) function as excellent retrieval cues for information from main memory. An examination of the data shows that IRUs occur in 6 of the 21 return pops. IRUs in combination with selectional restrictions leave only 2 cases of pronouns in return pops with competing antecedents.\\n\\nIn the remaining 2 cases, the competing antecedent is not and was never prominent in the discourse, i.e. it was never the discourse center, suggesting that it may never compete with the other cospecifier.\\n\\nIt should be possible to test how long it takes to resolve anaphors in return pops and under what conditions it can be done, considering the data presented here on competing referents, IRUs, explicit closing, and selectional restrictions. A probe just after a pronoun in a return pop and before the verb could determine whether the pronoun alone is an adequate retrieval cue, or whether selectional information from the verb is required or simply speeds processing.\\n\\nTo conclude, the analysis presented here suggests many hypotheses that could be empirically tested, which the currently available evidence does not enable us to resolve.\\n\\nDiscussion and Conclusion\\n\\nThis squib has discussed the role of limited attention in a computational model of discourse processing. The cache model was proposed as a computational implemention of human working memory and operations on attentional state are formulated as operations on a cache. Just as a cache can be used for processing the references and operations of a hierarchically structured program, so can a cache be used to model attentional state when discourse intentions are hierarchically structured.\\n\\nThe store and retrieve operations of the cache model casts discourse processing as a gradient phenomenon, predicting that the contents of the cache will change gradually, and that change requires processing effort. The notion of processing effort for retrieval operations on main memory makes predictions that can be experimentally tested. In the meantime, the notion of increased processing effort in the cache model explains the occurrence of a class of INFORMATIONALLY REDUNDANT utterances in discourse, as well as cases of infelicitous discourses constructed as variations on naturally occurring ones, while remaining consistent with evidence on human limited attentional capacity. Finally, the cache model appears to handle the class of ``return pops'' which prima facie should be problematic for the model.\\n\\nBibliography\\n\\nBaddeley, Alan.\\n\\n1986.\\n\\nWorking Memory.\\n\\nOxford University Press.\\n\\nCahn, Janet. 1991. The effect of intonation on pronoun referent resolution. Technical report, MIT Media Lab.\\n\\nClark, Herbert H. and C.J. Sengul. 1979. In search of referents for nouns and pronouns. Memory and Cognition, 7:35-41.\\n\\nDavis, James R. and Julia Hirschberg. 1988. Assigning intonational features in synthesized spoken directions. In ACL88.\\n\\nDi Eugenio, Barbara. 1990. Centering theory and the italian pronominal system. In COLING 90.\\n\\nFletcher, Charles R., John E. Hummel, and Chad J. Marsolek. 1990. Causality and the allocation of attention during comprehension. Journal of Experimental Psychology.\\n\\nFox, Barbara A. 1987. Discourse Structure and Anaphora: Written and Conversational English. Cambridge University Press.\\n\\nGreene, S.B., Gail McKoon, and R. Ratcliff. 1992. Pronoun resolution and discourse models. Journal of Experimental Psychology:Learning, Memory and Cognition.\\n\\nGrosz, Barbara J. 1977. The representation and use of focus in dialogue understanding. Technical Report 151, SRI International, 333 Ravenswood Ave, Menlo Park, Ca. 94025.\\n\\nGrosz, Barbara J. and Candace L. Sidner. 1986. Attentions, intentions and the structure of discourse. Computational Linguistics, 12:175-204.\\n\\nHobbs, Jerry R. 1985. On the coherence and structure of discourse. Technical Report CSLI-85-37, Center for the Study of Language and Information, Ventura Hall, Stanford University, Stanford, CA 94305.\\n\\nHuang, Xiorong. 1994. Planning references choices for argumentative texts. In The 7th International Conference on Natural Language Generation.\\n\\nKintsch, W. 1988. The role of knowledge in discourse comprehension: A construction-integration model. Psychological Review, 95:163-182.\\n\\nMalt, Barbara. 1984. The role of discourse structure in understanding anaphora. Journal of Memory and Language.\\n\\nMann, W.C. and S.A. Thompson. 1987. Rhetorical structure theory: Description and construction of text structures. In Gerard Kempen, editor, Natural Language Generation. Martinus Nijhoff, pages 83-96.\\n\\nMcKoon, Gail and Roger Ratcliff. 1992. Inference during reading. Psychological Review, 99(3):440-466.\\n\\nMiller, G. A. 1956. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Review, pages 81-97.\\n\\nPassonneau, Rebecca J. and Diane Litman. 1994. Empirical analysis of three dimension of spoken discourse: Segmentation, coherence and linguistic devices. In Donia Scott and Eduard Hovy, editors, Burning Issues in Discourse.\\n\\nPollack, Martha, Julia Hirschberg, and Bonnie Webber. 1982. User participation in the reasoning process of expert systems. In AAAI82.\\n\\nPrince, Ellen F. 1981. Toward a taxonomy of given-new information. In Radical Pragmatics. Academic Press, pages 223-255.\\n\\nRatcliff, Roger and Gail McKoon. 1988. A retrieval theory of priming in memory. Psychological Review, 95(3):385-408.\\n\\nReichman, Rachel. 1985. Getting Computers to Talk Like You and Me. MIT Press, Cambridge, MA.\\n\\nSachs, Jacqueline D. 1967. Recognition memory for syntactic and semantic aspects of connected discourse. Ph.D. thesis, University of California Berkeley.\\n\\nSidner, Candace L. 1979. Toward a computational theory of definite anaphora comprehension in English. Technical Report AI-TR-537, MIT.\\n\\nWalker, Marilyn A. 1992. Redundancy in collaborative dialogue. In Fourteenth International Conference on Computational Linguistics, pages 345-351.\\n\\nWalker, Marilyn A. 1993. Informational Redundancy and Resource Bounds in Dialogue. Ph.D. thesis, University of Pennsylvania.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9512003.xml'}),\n",
       " Document(page_content='Rapid Development of Morphological Descriptions for Full Language Processing Systems\\n\\nI describe a compiler and development environment for feature-augmented two-level morphology rules integrated into a full NLP system. The compiler is optimized for a class of languages including many or most European ones, and for rapid development and debugging of descriptions of new languages. The key design decision is to compose morphophonological and morphosyntactic information, but not the lexicon, when compiling the description. This results in typical compilation times of about a minute, and has allowed a reasonably full, feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system.\\n\\nIntroduction\\n\\nThe paradigm of two-level morphology (Koskenniemi, 1983) has become popular for handling word formation phenomena in a variety of languages. The original formulation has been extended to allow morphotactic constraints to be expressed by feature specification (Trost, 1990; Alshawi et al, 1991) rather than Koskenniemi\\'s less perspicuous device of continuation classes. Methods for the automatic compilation of rules from a notation convenient for the rule-writer into finite-state automata have also been developed, allowing the efficient analysis and synthesis of word forms. The automata may be derived from the rules alone (Trost, 1990), or involve composition with the lexicon (Karttunen, Kaplan and Zaenen, 1992).\\n\\nHowever, there is often a trade-off between run-time efficiency and factors important for rapid and accurate system development, such as perspicuity of notation, ease of debugging, speed of compilation and the size of its output, and the independence of the morphological and lexical components. In compilation, one may compose any or all of (a) the two-level rule set, (b) the set of affixes and their allowed combinations, and (c) the lexicon; see Kaplan and Kay (1994) for an exposition of the mathematical basis. The type of compilation appropriate for rapid development and acceptable run-time performance depends on, at least, the nature of the language being described and the number of base forms in the lexicon; that is, on the position in the three-dimensional space defined by (a), (b) and (c).\\n\\nFor example, English inflectional morphology is relatively simple; dimensions (a) and (b) are fairly small, so if (c), the lexicon, is known in advance and is of manageable size, then the entire task of morphological analysis can be carried out at compile time, producing a list of analysed word forms which need only be looked up at run time, or a network which can be traversed very simply. Alternatively, there may be no need to provide as powerful a mechanism as two-level morphology at all; a simpler device such as affix stripping (Alshawi, 1992, p119ff) or merely listing all inflected forms explicitly may be preferable.\\n\\nFor agglutinative languages such as Korean, Finnish and Turkish (Kwon and Karttunen, 1994; Koskenniemi, 1983; Oflazer, 1993), dimension (b) is very large, so creating an exhaustive word list is out of the question unless the lexicon is trivial. Compilation to a network may still make sense, however, and because these languages tend to exhibit few non-concatenative morphophonological phenomena other than vowel harmony, the continuation class mechanism may suffice to describe the allowed affix sequences at the surface level.\\n\\nMany European languages are of the inflecting type, and occupy still another region of the space of difficulty. They are too complex morphologically to yield easily to the simpler techniques that can work for English. The phonological or orthographic changes involved in affixation may be quite complex, so dimension (a) can be large, and a feature mechanism may be needed to handle such varied but interrelated morphosyntactic phenomena such as umlaut (Trost, 1991), case, number, gender, and different morphological paradigms. On the other hand, while there may be many different affixes, their possibilities for combination within a word are fairly limited, so dimension (b) is quite manageable.\\n\\nThis paper describes a representation and associated compiler intended for two-level morphological descriptions of the written forms of inflecting languages. The system described is a component of the Core Language Engine (CLE; Alshawi, 1992), a general-purpose language analyser and generator implemented in Prolog which supports both a built-in lexicon and access to large external lexical databases. In this context, highly efficient word analysis and generation at run-time are less important than ensuring that the morphology mechanism is expressive, is easy to debug, and allows relatively quick compilation. Morphology also needs to be well integrated with other processing levels. In particular, it should be possible to specify relations among morphosyntactic and morphophonological rules and lexical entries; for the convenience of developers, this is done by means of feature equations. Further, it cannot be assumed that the lexicon has been fully specified when the morphology rules are compiled. Developers may wish to add and test further lexical entries without frequently recompiling the rules, and it may also be necessary to deal with unknown words at run time, for example by querying a large external lexical database or attempting spelling correction (Alshawi, 1992, pp124-7). Also, both analysis and generation of word forms are required. Run-time speed need only be enough to make the time spent on morphology small compared to sentential and contextual processing.\\n\\nThese parameters - languages with a complex morphology/syntax interface but a limited number of affix combinations, tasks where the lexicon is not necessarily known at compile time, bidirectional processing, and the need to ease development rather than optimize run-time efficiency - dictate the design of the morphology compiler described in this paper, in which spelling rules and possible affix combinations (items (a) and (b)), but not the lexicon (item (c)), are composed in the compilation phase. Descriptions of French, Polish and English inflectional morphology have been developed for it, and I show how various aspects of the mechanism allow phenomena in these languages to be handled.\\n\\nThe Description Language\\n\\nMorphophonology\\n\\nThe Features in a rule is a list of\\n\\nFeature=Value equations. The allowed (finite) set of values of each feature must be prespecified. Value may be atomic or it may be a boolean expression.\\n\\nMembers of the surface and lexical strings may be characters or classes of single characters. The latter are represented by a single digit N in the string and an item N/ClassName in the Classes list; multiple occurrences of the same N in a single rule must all match the same character in a given application.\\n\\nBecause of the obligatory nature of change_e_1, and the fact that the orthographic feature restriction on the root cher, [cdouble=n], is consistent with the one on that rule, an alternative realisation chere, involving the use of the default rule in third position, is ruled out.\\n\\nWord Formation and Interfacing to Syntax\\n\\nThe allowed sequences of morphemes, and the syntactic and semantic properties of morphemes and of the words derived by combining them, are specified by morphosyntactic production rules (dimension (b)) and lexical entries both for affixes (dimension (b)) and for roots (dimension (c)), essentially as described by Alshawi (1992) (where the production rules are referred to as ``morphology rules\\'\\'). Affixes may appear explicitly in production rules or, like roots, they may be assigned complex feature-valued categories. Information, including the creation of logical forms, is passed between constituents in a rule by the sharing of variables. These feature-augmented production rules are just the same device as those used in the CLE\\'s syntactico-semantic descriptions, and are a much more natural way to express morphotactic information than finite-state devices such as continuation classes (see Trost and Matiasek, 1994, for a related approach).\\n\\nIrregular forms, either complete words or affixable stems, are specified by listing the morphological rules and terminal morphemes from which the appropriate analyses may be constructed, for example: irreg(dit,[dire,\\'PRESENT_3s\\'], [v_v_affix-only]). Here, PRESENT_3s is a pseudo-affix which has the same syntactic and semantic information attached to it as (one sense of) the affix ``t\\'\\', which is used to form some regular third person singulars. However, the spelling rules make no reference to PRESENT_3s; it is simply a device allowing categories and logical forms for irregular words to be built up using the same production rules as for regular words.\\n\\nCompilation\\n\\nAll rules and lexical entries in the CLE are compiled to a form that allows normal Prolog unification to be used for category matching at run time. The same compiled forms are used for analysis and generation, but are indexed differently. Each feature for a major category is assigned a unique position in the compiled Prolog term, and features for which finite value sets have been specified are compiled into vectors in a form that allows boolean expressions, involving negation as well as conjunction and disjunction, to be conjoined by unification (see Mellish, 1988; Alshawi, 1992, pp46-48).\\n\\nCompiling Spelling Patterns\\n\\nThis process results in a set of spelling patterns, one for each distinct application of the spelling rules to each affix sequence suggested by the production rules. A spelling pattern consists of partially specified surface and lexical root character sequences, fully specified surface and lexical affix sequences, orthographic feature constraints associated with the spelling rules and affixes used, and a pair of syntactic category specifications derived from the production rules used. One category is for the root form, and one for the inflected form.\\n\\nstripping off possible (surface) affix characters in the word and locating a spelling pattern that they index;\\n\\nmatching the remaining characters in the word against the surface part of the spelling pattern, thereby, through shared variables, instantiating the characters for the lexical part to provide a possible root spelling;\\n\\nchecking any orthographic feature constraints on that root;\\n\\nfinding a lexical entry for the root, by any of a range of mechanisms including lookup in the system\\'s own lexicon, querying an external lexical database, or attempting to guess an entry for an undefined word; and\\n\\nunifying the root lexical entry with the root category in the spelling pattern, thereby, through variable sharing with the other category in the pattern, creating a fully specified category for the inflected form that can be used in parsing.\\n\\nRepresenting Lexical Roots\\n\\nComplications arise in spelling rule application from the fact that, at compile time, neither the lexical nor the surface form of the root, nor even its length, is known. It would be possible to hypothesize all sensible lengths and compile separate spelling patterns for each. However, this would lead to many times more patterns being produced than are really necessary.\\n\\nLexical (and, after instantiation, surface) strings for the unspecified roots are therefore represented in a more complex but less redundant way: as a structure L1 ... Lm v(L,R) R1 ... Rn. Here the Li\\'s are variables later instantiated to single characters at the beginning of the root, and L is a variable, which is later instantiated to a list of characters, for its continuation. Similarly, the Ri\\'s represent the end of the root, and R is the continuation (this time reversed) leftwards into the root from the R1. The v(L,R) structure is always matched specially with a Kleene-star of the default spelling rule. For full generality and minimal redundancy, Lm and R1 are constrained not to match the default rule, but the other Li\\'s and Ri\\'s may. The values of nrequired are those for which, for some spelling rule, there are kcharacters in the target lexical string and n-k from the beginning of the right context up to (but not including) a boundary symbol. The lexical string of that rule may then match\\n\\nR1,...,Rk, and its\\n\\nright context match\\n\\nRk+1,...,Rn,+,.... The required values of m may be calculated similarly with reference to the left contexts of rules.\\n\\nApplying Obligatory Rules\\n\\nIn the absence of a lexical string for the root, the correct treatment of obligatory rules is another problem for compilation. If an obligatory rule specifies that lexical X must be realised as surface Y when certain contextual and feature conditions hold, then a partitioning where X is realised as something other than Y is only allowed if one or more of those conditions is unsatisfied. Because of the use of boolean vectors for both features and characters, it is quite possible to constrain each partitioning by unifying it with the complement of one of the conditions of each applicable obligatory rule, thereby preventing that rule from applying. For English, with its relatively simple inflectional spelling changes, this works well. However, for other languages, including French, it leads to excessive numbers of spelling patterns, because there are many obligatory rules with non-trivial contexts and feature specifications.\\n\\nFor this reason, complement unification is not actually carried out at compile time. Instead, the spelling patterns are augmented with the fact that certain conditions on certain obligatory rules need to be checked on certain parts of the partitioning when it is fully instantiated. This slows down run-time performance a little but, as we will see below, the speed is still quite acceptable.\\n\\nTimings\\n\\nThe compilation process for the entire rule set takes just over a minute for a fairly thorough description of French inflectional morphology, running on a Sparcstation 10/41 (SPECint92=52.6). Run-time speeds are quite adequate for full NLP, and reflect the fact that the system is implemented in Prolog rather than (say) C and that full syntactico-semantic analyses of sentences, rather than just morpheme sequences or acceptability judgments, are produced.\\n\\nAnalysis of French words using this rule set and only an in-core lexicon averages around 50 words per second, with a mean of 11 spelling analyses per word leading to a mean of 1.6 morphological analyses (the reduction being because many of the roots suggested by spelling analysis do not exist or cannot combine with the affixes produced). If results are cached, subsequent attempts to analyse the same word are around 40 times faster still. Generation is also quite acceptably fast, running at around 100 words per second; it is slightly faster than analysis because only one spelling, rather than all possible analyses, is sought from each call. Because of the separation between lexical and morphological representations, these timings are essentially unaffected by in-core lexicon size, as full advantage is taken of Prolog\\'s built-in indexing.\\n\\nDevelopment times are at least as important as computation times. A rule set embodying a quite comprehensive treatment of French inflectional morphology was developed in about one person month. The English spelling rule set was adapted from Ritchie et al (1992) in only a day or two. A Polish rule set is also under development, and Swedish is planned for the near future.\\n\\nSome Examples\\n\\nTo clarify further the use of the formalism and the operation of the mechanisms, we now examine several further examples.\\n\\nMultiple\\n\\n\\n\\nletter spelling changes\\n\\nIt is not necessary for the surface target to contain exactly one character for the blocking effect to apply, because the semantics of obligatoriness is that the lexical target and all contexts, taken together, make the specified surface target (of whatever length) obligatory for that partition. The reverse constraint, on the lexical target, does not apply.\\n\\nUsing features to control rule application\\n\\nFeatures can be used to control the application of rules to particular lexical items where the applicability cannot be deduced from spellings alone. For example, Polish nouns with stems whose final syllable has vowel  normally have inflected forms in which the accent is dropped. Thus in the nominative plural, krj (``style\\'\\') becomes kroje, br (``forest\\'\\') becomes bory, bj (``combat\\'\\') becomes boje. However, there are exceptions, such as zbj (``bandit\\'\\') becoming zbje. Similarly, some French verbs whose infinitives end in -eler take a grave accent on the first e in the third person singular future (modeler, ``model\\'\\', becomes modlera), while others double the l instead (e.g. appeler, ``call\\'\\', becomes appellera).\\n\\nThese phenomena can be handled by providing an obligatory rule for the case whether the letter changes, but constraining the applicability of the rule with a feature and making the feature clash with that for roots where the change does not occur. In the Polish case:\\n\\nDebugging the Rules\\n\\nThe debugging tools help in checking the operation of the spelling rules, either (1) in conjunction with other constraints or (2) on their own.\\n\\nFor case (2), the spelling rules may be applied directly, just as in rule compilation, to a specified surface or lexical character sequence, as if no lexical or morphotactic constraints existed. Feature constraints, and cases where the rules will not apply if those constraints are broken, are shown. For the lexical sequence cher+e+, for example, the output is as follows. Surface: \"chre\" [-] Lexical: \"cher\". Suffix: \"e\" c :: c [- default h :: h [- default :: e [- change_e_1 r :: r [- default :: + [- boundary Category: orth:[cdouble=n] e :: e [- default :: + [- boundary Surface: \"chere\" [-] Lexical: \"cher\". Suffix: \"e\" c :: c [- default h :: h [- default e :: e [- default  (breaks \"change_e_1\") r :: r [- default :: + [- boundary e :: e [- default :: + [- boundary This indicates to the user that if cher is given a lexical entry consistent with the constraint cdouble=n, then only the first analysis will be valid; otherwise, only the second will be.\\n\\nConclusions and Further Work\\n\\nThe rule formalism and compiler described here work well for European languages with reasonably complex orthographic changes but a limited range of possible affix combinations. Development, compilation and run-time efficiency are quite acceptable, and the use of rules containing complex feature-augmented categories allows morphotactic behaviours and non-segmental spelling constraints to be specified in a way that is perspicuous to linguists, leading to rapid development of descriptions adequate for full NLP.\\n\\nThe kinds of non-linear effects common in Semitic languages, where vowel and consonant patterns are interpolated in words (Kay, 1987; Kiraz, 1994) could be treated efficiently by the mechanisms described here if it proved possible to define a representation that allowed the parts of an inflected word corresponding to the root to be separated fairly cleanly from the parts expressing the inflection. The latter could then be used by a modified version of the current system as the basis for efficient lookup of spelling patterns which, as in the current system, would allow possible lexical roots to be calculated.\\n\\nAgglutinative languages could be handled efficiently by the current mechanism if specifications were provided for the affix combinations that were likely to occur at all often in real texts. A backup mechanism could then be provided which attempted a slower, but more complete, direct application of the rules for the rarer cases.\\n\\nThe interaction of morphological analysis with spelling correction (Carter, 1992; Oflazer, 1994; Bowden, 1995) is another possibly fruitful area of work. Once the root spelling patterns and the affix combinations pointing to them have been created, analysis essentially reduces to an instance of affix-stripping, which would be amenable to exactly the technique outlined by Carter (1992). As in that work, a discrimination net of root forms would be required; however, this could be augmented independently of spelling pattern creation, so that the flexibility resulting from not composing the lexicon with the spelling rules would not be lost.\\n\\nAcknowledgments\\n\\nThis research was partly funded by the Defence Research Agency, Malvern, UK, under Strategic Research Project M2YBT44X.\\n\\nReferences\\n\\nAbramson, H., (1992). ``A Logic Programming View of Relational Morphology\\'\\'. Proceedings of COLING-92, 850-854.\\n\\nAlshawi, H. (1992). The Core Language Engine (ed). MIT Press.\\n\\nAlshawi, H., D.J. Arnold, R. Backofen, D.M. Carter, J. Lindop, K. Netter, S.G. Pulman, J. Tsujii, and H. Uszkoreit (1991). Eurotra ET6/1: Rule Formalism and Virtual Machine Design Study. Commission of the European Communities, Luxembourg.\\n\\nBowden, T. (1995) ``Cooperative Error Handling and Shallow Processing\\'\\', these proceedings.\\n\\nCarter, D.M. (1992). ``Lattice-based Word Identification in CLARE\\'\\'. Proceedings of ACL-92.\\n\\nKaplan, R., and M. Kay (1994). ``Regular Models of Phonological Rule Systems\\'\\', Computational Linguistics, 20:3, 331-378.\\n\\nKay, M. (1987). ``Non-concatenative Finite-State Morphology\\'\\'. Proceedings of EACL-87.\\n\\nKarttunen, L., R.M. Kaplan, and A. Zaenen (1992). ``Two-level Morphology with Composition\\'\\'. Proceedings of COLING-92, 141-148.\\n\\nKiraz, G. (1994). ``Multi-tape Two-level Morphology\\'\\'. Proceedings of COLING-94, 180-186.\\n\\nKoskenniemi, K. (1983). Two-level morphology: a general computational model for word-form recognition and production. University of Helsinki, Department of General Linguistics, Publications, No. 11.\\n\\nKwon, H-C., and L. Karttunen (1994). ``Incremental Construction of a Lexical Transducer for Korean\\'\\'. Proceedings of COLING-94, 1262-1266.\\n\\nMellish, C. S. (1988). ``Implementing Systemic Classification by Unification\\'\\'. Computational Linguistics 14:40-51.\\n\\nOflazer, K. (1993). ``Two-level Description of Turkish Morphology\\'\\'. Proceedings of European ACL-93.\\n\\nOflazer, K. (1994). Spelling Correction in Agglutinative Languages. Article 9410004 in cmp-lg@xxx.lanl.gov archive.\\n\\nRitchie, G., G.J. Russell, A.W. Black and S.G. Pulman (1992). Computational Morphology. MIT Press.\\n\\nRuessink, H. (1989). Two Level Formalisms. Utrecht Working Papers in NLP, no. 5.\\n\\nTrost, H. (1990). ``The Application of Two-level Morphology to Non-Concatenative German Morphology\\'\\'. Proceedings of COLING-90, 371-376.\\n\\nTrost, H. (1991). ``X2MORF: A Morphological Component Based on Augmented Two-level Morphology\\'\\'. Proceedings of IJCAI-91, 1024-1030.\\n\\nTrost, H., and J. Matiasek (1994). ``Morphology with a Null-Interface\\'\\', Proceedings of COLING-94.\\n\\nFootnotes', metadata={'source': '../data/raw/cmplg-xml/9502006.xml'}),\n",
       " Document(page_content=\"User\\n\\n\\n\\nDefined Nonmonotonicity in Unification\\n\\n\\n\\nBased Formalisms\\n\\nBackground\\n\\nThis paper starts by providing the minimal properties required of a unification-based formalism when extending with nonmonotonic definitions. I then describe the approach of user-defined nonmonotonicity and illustrate how some commonly used nonmonotonic constructions can be defined within it. Finally I conclude with a discussion of the relation to Reiter's default logic and computational properties of the approach.\\n\\nPreliminaries\\n\\nOne very plausible subsumption order that can be used is the ordinary subsumption lattice of feature structures. It is, however, possible to use some other kind of subsumption order if that is more suitable for the domain to be modelled by the formalism. Examples of other subsumption orders that might be useful are typed feature structures, feature structures extended with disjunction, or simply an order based on sets and set inclusion.\\n\\nThe second constraint placed on the formalism, the possibility of defining an inheritance hierarchy, is not essential for the definition of nonmonotonic operations. It is, however, very useful when defining nonmonotonic constructions. The following notation will be used for describing an inheritance hierarchy.\\n\\nThus, each member in the inheritance hierarchy is called a class, which is defined by giving it a name and a parent in the hierarchy. It is also possible to define some constraints, called requirements, which must hold for a class. These requirements can be both structures in the subsumption order and nonmonotonic rules. The constraints on classes are inherited through the hierarchy. Every object in a class is assumed to contain at least the information given by the constraints specified for it and all its ancestors. For simplicity multiple inheritance between classes will not be allowed. This means that two classes where none of them is a subclass of the other, will always be considered inconsistent and thus yield a failure when unified.\\n\\nUser\\n\\n\\n\\nDefined Nonmonotonicity\\n\\nIn the approach described in this paper, the user is allowed to define the actual nonmonotonic rule that should be used for a particular operation by using the following syntax.\\n\\nThe when slot in the rule allows the user to decide when the rule is going to be applied, or in Young and Rounds' terminology, explained. I will make use of two values for the when-slot, immediate and posterior. Immediate means that the nonmonotonic rule is going to be applied each time a full unification task has been solved or whenever all information about an object in the defined inheritance hierarchy has been retrieved. Posterior explanation means that the explanation of the rule is postponed until reaching the result of some external process, for example, a parser or generator. There is however no hinder in excluding the use of other values here. One could, for example, imagine cases where one would want different nonmonotonic rules to be explained after a completed parse, a generation, or after resolving discourse referents.\\n\\nNote that although the when slot in the definition of a nonmonotonic rule allows the user to define when his rule is going to be applied we will still have an order independent nonmonotonic unification operator. This is the case because we follow Young and Rounds' approach and separate the unification operation from the explanation of a nonmonotonic rule. Therefore, what affects the final result of a computation is when one chooses to explain default rules and not the order of the unification operations occurring between such explanations.\\n\\nFormal Definitions\\n\\nA nonmonotonic sort is a structure containing both information from the basic subsumption order and information about default rules to be explained at a later point in the computation.\\n\\nAs seen by the definition a nonmonotonic sort is considered to be a pair of monotonic information from the subsumption order and nonmonotonic information represented as a set of nonmonotonic rules. The user can assign nonmonotonic information to a nonmonotonic sort by calling a nonmonotonic definition as defined in the previous section. The actual nonmonotonic rule occurring within the sort is a pair consisting of the when slot and the last part of the nonmonotonic definition, with the parameter variables instantiated according to the call made by the user.\\n\\nGiven the unification operation of objects within the subsumption order and the definition of nonmonotonic sorts it is possible to define an operation for nonmonotonic unification.\\n\\nThe nonmonotonic unification is computed by computing the unification of the monotonic parts of the two sorts and then taking the union of their nonmonotonic parts. The extra conditions used when forming the union of the nonmonotonic parts of the sorts are the same as in the definition of a nonmonotonic sort and their purpose is to remove nonmonotonic rules that are no longer applicable, or would have no effect when applied to the sort resulting from the unification.\\n\\nIt is important to note that this generalization of the original definition of nonmonotonic unification from Young and Rounds (1993) preserves the property of order independence for default unification.\\n\\nWhen using nonmonotonic sorts containing nonmonotonic rules, we also need to know how to merge the monotonic and nonmonotonic information within the sort. I will use the terminology w-application for applying one nonmonotonic rule to the sort and w-explanation when applying all possible rules.\\n\\nNote that the w in w-application should be considered as a variable. This means that only nonmonotonic rules whose first component is ware considered and that it is possible to choose which nonmonotonic rules should be applied in a particular point at some computation.\\n\\nNote also that the choice of which nonmonotonic rule to apply in each step of a w-explanation is nondeterministic. Consequently, it is possible to have conflicting defaults and multiple w-explanations for a nonmonotonic sort.\\n\\nNote also that the result of a w-explanation is allowed to be fail. Another option would be to interpret fail as if the application of the nonmonotonic rule should not be allowed. However, as seen in the next section, for many uses of nonmonotonic extensions within unification-based formalisms, the aim is to derive failure if the resulting structure does not fulfill some particular conditions. This makes it important to allow fail to be the result of applying a nonmonotonic rule.\\n\\nExamples\\n\\nThis default rule can be used when defining verbs. The rule is used for stating that verbs are active by default. I also define the two Swedish verbs skickade (sent) and skickades (was sent) to illustrate how this rule works.\\n\\nWhile retrieving the information for these two verbs we will obtain the following two feature structures containing nonmonotonic sorts:\\n\\nSince I have used immediate for the when-slot in the definition of the default rule, this nonmonotonic rule will be applied immediately after retrieving all information about a verb in the hierarchy. For the two structures above, the default rule can be applied for skickade, since active is consistent with [], but not for skickades, since active and passive are inconsistent. The result after applying immediate-explanation to the two structures above is shown below.\\n\\nOne use of value constraints in LFG is to assert a condition that some grammar rules can only be used for passive sentences. I will here assume that a representation for verbs where passive verbs have the value passive for the attribute form, but where other verbs have no value for this attribute. In the syntax used in this paper the constraint that a particular grammar rule can only be used for passive verbs would be expressed as below:\\n\\nThis would result in the nonmonotonic sort:\\n\\nThe next nonmonotonic structure I want to discuss is any-values. The inheritance hierarchy is used to be able to define any-values in a simple way.\\n\\nIn this small hierarchy it is assumed that all possible values of a structure is a subtype of value. We then divide this into none, which represents that a structure cannot have any value and any_value which contains all actual values. The class any_value is then further divided into a class called any_no_value, which only contains this single value, and the actual values of a structure. The class any_no_value should not be used when defining linguistic knowledge. However, when applying the default rule a value that has not been instantiated is compatible with this any_no_value. Therefore the default rule can make the conclusion that the structure is inconsistent, which is what we desire. Note that, as soon as a value has been further instantiated into a 'real' value, it will no longer be consistent with any_no_value, and the nonmonotonic rule cannot apply. Two examples will further illustrate this.\\n\\nThe last nonmonotonic operations I want to discuss are completeness and coherence as used in LFG. To be able to define these operations I assume the inheritance hierarchy above, without the nonmonotonic definition of any. I will, instead, make use of the two nonmonotonic definitions below.\\n\\nThe first of these rules is used to check coherence, and the effect is to add the value none to each attribute that has been defined to be relevant for coherence check, but has not been assigned a value in the lexicon. The second rule is used for checking completeness and it works similarly to the any-definition above.\\n\\nFinally, I will show how a fragment of a lexicon can be defined according to these rules. Note that in the definition of the transitive verb, the value any_value is given to the appropriate attributes. This means that they are inconsistent with none, and thus, the coherence rule cannot be applied.\\n\\nRelation to Default Logic\\n\\nIn this section I will discuss the relation of this work to Reiter's (1980) default logic. There will also be some discussion on the computational properties and limitations of the given approach.\\n\\nCompared with Reiter's default logic, our notion of nonmonotonic sorts corresponds to default theories. Unification of nonmonotonic sorts would then correspond to merging two default theories into one single theory and our notion of explaining a nonmonotonic sort corresponds to computing the extension of a default theory in default logic.\\n\\nAs mentioned previously there is with nonmonotonic sorts, as well as normal default logic, a possibility of conflicting defaults and thus multiple nonmonotonic extensions for a structure. One difference is that nonmonotonic sorts allow that the application of a nonmonotonic rule leads to fail, i.e. an inconsistent structure, while default logic does not allow this outcome. However, since fail is allowed as a valid explanation for a nonmonotonic sort, there is, as for normal default logic, always at least one explanation for a sort.\\n\\nThe two following examples will illustrate the difference between nonmonotonic rules giving multiple extensions and nonmonotonic rules giving a single explanation fail.\\n\\nExample a\\n\\nExample b\\n\\nIn example a the application of one rule, does not make the other inapplicable. Thus the only explanation for a structure is achieved by applying both these two rules and results in fail. In example b, however, the application of one of the rules would block the application of the other. Thus, in this case there are two explanations for the structure dependant on which of the rules that has been applied first. Note that even though there is an order dependency on the application order of nonmonotonic rules this does not affect the order independency on nonmonotonic unification between application of nonmonotonic rules.\\n\\nAllowing multiple extensions gives a higher computational complexity than allowing only theories with one extension. Since it is the user who defines the actual nonmonotonic theory multiple extensions must be allowed and it must be considered a task for the user to define his theory in the way he prefers.\\n\\nImprovements of the Approach\\n\\nI will start with two observations regarding the definitions given in section 3. First, it is possible to generalize these definitions to allow the first component of a nonmonotonic sort to contain substructures that are also nonmonotonic sorts. With the generalized versions of the definitions explanations that simultaneously explain all substructures of a nonmonotonic sort will be considered. Note that the explanation of default rules at one substructure might affect the explanation of rules at other substructures. Therefore the order on which nonmonotonic rules at different substructures are applied is important and all possible application orders must be considered.\\n\\nAs stated previously, nonmonotonic sorts allow multiple explanations of a nonmonotonic sort. If desired, it would be fairly easy to add priorities to the nonmonotonic rules, and thus induce a preference order on explanations.\\n\\nConclusion\\n\\nThe generality of the approach was demonstrated by defining some of the most commonly used nonmonotonic operations. We also gave formal definitions for the approach and provided a discussion on its computational properties.\\n\\nAcknowledgments\\n\\nThis work has been supported by the Swedish Research Council for Engineering Sciences (TFR). I would also like to thank Lars Ahrenberg and Patrick Doherty for comments on this work and Mark A. Young for providing me with much-needed information about his and Bill Rounds' work.\\n\\nBibliography\\n\\nGosse Bouma. 1990. Defaults in Unification Grammar. in Proceedings of the 1990 Conference of the Association for Computational Linguistics, pages 165-172.\\n\\nGosse Bouma.\\n\\n1992.\\n\\nFeature Structures and Nonmonotonicity.\\n\\nComputational Linguistics 18(2):183\\n\\n\\n\\n203.\\n\\nJochen Drre and Andreas Eisele. 1991. A Comprehensive Unification-Based Grammar Formalism. DYANA Report - Deliverable R3.1B. January 1991.\\n\\nMartin C. Emele, and Remi Zajac. 1990. Typed Unification Grammars. In Proceedings of the 13th International Conference on Computational Linguistics, Vol. 3, pages 293-298, Helsinki, Finland.\\n\\nRod Johnson and Michael Rosner. 1989. A Rich Environment for Experimentation with Unification Grammars. In Proceedings of the 4th Conference of the European Chapter of the Association for Computational Linguistics, pages 182-189, Manchester, England.\\n\\nR. Kaplan and J.Bresnan. 1983. A Formal System for Grammatical Representation. In: J Bresnan (ed. ), The Mental Representation of Grammatical Relations. MIT Press, Cambridge, Massachusetts.\\n\\nRay Reiter. 1980. A Logic for Default Reasoning. In Artificial Intelligence, 13:81-132.\\n\\nGraham Russel, Afzal Ballim, John Carrol and Susan Warwick-Armstrong. 1992. A Practical Approach to Multiple Default Inheritance for Unification-Based Lexicons. Computational Linguistics 18(3):311-337.\\n\\nLena Strmbck. 1994. Achieving Flexibility in Unification Grammars. In Proceedings of the 15th International Conference on Computational Linguistics, Vol. 3, pages 842-846, Kyoto, Japan.\\n\\nMark A Young and Bill Rounds. 1993. A Logical Semantics for Nonmonotonic Sorts. In Proceedings of the 1993 Conference of the Association for Computational Linguistics, pages 209-215\", metadata={'source': '../data/raw/cmplg-xml/9505033.xml'}),\n",
       " Document(page_content=\"RELATING COMPLEXITY TO PRACTICAL PERFORMANCE IN PARSING WITH WIDE-COVERAGE UNIFICATION GRAMMARS\\n\\nThe paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms, using a wide-coverage grammar. The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers.\\n\\nINTRODUCTION\\n\\nGeneral-purpose natural language (NL) analysis systems have recently started to use declarative unification-based sentence grammar formalisms; systems of this type include SRI's CLARE system (Alshawi et al., 1992) and the Alvey NL Tools (ANLT; Briscoe et al., 1987a). Using a declarative formalism helps ease the task of developing and maintaining the grammar (Kaplan, 1987). In addition to syntactic processing, the systems incorporate lexical, morphological, and semantic processing, and have been applied successfully to the analysis of naturally-occurring texts (e.g. Alshawi et al., 1992; Briscoe  Carroll, 1993).\\n\\nEvaluations of the grammars in these particular systems have shown them to have wide coverage (Alshawi et al., 1992; Taylor, Grover  Briscoe, 1989). However, although the practical throughput of parsers with such realistic grammars is important, for example when processing large amounts of text or in interactive applications, there is little published research that compares the performance of different parsing algorithms using wide-coverage unification-based grammars. Previous comparisons have either focussed on context-free (CF) or augmented CF parsing (Tomita, 1987; Billot  Lang, 1989), or have used relatively small, limited-coverage unification grammars and lexicons (Shann, 1989; Bouma van Noord, 1993; Maxwell  Kaplan, 1993). It is not clear that these results scale up to reflect accurately the behaviour of parsers using realistic, complex unification-based grammars: in particular, with grammars admitting less ambiguity parse time will tend to increase more slowly with increasing input length, and also with smaller grammars rule application can be constrained tightly with relatively simple predictive techniques. Also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent and so be a confounding factor in any general conclusions made.\\n\\nOther research directed towards improving the throughput of unification-based parsing systems has been concerned with the unification operation itself, which can consume up to 90% of parse time (e.g. Tomabechi, 1991) in systems using lexicalist grammar formalisms (e.g. HPSG; Pollard  Sag, 1987). However, parsing algorithms assume more importance for grammars having more substantial phrase structure components, such as CLARE (which although employing some HPSG-like analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et al., 1985), since the more specific rule set can be used to control which unifications are performed.\\n\\nIn NL analysis, the syntactic information associated with lexical items makes top-down parsing less attractive than bottom-up (e.g. CKY; Kasami, 1965; Younger, 1967), although the latter is often augmented with top-down prediction to improve performance (e.g. Earley, 1970; Lang, 1974; Pratt, 1975). Section 2 describes three unification-based parsers which are related to polynomial-complexity bottom-up CF parsing algorithms. Although incorporating unification increases their complexity to exponential on grammar size and input length (section 3), this appears to have little impact on practical performance (section 4). Sections 5 and 6 discuss these findings and present conclusions.\\n\\nTHE PARSERS\\n\\nThe three parsers in this study are: a bottom-up left-corner parser, a (non-deterministic) LR parser, and an LR-like parser based on an algorithm devised by Schabes (1991). All three parsers accept grammars written in the ANLT formalism (Briscoe et al., 1987a), and the first two are distributed as part of the ANLT package. The parsers create parse forests (Tomita, 1987) that incorporate subtree sharing (in which identical sub-analyses are shared between differing superordinate analyses) and node packing (where sub-analyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node).\\n\\nTHE BOTTOM-UP LEFT-CORNER PARSER Efficient rule invocation from cheap (static) rule indexing, using discrimination trees keyed on the feature values in each rule's first daughter to interleave rule access with unification and also to share unification results across groups of rules.\\n\\nDynamic indexing of partial and complete constituents on category types to avoid attempting unification or subsumption operations which static analysis shows will always fail.\\n\\nDynamic storage minimisation, deferring structure copying--e.g. required by the unification operation or by constituent creation--until absolutely necessary (e.g. unification success or parse success, respectively).\\n\\nTHE NON\\n\\n\\n\\nDETERMINISTIC LR PARSER\\n\\nBriscoe  Carroll (1993) describe a methodology for constructing an LR parser for a unification-based grammar, in which a CF `backbone' grammar is automatically constructed from the unification grammar, a parse table is constructed from the backbone grammar, and a parser is driven by the table and further controlled by unification of the `residue' of features in the unification grammar that are not encoded in the backbone. In this parser, the LALR(1) technique (Aho, Sethi  Ullman, 1986) is used, in conjunction with a graph-structured stack (Tomita, 1987), adapting for unification-based parsing Kipps' (1989) Tomita-like recogniser that achieves polynomial complexity on input length through caching.\\n\\nOn each reduction the parser performs the unifications specified by the unification grammar version of the CF backbone rule being applied. This constitutes an on-line parsing algorithm. In the general case, the off-line variant (in which all unifications are deferred until the complete CF parse forest has been constructed) is not guaranteed to terminate; indeed, it usually does not do so with the ANLT grammar. However, a drawback to the on-line algorithm is that a variant of Kipps' caching cannot be used, since the cache must necessarily assume that all reductions at a given vertex with all rules with the same number of daughters build exactly the same constituent every time; in general this is not the case when the daughters are unification categories. A weaker kind of cache on partial analyses (and thus unification results) was found to be necessary in the implementation, though, to avoid duplication of unifications; this sped the parser up by a factor of about three, at little space cost.\\n\\nTHE COMPILED\\n\\n\\n\\nEARLEY PARSER\\n\\nThe Compiled-Earley (CE) parser is based on a predictive chart-based CF parsing algorithm devised by Schabes (1991) which is driven by a table compiling out the predictive component of Earley's (1970) parser. The size of the table is related linearly to the size of the grammar (unlike the LR technique). Schabes demonstrates that this parser always takes fewer steps than Earley's, although its time complexity is the same: O(n[3]). The space complexity is also cubic, since the parser uses Earley's representation of parse forests.\\n\\nThe incorporation of unification into the CE parser follows the methodology developed for unification-based LR parsing described in the previous section: a table is computed from a CF `backbone', and a parser, augmented with on-line unification and feature-based subsumption operations, is driven by the table. To allow meaningful comparison with the LR parser, the CE parser uses a one-word lookahead version of the table, constructed using a modified LALR technique (Carroll, 1993).\\n\\nTo achieve the cubic time bound, the parser must be able to retrieve in unit time all items in the chart having a given state, and start and end position in the input string. However, the obvious array implementation, for say a ten word sentence with the ANLT grammar, would contain almost 500000 elements. For this reason, the implementation employs a sparse representation for the array, since only a small proportion of the elements are ever filled. In this parser, the same sort of duplication of unifications occurs as in the LR parser, so lists of partial analyses are cached in the same way.\\n\\nCOMPLEXITIES OF THE PARSERS\\n\\nThe two variables that determine a parser's computational complexity are the grammar and the input string (Barton, Berwick  Ristad, 1987). These are considered separately in the next two sections.\\n\\nGRAMMAR\\n\\n\\n\\nDEPENDENT COMPLEXITY\\n\\nThe term dependent on the grammar in the time complexity of the BU-LC unification-based parser described above is\\n\\nO(|C|[2|R|3]), where |C|is the number of categories implicit in the grammar, and |R|, the number of rules. The space complexity is dominated by the size of the parse forest, O(|C|) (these results are proved by Carroll, 1993). For the ANLT grammar, in which features are nested to a maximum depth of two, |C| is finite but nevertheless extremely large (Briscoe et al., 1987b).\\n\\nThe grammar-dependent complexity of the LR parser makes it also appear intractable: Johnson (1989) shows that the number of LR(0) states for certain (pathological) grammars is exponentially related to the size of the grammar, and that there are some inputs which force an LR parser to visit all of these states in the course of a parse. Thus the total number of operations performed, and also space consumed (by the vertices in the graph-structured stack), is an exponential function of the size of the grammar.\\n\\nTo avoid this complexity, the CE parser employs a table construction method which ensures that the number of states in the parse table is linearly related to the size of the grammar, resulting in the number of operations performed by the parser being at worst a polynomial function of grammar size.\\n\\nINPUT\\n\\n\\n\\nDEPENDENT COMPLEXITY\\n\\nAlthough the complexity of returning all parses for a string is always related exponentially to its length (since the number of parses is exponential, and they must all at least be enumerated), the complexity of a parser is usually measured for the computation of a parse forest (unless extracting a single analysis from the forest is worse than linear).\\n\\nPRACTICAL RESULTS\\n\\nTo assess the practical performance of the three unification-based parsers described above, a series of experiments were conducted using the ANLT grammar (Grover, Carroll  Briscoe, 1993), a wide-coverage grammar of English. The grammar is defined in metagrammatical formalism which is compiled into a unification-based `object grammar'--a syntactic variant of the Definite Clause Grammar formalism (Pereira  Warren, 1980)--containing 84 features and 782 phrase structure rules. Parsing uses fixed-arity term unification. The grammar provides full coverage of the following constructions: declarative sentences, imperatives and questions (yes/no, tag and wh-questions); all unbounded dependency types (topicalisation, relativisation, wh-questions); a relatively exhaustive treatment of verb and adjective complement types; phrasal and prepositional verbs of many complement types; passivisation; verb phrase extraposition; sentence and verb phrase modification; noun phrase complements and pre- and post-modification; partitives; coordination of all major category types; and nominal and adjectival comparatives.\\n\\nAlthough the grammar is linked to a lexicon containing definitions for 40000 base forms of words, the experiments draw on a much smaller lexicon of 600 words (consisting of closed class vocabulary and, for open-class vocabulary, definitions of just a sample of words which taken together exhibit the full range of possible complementation patterns), since issues of lexical coverage are of no concern here.\\n\\nCOMPARING THE PARSERS\\n\\nIn the first experiment, the ANLT grammar was loaded and a set of sentences was input to each of the three parsers. In order to provide an independent basis for comparison, the same sentences were also input to the SRI Core Language Engine (CLE) parser (Moore  Alshawi, 1992) with the CLARE2.5 grammar (Alshawi et al., 1992), a state-of-the-art system accessible to the author.\\n\\nThe sentences were taken from an initial sample of 175 representative sentences extracted from a corpus of approximately 1500 that form part of the ANLT package. This corpus, implicitly defining the types of construction the grammar is intended to cover, was written by the linguist who developed the ANLT grammar and is used to check for any adverse effects on coverage when the grammar is modified during grammar development. Of the initial 175 sentences, the CLARE2.5 grammar failed to parse 42 (in several cases because punctuation is strictly required but is missing from the corpus). The ANLT grammar also failed to parse three of these, plus an additional four. These sentences were removed from the sample, leaving 129 (mean length 6.7 words) of which 47 were declarative sentences, 38 wh-questions and other sentences with gaps, 20 passives, and 24 sentences containing co-ordination.\\n\\nTable 1 shows the total parse times and storage allocated for the BU-LC parser, the LR parser, and the CE parser, all with ANLT grammar and lexicon. All three parsers have been implemented by the author to a similar high standard: similar implementation techniques are used in all the parsers, the parsers share the same unification module, run in the same Lisp environment, have been compiled with the same optimisation settings, and have all been profiled with the same tools and hand-optimised to a similar extent. (Thus any difference in performance of more than around 15% is likely to stem from algorithmic rather than implementational reasons). Both of the predictive parsers employ one symbol of lookahead, incorporated into the parsing tables by the LALR technique. Table 1 also shows the results for the CLE parser with the CLARE2.5 grammar and lexicon. The figures include garbage collection time, and phrasal (where appropriate) processing, but not parse forest unpacking. Both grammars give a total of around 280 analyses at a similar level of detail.\\n\\nThe results show that the LR parser is approximately 35% faster than the BU-LC parser, and allocates about 30% less storage. The magnitude of the speed-up is less than might be expected, given the enthusiastic advocation of non-deterministic CF LR parsing for NL by some researchers (e.g. Tomita, 1987; Wright, Wrigley  Sharman, 1991), and in the light of improvements observed for predictive over pure bottom-up parsing (e.g. Moore  Dowding, 1991). However, on the assumption that incorrect prediction of gaps is the main avoidable source of performance degradation (c.f. Moore  Dowding), further investigation shows that the speed-up is near the maximum that is possible with the ANLT grammar (around 50%).\\n\\nThe throughput of the CE parser is half that of the LR parser, and also less than that of the BU-LC parser. However, it is intermediate between the two in terms of storage allocated. Part of the difference in performance between it and the LR parser is due to the fact that it performs around 15% more unifications. This might be expected since the corresponding finite state automaton is not determinised--to avoid theoretical exponential time complexity on grammar size--thus paying a price at run time. Additional reasons for the relatively poor performance of the CE parser are the overheads involved in maintaining a sparse representation of the chart, and the fact that with the ANLT grammar it generates less ``densely packed'' parse forests, since its parse table, with 14% more states (though fewer actions) than the LALR(1) table, encodes more contextual distinctions (Billot  Lang, 1989:146).\\n\\nGiven that the ANLT and CLARE2.5 grammars have broadly similar (wide) coverage and return very similar numbers of syntactic analyses for the same inputs, the significantly better throughput of the three parsers described in this paper over the CLE parser indicates that they do not contain any significant implementational deficiencies which would bias the results.\\n\\nSWAPPING THE GRAMMARS OVER\\n\\nA second experiment was carried out with the CLE parser, in which the built-in grammar and lexicon were replaced by versions of the ANLT object grammar and lexical entries translated (automatically) into the CLE formalism. (The reverse of this configuration, in which the CLARE2.5 grammar is translated into the ANLT formalism, is not possible since some central rules contain sequences of daughters specified by a single `list' variable, which has no counterpart in the ANLT and cannot directly be simulated). The throughput of this configuration was only one fiftieth of that of the BU-LC parser. The ANLT grammar contains more than five times as many rules than does the sentence-level portion of the CLARE2.5 grammar, and Alshawi (personal communication) points out that the CLE parser had not previously been run with a grammar containing such a large number of rules, in contrast to the ANLT parsers.\\n\\nTHE EFFECT OF SENTENCE LENGTH\\n\\nAlthough the mean sentence length in the first two experiments is much shorter than the 20-30 word length (depending on genre etc.) that is common in real texts, the test sentences cover a wide range of syntactic constructions and exhibit less constructional bias than would a set of sentences extracted at random from a single corpus. However, to investigate performance on longer sentences and the relationship between sentence length and parse time, a further set of 100 sentences with lengths distributed uniformly between 13 and 30 words was created by hand by the author and added to the previous test data. Table 2 shows the relationship between sentence length and mean parse time with the BU-LC and LR parsers.\\n\\nThe standard deviations for numbers of parses are also relatively large. The maximum number of parses was 2736 for one 29-word sentence, but on the other hand some of even the longest sentences had fewer than ten parses. (But note that since the time taken for parse forest unpacking is not included in parse times, the latter do not vary by such a large magnitude).\\n\\nThe results of this experiment are displayed graphically in Figure 1, together with a quadratic function. Comparison with the function suggests that, at least for the BU-LC parser, parse time is related roughly quadratically to input length.\\n\\nIn previous work with the ANLT (Briscoe  Carroll, 1993), throughput with raw corpus data was worse than that observed in these experiments, though probably only by a constant factor. This could be due to the fact that the vocabulary of the corpus concerned exhibits significantly higher lexical ambiguity; however, for sentences taken from a specific corpus, constructional bias observed in a training phase could be exploited to improve performance (e.g. Samuelsson  Rayner, 1991).\\n\\nDISCUSSION\\n\\nAll three of the parsers have theoretical worst-case complexities that are either exponential, or polynomial on grammar size but with an extremely large multiplier. Despite this, in the practical experiments reported in the previous section the parsers achieve relatively good throughput with a general-purpose wide-coverage grammar of a natural language. It therefore seems likely that grammars of the type considered in this paper (i.e. with relatively detailed phrase structure components, but comparatively simple from a unification perspective), although realistic, do not bring the parsing algorithms involved anywhere near the worst-case complexity.\\n\\nIn the experiments, the CE technique results in a parser with worse performance than the normal LR technique. Indeed, for the ANLT grammar, the number of states--the term that the CE technique reduces from exponential to linear on the grammar size--is actually smaller in the standard LALR(1) table. This suggests that, when considering the complexity of parsers, the issue of parse table size is of minor importance for realistic NL grammars (as long as an implementation represents the table compactly), and that improvements to complexity results with respect to grammar size, although interesting from a theoretical standpoint, may have little practical relevance for the processing of natural language.\\n\\nAlthough Schabes (1991:107) claims that the problem of exponential grammar complexity ``is particularly acute for natural language processing since in this context the input length is typically small (10-20 words) and the grammar size very large (hundreds or thousands of rules and symbols)'', the experiments indicate that, with a wide-coverage NL grammar, inputs of this length can be parsed quite quickly; however, longer inputs (of more than about 30 words in length)--which occur relatively frequently in written text--are a problem. Unless grammar size takes on proportionately much more significance for such longer inputs, which seems implausible, it appears that in fact the major problems do not lie in the area of grammar size, but in input length.\\n\\nkleene star is used only in a very limited context (for the analysis of coordination),\\n\\nmore than 90% of the rules in the grammar have no more than two daughters, and\\n\\nvery few rules license both left and right recursion (for instance of the sort that is typically used to analyse noun compounding, i.e. N -] N N).\\n\\nDespite little apparent theoretical difference between the CLE and ANLT grammar formalisms, and the fact that no explicit or formal process of `tuning' parsers and grammars to perform well with each other has been carried out in either of the ANLT or CLARE systems, the results of the experiment comparing the performance of the respective parsers using the ANLT grammar suggests that the parallel development of the software and grammars that has occurred nevertheless appears to have caused this to happen automatically. It therefore seems likely that implementational decisions and optimisations based on subtle properties of specific grammars can, and may very often be, more important than worst-case complexity when considering the practical performance of parsing algorithms.\\n\\nCONCLUSIONS\\n\\nThe research reported is in a similar vein to that of, for example, Moore Dowding (1991), Samuelsson  Rayner (1991), and Maxwell  Kaplan (1993), in that it relies on empirical results for the study and optimisation of parsing algorithms rather than on traditional techniques of complexity analysis. The paper demonstrates that research in this area will have to rely on empirical data until complexity theory is developed to a point where it is sufficiently fine-grained and accurate to predict how the properties of individual unification-based grammars will interact with particular parsing algorithms to determine practical performance.\\n\\nREFERENCES Aho, A., R. Sethi  J. Ullman (1986) Compilers: principles, techniques and tools. Reading, MA: Addison-Wesley.\\n\\nAlshawi, H., D. Carter, R. Crouch, S. Pulman, M. Rayner  A. Smith (1992) CLARE: a contextual reasoning and cooperative response framework for the Core Language Engine. SRI International, Cambridge, UK.\\n\\nBarton, G., R. Berwick  E. Ristad (1987) Computational complexity and natural language. Cambridge, MA: MIT Press.\\n\\nBillot, S.  B. Lang (1989) ``The structure of shared forests in ambiguous parsing.'' In Proceedings of the 27th Meeting of the Association for Computational Linguistics. 143-151.\\n\\nBouma, G.  G. van Noord (1993) ``Head-driven parsing for lexicalist grammars: experimental results.'' In Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics. 101-105.\\n\\nBriscoe, E., C. Grover, B. Boguraev  J. Carroll (1987a) ``A formalism and environment for the development of a large grammar of English.'' In Proceedings of the 10th International Joint Conference on Artificial Intelligence. 703-708.\\n\\nBriscoe, E., C. Grover, B. Boguraev  J. Carroll (1987b) ``Feature defaults, propagation and reentrancy.'' In Categories, Polymorphism and Unification, edited by E. Klein  J. van Benthem, Centre for Cognitive Science, Edinburgh University, UK. 19-34.\\n\\nBriscoe, E.  J. Carroll (1993) ``Generalised probabilistic LR parsing of natural language (corpora) with unification-based grammars.'' Computational Linguistics, 19(1): 25-59.\\n\\nCarroll, J. (1993) Practical unification-based parsing of natural language. Computer Laboratory, Cambridge University, UK, Technical Report 314.\\n\\nCarroll, J.  E. Briscoe (1992) ``Probabilistic normalisation and unpacking of packed parse forests for unification-based grammars.'' In Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language. 33-38.\\n\\nEarley, J. (1970) ``An efficient context-free parsing algorithm.'' Communications of the ACM, 13.2: 94-102.\\n\\nGazdar, G., E. Klein, G. Pullum  I. Sag (1985) Generalized phrase structure grammar. Oxford, UK: Blackwell.\\n\\nGrover, C., J. Carroll  E. Briscoe (1993) The Alvey natural language tools grammar (4th release). Computer Laboratory, Cambridge University, UK, Technical Report 284.\\n\\nJohnson, M. (1989) ``The computational complexity of Tomita's algorithm.'' In Proceedings of the 1st International Workshop on Parsing Technologies. 203-208.\\n\\nKaplan, R. (1987) ``Three seductions of computational psycholinguistics.'' In Linguistic Theory and Computer Applications, edited by P. Whitelock et al., New York: Academic Press. 149-188.\\n\\nKasami, J. (1965) An efficient recognition and syntax analysis algorithm for context-free languages. Air Force Cambridge Research Laboratory, Bedford, MA, Report AFCRL-65-758.\\n\\nKipps, J. (1989) ``Analysis of Tomita's algorithm for general context-free parsing.'' In Proceedings of the 1st International Workshop on Parsing Technologies. 193-202.\\n\\nLang, B. (1974) ``Deterministic techniques for efficient non-deterministic parsers.'' In Automata, Languages and Programming, Lecture Notes in Computer Science 14, edited by J. Loeckx, Berlin, Germany: Springer-Verlag. 255-269.\\n\\nMaxwell, J. III  R. Kaplan (1993) ``The interface between phrasal and functional constraints.'' Computational Linguistics, 19(4): 571-590.\\n\\nMoore, R.  H. Alshawi (1992) ``Syntactic and semantic processing.'' In The Core Language Engine, edited by H. Alshawi, Cambridge, MA: MIT Press. 129-148.\\n\\nMoore, R.  J. Dowding (1991) ``Efficient bottom-up parsing.'' In Proceedings of the DARPA Speech and Natural Language Workshop. 200-203.\\n\\nPereira, F.  D. Warren (1980) ``Definite clause grammars for language analysis--a survey of the formalism and a comparison with augmented transition networks.'' Artificial Intelligence, 13(3): 231-278.\\n\\nPollard, C.  I. Sag (1987) Information-based syntax and semantics: volume 1-fundamentals. Chicago, IL: University of Chicago Press.\\n\\nPratt, V. (1975) ``LINGOL - a progress report.'' In Proceedings of the 5th International Joint Conference on Artificial Intelligence. 422-428.\\n\\nSamuelsson, C.  M. Rayner (1991) ``Quantitative evaluation of explanation-based learning as an optimization tool for a large-scale natural language system.'' In Proceedings of the 12th International Joint Conference on Artificial Intelligence. 609-615.\\n\\nSchabes, Y. (1991) ``Polynomial time and space shift-reduce parsing of arbitrary context-free grammars.'' In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics. 106-113.\\n\\nTaylor, L., C. Grover  E. Briscoe (1989) ``The syntactic regularity of English noun phrases.'' In Proceedings of the 4th European Meeting of the Association for Computational Linguistics. 256-263.\\n\\nTomabechi, H. (1991) ``Quasi-destructive graph unification.'' In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics. 315-322.\\n\\nTomita, M. (1987) ``An efficient augmented-context-free parsing algorithm.'' Computational Linguistics, 13(1): 31-46.\\n\\nShann, P. (1989) ``The selection of a parsing strategy for an on-line machine translation system in a sublanguage domain. A new practical comparison.'' In Proceedings of the 1st International Workshop on Parsing Technologies. 264-276.\\n\\nWright, J., E. Wrigley  R. Sharman (1991) ``Adaptive probabilistic generalized LR parsing.'' In Proceedings of the 2nd International Workshop on Parsing Technologies. 154-163.\\n\\nYounger, D. (1967) ``Recognition and parsing of context-free languages in time n[3].'' Information and Control, 10(2): 189-208.\\n\\nFootnotes\\n\\nThis research was supported by SERC/DTI project 4/1/1261 `Extensions to the Alvey Natural Language Tools' and by EC ESPRIT BRA-7315 `ACQUILEX-II'. I am grateful to Ted Briscoe for comments on an earlier version of this paper, to David Weir for valuable discussions, and to Hiyan Alshawi for assistance with the CLARE system. For example, Taylor et al. demonstrate that the ANLT grammar is in principle able to analyse 96.8% of a corpus of 10,000 noun phrases taken from a variety of corpora. Schabes describes a table with no lookahead; the successful application of this technique supports Schabes' (1991:109) assertion that ``several other methods (such as LR(k)-like and SLR(k)-like) can also be used for constructing the parsing tables [...]'' Barton, Berwick  Ristad (1987:221) calculate that GPSG, also with a maximum nesting depth of two, licences more than 10[775] distinct syntactic categories. The number of categories is actually infinite in grammars that use a fully recursive feature system. This complexity measure does correspond to real world usage of a parser, since practical systems can usually afford to extract only a small number of parses from the frequently very large number encoded in a forest; this is often done on the basis of preference-based or probabilistic factors (e.g. Carroll  Briscoe, 1992).\\n\\nAlthough the ANLT parser is implemented in Common Lisp and the CLE parser in Prolog, comparing parse times is a valid exercise since current compiler and run-time support technologies for both languages are quite well-developed, and in fact the CLE parser takes advantage of Prolog's built-in unification operation which will have been very tightly coded. The ANLT's speed advantage over CLARE is less pronounced if the time for morphological analysis and creation of logical forms is taken into account, probably because the systems use different processing techniques in these modules.\", metadata={'source': '../data/raw/cmplg-xml/9405033.xml'}),\n",
       " Document(page_content=\"Efficient Algorithms for Parsing the DOP Model Introduction\\n\\nParsing using the DOP model is especially difficult. The model can be summarized as a special kind of Stochastic Tree Substitution Grammar (STSG): given a bracketed, labelled training corpus, let every subtree of that corpus be an elementary tree, with a probability proportional to the number of occurrences of that subtree in the training corpus. Unfortunately, the number of trees is in general exponential in the size of the training corpus trees, producing an unwieldy grammar.\\n\\nThis paper contains the first published replication of the full DOP model, i.e. using a parser which sums over derivations. It also contains algorithms implementing the model with significantly fewer resources than previously needed. Furthermore, for the first time, the DOP model is compared on the same data to a competing model.\\n\\nPrevious Research\\n\\nIn theory, the DOP model has several advantages over other models. Unlike a PCFG, the use of trees allows capturing large contexts, making the model more sensitive. Since every subtree is included, even trivial ones corresponding to rules in a PCFG, novel sentences with unseen contexts can still be parsed.\\n\\nUnfortunately, the number of subtrees is huge; therefore Bod randomly samples 5% of the subtrees, throwing away the rest. This significantly speeds up  parsing.\\n\\nReduction of DOP to PCFG\\n\\nUnfortunately, Bod's reduction to a STSG is extremely expensive, even when throwing away 95% of the grammar. Fortunately, it is possible to find an equivalent PCFG that contains exactly eight PCFG rules for each node in the training data; thus it is O(n). Because this reduction is so much smaller, we do not discard any of the grammar when using it. The PCFG is equivalent in two senses: first it generates the same strings with the same probabilities; second, using an isomorphism defined below, it generates the same trees with the same probabilities, although one must sum over several PCFG trees for each STSG tree.\\n\\nTo show this reduction and equivalence, we must first define some terminology. We assign every node in every tree a unique number, which we will call its address. Let A@k denote the node at address k, where A is the non-terminal labeling that node. We will need to create one new non-terminal for each node in the training data. We will call this non-terminal Ak. We will call non-terminals of this form ``interior'' non-terminals, and the original non-terminals in the parse trees ``exterior''.\\n\\nWe will give a simple small PCFG with the following surprising property: for every subtree in the training corpus headed by A, the grammar will generate an isomorphic subderivation with probability 1/a. In other words, rather than using the large, explicit STSG, we can use this small PCFG that generates isomorphic derivations, with identical probabilities.\\n\\nThe construction is as follows. For a node such as B@k AA C@l AA 2A@j we will generate the following eight PCFG rules, where the number in parentheses following a rule is its probability.\\n\\nWe will show that subderivations headed by A with external non-terminals at the roots and leaves, internal non-terminals elsewhere have probability 1/a. Subderivations headed by Aj with external non-terminals only at the leaves, internal non-terminals elsewhere, have probability 1/aj. The proof is by induction on the depth of the trees.\\n\\nFor trees of depth 1, there are two cases: B AA C AA 2A B AA C AA 2A@j Trivially, these trees have the required probabilities.\\n\\nWe call a PCFG tree isomorphic to a STSG tree if they are identical when internal non-terminals are changed to external non-terminals. Our main theorem is that this construction produces PCFG trees isomorphic to the STSG trees with equal probability. If every subtree in the training corpus occurred exactly once, this would be trivial to prove. For every STSG subderivation, there would be an isomorphic PCFG subderivation, with equal probability. Thus for every STSG derivation, there would be an isomorphic PCFG derivation, with equal probability. Thus every STSG tree would be produced by the PCFG with equal probability.\\n\\nHowever, it is extremely likely that some subtrees, especially trivial ones like NP AA VP AA 2S will occur repeatedly.\\n\\nIf the STSG formalism were modified slightly, so that trees could occur multiple times, then our relationship could be made one to one. Consider a modified form of the DOP model, in which when subtrees occurred multiple times in the training corpus, their counts were not merged: both identical trees are added to the grammar. Each of these trees will have a lower probability than if their counts were merged. This would change the probabilities of the derivations; however the probabilities of parse trees would not change, since there would be correspondingly more derivations for each tree. Now, the desired one to one relationship holds: for every derivation in the new STSG there is an isomorphic derivation in the PCFG with equal probability. Thus, summing over all derivations of a tree in the STSG yields the same probability as summing over all the isomorphic derivations in the PCFG. Thus, every STSG tree would be produced by the PCFG with equal probability.\\n\\nIt follows trivially from this that no extra trees are produced by the PCFG. Since the total probability of the trees produced by the STSG is 1, and the PCFG produces these trees with the same probability, no probability is ``left over'' for any other trees.\\n\\nParsing Algorithm\\n\\nFor a grammar with g nonterminals and training data of size T, the run time of the algorithm is\\n\\nO(Tn[2] + gn[3] + n[3]) since there are two layers of outer loops, each with run time at most n, and inner loops, over addresses (training data), nonterminals and n.  However, this is dominated by the computation of the Inside and Outside probabilities, which takes time O(rn[3]), for a grammar with rrules. Since there are eight rules for every node in the training data, this is O(Tn[3]).\\n\\nBy modifying the algorithm slightly to record the actual split used at each node, we can recover the best parse. The entry maxc[1, n] contains the expected number of correct constituents, given the model.\\n\\nExperimental Results and Discussion\\n\\nDOP does do slightly better on most measures. We performed a statistical analysis using a t-test on the paired differences between DOP and Pereira and Schabes performance on each run. On the minimally edited ATIS data, the differences were statistically insignificant, while on Bod's data the differences were statistically significant beyond the 98'th percentile. Our technique for finding statistical significance is more strenuous than most: we assume that since all test sentences were parsed with the same training data, all results of a single run are correlated. Thus we compare paired differences of entire runs, rather than of sentences or constituents. This makes it harder to achieve statistical significance.\\n\\nNotice also the minimum and maximum columns of the ``DOP-PS'' lines, constructed by finding for each of the paired runs the difference between the DOP and the Pereira and Schabes algorithms. Notice that the minimum is usually negative, and the maximum is usually positive, meaning that on some tests DOP did worse than Pereira and Schabes and on some it did better. It is important to run multiple tests, especially with small test sets like these, in order to avoid misleading results.\\n\\nTiming Analysis\\n\\nIn this section, we examine the empirical runtime of our algorithm, and analyze Bod's. We also note that Bod's algorithm will probably be particularly inefficient on longer sentences.\\n\\nNow, we note that the conditional probability of the most probable parse tree will in general decline exponentially with sentence length. We assume that the number of ambiguities in a sentence will increase linearly with sentence length; if a five word sentence has on average one ambiguity, then a ten word sentence will have two, etc. A linear increase in ambiguity will lead to an exponential decrease in probability of the most probable parse.\\n\\nSince the probability of the most probable parse decreases exponentially in sentence length, the number of random samples needed to find this most probable parse increases exponentially in sentence length. Thus, when using the Monte Carlo algorithm, one is left with the uncomfortable choice of exponentially decreasing the probability of finding the most probable parse, or exponentially increasing the runtime.\\n\\nWe admit that this is a somewhat informal argument. Still, the Monte Carlo algorithm has never been tested on sentences longer than those in the ATIS corpus; there is good reason to believe the algorithm will not work as well on longer sentences. Note that our algorithm has true runtime O(Tn[3]), as shown previously.\\n\\nAnalysis of Bod's Data\\n\\nIn the DOP model, a sentence cannot be given an exactly correct parse unless all productions in the correct parse occur in the training set. Thus, we can get an upper bound on performance by examining the test corpus and finding which parse trees could not be generated using only productions in the training corpus. Unfortunately, while Bod provided us with his data, he did not specify which sentences were test and which were training. We can however find an upper bound on average case performance, as well as an upper bound on the probability that any particular level of performance could be achieved.\\n\\nThe table is arranged from least generous to most generous: in the upper left hand corner is a technique Bod might reasonably have used; in that case, the probability of getting the test set he described is less than one in a million. In the lower right corner we give Bod the absolute maximum benefit of the doubt: we assume he used a parser capable of parsing unary branching productions, that he used a very overgenerating grammar, and that he used a loose definition of ``Exact Match.'' Even in this case, there is only about a 1.5% chance of getting the test set Bod describes.\\n\\nConclusion\\n\\nWe have given efficient techniques for parsing the DOP model. These results are significant since the DOP model has perhaps the best reported parsing accuracy; previously the full DOP model had not been replicated due to the difficulty and computational complexity of the existing algorithms. We have also shown that previous results were partially due to an unlikely choice of test data, and partially due to the heavy cleaning of the data, which reduced the difficulty of the task.\\n\\nOf course, this research raises as many questions as it answers. Were previous results due only to the choice of test data, or are the differences in implementation partly responsible? In that case, there is significant future work required to understand which differences account for Bod's exceptional performance. This will be complicated by the fact that sufficient details of Bod's implementation are not available.\\n\\nThis research also shows the importance of testing on more than one small test set, as well as the importance of not making cross-corpus comparisons; if a new corpus is required, then previous algorithms should be duplicated for comparison.\\n\\nBibliography\\n\\nJ.K. Baker. 1979. Trainable grammars for speech recognition. In Proceedings of the Spring Conference of the Acoustical Society of America, pages 547-550, Boston, MA, June.\\n\\nRens Bod. 1992. Mathematical properties of the data oriented parsing model. Paper presented at the Third Meeting on Mathematics of Language (MOL3), Austin Texas.\\n\\nRens Bod. 1993a. Data-oriented parsing as a general framework for stochastic language processing. In K. Sikkel and A. Nijholt, editors, Parsing Natural Language. Twente, The Netherlands.\\n\\nRens Bod. 1993b. Monte Carlo parsing. In Proceedings Third International Workshop on Parsing Technologies, Tilburg/Durbury.\\n\\nRens Bod. 1993c. Using an annotated corpus as a stochastic grammar. In Proceedings of the Sixth Conference of the European Chapter of the ACL, pages 37-44.\\n\\nRens Bod. 1995a. Enriching Linguistics with Statistics: Performance Models of Natural Language. University of Amsterdam ILLC Dissertation Series 1995-14. Academische Pers, Amsterdam.\\n\\nRens Bod. 1995b. The problem of computing the most probable tree in data-oriented parsing and stochastic tree grammars. In Proceedings of the Seventh Conference of the European Chapter of the ACL.\\n\\nEric Brill. 1993. A Corpus-Based Approach to Language Learning. Ph.D. thesis, University of Pennsylvania.\\n\\nJoshua Goodman. 1996. Parsing algorithms and metrics. In Proceedings of the 34th Annual Meeting of the ACL. To appear.\\n\\nCharles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990. The ATIS spoken language systems pilot corpus. In DARPA Speech and Natural Language Workshop, Hidden Valley, Pennsylvania, June. Morgan Kaufmann.\\n\\nK. Lari and S.J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35-56.\\n\\nDavid Magerman. 1994. Natural Language Parsing as Statistical Pattern Recognition. Ph.D. thesis, Stanford University University, February.\\n\\nFernando Pereira and Yves Schabes. 1992. Inside-Outside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the ACL, pages 128-135, Newark, Delaware.\\n\\nL.R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), February.\\n\\nR. Scha. 1990. Language theory and language technology; competence and performance. In Q.A.M. de Kort and G.L.J. Leerdam, editors,   Computertoepassingen in de Neerlandistiek. Landelijke Vereniging van Neerlandici (LVVN-jaarboek), Almere. In Dutch.\\n\\nYves Schabes, Michal Roth, and Randy Osborne. 1993. Parsing the Wall Street Journal with the Inside-Outside algorithm. In Proceedings of the Sixth Conference of the European Chapter of the ACL, pages 341-347.\\n\\nY. Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Proceedings of the 14th International Conference on Computational Linguistics.\\n\\nKhalil Sima'an. 1996. Efficient disambiguation by means of stochastic tree substitution grammars. In R. Mitkov and N. Nicolov, editors, Recent Advances in NLP 1995, volume 136 of Current Issues in Linguistic Theory. John Benjamins, Amsterdam.\\n\\nAndreas Stolcke. 1993. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Technical Report TR-93-065, International Computer Science Institute, Berkeley, CA.\\n\\nFootnotes\\n\\nA diff file between the original ATIS data and the cleaned up version, in a form usable by the ``ed'' program, is available by anonymous FTP from ftp://ftp.das.harvard.edu/pub/goodman/atis-ed/ ti_tb.par-ed and ti_tb.pos-ed. Note that the number of changes made was small. The diff files sum to 457 bytes, versus 269,339 bytes for the original files, or less than 0.2%. Ideally, we would exactly reproduce these experiments using Bod's algorithm. Unfortunately, it was not possible to get a full specification of the algorithm. A perl script for analyzing Bod's data is available by anonymous FTP from ftp://ftp.das.harvard.edu/pub/goodman/analyze.perl Actually, this is a slight overestimate for a few reasons, including the fact that the 75 sentences are drawn without replacement. Also, consider a sentence with a production that occurs only in one other sentence in the corpus; there is some probability that both sentences will end up in the test data, causing both to be ungeneratable.\", metadata={'source': '../data/raw/cmplg-xml/9604008.xml'}),\n",
       " Document(page_content=\"EVALUATING DISCOURSE PROCESSING ALGORITHMS\\n\\nIn order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study. We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues. We present the quantitative results of hand-simulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general. We illustrate the general difficulties encountered with quantitative evaluation. These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining.\\n\\nIntroduction\\n\\nIn the course of developing natural language interfaces, computational linguists are often in the position of evaluating different theoretical approaches to the analysis of natural language (NL). They might want to (a) evaluate and improve on a current system, (b) add a capability to a system that it didn't previously have, (c) combine modules from different systems.\\n\\nAlthough both algorithms are part of theories of discourse that posit the interaction of the algorithm with an inference or intentional component, we will not use reasoning in tandem with the algorithm's operation. We have made this choice because we want to be able to analyse the performance of the algorithms across different domains. We focus on the linguistic basis of these approaches, using only selectional restrictions, so that our analysis is independent of the vagaries of a particular knowledge representation. Thus what we are evaluating is the extent to which these algorithms suffice to narrow the search of an inference component. This analysis gives us some indication of the contribution of syntactic constraints, task structure and global focus to anaphoric processing.\\n\\nQuantitative Evaluation\\n\\n\\n\\nBlack Box\\n\\nThe Algorithms\\n\\nWhen embarking on such a comparison, it would be convenient to assume that the inputs to the algorithms are identical and compare their outputs. Unfortunately since researchers do not even agree on which phenomena can be explained syntactically and which semantically, the boundaries between two modules are rarely the same in NL systems. In this case the BFP centering algorithm and Hobbs algorithm both make  ASSUMPTIONS about other system components. These are, in some sense, a further specification of the operation of the algorithms that must be made in order to hand-simulate the algorithms. There are two major sets of assumptions, based on discourse segmentation and syntactic representation. We attempt to make these explicit for each algorithm and pinpoint where the algorithms might behave differently were these assumptions not well-founded.\\n\\nCentering algorithm\\n\\nIn the centering framework, the order of the forward-centers list is intended to reflect the salience of discourse entities. The BFP algorithm orders this list by grammatical relation of the complements of the main verb, i.e. first the subject, then object, then indirect object, then other subcategorized-for complements, then noun phrases found in adjunct clauses. This captures the intuition that subjects are more salient than other discourse entities.\\n\\nIn published texts, a paragraph is a new segment unless the first sentence has a pronoun in subject position or a pronoun where none of the preceding sentence-internal noun phrases match its syntactic features.\\n\\nIn the task-oriented dialogues, the action  PICK-UP marks task boundaries hence segment boundaries. Cue words like next, then, and now also mark segment boundaries. These will usually co-occur but either one is sufficient for marking a segment boundary.\\n\\nBFP never state that cospecifiers for pronouns within the same segment are preferred over those in previous segments, but this is an implicit assumption, since this line of research is derived from Sidner's work on local focusing. Segment initial utterances therefore are the only situation where the BFP algorithm will prefer a within-sentence noun phrase as the cospecifier of a pronoun.\\n\\nHobbs' algorithm\\n\\nThe order by which Hobbs' algorithm traverses the parse tree is the closest thing in his framework to predictions about which discourse entities are salient. In the main it prefers co-specifiers for pronouns that are within the same sentence, and also ones that are closer to the pronoun in the sentence. This amounts to a claim that different discourse entities are salient, depending on the position of a pronoun in a sentence. When seeking an intersentential co-specification, Hobbs algorithm searches the parse tree of the previous utterance breadth-first, from left to right. This predicts that entities realized in subject position are more salient, since even if an adjunct clause linearly precedes the main subject, any noun phrases within it will be deeper in the parse tree. This also means that objects and indirect objects will be among the first possible antecedents found, and in general that the depth of syntactic embedding is an important determiner of discourse prominence.\\n\\nSummary\\n\\nThe effects of some of the assumptions are measurable and we will attempt to specify exactly what these effects are, however some are not, e.g. we cannot measure the effect of Hobbs' syntax assumption since it is difficult to say how likely one is to get the wrong parse. We adopt the set collection assumption for both algorithms as well as the ability to recover the identity of speakers and hearers in dialogue.\\n\\nQuantitative Results of the Algorithms\\n\\nThe texts on which the algorithms are analysed are the first chapter of Arthur Hailey's novel Wheels, and the July 7, 1975 edition of Newsweek. The sentences in Wheels are short and simple with long sequences consisting of reported conversation, so it is similar to a conversational text. The articles from Newsweek are typical of journalistic writing. For each text, the first 100 occurrences of singular and plural third-person pronouns were used to test the performance of the algorithms. The task-dialogues contain a total of 81 uses of it and no other pronouns except for I and you. In the figures below note that possessives like his are counted along with he and that accusatives like him and her are counted as he and she.\\n\\nWe might wonder with what confidence we should view these numbers. A significant factor that must be considered is the contribution of  FALSE POSITIVES and  ERROR CHAINING. A FALSE POSITIVE is when an algorithm gets the right answer for the wrong reason. A very simple example of this phenomena is illustrated by this sequence from one of the task dialogues.\\n\\nAnother type of false positive example is ``Everybody and HIS brother suddenly wants to be the President's friend,'' said one aide. Hobbs gets this correct as long as one is willing to accept that Everybody is really the antecedent of his. It seems to me that this might be an idiomatic use.\\n\\nERROR CHAINING refers to the fact that once an algorithm makes an error, other errors can result. Consider:\\n\\nIt isn't possible to measure the effect of false positives, since in some sense they are subjective judgements. However one can and should measure the effects of error chaining, since reporting numbers that correct for error chaining is misleading, but if the error that produced the error chain can be corrected then the algorithm might show a significant improvement. In this analysis, error chains contributed 22 failures to Hobbs' algorithm and 19 failures to BFP.\\n\\nQualitative Evaluation\\n\\n\\n\\nGlass Box\\n\\nDistributions\\n\\nSince the main purpose of evaluation must be to improve the theory that we are evaluating, the most interesting cases are the ones on which the algorithms' performance varies and those that neither algorithm gets correct. We discuss these below.\\n\\nBoth\\n\\nIn the Wheels data, 4 examples rest on the assumption that the identities of speakers and hearers is recoverable. For example in The GM president smiled. ``Except Henry will be damned forceful and the papers won't print all HIS language. '', getting the his correct here depends on knowing that it is the GM president speaking. Only 4 examples rest on being able to produce collections or discourse entities, and 2 of these occurred with an explicit instruction to the hearer to produce such a collection by using the phrase them both.\\n\\nHobbs only\\n\\nThere are 21 cases that Hobbs gets that BFP don't, and of these these a few classes stand out. In every case the relevant factor is Hobbs' preference for intrasentential co-specifiers.\\n\\nAnother class, (n=7), are possessives. In some cases the possessive co-specified with the subject of the sentence, e.g. The SENATE took time from ITS paralyzing New Hampshire election debate to vote agreement, and in others it was within a relative clause and co-specified with the subject of that clause, e.g. The auto industry should be able to produce a totally safe, defect-free CAR that doesn't pollute ITS environment.\\n\\nOther cases seem to be syntactically marked subject matching with constructions that link two S clauses (n=8). These are uses of more-than in e.g. but Chamberlain grossed about $8.3 million more than HE could have made by selling on the home front. There also are S-if-S cases, as in Mondale said: ``I think THE MAFIA would be broke if IT conducted all its business that way.'' We also have subject matching in AS-AS examples as in ... and the resulting EXPOSURE to daylight has become as uncomfortable as IT was unaccustomed, as well as in sentential complements, such as But another liberal, Minnesota's Walter MONDALE, said HE had found a lot of incompetence in the agency's operations. The fact that quite a few of these are also marked with But may be significant.\\n\\nThe Hobbs algorithm will correctly choose the end as the antecedent for the second it. The BFP algorithm on the other hand will get two interpretations, one in which the second it co-specifies the red piece and one in which it co-specifies the end. They are both  CONTINUING interpretations since the first it co-specifies the  CB, but the constraints don't make a choice.\\n\\nBFP only\\n\\nAll of the examples on which BFP succeed and Hobbs fails have to do with extended discussion of one discourse entity. For instance:\\n\\nAnother example from the novel WHEELS is given below. On this one Hobbs gets the first use of he but then misses the next four, as a result of missing the second one by choosing a housekeeper as the co-specifier for HIS. ..An executive vice-president of Ford was preparing to leave for Detroit Metropolitan Airport. HE had already breakfasted, alone. A housekeeper had brought a tray to HIS desk in the softly lighted study where, since 5 a.m., HE had been alternately reading memoranda (mostly on special blue stationery which Ford vice-presidents used in implementing policy) and dictating crisp instructions into a recording machine. HE had scarcely looked up, either as the mail arrived, or while eating, as HE accomplished in an hour what would have taken...\\n\\nSince an executive vice-president is centered in the first sentence, and continued in each following sentence, the BFP algorithm will correctly choose the cospecifier.\\n\\nNeither\\n\\nAmong the examples that neither algorithm gets correctly are 20 examples from the task dialogues of it referring to the global focus, the pump. In 15 cases, these shifts to global focus are marked syntactically with a cue word such as Now, and are not marked in 5 cases. Presumably they are felicitous since the pump is visually salient. Besides the global focus cases, pronominal references to entities that were not linguistically introduced are rare. The only other example is an implicit reference to `the problem' of the pump not working:\\n\\nWe have only two examples of sentential or VP anaphora altogether, such as Madam Chairwoman, said Colby at last, I am trying to run a secret intelligence service. IT was a forlorn hope. Neither Hobbs algorithm nor BFP attempt to cover these examples.\\n\\nMost of the interchanges in the task dialogues consist of the client responding to commands with cues such as O.K. or Ready to let the expert know when they have completed a task. When both parties contribute discourse entities to the common ground, both algorithms may fail (n=4).\\n\\nConsider:\\n\\nModifiability\\n\\nTask structure in the pump dialogues is an important factor especially as it relates to the use of global focus. Twenty of the cases on which both algorithms fail are references to the pump, which is the global focus. We can include a global focus in the centering framework, as a separate notion from the current  CB. This means that in the 15 out of 20 cases where the shift to global focus is identifiably marked with a cue-word such as now, the segment rules will allow BFP to get the global focus examples.\\n\\nConclusion\\n\\nWe can benefit in two ways from performing such evaluations: (a) we get general results on a methodology for doing evaluation, (b) we discover ways we can improve current theories. A split of evaluation efforts into quantitative versus qualitative is incoherent. We cannot trust the results of a quantitative evaluation without doing a considerable amount of qualitative analyses and we should perform our qualitative analyses on those components that make a significant contribution to the quantitative results; we need to be able to measure the effect of various factors. These measurements must be made by doing comparisons at the data level.\\n\\nIn terms of general results, we have identified some factors that make evaluations of this type more complicated and which might lead us to evaluate solely quantitative results with care. These are: (a) To decide how to evaluate  UNDERSPECIFICATIONS and the contribution of ASSUMPTIONS, and (b) To determine the effects of  FALSE POSITIVES and ERROR CHAINING. We advocate an approach in which the contribution of each underspecification and assumption is tabulated as well as the effect of error chains. If a principled way could be found to identify false positives, their effect should be reported as well as part of any quantitative evaluation.\\n\\nIn addition, we have taken a few steps towards determining the relative importance of different factors to the successful operation of discourse modules. The percent of successes that both algorithms get indicates that syntax has a strong influence, and that at the very least we can reduce the amount of inference required. In 59% to 82% of the cases both algorithms get the correct result. This probably means that in a large number of cases there was no potential conflict of co-specifiers. In addition, this analysis has shown, that at least for task-oriented dialogues global focus is a significant factor, and in general discourse structure is more important in the task dialogues. However simple devices such as cue words may go a long way toward determining this structure.\\n\\nFinally, we should note that doing evaluations such as this allows us to determine the GENERALITY of our approaches. Since the performance of both Hobbs and BFP varies according to the type of the text, and in fact was significantly worse on the task dialogues than on the texts, we might question how their performance would vary on other inputs. An annotated corpus comprising some of the various NL input types such as those I discussed in the introduction would go a long way towards giving us a basis against which we could evaluate the generality of our theories.\\n\\nAcknowledgements\\n\\nDavid Carter, Phil Cohen, Nick Haddock, Jerry Hobbs, Aravind Joshi, Don Knuth, Candy Sidner, Phil Stenton, Bonnie Webber, and Steve Whittaker have provided valuable insights toward this endeavor and critical comments on a multiplicity of earlier versions of this paper. Steve Whittaker advised me on the statistical analyses. I would like to thank Jerry Hobbs for encouraging me to do this in the first place.\\n\\nBibliography\\n\\nJames F. Allen and C. Raymond Perrault. Analyzing intention in utterances. Artificial Intelligence, 15:143-178, 1980.\\n\\nAnne Abeille and Yves Schabes. Parsing idioms in lexicalized tags. In Proc. 4th Conference of the European Chapter of the ACL, Association of Computational Linguistics, pages 161-65, 1989.\\n\\nRoger Brown and Deborah Fish. The psychological causality implicit in language. Cognition, 14:237-273, 1983.\\n\\nSusan E. Brennan, Marilyn Walker Friedman, and Carl J. Pollard. A centering approach to pronouns. In Proc. 25th Annual Meeting of the ACL, Stanford, pages 155-162, 1987.\\n\\nDavid M. Carter. Interpreting Anaphors in Natural Language Texts. Ellis Horwood, 1987.\\n\\nPhillip R. Cohen. On knowing what to say: Planning speech acts. Technical Report 118, University of Toronto; Department of Computer Science, 1978.\\n\\nPhillip R. Cohen. The pragmatics of referring and the modality of communication. Computational Linguistics, 10:97-146, 1984.\\n\\nBarbara Grosz Deutsch. Typescripts of task oriented dialogs, August 1974.\\n\\nNils Dahlback and Arne Jonsson. Empirical studies of discourse representations for natural language interfaces. In Proc. 4th Conference of the European Chapter of the ACL, Association of Computational Linguistics, pages 291-298, 1989.\\n\\nBarbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. Providing a unified account of definite noun phrases in discourse. In Proc. 21st Annual Meeting of the ACL, Association of Computational Linguistics, pages 44-50, 1983.\\n\\nBarbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. Towards a computational theory of discourse interpretation. Unpublished Manuscript, 1986.\\n\\nBarbara J. Grosz. The representation and use of focus in dialogue understanding. Technical Report 151, SRI International, 333 Ravenswood Ave, Menlo Park, Ca. 94025, 1977.\\n\\nBarbara J. Grosz and Candace L. Sidner. Attentions, intentions and the structure of discourse. Computational Linguistics, 12:175-204, 1986.\\n\\nRaymonde Guindon, P. Sladky, H. Brunner, and J. Conner. The structure of user-adviser dialogues: Is there method in their madness? In Proc. 24th Annual Meeting of the ACL, Association of Computational Linguistics, pages 224-230, 1986.\\n\\nJulia Hirschberg and Diane Litman. Now lets talk about now: Identifying cue phrases intonationally. In Proc. 25th Annual Meeting of the ACL, Stanford, pages 163-171, Stanford University, Stanford, Ca., 1987.\\n\\nJerry R. Hobbs and Paul Martin. Local pragmatics. Technical report, SRI International, 333 Ravenswood Ave., Menlo Park, Ca 94025, 1987.\\n\\nJerry R. Hobbs. A computational approach to discourse analysis. Technical Report 76-2, Department of Computer Science, City College, City University of New York, 1976.\\n\\nJerry R. Hobbs. Pronoun resolution. Technical Report 76-1, Department of Computer Science, City College, City University of New York, 1976.\\n\\nJerry R. Hobbs. Why is discourse coherent? Technical Report 176, SRI International, 333 Ravenswood Ave., Menlo Park, Ca 94025, 1978.\\n\\nJerry R. Hobbs. On the coherence and structure of discourse. Technical Report CSLI-85-37, Center for the Study of Language and Information, Ventura Hall, Stanford University, Stanford, CA 94305, 1985.\\n\\nSusan B. Hudson, Michael K. Tanenhaus, and Gary S. Dell. The effect of the Discourse Center on the local coherence of a discourse. Technical Report.University of Rochester, 1986.\\n\\nJanet Pierrehumbert and Julia Hirschberg. The meaning of intonational contours in the interpretation of discourse. In Cohen, Morgan and Pollack, eds. Intentions in Communication, MIT Press, 1990.\\n\\nMartha Pollack. A model of plan inference that distinguishes between the beliefs of actors and observers. In Proc. 24th Annual Meeting of the Association of Computational Linguistics, pages 207-214, Columbia University, N.Y., N.Y, 1986.\\n\\nEllen F. Prince. Toward a taxonomy of given-new information. In Radical Pragmatics, pages 223-255. Academic Press, 1981.\\n\\nEllen F. Prince. Fancy syntax and shared knowledge. Journal of Pragmatics, pages 65-81, 1985.\\n\\nTanya Reinhart. The Syntactic Domain of Anaphora. PhD thesis, MIT, Cambridge Mass., 1976.\\n\\nRachel Reichman. Getting Computers to Talk Like You and Me. MIT Press, Cambridge, MA, 1985.\\n\\nCraige Roberts. Modal subordination and pronominal anaphora in discourse. Technical Report No. 127, CSLI, May,1988. Also appeared in Linguistics and Philosophy.\\n\\nDeborah Schiffrin.\\n\\nDiscourse Markers.\\n\\nCambridge University Press, 1987.\\n\\nCandace Sidner and David Israel. Recognizing intended meaning and speakers plans. In Proc. International Joint Conference on Artificial Intelligence, pages 203-208, Vancouver, BC, Canada, 1981.\\n\\nCandace L. Sidner. Toward a computational theory of definite anaphora comprehension in English. Technical Report AI-TR-537, MIT, 1979.\\n\\nBozena Henisz Thompson. Linguistic analysis of natural language communication with computers. In COLING80: Proc. 8th International Conference on Computational Linguistics. Tokyo, pages 190-201, 1980.\\n\\nBonnie Lynn Webber. Two steps closer to event reference. Technical Report MS-CIS-86-74, Linc Lab 42, Department of Computer and Information Science, University of Pennsylvania, 1986.\\n\\nSteve Whittaker and Phil Stenton. Cues and control in expert client dialogues. In Proc. 26th Annual Meeting of the ACL, Association of Computational Linguistics, pages 123-130, 1988.\\n\\nSteve Whittaker and Phil Stenton. User studies and the design of natural language systems. In Proc. 4th Conference of the European Chapter of the ACL, Association of Computational Linguistics, pages 116-123, 1989.\\n\\nThe Hobbs algorithm\\n\\nThe algorithm and an example is reproduced below. In it, NP denotes NOUN PHRASE and S denotes  SENTENCE.\\n\\nThe purpose of steps 2 and 3 is to observe the contra-indexing constraints. Let us consider a simple conversational sequence.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9410006.xml'}),\n",
       " Document(page_content=\"Does Baum\\n\\n\\n\\nWelch Re\\n\\n\\n\\nestimation Help Taggers?\\n\\nIn part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text. Early work in the field relied on a corpus which had been tagged by a human annotator to train the model. More recently, Cutting et al. (1992) suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities, by using Baum-Welch re-estimation to automatically refine the model. In this paper, I report two experiments designed to determine how much manual training information is needed. The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy. The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation. In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it. The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus (if any) and the corpus to be tagged. Heuristics for deciding how to use re-estimation in an effective manner are given. The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model. Citation details: appears in Proceedings of the 4th ACL Conference on Applied Natural Language Processing, Stuttgart, October 13-15th 1994, pp. 53-58.\\n\\nBackground\\n\\nPart-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus. One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM). The model is defined by two collections of parameters: the transition probabilities, which express the probability that a tag follows the preceding one (or two for a second order model); and the lexical probabilities, giving the probability that a word has a given tag without regard to words on either side of it. To tag a text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given the sequence of words is determined from the probabilities. Two algorithms are commonly used, known as the Forward-Backward (FB) and Viterbi algorithms. FB assigns a probability to every tag on every word, while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational efficiency. For an introduction to the algorithms, see Cutting et al. (1992), or the lucid description by Sharman (1990). There are two principal sources for the parameters of the model. If a tagged corpus prepared by a human annotator is available, the transition and lexical probabilities can be estimated from the frequencies of pairs of tags and of tags associated with words.\\n\\nAlternatively, a procedure called Baum-Welch (BW) re-estimation may be used, in which an untagged corpus is passed through the FB algorithm with some initial model, and the resulting probabilities used to determine new values for the lexical and transition probabilities. By iterating the algorithm with the same corpus, the parameters of the model can be made to converge on values which are locally optimal for the given text. The degree of convergence can be measured using a perplexity measure, the sum of plog2p for hypothesis probabilities p, which gives an estimate of the degree of disorder in the model. The algorithm is again described by Cutting et al. and by Sharman, and a mathematical justification for it can be found in Huang et al. (1990).\\n\\nThe Xerox tagger attempts to avoid the need for a hand-tagged training corpus as far as possible. Instead, an approximate model is constructed by hand, which is then improved by BW re-estimation on an untagged training corpus. In the above example, 8 iterations were sufficient. The initial model set up so that some transitions and some tags in the lexicon are favoured, and hence having a higher initial probability. Convergence of the model is improved by keeping the number of parameters in the model down. To assist in this, low frequency items in the lexicon are grouped together into equivalence classes, such that all words in a given equivalence class have the same tags and lexical probabilities, and whenever one of the words is looked up, then the data common to all of them is used. Re-estimation on any of the words in a class therefore counts towards re-estimation for all of them.\\n\\nThe aim of this paper is to examine the role that training plays in the tagging process, by an experimental evaluation of how the accuracy of the tagger varies with the initial conditions. The results suggest that a completely unconstrained initial model does not produce good quality results, and that one accurately trained from a hand-tagged corpus will generally do better than using an approach based on re-estimation, even when the training comes from a different source. A second experiment shows that there are different patterns of re-estimation, and that these patterns vary more or less regularly with a broad characterisation of the initial conditions. The outcome of the two experiments together points to heuristics for making effective use of training and re-estimation, together with some directions for further research.\\n\\nWork similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions. We will discuss this work below. The principal contribution of this work is to separate the effect of the lexical and transition parameters of the model, and to show how the results vary with different degree of similarity between the training and test data.\\n\\nThe tagger and corpora\\n\\nThe lexicon lists, for each word, all of tags seen in the training corpus with their probabilities. For words not found in the lexicon, all open-class tags are hypothesised, with equal probabilities. These words are added to the lexicon at the end of first iteration when re-estimation is being used, so that the probabilities of their hypotheses subsequently diverge from being uniform.\\n\\nTo measure the accuracy of the tagger, we compare the chosen tag with one provided by a human annotator. Various methods of quoting accuracy have been used in the literature, the most common being the proportion of words (tokens) receiving the correct tag. A better measure is the proportion of ambiguous words which are given the correct tag, where by ambiguous we mean that more than one tag was hypothesised. The former figure looks more impressive, but the latter gives a better measure of how well the tagger is doing, since it factors out the trivial assignment of tags to non-ambiguous words. For a corpus in which a fraction a of the words are ambiguous, and p is the accuracy on ambiguous words, the overall accuracy can be recovered from 1-a+pa. All of the accuracy figures quoted below are for ambiguous words only.\\n\\nThe training and test corpora were drawn from the LOB corpus and the Penn treebank. The hand tagging of these corpora is quite different. For example, the LOB tagset used 134 tags, while the Penn treebank tagset has 48. The general pattern of the results presented does not vary greatly with the corpus and tagset used.\\n\\nThe effect of the initial conditions\\n\\nSeveral follow-up experiments were used to confirm the results: using corpora from the Penn treebank, using equivalence classes to ensure that all lexical entries have a total relative frequency of at least 0.01, and using larger corpora. The specific accuracies were different in the various tests, but the overall patterns remained much the same, suggesting that they are not an artifact of the tagset or of details of the text.\\n\\nThe observations we can make about these results are as follows. Firstly, two of the tests, D2+T1 and D3+T1, give very poor performance. Their accuracy is not even as good as that achieved by picking the most frequent tag (although this of course implies a lexicon of D0 or D1 quality). It follows that if Baum-Welch re-estimation is to be an effective technique, the initial data must have either biasing in the transitions (the T0 cases) or in the lexical probabilities (cases D0+T1 and D1+T1), but it is not necessary to have both (D2/D3+T0 and D0/D1+T1).\\n\\nSecondly, training from a hand-tagged corpus (case D0+T0) always does best, even when the test data is from a different source to the training data, as it is for LOB-L. So perhaps it is worth investing effort in hand-tagging training corpora after all, rather than just building a lexicon and letting re-estimation sort out the probabilities. But how can we ensure that re-estimation will produce a good quality model? We look further at this issue in the next section.\\n\\nPatterns of re\\n\\n\\n\\nestimation\\n\\nThe tests were conducted in a similar manner to those of the first experiment, by building a lexicon and transitions from a hand tagged training corpus, and then applying them to a test corpus with varying degrees of degradation. Firstly, four different degrees of degradation were used: no degradation at all, D2 degradation of the lexicon, T1 degradation of the transitions, and the two together. Secondly, we selected test corpora with varying degrees of similarity to the training corpus: the same text, text from a similar domain, and text which is significantly different. Two tests were conducted with each combination of the degradation and similarity, using different corpora (from the Penn treebank) ranging in size from approximately 50000 words to 500000 words. The re-estimation was allowed to run for ten iterations.\\n\\nDiscussion If a hand-tagged training corpus is available, use it . If the test and training corpora are near-identical, do not use BW re-estimation; otherwise use for a small number of iterations.\\n\\nIf no such training corpus is available, but a lexicon with at least relative frequency data is available, use BW re-estimation for a small number of iterations.\\n\\nIf neither training corpus nor lexicon are available, use BW re-estimation with standard convergence tests such as perplexity. Without a lexicon, some initial biasing of the transitions is needed if good results are to be obtained.\\n\\nSimilar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions. As in the experiments above, BW re-estimation gave a decrease in accuracy when the starting point was derived from a significant amount of hand-tagged text. In addition, although Merialdo does not highlight the point, BW re-estimation starting from less than 5000 words of hand-tagged text shows early maximum behaviour. Merialdo's conclusion is that taggers should be trained using as much hand-tagged text as possible to begin with, and only then applying BW re-estimation with untagged text. The step forward taken in the work here is to show that there are three patterns of re-estimation behaviour, with differing guidelines for how to use BW effectively, and that to obtain a good starting point when a hand-tagged corpus is not available or is too small, either the lexicon or the transitions must be biased.\\n\\nWhile these may be useful heuristics from a practical point of view, the next step forward is to look for an automatic way of predicting the accuracy of the tagging process given a corpus and a model. Some preliminary experiments with using measures such as perplexity and the average probability of hypotheses show that, while they do give an indication of convergence during re-estimation, neither shows a strong correlation with the accuracy. Perhaps what is needed is a ``similarity measure'' between two models M and M', such that if a corpus were tagged with model M, M' is the model obtained by training from the output corpus from the tagger as if it were a hand-tagged corpus. However, preliminary experiments using such measures as the Kullback-Liebler distance between the initial and new models have again showed that it does not give good predictions of accuracy. In the end it may turn out there is simply no way of making the prediction without a source of information extrinsic to both model and corpus.\\n\\nAcknowledgements\\n\\nThe work described here was carried out at the Cambridge University Computer Laboratory as part of Esprit BR Project 7315 ``The Acquisition of Lexical Knowledge'' (Acquilex-II). The results were confirmed and extended at Sharp Laboratories of Europe. I thank Ted Briscoe for his guidance and advice, and the ANLP referees for their comments.\\n\\nBibliography\\n\\nEric Brill and Mitch Marcus (1992). Tagging an Unfamiliar Text With Minimal Human Supervision. In AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 10-16.\\n\\nEric Brill (1992). A Simple Rule-Based Part of Speech Tagger. In Third Conference on Applied Natural Language Processing. Proceedings of the Conference. Trento, Italy, pages 152-155, Association for Computational Linguistics.\\n\\nKenneth Ward Church (1988). A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. In Second Conference on Applied Natural Language Processing. Proceedings of the Conference, pages 136-143, Association for Computational Linguistics.\\n\\nDoug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun (1992). A Practical Part-of-Speech Tagger. In Third Conference on Applied Natural Language Processing. Proceedings of the Conference. Trento, Italy, pages 133-140, Association for Computational Linguistics.\\n\\nSteven J. DeRose (1988). Grammatical Category Disambiguation by Statistical Optimization. Computational Linguistics, 14(1):31-39.\\n\\nRoger Garside, Geoffrey Leech, and Geoffrey Sampson (1987). The Computational Analysis of English: A Corpus-based Approach. Longman, London.\\n\\nX. D. Huang, Y. Ariki, and M. A. Jack (1990). Hidden Markov Models for Speech Recognition. Edinburgh University Press.\\n\\nJ. M. Kupiec (1989). Probabilistic Models of Short and Long Distance Word Dependencies in Running Text. In Proceedings of the 1989 DARPA Speech and Natural Language Workshop, pages 290-295.\\n\\nJulian Kupiec (1992). Robust Part-of-speech Tagging Using a Hidden Markov Model. Computer Speech and Language, 6.\\n\\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz (1993). Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330.\\n\\nBernard Merialdo (1994). Tagging English Text with a Probabilistic Model. Computational Linguistics, 20(2):155-171.\\n\\nR. A. Sharman (1990). Hidden Markov Model Methods for Word Tagging. Technical Report UKSC 214, IBM UK Scientific Centre.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9410012.xml'}),\n",
       " Document(page_content=\"No Title\\n\\nThis paper investigates model merging, a technique for deriving Markov models from text or speech corpora. Models are derived by starting with a large and specific model and by successively combining states to build smaller and more general models. We present methods to reduce the time complexity of the algorithm and report on experiments on deriving language models for a speech recognition task. The experiments show the advantage of model merging over the standard bigram approach. The merged model assigns a lower perplexity to the test set and uses considerably fewer states.\\n\\nIntroduction\\n\\nThere are several methods to estimate model parameters. The first one is to use each word (type) as a state and estimate the transition probabilities between two or three words by using the relative frequencies of a corpus. This method is commonly used in speech recognition and known as word-bigram or word-trigram model. The relative frequencies have to be smoothed to handle the sparse data problem and to avoid zero probabilities.\\n\\nThe rest of the paper is structured as follows. We first give a short introduction to Markov models and  present the model merging technique. Then, techniques for reducing the time complexity are presented and we report two experiments using these techniques.\\n\\nMarkov Models\\n\\nA Markov model starts running in the start state qs, makes a transition at each time step, and stops when reaching the end state qe. The transition from one state to another is done according to the probabilities specified with the transitions. Each time a state is entered (except the start and end state) one of the outputs is chosen (again according to their probabilities) and emitted.\\n\\nAssigning Probabilities to Data\\n\\nFor the rest of the paper, we are interested in the probabilities which are assigned to sequences of outputs by the Markov models. These can be calculated in the following way.\\n\\nGiven a model M, a sequence of outputs\\n\\nand a sequence of states\\n\\n(of same length), the probability that the model running through the sequence of states and emitting the given outputs is\\n\\n(with q0 = qs). A sequence of outputs can be emitted by more than one sequence of states, thus we have to sum over all sequences of states with the given length to get the probability that a model emits a given sequence of outputs:\\n\\nPerplexity\\n\\nMarkov models assign rapidly decreasing probabilities to output sequences of increasing length. To compensate for different lengths and to make their probabilities comparable, one uses the perplexity\\n\\nof an output sequence instead of its probability. The perplexity is defined as\\n\\nThe probability is normalized by taking the k[th] root (k is the length of the sequence). Similarly, the log perplexity\\n\\nis\\n\\ndefined:\\n\\nHere, the log probability is normalized by dividing by the length of the sequence.\\n\\nand\\n\\nare defined such that higher perplexities (log perplexities, resp.) correspond to lower probabilities, and vice versa. These measures are used to determine the quality of Markov models. The lower the perplexity (and log perplexity) of a test sequence, the higher its probability, and thus the better it is predicted by the model.\\n\\nModel Merging\\n\\nBy estimating the topology, model merging groups words into categories, since all words that can be emitted by the same state form a category. The advantage of model merging in this respect is that it can recognize that a word (the type) belongs to more than one category, while each occurrence (the token) is assigned a unique category. This naturally reflects manual syntactic categorizations, where a word can belong to several syntactic classes but each occurrence of a word is unambiguous.\\n\\nThe Algorithm\\n\\nto the corpus. Since the model makes an implicit independence assumption between the utterances, the corpus probability is calculated by multiplying the utterance's probabilities, yielding\\n\\n.\\n\\nUsing Model Merging\\n\\nThe model merging algorithm needs several optimizations to be applicable to large natural language corpora, otherwise the amount of time needed for deriving the models is too large. Generally, there are O(l[2]) hypothetical merges to be tested for each merging step (l is the length of the training corpus). The probability of the training corpus has to be calculated for each hypothetical merge, which is O(l)with dynamic programming. Thus, each step of merging is O(l[3]). If we want to reduce the model from size l+2 (the trivial model, which consists of one state for each token plus initial and final states) to some fixed size, we need O(l) steps of merging. Therefore, deriving a Markov model by model merging is O(l[4]) in time.\\n\\nWe use two additional strategies to reduce the time complexity of the algorithm: a series of cascaded constraints on the merges and the variation of the starting point.\\n\\nConstraints\\n\\nWhen applying model merging one can observe that first mainly states with the same output are merged. After several steps of merging, it is no longer the same output but still mainly states that output words of the same syntactic category are merged. This behavior can be exploited by introducing constraints on the merging process. The constraints allow only some of the otherwise possible merges. Only the allowed merges are tested for each step of merging.\\n\\nWe consider constraints that divide the states of the current model into equivalence classes. Only states belonging to the same class are allowed to merge. E.g., we can divide the states into classes generating the same outputs. If the current model has N states and we divide them into k]1 nonempty equivalence classes\\n\\n, then, instead of N(N-1)/2, we have to test\\n\\nmerges only.\\n\\nThe best case for a model of size N is the division into N/2classes of size 2. Then, only N/2 merges must be tested to find the best merge.\\n\\nThe best division into k ] 1 classes for some model of size N is the creation of classes that all have the same size N/k (or an approximation if\\n\\n). Then,\\n\\nmust be tested for each step of merging.\\n\\nThus, the introduction of these constraints does not reduce the order of the time complexity, but it can reduce the constant factor significantly (see section about experiments).\\n\\nThe following equivalence classes can be used for constraints when using untagged corpora: 1. States that generate the same outputs (unigram constraint) 2. unigram constraint, and additionally all predecessor states must generate the same outputs (bigram constraint) 3. trigrams or higher, if the corpora are large enough 4. a variation of one: states that output words belonging to one ambiguity class, i.e. can be of a certain number of syntactic classes.\\n\\nMerging starts with one of the constraints. After a number of merges have been performed, the constraint is discarded and a weaker one is used instead.\\n\\nThe standard n-gram approaches are special cases of using model merging and constraints. E.g., if we use the unigram constraint, and merge states until no further merge is possible under this constraint, the resulting model is a standard bigram model, regardless of the order in which the merges were performed.\\n\\nIn practice, a constraint will be discarded before no further merge is possible (otherwise the model could have been derived directly, e.g., by the standard n-gram technique). Yet, the question when to discard a constraint to achieve best results is unsolved.\\n\\nThe Starting Point\\n\\nThe initial model of the original model merging procedure is the maximum likelihood or trivial model. This model has the advantage of directly representing the corpus. But its disadvantage is its huge number of states. A lot of computation time can be saved by choosing an initial model with fewer states.\\n\\nThe initial model must have two properties: 1. it must be larger than the intended model, and 2. it must be easy to construct. The trivial model has both properties. A class of models that can serve as the initial model as well are n-gram models. These models are smaller by one or more orders of magnitude than the trivial model and therefore could speed up the derivation of a model significantly.\\n\\nThis choice of a starting point excludes a lot of solutions which are allowed when starting with the maximum likelihood model. Therefore, starting with an n-gram model yields a model that is at most equivalent to one that is generated when starting with the trivial model, and that can be much worse. But it should be still better than any n-gram model that is of lower of equal order than the initial model.\\n\\nExperiments\\n\\nModel Merging vs. Bigrams\\n\\nThe first experiment compares model merging with a standard bigram model. Both are trained on the same data. We use\\n\\nNtrain = 14,421words of the Verbmobil corpus. The corpus consists of transliterated dialogues on business appointments. The models are tested on Ntest=2,436words of the same corpus. Training and test parts are disjunct.\\n\\nThe bigram model yields a Markov model with 1,440 states. It assigns a log perplexity of 1.20 to the training part and 2.40 to the test part.\\n\\nModel merging starts with the maximum likelihood model for the training part. It has 14,423 states, which correspond to the 14,421 words (plus an initial and a final state). The initial log perplexity of the training part is 0.12. This low value shows that the initial model is very specialized in the training part.\\n\\nWe start merging with the same-output (unigram) constraint to reduce computation time. After 12,500 merges the constraint is discarded and from then on all remaining states are allowed to merge. The constraints and the point of changing the constraint are chosen for pragmatic reasons. We want the constraints to be as week as possible to allow the maximal number of solutions but at the same time the number of merges must be manageable by the system used for computation (a SparcServer1000 with 250MB main memory). As the following experiment will show, the exact points of introducing/discarding constraints is not important for the resulting model.\\n\\nThere are\\n\\nhypothetical first merges in the unconstraint case. This number is reduced to\\n\\nwhen using the unigram constraint, thus by a factor of\\n\\n. By using the constraint we need about a week of computation time on a SparcServer 1000 for the whole merging process. Computation would not have been feasible without this reduction.\\n\\nThen, perplexity slowly increases. It can never decrease: the maximum likelihood model assigns the highest probability to the training part and thus the lowest perplexity.\\n\\nNote that the perplexity changes very slowly for the largest part, and then changes drastically during the last merges. There is a constant phase between 0 and 1,454 merges. Between 1,454 and\\n\\n11,000 merges the log perplexity roughly linearly increases with the number of merges, and it explodes afterwards.\\n\\nWhat happens to the test part? Model merging starts with a very special model which then is generalized. Therefore, the perplexity of some random sample of dialogue data (what the test part is supposed to be) should decrease during merging. This is exactly what we find in the experiment.\\n\\nModel merging finds a model with 113 states, which assigns a log perplexity of 2.26 to the test part. Thus, in addition to finding a model with lower log perplexity than the bigram model (2.26 vs. 2.40), we find a model that at the same time has less than 1/10 of the states (113 vs. 1,440).\\n\\nImprovements\\n\\nThe derivation of the optimal model took about a week although the size of the training part was relatively small. Standard speech applications do not use 14,000 words for training as we do in this experiment, but 100,000, 200,000 or more. It is not possible to start with a model of 100,000 states and to successively merge them, at least it is not possible on today's machines. Each step would require the test of\\n\\nmerges.\\n\\nThe derived models are not in any case equivalent (with respect to perplexity), regardless whether we start with the trivial model or the bigram model. We ascribe the equivalence in the experiment to the particular size of the training corpus. For a larger training corpus, the optimal model should be closer in size to the bigram model, or even larger than a bigram model. In such a case  starting with bigrams does not lead to an optimal model, and a trigram model must be used.\\n\\nConclusion\\n\\nWe investigated model merging, a technique to induce Markov models from corpora. The original procedure is improved by introducing constraints and a different initial model. The procedures are shown to be applicable to a transliterated speech corpus. The derived models assign lower perplexities to test data than the standard bigram model derived from the same training corpus. Additionally, the merged model was much smaller than the bigram model.\\n\\nThe experiments revealed a feature of model merging that allows for improvement of the method's time complexity. There is a large initial part of merges that do not change the model's perplexity w.r.t. the test part, and that do not influence the final optimal model. The time needed to derive a model is drastically reduced by abbreviating these initial merges. Instead of starting with the trivial model, one can start with a smaller, easy-to-produce model, but one has to ensure that its size is still larger than the optimal model.\\n\\nAcknowledgements\\n\\nI would like to thank Christer Samuelsson for very useful comments on this paper. This work was supported by the Graduiertenkolleg Kognitionswissenschaft, Saarbrcken.\\n\\nBibliography\\n\\nLalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179-190.\\n\\nLeonard E. Baum, Ted Petrie, George Soules, and Norman Weiss. 1970. A maximization technique occuring in the statistical analysis of probabilistic functions in markov chains. The Annals of Methematical Statistics, 41:164-171.\\n\\nThorsten Brants. 1995. Estimating HMM topologies. In Tbilisi Symposium on Language, Logic, and Computation, Human Communication Research Centre, Edinburgh, HCRC/RP-72.\\n\\nKenneth Ward Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proc. Second Conference on Applied Natural Language Processing, pages 136-143, Austin, Texas, USA.\\n\\nDoug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the 3rd Conference on Applied Natural Language Processing (ACL), pages 133-140.\\n\\nF. Jelinek. 1990. Self-organized language modeling for speech recognition. In A. Waibel and K.-F. Lee, editors, Readings in Speech Recognition, pages 450-506. Kaufmann, San Mateo, CA.\\n\\nS. M. Omohundro. 1992. Best-first model merging for dynamic learning and recognition. In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors,   Advances in Neural Information Processing Systems 4, pages 958-965. Kaufmann, San Mateo, CA.\\n\\nFernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of english words. In Proceedings of the 31st ACL, Columbus, Ohio.\\n\\nL. R. Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. In Proceedings of the IEEE, volume 77(2), pages 257-285.\\n\\nAndreas Stolcke and Stephen M. Omohundro. 1994. Best-first model merging for hidden markov model induction. Technical Report TR-94-003, International Computer Science Institute, Berkeley, California, USA.\\n\\nA. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. In IEEE Transactions on Information Theory, pages 260-269.\\n\\nFootnotes\\n\\nMany thanks to the Verbmobil project for providing these data. We use dialogues that were recorded in 1993 and 94, and which are now available from the Bavarian Archive for Speech Signals BAS (http://www.phonetik.uni-muenchen.de/ Bas/BasHomeeng.html).\", metadata={'source': '../data/raw/cmplg-xml/9604005.xml'}),\n",
       " Document(page_content=\"Conserving Fuel in Statistical Language Learning: Predicting Data Requirements 1\\n\\nThe paradigm for  NLP known as  STATISTICAL LANGUAGE LEARNING ( SLL) has flourished in recent times, being seen as a quick and easy way to get off the ground. Research systems have been launched at many  NLP problems including sense disambiguation (Yarowsky, 1992), anaphora resolution (Dagan and Itai, 1990), prepositional phrase attachment (Hindle and Rooth, 1993) and lexical acquisition (Brent, 1993). This has all been fueled by the large text corpora which are increasingly available (Marcus et al., 1993). Since these systems learn to navigate language by consuming text, they are critically dependent on the data that drives them. In this paper I address the practical concern of predicting how much training data is sufficient for a given system. First, I briefly review earlier results and show how these can be combined to bound the expected accuracy of a mode-based learner as a function of the volume of training data. I then develop a more accurate estimate of the expected accuracy function under the assumption that inputs are uniformly distributed. Since this estimate is expensive to compute, I also give a close but cheaply computable approximation to it. Finally, I report on a series of simulations exploring the effects of inputs that are not uniformly distributed.\\n\\nBackground Do We Need To Know?\\n\\nEven though text is becoming increasingly available, it is often expensive, especially if it must be annotated. Consider the decisions facing the  SLL technology consumer, that is, the architect of a planned commercial  NLP system. For each module which is to employ  SLL, an appropriate technique must be selected. If different techniques require different amounts of data to achieve a given accuracy, the architect would like to know what these requirements are in advance in order to make an informed choice.\\n\\nFurther, once the technique is chosen, she must decide how much data to collect or purchase for training. Because this data can be expensive, foreknowledge of data requirements is highly valuable. Thus, in order to make statistical  NLP technology practical, a predictive theory of data requirements is needed. Despite this need, very little attention has been paid to the problem.\\n\\nFoundations For A Theory\\n\\nAll the  SLL systems mentioned above employ knowledge gained from a corpus to make decisions. Abstractly, this knowledge can be represented as a mapping from observable features (inputs) to decision outcomes (outputs). Following Lauer (1995) I will call each distinguished input a  BIN and each possible output a  VALUE. There is a probability distribution across the bins representing how instances fall into bins. Also, for each bin, there is a probability distribution across the set of values representing how instances in that bin take on values. For the system to perform accurately, most (but not necessarily all) of the instances falling in a particular bin must have the same value.\\n\\nIn what follows I will make several assumptions: Training and test data are drawn from the same distributions. The set of possible values is binary (examples include Hindle and Rooth, 1993 and Lauer, 1994). The probability of the most likely value in each bin is constant. Finally, I will only consider a simple learning algorithm: collect the training instances falling into each bin and then select the most frequent value for each. This mode-based learner is employed directly in the unigram tagger of Charniak (1993, p49) and is at the heart of many systems.\\n\\nOptimal Accuracy\\n\\nThere are two sources of error in statistical language learners of the kind we are considering. First, since the values are not necessarily fully determined by the bins, no matter what value the learner assigns to a bin there will always be errors (the optimal error rate). Second, since training data is limited, the learner may not have sufficient data available to acquire accurate rules. The combination of these sources of error results in some degree of inaccuracy for the system. We are interested in estimating the accuracy for various volumes of training data. Since the optimal error rate is independent of the amount of training data,  it will always exist no matter how much data is used. As the amount of training data increases we expect the accuracy to get closer to this optimal.\\n\\nLet B be the set of bins, V the set of values,\\n\\nthe probability that an instance falls into the bin b and\\n\\nthe probability of the value v given the bin b. If we denote the most likely value in each bin as\\n\\n, then the expected value of the optimal accuracy is determined by the likelihood of this value occurring in each bin.\\n\\nIf we know the probability that an algorithm will learn the value v for the bin b (denote this\\n\\n), then we can also calculate the expected accuracy rate:\\n\\nExisting Work\\n\\nEmpty Bins and Non\\n\\n\\n\\nempty Bins\\n\\nThe most severe result of insufficient training data is that some bins can go without any training instances. Since the learner has no indications about likely values for the bin it will be forced to guess. To estimate how often this will occur, consider the way in which m training instances would fall into the bins. For each bin, the probability that no training instances fall into it is:\\n\\nI will call such bins  EMPTY BINS.\\n\\nIn Lauer (1995) it is shown that for any bin b:\\n\\nLauer (1995) also bounds the expected accuracy of the mode-based learner when all bins are guaranteed to have at least one training instance. When this is the case, it is shown that the expected error rate is always no worse than twice the optimal error rate.\\n\\nThis is quite a useful result, since we expect the optimal accuracy to be fairly high. If the optimal predictions are 90% accurate, then a mode-based learner will be at least 80% accurate after learning on just one instance per bin.\\n\\nOverall Expected Accuracy\\n\\nis constant (call this p), we can infer that the optimal accuracy for the non-empty bins is the same as the optimal accuracy on all bins. Thus: \\\\mbox{EA}  =  \\\\Pr(\\\\mbox{non-empty}) \\\\mbox{EA}(\\\\mbox{non-empty}) + \\\\Pr(\\\\mbox{empty}) \\\\mbox{EA}(\\\\mbox{empty}) \\\\nonumber \\\\\\\\ \\\\ge  (1-e^{-m/{\\\\mid B \\\\mid}}) \\\\mbox{EA}(\\\\mbox{non-empty}) + (e^{-m/{\\\\mid B \\\\mid}}) \\\\mbox{EA}(\\\\mbox{empty}) \\\\nonumber \\\\\\\\ \\\\ge  (1-e^{-m/{\\\\mid B \\\\mid}}) (1-2(1-\\\\mbox{OA})) + \\\\frac{1}{2}e^{-m/{\\\\mid B \\\\mid}} \\\\nonumber \\\\\\\\ =  (1-e^{-m/{\\\\mid B \\\\mid}}) (2p-1) + \\\\frac{1}{2}e^{-m/{\\\\mid B \\\\mid}} \\\\end{eqnarray} --> The second step follows from the fact that\\n\\nTheory\\n\\nEstimating Expected Accuracy\\n\\n). Let the total number of training instances in a bin b be nand the number of these instances with value v be\\n\\n:\\n\\nIf n is even, we must also add an additional term of\\n\\nThis is because when there are equal numbers of both values in the bin, a random guess yields an expected accuracy of 50%. In the arguments below, I will treat all values of n as odd in order to simplify. The reader may check for herself that the results hold generally when the above extra term is included.\\n\\nUsing the fact that V is binary, the total expected accuracy for test instances in bin bwhen it contains n training instances is:\\n\\nBy summing over all possible numbers of training instances in a bin, we can arrive at an expression for the expected accuracy across all bins as follows:\\n\\nwhere\\n\\nTo simplify this I have defined a function as follows:\\n\\nA result which may be easily obtained by expansion is:\\n\\n.\\n\\nA Computable Bound for G\\n\\nThe main difficulty with the function G is the appearance of\\n\\n. Most corpus-based language learners use large corpora, so we expect the number of training instances, m, to be very large. So we need a more easily computable version of G.  The following argument leads to a fairly tight lower bound to G for suitably chosen values of kj (see below):\\n\\nThe first step rearranges the order of addition. The final step introduces a series of variables which limit the number of terms in the inner sum. The inequality holds for all\\n\\n. Notice that the kj may vary for each term of the outer sum. Since\\n\\nwe can use\\n\\nthe following relation:\\n\\nLetting\\n\\nwe can simplify as\\n\\nfollows:\\n\\nThe last step introduces g and holds for all\\n\\nThis is because in practice only the first few terms of the outer sum are significant. Thus for suitably chosen g, kj this is a cheaply computable lower bound for G.  A program to compute this to a high degree of accuracy has been implemented.\\n\\nExperiment\\n\\nSkewed Bins\\n\\nThe assumption that bin probabilities are uniform is problematic. When bins are uniformly probable, the expected number of training instances in the same bin as a random test instance is\\n\\n(\\n\\n). But most distributions in language are highly skewed. Zipf's law states that word types are distributed logarithmically (the nth most frequent word has probability proportional to\\n\\n). When this is true the expected number of training instances in the same bin as a random test instance is approximately\\n\\n(\\n\\n). Thus we can expect much more information to be available about typical test cases.\\n\\nSimulations\\n\\nThese simulations use a fixed number of bins (10,000), allocating m training instances to the bins according to either a uniform or logarithmic distribution. It then measures the correctness of the mode-based learner on 1000 randomly generated test instances to arrive at an observed correctness rate.\\n\\nThis process (training and testing) is repeated 30 times for each run, with the mean being recorded as the observed accuracy. The standard deviation is used to estimate a 5% t-score confidence interval.\\n\\nResults\\n\\nConclusion\\n\\nIf commercial  NLP systems are to be developed from the current batch of research prototypes for  SLL, then a predictive theory of the data requirements of such systems is necessary. In this paper I have explored the dependence of the expected accuracy of a simple statistical learner on the volume of training data. When the probability distribution of inputs is uniform, I have shown how to compute the expected accuracy, a result backed up by simulations. In particular, an average of four training instances per bin can be expected to yield an error rate only 50% worse than the optimal error rate.\\n\\nWhen the distribution is non-uniform, simulations show that convergence can be much more rapid. Error rates only 50% worse than optimal result from only three training instances per bin. However, when data is abundant, non-uniform distributions result in higher error rates than the estimate produced by assuming uniformity.\\n\\nAcknowledgements\\n\\nI am grateful to Mark Johnson, without whom this work would not exist, and also to Robert Dale, Mark Dras, Mike Johnson and John Potter. Financial support is gratefully acknowledged from the Australian Government and the Microsoft Institute.\\n\\nBibliography\\n\\n1 Brent, Michael. 1993. From Grammar to Lexicon: Unsupervised Learning of Lexical Syntax. In Computational Linguistics, Vol 19(2), pp243-62.\\n\\n2\\n\\nCharniak, Eugene.\\n\\n1993.\\n\\nStatistical Language Learning.\\n\\nMIT Press, Cambridge, MA.\\n\\n3 Dagan, I. and Itai, A. 1990. A Statistical Filter For Resolving Pronoun References. In Proceedings of the Seventh Israeli Conference on Artificial Intelligence and Computer Vision, Ramat Gan, Israel. pp125-35.\\n\\n4 de Haan, Pieter. 1992. Optimum Corpus Sample Size? In Leitner, Gerhard (ed.) New Directions in English Language Corpora. Mouton de Gruyter, Berlin.\\n\\n5 Hindle, D. and Rooth, M. 1993. Structural Ambiguity and Lexical Relations. In Computational Linguistics Vol. 19(1), pp103-20.\\n\\n6 Lauer, Mark. 1995. How Much Is Enough? Data Requirements for Statistical NLP. In Proceedings of the 2nd Conference of the Pacific Association for Computational Linguistics, Brisbane, Australia. cmp-lg/9509001\\n\\n7 Lauer, M. and Dras, M. 1994. A Probabilistic Model of Compound Nouns. In Proceedings of the 7th Australian Joint Conference on Artificial Intelligence, Armidale, NSW, Australia. World Scientific Press, pp474-81. cmp-lg/9409003\\n\\n8 Marcus, M., Marcinkiewicz, M. A. and Santorini, B. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. In Computational Linguistics Vol 19(2), pp313-30.\\n\\n9 Yarowsky, David. 1992. Word-Sense Disambiguation Using Statistical Models of Roget's Categories Trained on Large Corpora. In Proceedings of COLING-92, France, pp454-60.\\n\\nFootnotes\\n\\nThis paper has been accepted for publication at the Eigth Australian Joint Conference on Artificial Intelligence, Canberra, 1995. See de Haan (1992) for an investigation of sample sizes for linguistic studies. Note that this does not require that the most likely value be the same value in each bin; only that whatever the most likely value is has a constant probability. The results were generated using an optimal value probability of p = 0.9 (thus the optimal accuracy rate is 90%). Simulations with other values of p did not differ qualitatively.\", metadata={'source': '../data/raw/cmplg-xml/9509002.xml'}),\n",
       " Document(page_content=\"A State\\n\\n\\n\\nTransition Grammar for Data\\n\\n\\n\\nOriented Parsing\\n\\nThis paper presents a grammar formalism designed for use in data-oriented approaches to language processing. It goes on to investigate ways in which a corpus pre-parsed with this formalism may be processed to provide a probabilistic language model for use in the parsing of fresh texts.\\n\\nIntroduction\\n\\nRecent years have seen a resurgence of interest in probabilistic techniques for automatic language analysis. In particular, there has arisen a distinct paradigm of processing on the basis of pre-analyzed data which has taken the name   Data-Oriented Parsing.\\n\\n``Data Oriented Parsing (DOP) is a model where no abstract rules, but language experiences in the form of an analyzed corpus, constitute the basis for language processing.''\\n\\nThere is not space here to present full justification for adopting such an approach or to detail the advantages that it offers. The main claim it makes is that effective language processing requires a consideration of both the structural and statistical aspects of language, whereas traditional competence grammars rely only on the former, and standard  statistical techniques such as n-gram models only on the latter. DOP attempts to combine these two traditions and produce ``performance grammars'', which:\\n\\n``... should not only contain information on the structural possibilities of the general language system, but also on details of actual language use in a language community...''\\n\\nThis approach entails however that a corpus has first to be pre-analyzed (ie. hand-parsed), and the question immediately arises as to the formalism to be used for this. There is no lack of competing competence grammars available, but also no reason to expect that such grammars should be suited to a DOP approach, designed as they were to characterize the nature of linguistic competence rather than performance.\\n\\nThe next section sets out some of the properties that we might require from such a ``performance grammar'' and offers a formalism which attempts to satisfy these requirements.\\n\\nA Formalism for DOP\\n\\nGiven that we are attempting to construct a formalism that will do justice to both the statistical and structural aspects of language, the features that we would wish to maximize will include the following:\\n\\n1. The formalism should be easy to use with probabilistic processing techniques, ideally having a close correspondence to a simple probabilistic model such as a Markov process. 2. The formalism should be fine-grained, ie. responsive to the behaviour of individual words (as n-gram models are). This suggests a radically lexicalist approach (cf. Karttunen, 1990) in which all rules are encoded in the lexicon, there being no phrase structure rules which do not introduce lexical items. 3. It should be capable of capturing fully the linguistic intuitions of language users. In other words, using the formalism one should be able to characterize the structural regularities of language with at least the sophistication of modern competence grammars. 4. As it is to be used with real data, the formalism should be able to characterize the wide range of syntactic structures found in actual language use, including those normally excluded by competence grammars as belonging to the ``periphery'' of the language or as being ``ungrammatical''. Ideally every interpretable utterance should have one and only one analysis for any interpretation of it. Considering the first of these points, namely a close relation to a simple probabilistic model, a good place to start the search might be with a right-branching finite-state grammar. In this class of grammars every rule has the form A\\n\\na B (A,B\\n\\n{non\\n\\n\\n\\nterminals}, a\\n\\n{terminals}) and all trees have the simple structure :\\n\\nOr: equivalent vertical alignment, henceforth to be used in this paper, on the right)\\n\\nIn probabilistic terms, a finite-state grammar corresponds to a first-order Markov process, where given a sequence of states Si, Sj,... drawn from a finite set of possible states {S0,..,Sn} the probability of a particular state occurring depends solely on the identity of the previous state. In the finite-state grammar each word is associated with a transition between two categories, in the tree above `a' with the transition A\\n\\nB and so on. To calculate the probability that a string of words x1, x2, x3,... xn has the parse represented by the string of category-states S1, S2, S3,...Sn, we simply take the product of the probability of each transition: ie.\\n\\nIn addition to satisfying our first criterion, a finite-state grammar also fulfills the requirement that the formalism be radically lexicalist, as by definition every rule introduces a lexical item.\\n\\nAccounting for Linguistic Structure\\n\\nIf a finite-state grammar is chosen however, the third criterion, that of linguistic adequacy, seems to present an insurmountable stumbling block. How can such a simple formalism, in which syntax is reduced to a string of category-states, hope to capture even the basic hierarchical structure, the familiar ``tree structure'', of linguistic expressions?\\n\\nIndeed, if the non-terminals are viewed as atomic categories then there is no way this can be done. If however, in line with most current theories, categories are taken to be bundles of features and crucially if one of these features has the value of a stack of categories, then this hierarchical structure can indeed be represented.\\n\\nUsing the notation A [B] to represent a state of basic category A carrying a category B on its stack, the hierarchical structure of the sentence:\\n\\n(1) The man gave the dog a bone.\\n\\ncan be represented as:\\n\\nThe \\t\\tS \\t\\t[ ] man \\t\\tN \\t\\t[VP] gave \\t\\tVP \\t\\t[ ]  (1a)\\t\\tthe \\t\\tNP \\t\\t[NP] dog \\t\\tN \\t\\t[NP] a \\t\\tNP \\t\\t[ ] bone \\t\\tN \\t\\t[ ]\\n\\nIntuitively, syntactic links between non-adjacent words, impossible in a standard finite-state grammar, are here established by passing categories along on the stack ``through'' the state of intervening words. That such a formalism can fully capture basic linguistic structures is confirmed by the proof in Aho (1968) that an   indexed grammar (ie. one where categories are supplemented with a stack of unbounded length, as above), if restricted to right linear trees (also as above), is equivalent to a   context-free grammar.\\n\\nA perusal of the state transitions associated with individual words in (1a) reveals an obvious relationship to the ``types'' of categorial grammar. Using\\n\\nto represent a list of categories (possibly null), we arrive at the following transitions (with their corresponding categorial types alongside).\\n\\nThe ditransitive verb `gave' is\\n\\nVP [\\n\\n]\\n\\nNP [NP,\\n\\n]   (VP/NP)/NP\\n\\nDeterminers in complement position are both:\\n\\nNP [\\n\\n]\\n\\nN\\n\\n[\\n\\n]   NP/N\\n\\nDeterminer in subject position is `type-raised' to:\\n\\nS [\\n\\n]\\n\\nN [VP,\\n\\n]\\n\\n(S/VP)/N\\n\\nThe common nouns are all:\\n\\nN [\\n\\n]\\n\\nN\\n\\nIn fact as no intermediate constituents are formed in the analysis, an even closer parallel is to a dependency syntax where only rightward pointing arrows are allowed, of which the formalism as presented above is a notational variant. This lack of intermediate constituents has the added benefit that no ``spurious ambiguities'' can arise.\\n\\nKnowing now that the addition of a stack-valued feature suffices to capture the basic hierarchical structure of language, additional features can be used to deal with other syntactic relations. For example, following the example of GPSG,  unbounded dependencies can be captured using ``slashed'' categories. If we represent a ``slashed'' category X  with the lower case x, and use the notation A(b) for a category A carrying a feature b, then the topicalized sentence:\\n\\n(2) This bone the man gave the puppy.\\n\\nwill have the analysis:\\n\\nThis \\t\\tS \\t\\t[ ] bone \\t\\tN \\t\\t[S(np)] the \\t\\tS(np) \\t\\t[ ]  (2a)\\t\\tman \\t\\tN \\t\\t[VP(np)] gave \\t\\tVP(np) \\t\\t[ ] the \\t\\tNP \\t\\t[ ] puppy \\t\\tN \\t\\t[ ]\\n\\nAlthough there is no space in this paper to go into greater detail, further constructions involving unbounded dependency and complement control phenomena can be captured in similar ways.\\n\\nCoverage\\n\\nThe criterion that remains to be satisfied is that of width of coverage: can the formalism cope with the many ``peripheral'' structures found in real written and spoken texts? As it stands the formalism is weakly equivalent to a context-free grammar and as such will have problems dealing with phenomena like discontinuous constituents, non-constituent coordination and gapping. Fortunately if extensions are made to the formalism, necessarily taking it outside weak equivalence to a context-free grammar, natural and general analyses present themselves for such constructions. Two of these will now be sketched.\\n\\nDiscontinuous Constituents\\n\\nConsider the pair of sentences  (3) and (4), identical in interpretation, but the latter containing a discontinuous noun phrase and the former not:\\n\\n(3) I saw a dog which had no nose yesterday.\\n\\n(4)  I saw a dog yesterday which had no nose.\\n\\nwhich have the respective analyses: I \\t\\tS \\t\\t[ ] saw \\t\\tVP \\t\\t[ ] a \\t\\tNP \\t\\t[NP(t)]\\t\\t`t' = dog \\t\\tN \\t\\t[NP(t)]\\t\\t`time adjunct'  (3a)\\t\\twhich \\t\\tS(rel) \\t\\t[NP(t)] \\t\\t `rel' = had \\t\\tVP \\t\\t[NP(t)] \\t\\t `relative' no \\t\\tNP \\t\\t[NP(t)] nose \\t\\tN \\t\\t[NP(t)] yesterday \\t\\tNP(t) \\t\\t[ ]\\n\\nI \\t\\tS \\t\\t[ ] saw \\t\\tVP \\t\\t[ ] a \\t\\tNP \\t\\t[NP(t)] dog \\t\\tN \\t\\t[NP(t)]  (4a)\\t\\tyesterday \\t\\tNP(t) \\t\\t[S(rel)] which \\t\\tS(rel) \\t\\t[ ] had \\t\\tVP \\t\\t[ ] no \\t\\tNP \\t\\t[ ] nose \\t\\tN \\t\\t[ ]\\n\\nThe only transition in (4a) that differs from that of the corresponding word in the `core' variant (3a) is that of `dog' which has the respective transitions:\\n\\nN [NP(t)]\\n\\nS(rel)\\n\\n[NP(t)]  (in 3a)\\n\\nN [NP(t)]\\n\\nNP(t)\\n\\n[S(rel)]  (in 4a)\\n\\nBoth nouns introduce a relative clause modifier S(rel), the difference being that in the discontinuous variant a category has been taken off the stack at the same time as the modifier has been placed on the stack. It has been assumed so far that we are using a right-linear indexed grammar, but such a rule is expressly disallowed in an indexed grammar and so allowing transitions of this kind ends the formalism`s weak equivalence to the context-free grammars.\\n\\nOf course, having allowed such crossed dependencies, there is nothing in the formalism itself that will disallow a similar analysis for a discontinuity unacceptable in English such as:\\n\\n(5) I saw a yesterday dog.\\n\\nThis does not present a problem, however, as in DOP it is information in the parsed corpus which determines the structures that are possible. There is no need to explicitly rule out (5), as the transition NP [\\n\\n]\\n\\n[N] will be vanishingly rare in any corpus of even the most garbled speech, while the transition N [\\n\\n]\\n\\n[S(rel)] is commonly met with in both written and spoken English.\\n\\nNon\\n\\n\\n\\nConstituent Coordination\\n\\nThe analysis of standard coordination is shown in (6):\\n\\nFido \\t\\tS \\t\\t[ ] gnawed \\t\\tVP \\t\\t[ ] a \\t\\tNP \\t\\t[VP(+)]  (6)\\t\\tbone \\t\\tN \\t\\t[VP(+)] and \\t\\tVP(+) \\t\\t[ ] barked \\t\\tVP \\t\\t[ ]\\n\\nInstead of a typical transition for `gnawed' of VP\\n\\nNP, we have a transition introducing a coordinated VP:   VP\\n\\nNP [VP(+)]\\n\\nIn general for any transition X\\n\\nY , where X is a category and Y a list of categories (possibly empty), there will be a transition introducing coordination: X\\n\\nY [X(+)]\\n\\nNon-constituent coordinations such as (7) present serious problems for phrase-structure approaches:\\n\\n(7) Fido had a bone yesterday and biscuit today.\\n\\nHowever if we generalize the schema already obtained for standard coordination by allowing X to be not only a single category, but a list of categories, it is found to suffice for non-constituent coordination as well.\\n\\nFido \\t\\tS \\t\\t[ ] had \\t\\tVP \\t\\t[ ] a \\t\\tNP \\t\\t[NP(t)]  (7a)\\t\\tbone \\t\\tN \\t\\t[NP(t)] yesterday \\t\\tNP(t) \\t\\t[N(+) [NP(t)]] and \\t\\tN(+) \\t\\t[NP(t)] biscuit \\t\\tN \\t\\t[NP(t)] today \\t\\tNP(t) \\t\\t[ ]\\n\\nIn this analysis instead of a regular transition for `bone' of: N [NP(t)]\\n\\nNP(t) [ ]\\n\\nthere is instead a transition introducing coordination: N [NP(t)]\\n\\nNP(t) [N(+) [NP(t)]]\\n\\nAllowing categories on the stack to themselves have non-empty stacks moves the formalism one step further from being an indexed grammar. This is the final incarnation of the formalism, being the   State-Transition Grammar of the title.\\n\\nSimilar schemas are being investigated to characterize gapping constructions.\\n\\nCentre\\n\\n\\n\\nEmbedding\\n\\nIt should be noted that an indefinite amount of   centre-embedding can be described, but only at the expense of unlimited growth in the length of states:\\n\\nThe \\t\\tS \\t\\t[ ] fly \\t\\tN \\t\\t[VP] the \\t\\tS(np) \\t\\t[VP] dog \\t\\tN \\t\\t[VP(np),VP] (8)\\t\\tthe \\t\\tS(np) \\t\\t[VP(np),VP] cat \\t\\tN \\t\\t[VP(np),VP(np),VP] scratched \\t\\tVP(np) \\t\\t[VP(np),VP] swallowed \\t\\tVP(np) \\t\\t[VP] died \\t\\tVP \\t\\t[ ]\\n\\nThis contrasts with unlimited right-recursion where there is no growth in state length:\\n\\nI \\t\\tS \\t\\t[ ] saw \\t\\tVP \\t\\t[ ] the \\t\\tNP \\t\\t[ ] cat \\t\\tN \\t\\t[ ] (9)\\t\\tthat \\t\\tS(rel) \\t\\t[ ] scratched \\t\\tVP \\t\\t[ ] the \\t\\tNP \\t\\t[ ] dog \\t\\tN \\t\\t[ ] that \\t\\tS(rel) \\t\\t[ ]  \\t\\t... \\t\\t... As the model is to be trained from real data, transitions involving long states as in (8) will have an ever smaller and eventually effectively nil probability. Therefore, when tuned to any particular language corpus the resulting grammar will be effectively finite-state.\\n\\nParsing\\n\\nAssuming that we now have a corpus parsed  with the state-transition grammar, how can this information be used to parse fresh text?\\n\\nFirstly, for each word type in the corpus we can collect the transitions with which it occurs and calculate its probability distribution over all possible transitions (an infinite number of which will be zero). To make this concrete, there are five tokens of the word `dog' in the examples thus far, and so `dog' will have the transition probability distribution: N [VP(np),VP]\\n\\nN [NP]\\n\\nNP [ ]\\t\\t0.2 N [NP(t)]\\n\\nS(rel) [NP(t)]\\t\\t0.2 N [NP(t)]\\n\\nNP(t) [S(rel)]\\t\\t0.2 N [VP(np),VP]\\n\\nS(np) [VP(np),VP]\\t\\t0.2 N [ ]\\n\\nS(rel) [ ]\\t\\t0.2\\n\\nTo find the most probable parse for a sentence, we simply find the path from word to word which maximizes the product of the state transitions (as we have a first order Markov process).\\n\\nHowever this simple-minded approach, although easy to implement, in other ways leaves much to be desired. The probability distributions are far too ``gappy'' and even if a huge amount of data were collected, the chances that they would provide the desired path for a sentence of any reasonable length are slim. The process of generalizing or smoothing the transition probabilities is therefore seen to be indispensable.\\n\\nSmoothing Probability Distributions\\n\\nAlthough far from exhausting the possible methods for smoothing, the following three are those used in the implementation described at the end of the paper.\\n\\n1. Factor out elements on the stack which are merely carried over from state to state (which was done earlier in looking at the correspondence of state transitions to categorial types). The previous transitions for `dog' then become:\\n\\naaaaaaN [\\n\\n]\\n\\nS(rel)\\t\\tN [\\n\\n]\\n\\n[ ]\\t\\t0.2 \\t\\tN [\\n\\n]\\n\\n[S(rel)]\\t\\t0.2 \\t\\tN [\\n\\n]\\n\\nS(np) [\\n\\n]\\t\\t0.2 \\t\\tN [\\n\\n]\\n\\nS(rel) [\\n\\n]\\t\\t0.4\\n\\n2. Factor out other features which are merely passed from state to state. For instance in the example sentences, `the' has the generalized transitions:\\n\\nS [\\n\\n]\\n\\nN [VP,\\n\\n] \\t\\tS(np) [\\n\\n]\\n\\nN [VP(np),\\n\\n]\\n\\nwhich can be further generalized to the single transition:\\n\\naaS(\\n\\n) [\\n\\n]\\n\\nN\\t\\tS(\\n\\n) [\\n\\n]\\n\\nN  [VP(\\n\\n),\\n\\n]\\n\\n= set of features\\n\\n3. Establish word paradigms, ie. classes of words which occur with similar transitions. The probability distribution for individual words can then be smoothed by suitably blending in the paradigmatic distribution. These paradigms  will correspond to a great extent to the word classes of rule-based grammars. The advantage would be retained however that the system is still fine-grained enough to reflect the idiosyncratic patterns of individual words and could override this paradigmatic information if sufficient data were available.\\n\\nWords hitherto unknown to the system can be treated as being extreme examples of words lacking sufficient transition data and they might then be given a transition distribution blended from the open class word paradigms.\\n\\nProblems Arising from Smoothing\\n\\nAlthough essential for effective processing, the smoothing operations may give rise to new problems. For example, factoring out items on the stack, as in (1), removes from the model the disinclination for long states inherent in the original corpus. To recapture this discarded aspect of the language, it would be sufficient to introduce into the model a probabilistic penalty based on state length. This penalty may easily be calculated according to the lengths of states in the parsed corpus.\\n\\nNot only would this allow the modelling of the restriction on centre-embedding, but it would also allow many other ``processing'' phenomena to be accurately characterized. Taking as an example ``heavy-NP shift'',  suppose that the corpus contained two distinct transitions for the word `threw', with the particle `out' both before and after the object.\\n\\nthrew \\t\\tVP\\n\\nNP, X(out) \\t\\tprob: p1\\n\\nVP\\n\\nX(out), NP \\t\\tprob: p2\\n\\nEven if p1 were considerably greater than p2, the cumulative negative effect of the longer states in (10) would eventually lead to the model giving the sentence with the shifted NP (11) a higher probability.\\n\\nI \\t\\tS \\t\\t[ ] threw \\t\\tVP \\t\\t[ ] the \\t\\tNP \\t\\t[X(out)] bacon \\t\\tN \\t\\t[X(out)]  (10)\\t\\tthat \\t\\tS(rel) \\t\\t[X(out)] Fido \\t\\tS(np) \\t\\t[X(out)] had \\t\\tVP(np) \\t\\t[X(out)] chewed \\t\\tVP(np) \\t\\t[X(out)] out \\t\\tX(out) \\t\\t[ ]\\n\\nI \\t\\tS \\t\\t[ ] threw \\t\\tVP \\t\\t[ ] out \\t\\tX(out) \\t\\t[NP] the \\t\\tNP \\t\\t[ ]  (11)\\t\\tbacon \\t\\tN \\t\\t[ ] that \\t\\tS(rel) \\t\\t[ ] Fido \\t\\tS(np) \\t\\t[ ] had \\t\\tVP(np) \\t\\t[ ] chewed \\t\\tVP(np) \\t\\t[ ]\\n\\nCapturing Lexical Preferences\\n\\nOne strength of n-gram models is that they can capture a certain amount of lexical preference information. For example, in a bigram model trained on sufficient data the probability of the bigram `dog barked' could be expected to be significantly higher than `cat barked', and this slice of ``world knowledge'' is something our model lacks.\\n\\nIt would not be difficult to make a small extension to the present model to capture such information, namely by introducing an additional feature containing the ''lexical value'' of the head of a phrase. Abandoning the shorthand `VP' and representing a subject explicitly as a ``slashed'' NP, a sentence with added lexical head features would appear as:\\n\\nThe \\t\\tS \\t\\t[ ] dog \\t\\tN(dog) \\t\\t[S(np(dog))] which \\t\\tS(rel,np(dog)) \\t\\t[S(np(dog))] (12)\\t\\tchased \\t\\tS(np(dog)) \\t\\t[S(np(dog))] the \\t\\tNP(cat) \\t\\t[S(np(dog))] cat \\t\\tN(cat) \\t\\t[S(np(dog))] barked \\t\\tS(np(dog)) \\t\\t[ ]\\n\\nIn contrast to n-grams, where this sentence would cloud somewhat the ``world knowledge'', containing as it does the bigram `cat barked', the added structure of our model allows the lexical preference to be captured no matter how far the head noun is from the head verb. From (12) the world knowledge of the system would be reinforced by the two stereotypical transitions:\\n\\n`chased'    S(np(dog))\\n\\nNP(cat)\\n\\n`barked'   S(np(dog))\\n\\n[ ]\\n\\nPresent Implementation\\n\\n16,000+ running words from section N of the Brown corpus (texts N01-N08) were hand-parsed using the state-transition grammar. The actual formalism used was much fuller than the rather schematic one given above, including many additional features such as case, tense, person and number. Transition probabilities were generalized in the ways discussed in the previous section.\\n\\nResults\\n\\n100 sentences of less than 15 words were chosen randomly from other texts in section N of the Brown corpus (N09-N14) and fed to the parser without alteration. Unknown words in the input, of which there were obviously many,  were assigned to one of seven orthographic classes and given appropriate transitions calculated from the corpus.\\n\\n27 were parsed correctly, ie. exactly the same as the hand parse or differing in only relatively insignificant ways which the model could not hope to know.\\n\\n23 were parsed wrongly, ie. the analysis differed from the hand parse in some non-trivial way.\\n\\n50 were not parsed at all, ie. one or more of the transitions necessary to find a parse path was lacking, even after generalizing the transitions.\\n\\nFuture Development\\n\\nAlthough the results at present are extremely modest, it should be borne in mind both that the amount of data the system has to work on is very small and that the smoothing of transition probabilities is still far from optimal. The present target is to achieve such a level of performance that the corpus can be extended by hand-correction of the parser output, rather than hand-parsing from scratch. Not only will this hopefully save a certain amount of drudgery, it should also help to minimize errors and maintain consistency.\\n\\nA more distant goal is to ascertain whether the performance of the model can improve after parsing new texts and processing the data therein even without hand-correction of the parses, and if so what the limits are to such ``self-improvement''.\\n\\nReferences\\n\\nAHO A.V. 1968. Indexed Grammars. Journal of the ACM, 15: 647-671.\\n\\nBOD, RENS 1992. A Computational Model of Language Performance: Data Oriented Parsing. COLING-92.\\n\\nKARTTUNEN L. 1990. Radical Lexicalism. In Baltin  Kroch (eds),     Alternative conceptions of phrase structure, Univ of Chicago Press, pp 43-65.\\n\\nKRAUWER, STEVEN  DES TOMBES, LOUIS 1981. Transducers and Grammars as Theories of Language. Theoretical Linguistics, 8, 173-202.\\n\\nMILWARD, DAVID 1990. Coordination in an Axiomatic Grammar. COLING-90.\\n\\nMILWARD, DAVID 1994. Non-constituent Coordination: Theory and Practice. COLING-94.\\n\\nFootnotes\\n\\nThis research was funded by a research studentship from the ESRC. My thanks also for discussion and comments to Matt Crocker, Chris Brew, David Milward and Anna Babarczy. Bod, 1992. ibid. ``VP'' is used here and henceforth as a shorthand for an S with a missing (ie. ``slashed'') subject. The unidirectionality of the formalism results in an automatic type-raising of all categories appearing before their heads. There is in general no upper limit to the  length of this list, eg. ``I gave Fido a biscuit yesterday in the house and Rover a bone today in his kennel.'' Milward (1990) introduces a  formalism essentially identical to the one presented here, although viewed from a very different perspective. Milward (1994) shows how it handles a wide range of  non-constituent co-ordinations. This may be compared to the claim in Krauwer Des Tombes (1981) that finite-state automata offer a more satisfactory characterization of language than context-free grammars. Such as the system postulating that ``Jess''  was a surname, as against the hand-parser's guess of a masculine first name.\", metadata={'source': '../data/raw/cmplg-xml/9502037.xml'}),\n",
       " Document(page_content='TEMPORAL RELATIONS: REFERENCE OR DISCOURSE COHERENCE?\\n\\nThe temporal relations that hold between events described by successive utterances are often left implicit or underspecified. We address the role of two phenomena with respect to the recovery of these relations: (1) the referential properties of tense, and (2) the role of temporal constraints imposed by coherence relations. We account for several facets of the identification of temporal relations through an integration of these.\\n\\nIntroduction\\n\\nMax slipped. He spilt a bucket of water.\\n\\nMax slipped. He had spilt a bucket of water.\\n\\nMax slipped because he spilt a bucket of water.\\n\\nThe Account\\n\\nWe postulate rules characterizing the referential nature of tense and the role of discourse relations in further constraining the temporal relations between clauses. The rules governing tense are:\\n\\n1. Main verb tenses are indefinitely referential, creating a new temporal entity under constraints imposed by its type (i.e., past, present, or future) in relation to a discourse reference time tR. For instance, a main verb past tense introduces a new temporal entity t under the constraint prior-to(t,tR). For simple tenses tR is the speech time, and therefore simple tenses are not anaphoric.\\n\\n2. Tensed auxiliaries in complex tenses are anaphoric, identifying tR as a previously existing temporal entity. The indefinite main verb tense is then ordered with respect to this tR. The tenses used may not completely specify the implicit temporal relations between the described events. We claim that these relations may be further refined by constraints imposed by the coherence relation operative between clauses. We describe three coherence relations relevant to the examples in this paper and give temporal constraints for them.\\n\\nNarration:\\n\\nIf\\n\\nNarration(A,B) then\\n\\ntA [ tB\\n\\nParallel:\\n\\nExplanation:\\n\\nIf Explanation(A,B) then\\n\\ntB [ tA\\n\\nTo summarize the analysis, we claim that tense operates as indefinite reference with respect to a possibly anaphorically-resolved discourse reference time. The temporal relations specified may be further refined as a by-product of establishing the coherence relationship extant between clauses, Narration being but one such relation.\\n\\nExamples\\n\\nWe now analyze the examples presented in Section 1, repeated below, using this approach:\\n\\nMax slipped. He spilt a bucket of water.\\n\\nMax slipped. He had spilt a bucket of water.\\n\\nMax slipped because he spilt a bucket of water.\\n\\nMax slipped because he had spilt a bucket of water.\\n\\nMax had spilt a bucket of water. Intuitively, such usage is infelicitous because of a dependency on a contextually salient time which has not been previously introduced. This is not captured by the LA account because sentences containing the past perfect are treated as sententially equivalent to those containing the simple past. On the other hand, sentences in the simple past are perfectly felicitous in standing alone or opening a discourse, introducing an asymmetry in accounts treating the simple past as anaphoric to a previously evoked time. All of these facts are explained by the account given here.\\n\\nConclusion\\n\\nWe have given an account of temporal relations whereby (1) tense is resolved indefinitely with respect to a possibly anaphorically-resolved discourse reference time, and (2) the resultant temporal relations may be further refined by constraints that coherence relations impose. This work is being expanded to address issues pertaining to discourse structure and inter-segment coherence.\\n\\nAcknowledgments\\n\\nThis work was supported in part by National Science Foundation Grant IRI-9009018, National Science Foundation Grant IRI-9350192, and a grant from the Xerox Corporation. I would like to thank Stuart Shieber and Barbara Grosz for valuable discussions and comments on earlier drafts.\\n\\nBibliography\\n\\nErhard Hinrichs. 1986. Temporal anaphora in discourses of english. Linguistics and Philosophy, 9:63-82.\\n\\nJerry Hobbs.\\n\\n1979.\\n\\nCoherence and coreference.\\n\\nCognitive Science, 3:67\\n\\n\\n\\n90.\\n\\nMegumi Kameyama, Rebecca Passoneau, and Massimo Poesio. 1993. Temporal centering. In Proceedings of the 31st Conference of the Association for Computational Linguistics (ACL-93), pages 70-77, Columbus, Ohio, June.\\n\\nAlex Lascarides and Nicolas Asher. 1993. Temporal interpretation, discourse relations, and common sense entailment. Linguistics and Philosophy, 16(5):437-493.\\n\\nJohn Nerbonne. 1986. Reference time and time in narration. Linguistics and Philosophy, 9:83-95.\\n\\nBarbara Partee.\\n\\n1984.\\n\\nNominal and temporal anaphora.\\n\\nLinguistics and Philosophy, 7:243\\n\\n\\n\\n286.\\n\\nHans Reichenbach.\\n\\n1947.\\n\\nElements of Symbolic Logic.\\n\\nMacmillan, New York.\\n\\nBonnie Lynn Webber.\\n\\n1988.\\n\\nTense as discourse anaphor.\\n\\nComputational Linguistics, 14(2):61\\n\\n\\n\\n73.\\n\\nFootnotes', metadata={'source': '../data/raw/cmplg-xml/9405002.xml'}),\n",
       " Document(page_content=\"Noun Phrase Reference in Japanese-to-English Machine Translation\\n\\nThis paper shows the necessity of distinguishing different referential uses of noun phrases in machine translation. We argue that differentiating between the generic, referential and ascriptive uses of noun phrases is the minimum necessary to generate articles and number correctly when translating from Japanese to English. Heuristics for determining these differences are proposed for a Japanese-to-English machine translation system. Finally the results of using the proposed heuristics are shown to have raised the percentage of noun phrases generated with correct use of articles and number in the Japanese-to-English machine translation system ALT-J/E from 65% to 77%.\\n\\nIntroduction\\n\\nThis paper is structured as follows. First, we define the three kinds of referentiality which we distinguish and justify the definitions on theoretical and practical grounds, comparing them with those suggested by other researchers. We then describe in detail a heuristic method for determining noun phrase reference in Japanese sentences. Next, we show how the distinction is used in a Japanese to English machine translation system to generate articles and number. Finally, we look at experimental results gained by implementing the proposed methods and compare them to those achieved by an earlier version of the same system, and by other systems.\\n\\nDefinition of noun phrase reference\\n\\nReferential: Referential noun phrases are those that refer to some entity or entities in the discourse world: e.g. a mammoth in There is a mammoth in my garden! Referential noun phrases are plural if there is more than one discrete referent, and are marked for definiteness. Ascriptive: Ascriptive noun phrases are used with a copular verb, or in an appositive expression, to ascribe a property to their subject: e.g. a mammoth in That animal is a mammoth. Because ascriptive noun phrases are non-referring they cannot be the antecedent of other noun phrases.\\n\\nDetermination of noun phrase reference\\n\\nIt is possible for the algorithm to be applied to the Japanese parse tree as part of the semantic analysis. In   ALT-J/E, however, the algorithm is applied after the semantic analysis has finished, during the transfer stage, because much of the semantic information is stored in the transfer dictionaries where the combination of Japanese and English makes it easy to disambiguate word senses. The overall process of translation in ALT-J/E is divided into seven parts. First, the system splits the Japanese text into morphemes and assigns parts of speech. Second, it parses the segmented text, often giving multiple possible interpretations. Third, it rewrites complicated Japanese expressions into simpler ones. Fourth, ALT-J/E semantically evaluates the various interpretations. Fifth, syntactic and semantic criteria are used to select the best interpretation. Sixth, the selected interpretation is transferred into English. Finally, the English sentence is adjusted to give the correct inflectional forms. The algorithm described in this section has been implemented as part of the sixth stage. However, it could be implemented as part of the fifth stage.\\n\\nThe default assumption is that a noun phrase will be used to refer to some specific entity or entities in the discourse world, i.e. that it is REFERENTIAL.\\n\\nUsing noun phrase referentiality to select articles and    determine number\\n\\nKnowledge of a noun phrase's referential use is essential when translating from Japanese to English, as it plays a large part in determining how a noun phrase is expressed in English. In this section we show how articles and number are generated differently for the three different referentialities in the machine translation system ALT-J/E. Correct generation of articles and number is important not only to express meaning accurately, but because it is one of the major factors in determining the readability of Japanese-to-English translations.\\n\\nTranslation of generic noun phrases\\n\\nThe use of all three kinds of  GENERIC noun phrases is not acceptable in some contexts, for example *a mammoth evolved. Sometimes a noun phrase can be ambiguous, for example I like the elephant, where the speaker could like a particular elephant, or all elephants.\\n\\nTranslation of referential noun phrases\\n\\nWhether or not a  REFERENTIAL noun phrase is definite or not is determined using heuristic criteria based on whether there is enough information to uniquely identify the noun phrase's referent, such as the following:\\n\\nif the head noun is marked in the lexicon as being unique: the earth\\n\\nif the noun phrase is made logically unique by a modifier: the best price\\n\\nif the noun phrase's referent is restrictively described: the man who came to dinner, the aim of this research\\n\\ndirect and indirect anaphoric reference: I saw a cat and a dog. The dog chased the cat.\\n\\nAs the above criteria are only meaningful for  REFERENTIAL noun phrases, it is essential to determine whether the noun phrase is referential as a first step.\\n\\nTranslation of ascriptive noun phrases\\n\\nResults\\n\\nNew shows the results using the proposed method.\\n\\nOld shows the results using the unmodified system.\\n\\nWe tested the system on newspaper articles, in the articles tested, there were an average of 7 noun phrases in each sentence. The articles were translated by ALT-J/E and the raw output examined by an English native speaker. Each noun phrase was given one of the following scores: STRUCTURE: problem with structure or choice of translation  BEST: the most appropriate article/number ARTICLE: inappropriate article NUMBER: inappropriate number POSSESSIVE: inappropriate use of possessive determiner COUNTABILITY: problem with countability REFERENCE: problem with referential property For the purpose of evaluating the generation of articles and number, noun phrases that were either the  BEST possible translation, or that had a problem only with  STRUCTURE/CHOICE OF TRANSLATION, were judged to be successful. A third-party evaluator gave the success rates as 77% for the system with the proposed method and 65% for the original system. The method of evaluation described above does not give a reproducible, absolute level of success. It does, however, successfully show the overall level of improvement/degradation, and help to identify the remaining problems.\\n\\nDiscussion\\n\\nIn this section we discuss the remaining errors and compare the results to two other systems.\\n\\nmachine translation of the newspaper articles\\n\\nOverall, the largest sources of errors are problems with the source language analysis and dictionaries (22% each). These are not problems with the proposed algorithm but with the machine translation system as a whole. Another major source of errors is the translation of numerical expressions (12%). The processing for handling numerical expressions is currently being overhauled. The errors caused by lack of information in the dictionaries are solvable immediately, which will reduce the number of errors by around 20%.\\n\\nIn the generation of articles and numbers for  REFERENTIAL noun phrases some of the errors can simply be solved by the addition of new rules: for example, adding rules which use the meaning of adverbs to determine number or rules using pre-head modifiers to determine definiteness. The problems of common sense deduction and indirect anaphora, however, require a large scale knowledge base and inference rules. While both are being researched at the moment, they are unlikely to be implemented soon. We estimate that the number of errors caused by insufficiencies in the generation of articles and numbers for  REFERENTIAL noun phrases can be reduced at least a quarter, thus reducing the total number of errors by around 8%.\\n\\nCombining the above figures, we predict it is possible to reduce the errors by around 30%, bringing the total success rate to 84% for a window test. To go beyond this needs new processing to improve the source language analysis, the translation of numerical expressions and more use of contextual inferences.\\n\\nIn addition examining even this small sample of text we came up with one major addition to the algorithm for determining noun phrase reference. Therefore the algorithm needs to be tested on a wider range of texts before the rules can be considered comprehensive. We have started testing the algorithm on a larger corpus of newspaper articles and are investigating methods for automatically learning rules.\\n\\nConclusion\\n\\nThis paper proposes a method that uses the information available in a Japanese sentence to identify a noun phrase as being used either GENERICALLY,  REFERENTIALLY or  ASCRIPTIVELY. This distinction is shown to be both theoretically justified and practically useful. The three way distinction in noun phrase reference is used as a base to determine a noun phrase's number and to generate appropriate articles and possessive pronouns when translating from Japanese to English. Incorporating this method into the machine translation system ALT-J/E helped to improve the percentage of noun phrases with correctly generated articles and number from 65% to 77%. It is shown that the proposed method can be extended straightforwardly to increase the success rate to 84%.\\n\\nSeveral problems remain to be explored. We consider the following to of primary importance: 1. Extension of the algorithm to translate texts as coherent passages, not just as single sentences. 2. Improvement of the reproducibility of the evaluation method. 3. Investigation of the coverage of the algorithm on a wider collection of texts.\\n\\nAcknowledgments\\n\\nThe paper has benefited greatly from the comments of the anonymous reviewers for TMI, Graham, Monique and Mitsuyo Bond, Satoru Ikehara, Roly Sussex and especially Tsuneko Nakazawa. We would like to thank Toshiaki Nebashi, Kazuya Fukamachi and Yoshitake Ichii for their invaluable help in implementing the processing described here.\\n\\nBibliography\\n\\nBOND, FRANCIS,  KENTARO OGURA,   SATORU IKEHARA. 1994. Countability and number in Japanese-to-English machine translation. In Proceedings of the 15th International Conference on Computational Linguistics (COLING '94), 32-38. (cmp-lg/9511001).\\n\\nBOND, FRANCIS,  KENTARO OGURA, SATORU IKEHARA. 1995. Possessive pronouns as determiners in Japanese-to-English machine translation. In Proceedings of the 2nd Pacific Association for Computational Linguistics Conference (PACLING '95). (cmp-lg/9601006).\\n\\nBOND, FRANCIS,  KENTARO OGURA, SATORU IKEHARA,   SATOSHI SHIRAI. 1993. Using the meanings of verbs to select the countability of English noun phrases. In Proceedings of the 1993 IEICE Fall Conference, 6:61-62. IEICE.\\n\\nHAWKINS, JOHN A. 1991. On (in)definite articles: implicatures and (un)grammaticality prediction. Journal of Linguistics 27.405-442.\\n\\nHUDDLESTON, RODNEY. 1984. Introduction to the Grammar of English. Cambridge textbooks in linguistics. Cambridge: Cambridge University Press.\\n\\nIKEHARA, SATORU,  SATOSHI SHIRAI,  AKIO YOKOO, HIROMI NAKAIWA. 1991. Toward an MT system without pre-editing - effects of new methods in ALT-J/E-. In Proceedings of MT Summit III, 101-106. (cmp-lg/9510008).\\n\\nKNIGHT, KEVIN,   ISHWAR CHANDER. Automated postediting of documents. In Proceedings of AAAI '94.\\n\\nKUNO, SUSUMU. 1973. The Structure of the Japanese Language. Cambridge, Massachusetts, and London, England: MIT Press.\\n\\nLYONS, JOHN.\\n\\n1977.\\n\\nSemantics, volume 2.\\n\\nCambridge: Cambridge University Press.\\n\\nMURATA, MASAKI, 1993. Research into the determination of referential property and number of nouns using Japanese structure as a guide. Bachelor's thesis, Kyoto University, Kyoto, Japan. (in Japanese)\\n\\nMURATA, MASAKI,   MAKOTO NAGAO. 1993. Determination of referential property and number of nouns in Japanese sentences for machine translation into English. In Proceedings of the Fifth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI '93), 218-25.\\n\\nOGURA, KENTARO,  AKIO YOKOO,  SATOSHI SHIRAI, SATORU IKEHARA. 1993. Japanese to English machine translation and dictionaries. In Proceedings of the 44th Congress of the International Astronautical Federation, Graz, Austria.\\n\\nQUIRK, RANDOLPH,  SIDNEY GREENBAUM,  GEOFFREY LEECH, JAN SVARTVIK. 1985. A Comprehensive Grammar of the English Language. Essex: Longman.\\n\\nSHIRAI, SATOSHI,  SATORU IKEHARA,   TSUKASA KAWAOKA. 1993. Effects of automatic rewriting of source language within a Japanese to English MT system. In Proceedings of the Fifth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI '93), 226-239.\\n\\nZELINSKY-WIBBELT, CORNELIA. 1992. Exploiting linguistic iconism for article selection in machine translation. In Proceedings of the 14th International Conference on Computational Linguistics (COLING '92), 792-798.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9601008.xml'}),\n",
       " Document(page_content='Stochastic HPSG\\n\\nIn this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by Pollard and Sag. We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures, then provide an extended interpretation which allows them. We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated  from corpora.\\n\\nIntroduction\\n\\nProbabilistic interpretation of PCFGs\\n\\nWe review the standard probabilistic interpretation of PCFGs .\\n\\nP(N1) = 1.\\n\\nIn this definition R acts as a specification of the accessibility relationships which can hold between nodes of the trees admitted by the grammar. The rule probabilities specify the cost of making particular choices about the way in which the rules develop. It is going to turn out that an exactly analogous system of accessibility relations is present in the probabilistic type hierarchies which we define later.\\n\\nLimitations of PCFGs\\n\\nThe definition of PCFGs implies that the probability of a phrase marker depends only on the choice of rules used in expanding non-terminal nodes. In particular, the probability does not depend on the order in which the rules are applied. This has the arguably unwelcome consequence that PCFGs are unable to make certain discriminations between trees which differ only in their configuration . The models developed in this paper build in similar independence assumptions. A large part of the art of probabilistic language modelling resides in the management of the trade-off between descriptive power (which has the merit of allowing us to make the discriminations which we want) and independence assumptions (which have the merit of making training practical by allowing us to treat similar situations as equivalent).\\n\\nApplying a PCFG to a simple corpus Probabilistic type hierarchies ALE signatures What the ALE signature tells us\\n\\nThe inheritance information tells us that a sign is a forced choice between a sentence and a phrase, that a phrase is a forced choice between a noun-phrase (np) and a verb-phrase (vp) and that number values (num) are partitioned into singular (sing) and plural (pl). The features which are defined are left,right, and num, and the appropriateness information says that the feature num introduces a new instance of the type num on all phrases, and that left and right introduce np and vp respectively on sentences.\\n\\nThe parallel with PCFGs\\n\\nOne difference is that num is explicitly introduced as a feature in the hierarchy, where at is only implicitly present in the original grammar. The other difference is the use of left and right as models of the dominance relationships between nodes.\\n\\nA probabilistic interpretation of  typed feature-structures\\n\\nFor our purposes, a probabilistic type hierarchy (PTH) is a four-tuple\\n\\n[ MT, NT, NT1, I ]\\n\\nAs things stand this definition is nearly isomorphic to that given for PCFGs, with the major differences being two changes which move us from rules to introduction relationships. Firstly, we relax the stipulation that the items on the right hand side of the rules are strings, allowing them instead to be multisets. Secondly, we introduce an additional term in the head of introduction rules to signal the fact that when we apply a particular introduction relationship to a node we also specialize the type of the node  by picking exactly one of the direct subtypes of its current type. Finally, we need to deal with the case where T[j] is non-maximal. This is simply achieved by defining the iterated introduction relationships from T[i] as being those corresponding to the chains of introduction relationships from T[i] which refine the type to a maximal type. In the probabilistic type hierarchy, it is the iterated introduction relationships which correspond to the context-free rewrite rules of a PCFG. A useful side-effect of this is that we can preserve the invariant that all types except those at the fringe of the structure are maximal.\\n\\nP(NT1) = 1.\\n\\nWe still model the distribution observed in the corpus by assuming two independent decisions.\\n\\nWe still get a strange ranking of the parses, which favours number disagreement,in spite of the fact that the grammar which generated the corpus enforces number agreement.\\n\\nThe hierarchy uses bot rather than s as its start symbol. The probabilities tell us that the corpus contains no free-standing structures of type num.\\n\\nSince items of type phrase are never introduced at that type, but only in the form of sub-types, there are no transitions from phrase in the corpus. Therefore the initial estimates of the probabilities of such transitions are unaffected by training.\\n\\nIn the PCFG the symmetry between the expansions of np and vp to singular and plural variants is implicit, whereas in the PTH the distribution of singular and plural variants is encoded at a single location, namely that at which num is refined.\\n\\nClarity: The decisions which we have made lead to a system with a clear probabilistic semantics.\\n\\nTrainability:  the number of parameters which must be estimated for a grammar is a linear function of the size of the type hierarchy\\n\\nEasy extensibility:  There is a clear route to a more finely grained account if we allow the expansion probabilities to be conditioned on surrounding context. This would increase the number of parameters to be estimated, which may or may not prove to be a problem.\\n\\nAdding re\\n\\n\\n\\nentrancies\\n\\nThe essential insight is that the choice of a fully inequated feature structure involving a set of nodes is the same thing as the choice of an arbitrary equivalence relation over these nodes, and this is in turn equivalent to the choice of a partition of the set of nodes into a set of non-empty sets. These sets of nodes are equivalence classes. The standard recursive procedure for generating partitions of k+1elements is to non-deterministically add the k+1thq node to each of the equivalence classes of each of the partitions of k nodes, and also to nondeterministically consider the new node as a singleton set. The basis of the stochastic procedure for generating fully-inequated feature structures is to interleave the generation of equivalence classes with the expansion from the initial node as described above.\\n\\nFor the purposes of the expansion algorithm, a fully inequated feature structure consists of a feature tree (as before) and an equivalence relation over all the maximal nodes in that tree. The task of the algorithm is to generate all such structures and to equip them with probabilities. We proceed as in the case without re-entrancy, except that we only ever expand sub-trees in the case where the new node begins a new equivalence class. This avoids the double counting which was a problem earlier.\\n\\nThe remaining task is that of assigning scores to equivalence relations. We do not have a fully satisfactory solution to this problem. The reason for this is that we would ideally like to assign probabilities to intermediate structures in such a way that the probabilities of fully expanded structures are independent of the route by which they were arrived at. This can be done, and the method which we adopt has the merit of simplicity.\\n\\nScoring re\\n\\n\\n\\nentrancies\\n\\nEvaluation\\n\\nEven a crude account of re-entrancy is better than completely ignoring the issue, and the one proposed gets the right result for cases of double counting such as those discussed above, but it should be obvious that there is room for improvement in the treatment which we provide. Intuitively what is required is a parametrisable means of distributing probability mass among the distinct equivalence relations which extend the current structure. One attractive possibility would be to enumerate the relations which can be obtained by adding the current node to the various different equivalence classes which are available, apply some scoring function to each class, and then normalize such that the total score over all alternatives is one. But this might introduce unpleasant dependencies of the probabilities of feature structures on the order in which the stochastic procedure chooses to expand nodes, because the normalisation is carried out before we have full knowledge of the equivalence classes with which the current node might become associated. It may be that an appropriate choice of scoring function will circumvent this difficulty, but this is left as a matter for further research.\\n\\nConclusions\\n\\nWe have presented two proposals for the association of probabilities with typed feature-structures of the form used in HPSG. As far as we know these are the most detailed of their type, and the ones which are most likely to be able to exploit standard training and parsing algorithms. For typed feature structures lacking re-entrancy we believe our proposal to be the simplest and most natural which is available. The proposal for dealing with re-entrancy is less satisfactory but offers a basis for empirical exploration, and has definite advantages over the straightforward use of PCFGs. We plan to follow up the current work by  training and testing a suitable instantiation of our framework  against manually annotated corpora.\\n\\nAcknowledgements\\n\\nI acknowledge the support of the Language Technology Group of the Human Communication Research Centre, which is a UK ESRC funded institution.\\n\\nBibliography\\n\\nBob Carpenter. 1992. The Logic of Typed Feature Structures. Cambridge Tracts in Theoretical Computer Science. CUP. With Applications to Unification Grammars, Logic Programs and Constraint Resolution.\\n\\nEugene Charniak.\\n\\n1993.\\n\\nStatistical Language Learning.\\n\\nThe MIT Press.\\n\\nAlbert Kim. 1994. Graded unification: A framework for interactive processing. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 313-315, June.\\n\\nC.S. Mellish. 1988. Implementing systemic classification by unification. Computational Linguistics, 14(1):40-51. Winter.\\n\\nCarl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. CSLI and University of Chicago Press, Stanford, Ca. and Chicago, Ill.\\n\\nFootnotes', metadata={'source': '../data/raw/cmplg-xml/9502022.xml'}),\n",
       " Document(page_content=\"APPORTIONING DEVELOPMENT EFFORT IN A PROBABILISTIC LR PARSING SYSTEM THROUGH EVALUATION\\n\\nWe describe an implemented system for robust domain-independent syntactic parsing of English, using a unification-based grammar of part-of-speech and punctuation labels coupled with a probabilistic LR parser. We present evaluations of the system's performance along several different dimensions; these enable us to assess the contribution that each individual part is making to the success of the system as a whole, and thus prioritise the effort to be devoted to its further enhancement. Currently, the system is able to parse around 80% of sentences in a substantial corpus of general text containing a number of distinct genres. On a random sample of 250 such sentences the system has a mean crossing bracket rate of 0.71 and recall and precision of 83% and 84% respectively when evaluated against manually-disambiguated analyses.\\n\\nINTRODUCTION\\n\\nThis work is part of an effort to develop a robust, domain-independent syntactic parser capable of yielding the unique correct analysis for unrestricted naturally-occurring input. Our goal is to develop a system with performance comparable to extant part-of-speech taggers, returning a syntactic analysis from which predicate-argument structure can be recovered, and which can support semantic interpretation. The requirement for a domain-independent analyser favours statistical techniques to resolve ambiguities, whilst the latter goal favours a more sophisticated grammatical formalism than is typical in statistical approaches to robust analysis of corpus material.\\n\\nBriscoe  Carroll (1993) describe a probablistic parser using a wide-coverage unification-based grammar of English written in the Alvey Natural Language Tools (ANLT) metagrammatical formalism (Briscoe et al., 1987), generating around 800 rules in a syntactic variant of the Definite Clause Grammar formalism (DCG, Pereira  Warren, 1980) extended with iterative (Kleene) operators. The ANLT grammar is linked to a lexicon containing about 64K entries for 40K lexemes, including detailed subcategorisation information appropriate for the grammar, built semi-automatically from a learners' dictionary (Carroll Grover, 1989). The resulting parser is efficient, constructing a parse forest in roughly quadratic time (empirically), and efficiently returning the ranked n-most likely analyses (Carroll, 1993, 1994). The probabilistic model is a refinement of probabilistic context-free grammar (PCFG) conditioning CF `backbone' rule application on LR state and lookahead item. Unification of the `residue' of features not incorporated into the backbone is performed at parse time in conjunction with reduce operations. Unification failure results in the associated derivation being assigned a probability of zero. Probabilities are assigned to transitions in the LALR(1) action table via a process of supervised training based on computing the frequency with which transitions are traversed in a corpus of parse histories.\\n\\nExperiments with this system revealed three major problems which our current research is addressing. Firstly, improvements in probabilistic parse selection will require a `lexicalised' grammar/parser in which (minimally) probabilities are associated with alternative subcategorisation possibilities of individual lexical items. Currently, the relative frequency of subcategorisation possibilities for individual lexical items is not recorded in wide-coverage lexicons, such as ANLT or COMLEX (Grishman et al., 1994). Secondly, removal of punctuation from the input (after segmentation into text sentences) worsens performance as punctuation both reduces syntactic ambiguity (Jones, 1994) and signals non-syntactic (discourse) relations between text units (Nunberg, 1990). Thirdly, the largest source of error on unseen input is the omission of appropriate subcategorisation values for lexical items (mostly verbs), preventing the system from finding the correct analysis. The current coverage--the proportion of sentences for which at least one analysis was found--of this system on a general corpus (e.g. Brown or LOB) is estimated to be around 20% by Briscoe (1994). Therefore, we have developed a variant probabilistic LR parser which does not rely on subcategorisation and uses punctuation to reduce ambiguity. The analyses produced by this parser can be utilised for phrase-finding applications, recovery of subcategorisation frames, and other `intermediate' level parsing problems.\\n\\nPART\\n\\n\\n\\nOF\\n\\n\\n\\nSPEECH TAG SEQUENCE GRAMMAR\\n\\nTEXT GRAMMAR AND PUNCTUATION\\n\\nFurther details of the text grammar are given in Briscoe  Carroll (1994, 1995). The text grammar has been tested on the Susanne corpus and covers 99.8% of sentences. (The failures are mostly text segmentation problems). The number of analyses varies from one (71%) to the thousands (0.1%). Just over 50% of Susanne sentences contain some punctuation, so around 20% of the singleton parses are punctuated. The major source of ambiguity in the analysis of punctuation concerns the function of commas and their relative scope as a result of a decision to distinguish delimiters and separators (Nunberg 1990:36). Therefore, a text sentence containing eight commas (and no other punctuation) will have 3170 analyses. The multiple uses of commas cannot be resolved without access to (at least) the syntactic context of occurrence.\\n\\nTHE INTEGRATED GRAMMAR\\n\\nDespite Nunberg's observation that text grammar is distinct from syntax, text grammatical ambiguity favours interleaved application of text grammatical and syntactic constraints. Integrating the text and the PoS sequence grammars is straightforward and the result remains modular, in that the text grammar is `folded into' the PoS sequence grammar, by treating text and syntactic categories as overlapping and dealing with the properties of each using disjoint sets of features, principles of feature propagation, and so forth. In addition to the core text-grammatical rules which carry over unchanged from the stand-alone text grammar, 44 syntactic rules (of pre- and post- posing, and coordination) now include (often optional) comma markers corresponding to the purely `syntactic' uses of punctuation.\\n\\nThe approach to text grammar taken here is in many ways similar to that of Jones (1994). However, he opts to treat punctuation marks as clitics on words which introduce additional featural information into standard syntactic rules. Thus, his grammar is thoroughly integrated and it would be harder to extract an independent text grammar or build a modular semantics. Our less-tightly integrated grammar is described in more detail in Briscoe  Carroll (1994).\\n\\nPARSING THE SUSANNE AND SEC CORPORA\\n\\nWe have used the integrated grammar to parse the Susanne corpus and the quite distinct Spoken English Corpus (SEC; Taylor  Knowles, 1988), a 50K word treebanked corpus of transcribed British radio programmes punctuated by the corpus compilers. Both corpora were retagged using the Acquilex HMM tagger (Elworthy, 1993, 1994) trained on text tagged with a slightly modified version of CLAWS-II labels (Garside et al., 1987). In contrast to previous systems taking as input fully-determinate sequences of PoS labels, such as Fidditch (Hindle, 1989) and MITFP (de Marcken, 1990), for each word the tagger returns multiple label hypotheses, and each is thresholded before being passed on to the parser: a given label is retained if it is the highest-ranked, or, if the highest-ranked label is assigned a likelihood of less than 0.9, if its likelihood is within a factor of 50 of this. We thus attempt to minimise the effect of incorrect tagging on the parsing component by allowing label ambiguities, but control the increase in indeterminacy and concomitant decrease in subsequent processing efficiency by applying the thresholding technique. On Susanne, retagging allowing only a single label per word results in a 97.90% label/word assignment accuracy, whereas multi-label tagging with this thresholding scheme results in 99.51% accuracy.\\n\\nIn an earlier paper (Briscoe  Carroll, 1995) we gave results for a previous version of the grammar and parsing system. We have made a number of significant improvements to the system since then, the most fundamental being the use of multiple labels for each word. System accuracy evaluation results are also improved since we now output trees that conform more closely to the annotation conventions employed in the test treebank.\\n\\nCOVERAGE AND AMBIGUITY\\n\\nTo examine the efficiency and coverage of the grammar we applied it to our retagged versions of Susanne and SEC. We used the ANLT chart parser (Carroll, 1993), but modified just to count the number of possible parses in the parse forests (Billot  Lang, 1989) rather than actually unpacking them. We also imposed a per-sentence time-out of 30 seconds CPU time, running in Franz Allegro Common Lisp 4.2 on an HP PA-RISC 715/100 workstation with 128 Mbytes of physical memory.\\n\\nAs the grammar was developed solely with reference to Susanne, coverage of SEC is quite robust. The two corpora differ considerably since the former is drawn from American written text whilst the latter represents British transcribed spoken material. The corpora overall contain material drawn from widely disparate genres / registers, and are more complex than those used in DARPA ATIS tests, and more diverse than those used in MUCs and probably also the Penn Treebank. Black et al. (1993) report a coverage of around 95% on computer manuals, as opposed to our coverage rate of 70-80% on much more heterogeneous data and longer sentences. The APBs for Susanne and SEC of 1.313 and 1.300 respectively indicate that sentences of average length in each corpus could be expected to be assigned of the order of 238 and 376 analyses (i.e.\\n\\n1.313[20.1] and\\n\\n1.300[22.6]).\\n\\nThe parser throughput on these tests, for sentences successfully analysed, is around 25 words per CPU second on an HP PA-RISC 715/100. Sentences of up to 30 tokens (words plus sentence-internal punctuation) are parsed in an average of under 1 second each, whilst those around 60 tokens take on average around 7 seconds. Nevertheless, the relationship between sentence length and processing time is fitted well by a quadratic function, supporting the findings of Carroll (1994) that in practice NL grammars do not evince worst-case parsing complexity.\\n\\nGrammar Development  Refinement\\n\\nSince the coverage on SEC is increasing at the same time as on Susanne, we can conclude that the grammar has not been specifically tuned to the particular sublanguages or genres represented in the development corpus. Also, although the almost-50% initial coverage on the heterogeneous text of Susanne compares well with the state-of-the-art in grammar-based approaches to NL analysis (e.g. see Taylor et al., 1989; Alshawi et al., 1992), it is clear that the subsequent grammar refinement phases have led to major improvements in coverage and reductions in spurious ambiguity.\\n\\nWe have experimented with increasing the richness of the lexical feature set by incorporating subcategorisation information for verbs into the grammar and lexicon. We constructed randomly from Susanne a test corpus of 250 in-coverage sentences, and in this, for each word tagged as possibly being an open-class verb (i.e. not a modal or auxiliary) we extracted from the ANLT lexicon (Carroll  Grover, 1989) all verbal entries for that word. We then mapped these entries into our PoS grammar experimental subcategorisation scheme, in which we distinguished each possible pattern of complementation allowed by the grammar (but not control relationships, specification of prepositional heads of PP complements etc. as in the full ANLT representation scheme). We then attempted to parse the test sentences, using the derived verbal entries instead of the original generic entries which generalised over all the subcategorisation possibilities. 31 sentences now failed to receive a parse, a decrease in coverage of 12%. This is due to the fact that the ANLT lexicon, although large and comprehensive by current standards (Briscoe  Carroll, 1996), nevertheless contains many errors of omission.\\n\\nPARSE SELECTION\\n\\nA probabilistic LR parser was trained with the integrated grammar by exploiting the Susanne treebank bracketing. An LR parser (Briscoe Carroll, 1993) was applied to unlabelled bracketed sentences from the Susanne treebank, and a new treebank of 1758 correct and complete analyses with respect to the integrated grammar was constructed semi-automatically by manually resolving the remaining ambiguities. 250 sentences from the new treebank, selected randomly, were kept back for testing. The remainder, together with a further set of analyses from 2285 treebank sentences that were not checked manually, were used to train a probabilistic version of the LR parser, using  Good-Turing smoothing to estimate the probability of unseen transitions in the LALR(1) table (Briscoe  Carroll, 1993; Carroll, 1993). The probabilistic parser can then return a ranking of all possible analyses for a sentence, or efficiently return just the n-most probable (Carroll, 1993).\\n\\nshows the results of this test--with respect to the original Susanne bracketings--using the Grammar Evaluation Interest Group scheme (GEIG, see e.g. Harrison et al., 1991). This compares unlabelled bracketings derived from corpus treebanks with those derived from parses for the same sentences by computing recall, the ratio of matched brackets over all brackets in the treebank; precision, the ratio of matched brackets over all brackets found by the parser; mean crossings, the number of times a bracketed sequence output by the parser overlaps with one from the treebank but neither is properly contained in the other, averaged over all sentences; and zero crossings, the percentage of sentences for which the analysis returned has zero crossings.\\n\\nBlack et al. (1993:7) use the crossing brackets measure to define a notion of structural consistency, where the structural consistency rate for the grammar is defined as the proportion of sentences for which at least one analysis--from the many typically returned by the grammar--contains no crossing brackets, and report a rate of around 95% for the IBM grammar tested on the computer manual corpus. However, a problem with the GEIG scheme and with structural consistency is that both are still weak measures (designed to avoid problems of parser/treebank representational compatibility) which lead to unintuitive numbers whose significance still depends heavily on details of the relationship between the representations compared (e.g. between structure assigned by a grammar and that in a treebank). One particular problem with the crossing bracket measure is that a single attachment mistake embedded n levels deep (and perhaps completely innocuous, such as an ``aside'' delimited by dashes) can lead to n crossings being assigned, whereas incorrect identification of arguments and adjuncts can go unpunished in some cases.\\n\\nSchabes et al. (1993) and Magerman (1995) report results using the GEIG evaluation scheme which are numerically similar in terms of parse selection to those reported here, but achieve 100% coverage. However, their experiments are not strictly comparable because they both utilise more homogeneous and probably simpler corpora. (The appendix gives an indication of the diversity of the sentences in our corpus). In addition, Schabes et al. do not recover tree labelling, whilst Magerman has developed a parser designed to produce identical analyses to those used in the Penn Treebank, removing the problem of spurious errors due to grammatical incompatibility. Both these approaches achieve better coverage by constructing the grammar fully automatically, but as an inevitable side-effect the range of text phenomena that can be parsed becomes limited to those present in the training material, and being able to deal with new ones would entail further substantial treebanking efforts.\\n\\nTo date, no robust parser has been shown to be practical and useful for some NLP task. However, it seems likely that, say, rule-to-rule semantic interpretation will be easier with hand-constructed grammars with an explicit, determinate rule-set. A more meaningful parser comparison would require application of different parsers to an identical and extended test suite and utilisation of a more stringent standard evaluation procedure sensitive to node labellings.\\n\\nTraining Data Size and Accuracy\\n\\nStatistical HMM-based part-of-speech taggers require of the order of 100K words and upwards of training data (Weischedel et al., 1993:363); taggers inducing non-probabilistic rules (e.g. Brill, 1994) require similar amounts (Gaizauskas, pc). Our probabilistic disambiguation system currently makes no use of lexical frequency information, training only on structural configurations. Nevertheless, the number of parameters in the probabilistic model is large: it is the total number of possible transitions in an LALR(1) table containing over 150000 actions. It is therefore interesting to investigate whether the system requires more or less training data than a tagger.\\n\\nThe results show convincingly that the system is extremely robust when confronted with limited amounts of training data: when using a mere one sixty-fourth of the full amount (59 trees), accuracy was degraded by only 10-20%. However, there is a large decrease in accuracy with no training data (i.e. random choice). Conversely, accuracy is still improving at 3800 trees, with no sign of over-training, although it appears to be approaching an upper asymptote. To determine what this might be, we ran the system on a set of 250 sentences randomly extracted from the training corpus. On this set, the system achieves a zero crossings rate of 60.0%, mean crossings 0.88, and recall and precision of 77.0% and 75.2% respectively, with respect to the original Susanne bracketings. Although this is a different set of sentences, it is likely that the upper asymptote for accuracy for the test corpus lies in this region. Given that accuracy is increasing only slowly and is relatively close to the asymptote it is therefore unlikely that it would be worth investing effort in increasing the size of the training corpus at this stage in the development of the system.\\n\\nCONCLUSIONS\\n\\nIn this paper we have outlined an approach to robust domain-independent parsing, in which subcategorisation constraints play no part, resulting in coverage that greatly improves upon more conventional grammar-based approaches to NL text analysis. We described an implemented system, and evaluated its performance along several different dimensions. We assessed its coverage and that of previous versions on a development corpus and an unseen corpus, and demonstrated that the grammar refinement we have carried out has led to substantial improvements in coverage and reductions in spurious ambiguity. We also evaluated the accuracy of parse selection with respect to treebank analyses, and, by varying the amount of training material, we showed that it requires comparatively little data to achieve a good level of accuracy.\\n\\nWe have made good progress in increasing grammar coverage, though we have now reached a point of diminishing returns. Further significant improvements in this area would require corpus-specific additions and tuning whose benefit would not necessarily carry over to other corpora. In the application we are currently using the system for--automatic extraction of subcategorisation frames, and more generally argument structure, from large amounts of text (Briscoe Carroll, 1996)--we do not need full coverage; 70-80% appears to be sufficient. However, further improvements in coverage will require some automated approach to rule induction driven by parse failure. Since our evaluations indicate that our system achieves a good level of accuracy with little treebank data, and that 67-75% coverage was achieved for English quite early in the grammar refinement effort, porting the current system to other languages should be possible with small-to-medium-sized treebanks (around 20K words) and feasible manual effort (of the order of 12 person-months for grammar-writing and treebanking). This may yield a system accurate enough for some types of application, given that the system is not restricted to returning the single highest ranked analysis but can return the n-highest ranked for further application-specific selection.\\n\\nAlthough we report promising results, parse selection that is sufficiently accurate for many practical applications will require a more lexicalised system. Magerman's (1995) parser is an extension of the history-based parsing approach developed at IBM (Black et al., 1993) in which rules are conditioned on lexical and other (essentially arbitrary) information available in the parse history. In future work, we intend to explore a more restricted and semantically-driven version of this approach in which, firstly, probabilities are associated with different subcategorisation possibilities, and secondly, alternative predicate-argument structures derived from the grammar are ranked probabilistically. However, the massively increased coverage obtained here by relaxing subcategorisation constraints underlines the need to acquire accurate and complete subcategorisation frames in a corpus-driven fashion, before such constraints can be exploited robustly and effectively with free text.\\n\\nFootnotes\\n\\nSome of this work was carried out while the second author was visiting Rank Xerox, Grenoble. The work was also supported by UK DTI/SALT project 41/5808 `Integrated Language Database', and by SERC/EPSRC Advanced Fellowships to both authors. Geoff Nunberg provided encouragement and much advice on the analysis of punctuation, and Greg Grefenstette undertook the original corpus tokenisation and segmentation for the punctuation experiments. Bernie Jones and Kiku Ribas made helpful comments on an earlier draft. We are responsible for any mistakes. Briscoe  Carroll (1995) note that ``coverage'' is a weak measure since discovery of one or more global analyses does not entail that the correct analysis is recovered. The appendix contains a random sample of sentences from the test corpus. We would like to thank Phil Harrison for supplying the evaluation software.\", metadata={'source': '../data/raw/cmplg-xml/9604004.xml'}),\n",
       " Document(page_content=\"AUTOMATED TONE TRANSCRIPTION\\n\\nINTRODUCTION\\n\\nTONE TRANSCRIPTION\\n\\nGeneration and Recognition\\n\\nIn the present context, the emphasis is not on automatic speech recognition but on a tool to support phonologists working with tone. As we shall see in the next section, once the phonologist has identified the salient location to measure the `F0 value' of a syllable (or some other phonological unit), the task will be to automatically map a string of these values to a string of tones.\\n\\nA Tool for Phonologists\\n\\nF0 SCALING\\n\\nConsider again the F0 contour in Figure 1. In particular, note that the F0 decay seems to be to a non-zero asymptote, and that H and L appear to have different asymptotes which we symbolise as hand l respectively. These observations are clearer in Figure 2, which (roughly speaking) displays the peaks and valleys from Figure 1.\\n\\nNow suppose that we have a sequence T of tones where ti is the ith tone (H or L) and a sequence X of F0 values where xi is the F0 value corresponding to ti. Then we would like a formula which predicts xi given xi-1, ti and ti-1 (i]1). We express this as follows:\\n\\nThe question, now, is what should this function look like? Suppose for sake of argument that the ratio of L to the immediately preceding H in Figure 2 is constant, with respect to the baselines for H and L, namely h and l.  Then we have:\\n\\nTONE AND F0 IN  BAMILEKE DSCHANG\\n\\nIn a recent field trip to Western Cameroon to study the Bamileke Dschang noun associative construction, I was able to collect a small amount of data relating to F0 scaling throughout a particular informant's pitch range. Following Liberman et al., voice pitch was varied by getting the informant to speak at different volumes and by adjusting the recording level appropriately. However, rather than asking the informant to imagine speaking to a subject at different distances, I controlled the volume by having the informant wear headphones and played white noise from a detuned radio. Thus, I could set the informant's voice pitch by using the volume control on my radio. My hypothesis is that this technique produces more consistent volume (and hence, pitch scaling) over long utterances and may make informants less self-conscious about speaking loudly than simply asking them to imagine speaking to subjects at various distances away. Measurements were taken from the following data.\\n\\nIMPLEMENTATIONS\\n\\nA Genetic Algorithm\\n\\nFor a cogent introduction to genetic search and an explanation of why it works, the reader is referred to South93. Before presenting the version of the algorithm used in the implementation, I shall informally define the key data types it uses along with the standard operations on those types.\\n\\nAn Annealing Algorithm\\n\\nequilibrium At each temperature, the system is required to reach `thermal equilibrium' before the temperature is lowered. In the present context, equilibrium is reached if no more than one of the last eight perturbations yielded a new state that was accepted.\\n\\nPerformance Results\\n\\nThe heavily shaded bars corresponding to evaluations less than 1 are the most important. These indicate the number of times out of 100 that the programs found a transcription with an evaluation less than 1. This evaluation means that the average of the squared difference between the predicted F0 values and the actual F0 values was less than 1Hz. Observe that the annealing search program performs significantly better in all cases. Note that the mutation operation in the genetic search program treats each bit in the parameter encodings equally, while the perturbation operation in the annealing search program is sensitive to the distinction between more significant vs. less significant bits. This may explain the better convergence behaviour of the annealing search.\\n\\nNotice also in Figure 6 that performance does not degrade with transcription length as the length doubles from 10 to 20. This is probably because a randomly generated sequence will contain downsteps on every second tone (on average) causing a general downtrend in the F0 values and severely limiting the combinatorial explosion of possible transcriptions.\\n\\nTrial 2: Artificial Data with Upstep. Trial 2 was the same as trial 1 except that this time upstep was permitted as well. The results are displayed in Figure 7. Again the annealing program fares better than the genetic program. Consider again the bars corresponding to evaluations less than 1. For both programs, however, observe that the performance degrades more uniformly than in trial 1, probably because the inclusion of upstep greatly increases the number of possible transcriptions (and hence, the number of local optima).\\n\\nPerformance results are given in Figure 8. Notice that the interpretation of the shading in this figure is different from that in previous figures. This is because evaluations near zero were less likely with real data. In fact, the annealing program never found an evaluation less than 3 while the genetic program never found an evaluation less than 4.\\n\\nSince the programs performed about equally on finding transcriptions with an evaluation less than 7, I shall display these transcriptions along with an indication of how many times each program found the transcription (G = genetic, A = annealing). I give transcriptions which occurred at least twice in one of the programs, during 100 executions of each.\\n\\nThe results from trial 1 deserve special attention. In trial 1, three transcriptions were found by both programs. The best evaluations found are given below:\\n\\nTherefore, there are encouraging signs that the program is living up to its promise of producing alternative, equally acceptable transcriptions, as desired from an analytical standpoint.\\n\\nMultiple Solutions\\n\\nAlthough we have seen more than one transcription for a given F0 sequence, it is inconvenient to be required to run the programs several times in order to see if more than one solution can be found. Furthermore, the programs are designed not to get caught in local optima, which is a problem since interesting alternative transcriptions may actually be local optima. Therefore, both programs are set up to report the k best solutions, where the user specifies the number of solutions desired. The program ensures that the same area of the search space is not re-explored by subsequent searches. This is done by defining a distance metric on transcriptions which counts the number of tones in one transcription that have to be changed in order to make it identical to the other transcription. That part of the search space within a distance of n/3 from any previously found solution is not explored again. The programs give up before finding k solutions if 5 randomly generated transcriptions all fall within distance n/3 of previous solutions.\\n\\nNow, consider the following randomly generated sequence of tones:\\n\\nThe annealing program was set the task of finding ten transcriptions of this tone sequence. The program was run only twice, and it reported the following solutions with evaluations less than or equal to 1. Both runnings of the program found the same solutions, and in the same order. (Note that two transcriptions are taken to be the same if one or both begin with an initial upstep or downstep; this has no effect on the phonetic interpretation). In the following displays, the predicted F0 values are given below each solution to facilitate comparison with the input sequence.\\n\\nSince all executions to this point have been based on the first table of R values, it was decided to try a test with the second table of R values to see if the performance was different. Interestingly, the third solution in both of the above executions was not found, though two new solutions were found.\\n\\nObserve that the value of d in the above solutions clusters around 0.66 and 0.87. Similar clustering may be occurring with the ratio h/l. However, an analysis of the relationship between the kinds of solutions found, the two R tables and the parameter values h, l and d has not been attempted.\\n\\nAreas for Further Improvement\\n\\nIt is rather unsatisfying that the performance of the two programs is heavily dependent on the setting of several search parameters, and it seems to be a combinatorial optimisation problem in itself to find good parameter settings. My trial-and-error approach will not necessarily have found optimal parameter values, and so it would be premature to conclude from the performance comparison that annealing search is better than genetic search for the problem of tone transcription. A more thoroughgoing comparison of these two approaches to the problem needs to be undertaken.\\n\\nFinally, it would be interesting to integrate a system like either of the ones presented here into a speech workstation. As the phonologist identifies salient points with a cursor the system would do the transcription, incrementally and interactively.\\n\\nCONCLUSION\\n\\nACKNOWLEDGEMENTS\\n\\nThis research is funded by the UK Economic and Social Research Council, under grant R00023 4439 A Computational Model for the Phonology-Phonetics Interface in Tone Languages. I am indebted to SIL Cameroon for their logistical support on my field trip in September and October of 1993, during which the data presented in the paper (and much other data besides) was gathered, and especially to Nancy Haynes, Gretchen Harro for helping me collect the data and Jean-Claude Gnintedem who endured many recording sessions. I am grateful to John Coleman, Michael Gasser and Marie South for helpful comments on an earlier version of this paper. The F0 data was extracted using the ESPS Waves+ package in the Edinburgh University Phonetics Laboratory.\\n\\nBibliography\\n\\nBird, S.  Stegen, O. (1993). Tone in the Bamileke Dschang Associative Construction: An Electrolaryngographic Study and Comparison with Hyman (1985). RP 57, University of Edinburgh, Centre for Cognitive Science.\\n\\nClements, G. N. (1979).\\n\\nThe description of terraced\\n\\n\\n\\nlevel tone languages.\\n\\nLanguage, 55, 536\\n\\n\\n\\n558.\\n\\nConnell, B.  Ladd, D. R. (1990). Aspects of pitch realisation in Yoruba. Phonology, 7, 1-29.\\n\\nEllison, T. M. (1992). Machine Learning of Phonological Structure. PhD thesis, University of Western Australia.\\n\\nGoldberg, D. E. (1989). Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley.\\n\\nHuang, X. D., Ariki, Y.,  Jack, M. (1990). Hidden Markov Models for Speech Recognition. Edinburgh Information Technology Series. Edinburgh University Press.\\n\\nHyman, L. M. (1979). A reanalysis of tonal downstep. Journal of African Languages and Linguistics, 1, 9-29.\\n\\nHyman, L. M. (1985). Word domains and downstep in Bamileke-Dschang. Phonology Yearbook, 2, 45-83.\\n\\nHyman, L. M.  Schuh, R. G. (1974). Universals of tone rules: evidence from West Africa. Linguistic Inquiry, 5, 81-115.\\n\\nLiberman, M., Schultz, J. M., Hong, S.,  Okeke, V. (1993). The Phonetic Interpretation of Tone in Igbo. Phonetica, 50, 147-160.\\n\\nPierrehumbert, J.  Beckman, M. (1988). Japanese Tone Structure. Cambridge Mass. : MIT Press.\\n\\nSouth, M. C., Wetherill, G. B.,  Tham, M. T. (1993). Hitch-hiker's guide to genetic algorithms. Journal of Applied Statistics, 20, 153-175.\\n\\nStewart, J. M. (1993). Dschang and Ebri as Akan-type total downstep languages. In H. van der Hulst  K. Snider (Eds. ), The Phonology of Tone - The Representation of Tonal Register  (pp. 185-244). Berlin; New York: Mouton de Gruyter. Linguistic models, Volume 17.\\n\\nvan Laarhoven, P. J. M.  Aarts, E. H. L. (1987). Simulated Annealing. Dordrecht:Reidel.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9410022.xml'}),\n",
       " Document(page_content=\"Toward an MT System without Pre-Editing -- Effects of New Methods in ALT-J/E --\\n\\nRecently, several types of Japanese-to-English machine translation systems have been developed, but all of them require an initial process of rewriting the original text into easily translatable Japanese. Therefore these systems are unsuitable for translating information that needs to be speedily disseminated. To overcome this limitation, a Multi-Level Translation Method based on the Constructive Process Theory has been proposed. This paper describes the benefits of using this method in the Japanese-to-English machine translation system ALT-J/E. In comparison with conventional compositional methods, the Multi-Level Translation Method emphasizes the importance of the meaning contained in expression structures as a whole. It is shown to be capable of translating typical written Japanese based on the meaning of the text in its context, with  comparative ease. We are now hopeful of carrying out useful machine translation with no manual pre-editing.\\n\\nIntroduction\\n\\nJapanese-to-English machine translation has reached the stage where sentences that allow word-by-word transfer from Japanese to English, followed by assembly into the final sentence form (i.e. where literal translation is possible), can be translated by current technology. But there is a wide difference in the thought process constituting the background of linguistic expression between the Japanese and English languages. Therefore, translations using existing systems require pre-editing to re-write the original Japanese sentences into a form that will enable application of the elementary compositional method, or in other words, a form that can undergo literal translation.\\n\\nThis method focuses attention on the fact that many expressions have meanings that cannot be deduced directly from the combination of the meanings of the individual words. It is a method of translation which grasps the structure and meanings of expressions as a whole. The meanings of words will vary according to the manner and context in which the words are used. Many expressions have meanings that cannot be explained directly from the meanings of each individual word. With attention focused on these characteristics, those units having structural meanings have been arranged systematically into a form of linguistic knowledge. This knowledge is being used in analysis of the Japanese language and conversion of the Japanese into English. As such, it represents a big step forward towards the fundamental solution of previously existing problems, hitherto only solvable by pre-editing.\\n\\nThe Constructive Process Theory and the Multi-Level Translation  Method The Constructive Process Theory of Language Problems of Conventional Translation Systems The Concept of the Constructive Process Theory of Language\\n\\nAccording to the Constructive Process Theory of Language, language is to be understood as a compound body of processes as in the field of physics, and can be viewed as the relationship between the `object', `(speaker's) recognition' and `expression'. The relationship between `object' and `recognition' can be explained by `Epistemology' or `Reflection Theory', and between `recognition' and `expression' by `Linguistic Norm'. The sole element that is common between two differing languages would be the `object' and since there are differences in how the `object' is viewed and understood between languages, everything beyond `recognition' will differ depending on the language in question. The very existence of `deep structure' which is neither `object' nor `recognition' is denied altogether.\\n\\nWhen language is regarded thus as a compound body of various processes, the following two points, placing importance on the meaning, are seen as important for machine translation.\\n\\n1. Expressions are classified into   `subjective' which are a direct expression of the emotions, intentions, and judgment of the speaker and `objective' which express the object in the form of a concept, and reproduce it within the framework of the target language.\\n\\n2. The structure, which involved with the object, is reflected by its recognition and this is further reflected in the structure of the expression. Therefore, the structure of an expression is to be considered as a part of its meaning, and the meaning is to be handled accordingly.\\n\\nThe Multi\\n\\n\\n\\nLevel Translation Method\\n\\nOrganization of Linguistic Knowledge\\n\\nSemantic Categories of Words\\n\\nNouns are used to express existing objects as concepts. Depending on how the object is viewed and understood, various profiles of the object are picked up or discarded. Which noun is to be used is selected based on a profile corresponding to the view of the speaker.\\n\\nIn conceiving the object, the special and individual characteristics are discarded and the features are recognized as a single unit. Among the concepts analyzing semantic features, there have been attempts to explain the meaning of nouns as a bundle of detailed meanings or features. But the concept that is represented by a noun is a single conclusive unit of recognition. It is, therefore, to be handled as an irreducible concept, that can only be captured as a whole. We classify these concepts as  SEMANTIC CATEGORIES.\\n\\nFor example, the objective concept represented by the word   school would include ``the school as an organization'' and ``the school as a given location''. In machine translation, there is a need to know which of these the word school signifies. In order to do this, thought was given to what type of profile is conceived for the object when it is used. These profiles were then classified as semantic categories held by each noun.\\n\\nAround 3,000 categories were specified, about the number of important words which the normal person feels comfortable in using. The semantic categories are ordered into two  IS-A hierarchies. These are the common noun semantic categories, some 2,800 categories (12 levels deep), and the proper noun semantic categories, some 200 categories (9 levels deep). Based on this ontology, a semantic word dictionary was compiled with 400,000 index words. The maximum number of semantic categories per word is 5 common noun categories and 10 proper noun categories. Overall, an average of 2 categories are assigned to each noun in the dictionary.\\n\\nThe Meaning of Expression Structures  as viewed from Declinable Words\\n\\nIn Japanese both verbs and adjectives are declinable. The basic structure of Japanese sentences revolves mainly around predicates. Looking at the declinable words, the meanings of the predicates themselves, and of their basic structure, can be understood by examining the types and meaning of nouns that fill the predicate's case frames. A semantic structure dictionary with some 6,000 index words (verbs and adjectives) consisting of 15,000 patterns has been prepared for use in analysis, transfer and generation.\\n\\nWith this method, analysis is performed by having units of semantics and structure correspond to one another so that ambiguity in structural analysis is reduced. Each Japanese entry has an English translation. As soon as the structure of the Japanese is determined in the source language analysis, the basic English structure can be determined from the English form structure in the semantic structure dictionary. This is helpful in avoiding the need for an additional conversion process.\\n\\nRealization of New Functions\\n\\nAmong the functions which have been realized through this method, the following  will solve problems previously requiring pre-editing.\\n\\nPrecise Selection of Translation    According to Meaning\\n\\nIt has also become possible to translate typically Japanese expressions which were previously difficult to translate into English as well as to differentiate between translation of idiomatic expressions and general expressions.\\n\\nAutomatic Re\\n\\n\\n\\nWriting Function in Japanese\\n\\nThere are many cases in which typical Japanese expressions, where two or more words are combined to form idiomatic expressions, cannot be literally translated and even if they were literally translated, would be inappropriate in the English language. It would be advantageous to have such expressions automatically converted within the system into more easily translatable Japanese. But previous attempts to do this have foundered due to the problems of unwanted side effects.\\n\\nThe Multi-Level Translation Method has enabled a precise enumeration of conditions for the application of rules through detailed semantic categories. This has enabled side effects to be reduced and effectively re-writes the Japanese prior to translation.\\n\\nSupplementation of ellipsed elements through Context Processing\\n\\nThe Japanese language normally omits elements that are easily recoverable from context, particularly subjects and objects. But in English, these elements are in most cases obligatory. Previously, supplementing these constituted an important part of pre-editing.\\n\\nALT-J/E has, in addition to the semantic structure dictionary and semantic categories, introduced an analysis of the semantic categories of predicates which allows the supplemention of ellipses using the semantic relations between sentences.\\n\\nTranslation  of Compound Words\\n\\nThe Japanese language generates new words (compound words) which are an amalgamation of a number of nouns, prefixes and suffixes (a characteristic of agglutinative languages). This type of compound word is generated without limitation and so it is impossible to have them all registered in a dictionary in advance. With conventional translation methods, registration of these compound words in the dictionary was an important issue for pre-processing.\\n\\nALT-J/E uses semantic categories to analyze compound words to find the semantic relationships of their constituents. This function makes the translation of unknown compound words possible. It also enables the automatic translation of compound words whose meanings vary  depending on the manner in which they are used within a sentence.\\n\\nThe Benefits of the Multi-Level Translation  Method and Future Issues Benefits of the Multi-Level Translation Method\\n\\nThe experimental Japanese-to-English machine translation system   ALT-J/E, based on the Multi-Level Translation Method, is currently being debugged. To examine the potential of this method, newspaper lead sentences (a summary preceding the newspaper article proper, generally consisting of 3 to 5 sentences per article, and averaging 20 words per sentence) were translated in the following experiments.\\n\\nBlind Test: (BT) Experiments conducted with articles chosen at random with no registration of unknown words, nor rule revisions.\\n\\nWindow Test: (WT) Experiments on a sample of text with revision of the system allowed. Registration of unknown words and rule revisions are conducted during the test.\\n\\n(In both cases, the original text was translated without any pre-editing)\\n\\nThe condition for a passing grade was that the meaning could be understood by looking only at the translation. Thus, sentences that were ruled as passing are not guaranteed to be stylistically appropriate (or even grammatical). But it is estimated that a quality level equal to or better than that of existing Japanese-to-English machine translation systems has been achieved.\\n\\nAccording to this test, the pass rates for the blind test were 40 to 50% , and over 60% for the window test. This indicates a passing ratio of about double that of existing Japanese-to-English machine translation systems. For tests pertaining to technical subjects (which are easier to translate than the newspaper lead sentences), a pass rate of 80% was achieved.\\n\\nBased on the above results, we judge that with the Multi-Level Translation Method, a major step toward realization of a Japanese-to-English machine translation system requiring no pre-editing has been achieved.\\n\\nFuture Issues\\n\\nThe major problem currently being faced is the need for improvement of the translation quality of long sentences (of 30 words or longer) and for the overall improvement of the English in the translated text. To meet this challenge, research efforts are presently being focussed on an extended Japanese-to-English transfer method designed to analyze the meaning of the structure of declinable words and to directly establish an appropriate English structure to correspond to this. This direct parse-tree transfer method will be adding a new path to the three transfer paths for objective expression in the Multi-Level Transfer Method, further improving and strengthening it.\\n\\nOver the long term, research efforts are being extended to include a review of the system of parts of speech in the Japanese language and to extend the semantic hierarchy to multiple dimensions.\\n\\nSummary\\n\\nThis paper has presented the results of using the Multi-Level Translation Method, based on the Constructive Process Theory. It has shown that the method enables a Japanese-to-English machine translation system to function effectively without manual pre-editing. In fact, the major reasons for pre-editing the source text are no longer valid. But there remain problems with translating typically long Japanese sentences and a need to improve the quality of finished translations.\\n\\nWe call the limited use of semantic information used in the Multi-Level Translation Method meaning analysis. It is estimated that this level of technology is limited to a maximum success rate of approximately 80%. To attain a higher level of accuracy it is essential to establish an understanding of meaning in context, based on the expansion of general and specialized knowledge of the target domains. We call this meaning comprehension. However, since it is difficult to establish the comprehension of meaning in extremely broad or general fields, it is planned to establish the limits for processing based on meaning analysis first, and then follow up with research into the area of meaning comprehension.\\n\\nAcknowledgment\\n\\nThe authors wish to thank Masahiro Miyazaki, Kentaro Ogura and other members of the research group on machine translation for their valuable contribution to discussions; and especial thanks to Francis Bond for revising this paper before archiving it.\\n\\nBibliography\\n\\nALLWOOD, J.,  L.G. ANDERSON,   O. DAHL. 1971. Logics in Linguistics. Cambridge Univ. Press.\\n\\nAUTOMATIC LANGUAGE PROCESSING ADVISORY COMMITTEE. 1966. Language and machine: Computers in translation and linguistics. Technical report, National Academy of sciences, U.S. National Research Council.\\n\\nBARWISE, J.,   J. PERRY. 1981. Situation and attotudes. Journal of Philosophy 78.668-691.\\n\\nBRESNAN, J. (ed.) 1982. The Mental Representation of Grammatical Relations. Cambridge, Mass. : MIT Press.\\n\\nCHOMSKY, NOAM. 1956. Three Models for the Description of Language, volume IT-2. IRE Trans.\\n\\nCHOMSKY, NOAM. 1965. Aspects of the Theory of Syntax. MIT Press.\\n\\nCRESSWELL, M.J. 1973. Logics and Languages. London: Methuen  Co. Ltd.\\n\\nEDR. 1990. Concept dictionary. Technical report, Japan Electronic Dictionary Research Institute, Ltd.\\n\\nFILLMORE, C.J. 1975. TOWARD A MODERN THEORY OF CASE  OTHER ARTICLES. Sanseidou Publishing. (Japanese Translation).\\n\\nIKEHARA, SATORU.\\n\\n1989.\\n\\nMulti\\n\\n\\n\\nlevel machine translation system.\\n\\nFuture Computer Systems 1.261\\n\\n\\n\\n274.\\n\\nIKEHARA, SATORU,  MASAHIRO MIYAZAKI,  SATOSHI SHIRAI,   Y. HAYASHI. 1987. Speaker's recognition and multi-level-translating method based on it. Transactions of the Information Processing Society of Japan 28. (in Japanese).\\n\\nIKEHARA, SATORU,  MASAHIRO MIYAZAKI,  SATOSHI SHIRAI,   AKIO YOKOO. 1989. An approach to machine translation method based on constructive process theory. Review of the Electrical Communications Laboratories 37.39-44.\\n\\nLANCELOT, CLAUDE,   ANTOINE ARNAULD. 1972. Grammaire generale et raisonnee, les fondements de l'art de parler. In Port Royal Grammar, ed. by Paul Reitsch. Tokyo: Taishukan Publishing. (Japanese Translation).\\n\\nMENDELSON, E.\\n\\n1979.\\n\\nIntroduction to Mathematical Logics.\\n\\nD. Van Nostrand Company.\\n\\nMIURA, T. 1967. Theory of Recognition and Languages. Keiso Shobou. (in Japanese).\\n\\nMOTOORI, NORINAGA.\\n\\n1779.\\n\\nKotoba no tama\\n\\n\\n\\nno\\n\\n\\n\\no.\\n\\nChikuma Publishing.\\n\\n(in Japanese).\\n\\nMT SUMMIT\\n\\n\\n\\nI (ed.)\\n\\n1987.\\n\\nProceedings of MT Summit\\n\\n\\n\\nI.\\n\\nMT SUMMIT\\n\\n\\n\\nII (ed.)\\n\\n1989.\\n\\nProceedings of MT Summit\\n\\n\\n\\nII.\\n\\nNAGAO, MAKOTO. 1985. Evaluation of the quality of machine-translated sentences and the control of language. Journal of the Information Processing Society of Japan 26.1197-1202.\\n\\nNOMOTO, K.\\n\\n1986.\\n\\nFrege's Philosophy of Languages.\\n\\nKeiso publishing.\\n\\n(in Japanese).\\n\\nSAUSSURE, F.D.\\n\\n1909.\\n\\nCOURS DE LINGUISTIQUE GENERALE.\\n\\nKeiso Publishing.\\n\\n(Japanese Translation).\\n\\nTOKIEDA, MOTONAGA.\\n\\n1941.\\n\\nThe Principle of Linguistics.\\n\\nIwanami Bookstore.\\n\\n(in Japanese).\\n\\nTOMABECHI, H.\\n\\n1987.\\n\\nDirect memory translation.\\n\\nIn Proceedings of IJCAI\\n\\n\\n\\n87.\\n\\nTOMITA, M.\\n\\n1987.\\n\\nAn efficient augmented\\n\\n\\n\\ncontext\\n\\n\\n\\nfree parsing algorithm.\\n\\nComputational Linguistics 13.31\\n\\n\\n\\n46.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9510008.xml'}),\n",
       " Document(page_content=\"RECOGNIZING TEXT GENRES WITH SIMPLE METRICS USING DISCRIMINANT ANALYSIS\\n\\nA simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus. Discriminant analysis makes it possible use a large number of parameters that may be specific for a certain corpus or information stream, and combine them into a small number of functions, with the parameters weighted on basis of how useful they are for discriminating text genres. An application to information retrieval is discussed.\\n\\nText Types\\n\\nThere are different types of text. Texts ``about'' the same thing may be in differing genres, of different types, and of varying quality. Texts vary along several parameters, all relevant for the general information retrieval problem of matching reader needs and texts. Given this variation, in a text retrieval context the problems are (i) identifying genres, and (ii) choosing criteria to cluster texts of the same genre, with predictable precision and recall. This should not be confused with the issue of identifying topics, and choosing criteria that discriminate one topic from another. Although not orthogonal to genre-dependent variation, the variation that relates directly to content and topic is along other dimensions. Naturally, there is co-variance. Texts about certain topics may only occur in certain genres, and texts in certain genres may only treat certain topics; most topics do, however, occur in several genres, which is what interests us here.\\n\\nDouglas Biber has studied text variation along several parameters, and found that texts can be considered to vary along five dimensions. In his study, he clusters features according to covariance, to find underlying dimensions (1989). We wish to find a method for identifying easily computable parameters that rapidly classify previously unseen texts in general classes and along a small set - smaller than Biber's five - of dimensions, such that they can be explained in intuitively simple terms to the user of an information retrieval application. Our aim is to take a set of texts that has been selected by some sort of crude semantic analysis such as is typically performed by an information retrieval system and partition it further by genre or text type, and to display this variation as simply as possible in one or two dimensions.\\n\\nMethod\\n\\nWe start by using features similar to those first investigated by Biber, but we concentrate on those that are easy to compute assuming we have a part of speech tagger (Cutting et al, 1992; Church, 1988), such as such as third person pronoun occurrence rate as opposed to 'general hedges' (Biber, 1989). More and more of Biber's features will be available with the advent of more proficient analysis programs, for instance if complete surface syntactic parsing were performed before categorization (Voutilainen Tapanainen, 1993).\\n\\nWe then use discriminant analysis, a technique from descriptive statistics. Discriminant analysis takes a set of precategorized individuals and data on their variation on a number of parameters, and works out a set discriminant functions which distinguishes between the groups. These functions can then be used to predict the category memberships of new individuals based on their parameter scores (Tatsuoka, 1971; Mustonen, 1965).\\n\\nEvaluation\\n\\n2 categories\\n\\n4 categories\\n\\n15 (or 10) categories\\n\\nValidation of the Technique\\n\\nIt is important to note that this experiment does not claim to show how genres in fact differ. What we show is that this sort of technique can be used to determine which parameters to use, given a set of them. We did not use a test set disjoint from the training set, and we do not claim that the functions we had the method extract from the data are useful in themselves. We discuss how well this method categorizes a set text, given a set of categories, and given a set of parameters.\\n\\nThe error rates climb steeply with the number of categories tested for in the corpus we used. This may have to do with how the categories are chosen and defined. For instance, distinguishing between different types of fiction by formal or stylistic criteria of this kind may just be something we should not attempt: the fiction types are naturally defined in terms of their content, after all.\\n\\nThe statistical technique of factor analysis can be used to discover categories, like Biber has done. The problem with using automatically derived categories is that even if they are in a sense real, meaning that they are supported by data, they may be difficult to explain for the unenthusiastic layman if the aim is to use the technique in retrieval tools.\\n\\nOther criteria that should be studied are second and higher order statistics on the respective parameters. Certain parameters probably vary more in certain text types than others, and they may have a skewed distribution as well. This is not difficult to determine, although the standard methods do not support automatic determination of standard deviation or skewness as discrimination criteria. Together with the investigation of several hitherto untried parameters, this is a next step.\\n\\nReadability Indexing\\n\\nNot unrelated to the study of genre is the study of readability which aims to categorize texts according to their suitability for assumed sets of assumed readers. There is a wealth of formul to compute readability. Most commonly they combine easily computed text measures, typically average or sampled average sentence length combined with similarly computed word length, or incidence of words not on a specified ``easy word list'' (Chall, 1948; Klare, 1963). In spite of Chall's warnings about injudicious application to writing tasks, readability measurement has naively come to be used as a prescriptive metric of good writing as a tool for writers, and has thus come into some disrepute among text researchers. Our small study confirms the basic findings of the early readability studies: the most important factors of the ones we tested are word length, sentence length, and different derivatives of these two parameters. As long as readability indexing schemes are used in descriptive applications they work well to discriminate between text types.\\n\\nApplication\\n\\nIn any specific application area it will be unlikely that the text database to be accessed will be completely free form. The texts under consideration will probably be specific in some way. General text types may be useful, but quite probably there will be a domain- or field-specific text typology. In an envisioned application, a user will employ a cascade of filters starting with filtering by topic, and continuing with filters by genre or text type, and ending by filters for text quality, or other tentative finer-grained qualifications.\\n\\nThe IntFilter Project\\n\\nThe IntFilter Project at the departments of Computer and Systems Sciences, Computational Linguistics, and Psychology at Stockholm University is at present studying texts on the USENET News conferencing system. The project at present studies texts which appear on several different types of USENET News conferences, and investigates how well the classification criteria and categories that experienced USENET News users report using (IntFilter, 1993) can be used by a newsreader system. To do this the project applies the method described here. The project uses categories such as ``query'', ``comment'', ``announcement'', ``FAQ'', and so forth, categorizing them using parameters such as different types of length measures, form word content, quote level, percentage quoted text and other USENET News specific parameters.\\n\\nAcknowledgements\\n\\nThanks to Hans Karlgren, Gunnel Kllgren, Geoff Nunberg, Jan Pedersen, and the Coling referees, who all have contributed with suggestions and methodological discussions.\", metadata={'source': '../data/raw/cmplg-xml/9410008.xml'}),\n",
       " Document(page_content=\"The Role of the Gricean Maxims in the Generation of Referring Expressions\\n\\nGrice's maxims of conversation [Grice 1975] are framed as directives to be followed by a speaker of the language. This paper argues that, when considered from the point of view of natural language generation, such a characterisation is rather misleading, and that the desired behaviour falls out quite naturally if we view language generation as a goal-oriented process. We argue this position with particular regard to the generation of referring expressions.\\n\\nIntroduction\\n\\nThe position taken in this paper can be summarised as follows. 1. Grice's maxims [Grice 1975] are framed as directives to the speaker, and so it is natural to consider how they might impact on the task of natural language generation ( NLG). 2. A number of the maxims can collectively be expressed by the imperative ``Don't say too much and don't say too little.'' This focusses our attention on the language generation subtask of  CONTENT DETERMINATION; and one of the more constrained and well-explored aspects of content determination is the generation of referring expressions. However, if we look at this task in detail, it becomes clear that there are problems with enforcing a literal interpretation of the maxims. 3. We review some of our previous work that has tried to address this problem, but go on to suggest a rather more radical position: that Grice's maxims are unnecessary directives from the point of view of referring expression generation, and that, provided the register and sublanguage conventions of the genre in force are conformed to, the behaviour the maxims characterise actually falls out quite naturally from viewing  NLG as a goal-oriented process.\\n\\nUnder this view, the maxims are no more than post hoc characterisations of the way language works, and their framing as directives is ultimately rather misleading.\\n\\nAdequate and Efficient Referring  Expressions Grice and Reference:  Deciding What To Say\\n\\nIt has become commonplace to view language generation as encompassing two kinds of concerns: deciding what to say, and deciding how to say it. The considerations that arise in deciding what to say are often expressed in terms that echo Grice's Maxims of Quantity: don't say too much, and don't say too little. There are many reasons why such imperatives are worth attending to in the development of algorithms to be used by computational systems which generate natural language; for example, we want to make sure that we have given the hearer the information that she needs, but we don't want to bore her with a flood of unnecessary statements. From the point of view of implicatures, however, an additional concern is that saying too much might lead the hearer to read between the lines in ways that were unintended by the underlying system. Ultimately, language generation systems should be as capable of exploiting the notion of conversational implicature as much as people do; but before we can achieve goals of that kind, it's important that we know when we are obeying the maxims. In other words, our first priority is to ensure that the generated text does not unintentionally contain false implicatures.\\n\\nThe task of generating referring expressions--and in particular, anaphoric referring expressions--provides an arena where we can move towards a more formal specification of what this really involves. Given some internal symbol that corresponds to an intended referent, the job of a referring expression generator is to determine the semantic content of a noun phrase that will identify the intended referent to the hearer. The first serious consideration of this issue in the  NLG literature was probably McDonald's [1980] discussion of  Potential distractors--other entities in the context we might mistakenly refer to--when deciding whether or not it was safe to use a pronoun to refer to an entity. Appelt [1982] and Novak [1988] looked at determining the content of definite noun phrase referring expressions in situations where (for whatever reason) a pronoun could not be used. In Dale [1989], one of us characterised the task of determining the content of a referring expression as being constrained by three Grice-like principles: the  PRINCIPLE OF SENSITIVITY, which states that the referring expression chosen should take account of the state of the hearer's knowledge; the  PRINCIPLE OF ADEQUACY, which states that the referring expression chosen should be sufficient to identify the intended referent; and the  PRINCIPLE OF EFFICIENCY, which states that the referring expression chosen should provide no more information than is necessary for the identification of the intended referent.\\n\\nAn Algorithm for Saying The Right Amount Cooperative Behaviour as an Epiphenomenon Allowing in Redundancy\\n\\nOne response to Reiter's objection is to take the view that the notion of minimality sought in the algorithm above is too strong. In subsequent work we took a step back and asked: what do people actually do when they construct referring expressions? It is very difficult to make any strong claims on the basis of the experiments that have been done, but it does seem to be the case that people do not build minimal distinguishing descriptions in the strong sense suggested above. We explored these considerations in more detail in [Dale and Reiter 1995], where we proposed a revised algorithm which is computationally efficient at the cost of producing some informational redundancy in the referring expressions it generates.\\n\\nImplicit vs Explicit Pursuance of the Maxims\\n\\nSo:  obeying the Gricean maxims looks computationally problematic, and it seems not to be what people do (with some caveats: we are assuming a literal interpretation of the maxims, and assuming that it is possible to generalise from the experimental results).\\n\\nOur early attempts to generate referring expressions (as presented, for example, in Dale [1989] and Reiter [1990]) explicitly enforced variants of the Gricean Maxim of Quantity. However, our current hypothesis is that this is in fact unnecessary. We now take the view that it is a mistake to view the Gricean Maxims as directives; they are really no more than post hoc characterisations of what is going on. They may even mislead us in the construction of mechanisms that cooperate conversationally.\\n\\nGetting the hearer to identify the intended referent r.\\n\\nThe fact that information can appear in a noun phrase for purposes other than referent identification means, of course, that the hearer has to do some work in determining what the role of each provided descriptor is. In the case of an utterance like Give me the red pen, the speaker may be providing the term red in order to distinguish the intended referent from another pen which is green. It is equally possible, though, that there is only one pen in the context, and red is included in the description because colour has special salience (it may be easier for the speaker to first look for red objects, and then find the particular red object which is a pen). Another possibility--perhaps a little tenuous in the current example, but clearly a possibility nonetheless--is that the hearer may be red-green colour blind, and the speaker is imparting additional information about the colour of the pen which the hearer may be able to make use of later. A more common clue to descriptor purpose is that fact that some properties are more likely to be used for referent identification than others. In the utterance Sit by the newly-painted table, for example, the property newly-painted could be being used to distinguish the intended referent from other tables in the context, but it is rather more likely that its purpose is to warn the hearer not to put her elbows on the table.\\n\\nIn addition to goal orientation, aspects of genre such as register and sublanguage also play an important role in determining appropriate referring expressions. In particular, whether a specific referring expression is interpreted by the hearer as being purely for identification or not may depend on the current genre. For example, in casual conversation, a hearer might interpret Give me the Staedtler pen as having some purpose beyond simple identification (perhaps informing the hearer that the speaker prefers pens made by Staedtler), since manufacturer is not a commonly used attribute in identification-only referring expressions in this genre. In an inventory-stocking context, on the other hand, Give me the Staedtler pens might be construed as purely referential, since manufacturer is often used as an identifying attribute in this genre. Consequently, an  NLG system that is generating an identification-only referring expression should if possible use only those attributes that are typically used for identification in the target genre; otherwise, false implicatures may arise. However, again we believe that there is no need to explicitly model this phenomenon as an implicature; it is sufficient to design the system so that it uses the identifying attributes preferred in its target genre (as is done via the  PREFERREDATTRIBUTES list in the algorithm of [Dale and Reiter 1995]).\\n\\nAlthough in the above examples the hearer may have to perform some potentially complex inferencing to determine what the speakers' goals are, note that there is no need for the speaker to do anything other than satisfy the list of goals using resources appropriate to the current genre. Nowhere is there an explicit attempt to adhere to the maxims.\\n\\nReassessing the Maxims\\n\\nIn the light of the above discussion, we revisit Grice's maxims in this section and comment on how each might be best interpreted in the context of natural language generation.\\n\\nThe Maxim of Quality\\n\\nTry to make your contribution one that is true. More specifically: 1. Do not say what you believe to be false. 2. Do not say that for which you lack adequate evidence. No natural language generation systems that we are aware of deliberately say things that are false:  this can happen by accident, of course, but then it is not intentional.\\n\\nAn arguable exception to this claim is the work of Jameson [1987], whose Imp system injects bias into its utterances in order to mislead; but even here this is not done by telling lies. Certainly, in principle one could construct a generation system that `lied' for purposes such as advertising or manipulation, or that produced descriptions that were `correct' relative to the hearer's knowledge even if they were untrue in the world; for example, we might want to build a system which could generate the man drinking a martini to refer to a man who was actually drinking water from a martini glass. This would require explicit programming, however; the default behaviour of all systems we are aware of is to automatically obey the Maxim of Quality.\\n\\nThe Maxim of Quantity\\n\\n1. Make your contribution as informative as is required (for the current purposes of the exchange). 2. Do not make your contribution more informative than is required. The first part of the Maxim of Quantity is automatically fulfilled by a goal-oriented system: the goal will not be satisfied until sufficient information is provided. What we should say about the second part of the maxim, however, depends on how strongly or literally we choose to interpret it. If we insist that referring expressions or other utterances contain no unnecessary words, then we will probably have to explicitly enforce this as a constraint in our NLG system; in general,  NLG systems will not automatically obey this rule. On the other hand, if we interpret the second part of the Maxim of Quality as meaning `do not go out of your way to add extra information that is not needed', then this behaviour once more comes for free with goal-orientation. Our experience suggests that, at least for the task of generating referring expressions, the second interpretation is the best one.\\n\\nThe Maxim of Relevance\\n\\nBe relevant.\\n\\nYet again, this follows directly from goal-oriented behaviour:  there is no reason why the system should consider saying something that is not relevant. It is possible, of course, that an algorithm might unintentionally include irrelevant information; this is also true of human linguistic behaviour.\\n\\nThe Maxim of Manner\\n\\nBe perspicuous. More specifically: 1. Avoid obscurity of expression. 2. Avoid ambiguity. 3. Be brief (avoid unnecessary prolixy). 4. Be orderly. These are places where an anticipation feedback mechanism of the kind proposed by Jameson and Wahlster [1982] might be appropriate:  i.e, we might like a system to subject its proposed utterance to a self-monitoring stage, to make sure that it is not ambiguous and so on. Of all the maxims, it is perhaps these (with the exception of the brevity submaxim, to which our response is the same as our response to the Maxim of Quantity) which are most amenable to explicit modelling in the generation process; but even here it is equally possible that over time we learn heuristics that do the job for us, so that the generation task more or less naturally produces results that have the required characteristics (see Levelt's [1989] comments on this as a possible characteristic of the human language production mechanism).\\n\\nExploiting Violations\\n\\nThe goal-oriented approach has the additional benefit that using violations of the maxims in order to get some other point across falls out as part of the same mechanism; again, see Jameson's [1987] work in this regard. From the point of view of the generator they are not violations at all.\\n\\nConclusions\\n\\nWe have argued that Grice's Maxims do not need to be explicitly enforced or modelled in a natural language generation system. Instead, they should be replaced by the following system construction principles.\\n\\nGrice and Generation\\n\\nA generation system should be goal-driven, and conform to the current genre. As a general architecture, this suggests a process which builds an agenda of goals, and then searches for communicative and linguistics resources in the target genre which can be used to realize these goals. There is the possibility that such a system may end up saying something beyond what was intended. That is acceptable. Minimality is not necessary; provided the information that is provided is there because it serves some purpose, hearers will not make inappropriate inferences.\\n\\nGrice and Interpretation\\n\\nAs a corollary to the goal-oriented view of generation, the hearer should assume that every informational element in the speaker's utterance is there with some intended purpose. The hearer's job is then to work out what the speaker's intended purpose is. If we are in a context, for example, where referent identification is obviously the task, and the expression contains information unnecessary for identification, the hearer must consider the possibility that this information has been provided by the speaker for some other purpose--but note that it might not be; it might have been put in to help with identification even if it turns out that the hearer did not make use of it for that purpose. Properties which are clearly not able to help us in identifying the intended referent must be doing something else. In the context of our newly-painted table example, it many cases it will be impossible to determine that something is newly-painted simply by looking; and it is unlikely that the speaker intends us to go around actually touching all the tables to identify which one has that property; so it is reasonable to assume that the property has been provided for some other purpose.\\n\\nIn Summary\\n\\nUltimately, for the generator, Grice's maxims taken collectively mean Don't include elements that don't do anything. Our position is that, under a goal-oriented view of language generation, there is no need to explicitly follow such a directive at all; the desired behaviour just falls out of the mechanism. We have argued, in the present paper, that this is true of the referring expression generation task; it remains to be seen whether the same story can be told of all language generation, and what the impact of this is on models of language understanding.\\n\\nReferences\\n\\nDouglas Appelt [1982] Planning Natural Language Utterances to Satisfy Multiple Goals. Technical Note 259, SRI International, Menlo Park, California. Robert Dale [1989] Cooking Up Referring Expressions. In Proceedings of 27th Annual Meeting of the Association for Computational Linguistics, Vancouver  BC. Robert Dale and Ehud Reiter [1995] Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions. Cognitive Science, 19(2), pp233-263. H Paul Grice [1975] Logic and conversation. In P Cole and J Morgan, editors, Syntax and Semantics: Vol 3, Speech Acts, pages 43-58. New York:  Academic Press. Anthony Jameson [1987] How to Appear to be Conforming to the Maxims Even if you Prefer to Violate Them. Chapter 2 in G Kempen (ed), Natural Language Generation, Dordrecht:  Martinis Nijhoff Publishers. Anthony Jameson and Wolfgang Wahlster [1982] User Modelling in Anaphora Generation:  Ellipsis and Definite Description. In Proceedings of the Fifth European Conference on Artificial Intelligence, Pisa, Italy. Willem Levelt [1989] Speaking:  From Intention to Articulation. Cambridge,  MA:  MIT Press. David McDonald [1980] Natural Language Generation as a Process of Decision Making Under Constraints. PhD Thesis,  MIT. Hans-Joachim Novak [1988] Generating Referring Phrases in a Dynamic Environment. Chapter 5 in M Zock and G Sabah (eds), Advances in Natural Language Generation, Volume 2, pp76-85. Pinter Publishers.\\n\\nEhud Reiter [1990] Generating Appropriate Natural Language Object Descriptions. PhD Thesis, Harvard University. Jacques Robin [1994] Revision-Based Generation of Natural Language Summaries Providing Historical Background. PhD Thesis, Columbia University.\\n\\nFootnotes\\n\\nWe take no particular stance in the present discussion as to how this distinction impacts on the modularity of the architecture of a language generation system. This example is due to Bonnie Webber. Returning to our earlier red pen example, suppose the hearer just happens to be focussed on the red pen when the utterance is produced. Then, the property of redness may have been included by the speaker in order to help the hearer identify the intended referent even if it is not used for this purpose.\", metadata={'source': '../data/raw/cmplg-xml/9604006.xml'}),\n",
       " Document(page_content=\"SEMHE: A Generalised Two\\n\\n\\n\\nLevel System\\n\\nThis paper presents a generalised two-level implementation which can handle linear and non-linear morphological operations. An algorithm for the interpretation of multi-tape two-level rules is described. In addition, a number of issues which arise when developing non-linear grammars are discussed with examples from Syriac.\\n\\nIntroduction\\n\\nThe past decade has seen a number of proposals for handling non-linear morphology; however, none (apart from Beesley's work) seem to have been implemented over large descriptions, nor have they provided means by which the grammarian can develop non-linear descriptions using higher level notation.\\n\\nTo test the validity of one's proposal or formalism, minimally a medium-scale description is a desideratum. SemHe fulfils this requirement. It is a generalised multi-tape two-level system which is being used in developing non-linear grammars.\\n\\nLinguistic Descriptions\\n\\nThe linguist provides SemHe with three pieces of data: a lexicon, two-level rules and word formation grammar. All entries take the form of Prolog terms. (Identifiers starting with an uppercase letter denote variables, otherwise they are instantiated symbols.) A lexical entry is described by the term\\n\\nA lexical string maps to a surface string iff (1) they can be partitioned into pairs of lexical-surface subsequences, where each pair is licenced by a rule, and (2) no partition violates an obligatory rule.\\n\\nAlphabet declarations take the form tl_alphabet( tape, symbol_list), and variable sets are described by the predicate tl_set(id, symbol_list). Word formation rules take the form of unification-based CFG rules,\\n\\nThe following example illustrates the derivation of Syriac /ktab/ `he wrote' (in the  simple p`al measure) from the pattern morpheme {cvcvc} `verbal pattern', root {ktb} `notion of writing', and vocalism {a}. The three morphemes produce the underlying form */katab/, which surfaces as /ktab/ since short vowels in open unstressed syllables are deleted. The process is illustrated in +1.\\n\\nThe pa``el measure of the same verb, viz. /katteb/, is derived by the gemination of the middle consonant (i.e. t) and applying the appropriate vocalism {ae}.\\n\\nImplementation\\n\\nInternal Representation\\n\\nTwo-level predicates are converted into an internal representation: (1) every left-context expression is reversed and appended to an uninstantiated tail; (2) every right-context expression is appended to an uninstantiated tail; and (3) each rule is assigned a 6-bit `precedence value' where every bit represents one of the six lexical and surface expressions. If an expression is not an empty list (i.e. context is specified), the relevant bit is set. In analysis, surface expressions are assigned the most significant bits, while lexical expressions are assigned the least significant ones. In generation, the opposite state of affairs holds. Rules are then reasserted in the order of their precedence value. This ensures that rules which contain the most specified expressions are tested first resulting in better performance.\\n\\nThe Interpreter Algorithm\\n\\nIn order to minimise accumulator-passing arguments, we assume the following initially-empty stacks: ParseStack accumulates the category structures of the morphemes identified, and FeatureStack maintains the rule features encountered so far. (`+' indicates concatenation.)\\n\\nPARTITION partitions a two-level analysis into sequences of lexical-surface pairs, each licenced by a rule. The base case of the predicate is given in Listing 2, and the recursive case in Listing 3.\\n\\nThe recursive  COERCE predicate ensures that no partition is violated by an obligatory rule. It takes three arguments: Result is the output of  PARTITION (usually reversed by the calling predicate, hence,  COERCE deals with the last partition first), PrevCats is a register which keeps track of the last morpheme category encountered, and Partition returns selected elements from Result. The base case of the predicate is simply COERCE([], _, []) - i.e., no more partitions. The recursive case is shown in Listing 4. CurrentCats keeps track of the category of the morpheme which occures in the current partition. The invalidity of a partition is determined by INVALID-PARTITION (Listing 5).\\n\\nTWO-LEVEL-ANALYSIS (Listing 6) is the main predicate. It takes a surface string or lexical string(s) and returns a list of partitions and a morphosyntactic parse tree. To analyse a surface form, one calls  TWO-LEVEL-ANALYSIS(+Surf, -Lex, -Partition, -Parse). To generate a surface form, one calls TWO-LEVEL-ANALYSIS(-Surf, +Lex, -Partition, -Parse).\\n\\nDeveloping Non\\n\\n\\n\\nLinear Grammars\\n\\nWhen developing Semitic grammars, one comes across various issues and problems which normally do not arise with linear grammars. Some can be solved by known methods or `tricks'; others require extensions in order to make developing grammars easier and more elegant. This section discuss issues which normally do not arise when compiling linear grammars.\\n\\nLinearity vs. Non\\n\\n\\n\\nLinearity\\n\\nWe resolve this by allowing the user to write expansion rules of the from\\n\\nVocalisation\\n\\nMorphosyntactic Issues\\n\\nStems and\\n\\n\\n\\nTheory\\n\\nA pattern, a root and a vocalism do not alway produce a free stem which can stand on its own. In Syriac, for example, some verbal forms are bound: they require a stem morpheme which indicates the measure in question, e.g. the prefix {a} for af`el stems. Additionally, passive forms are marked by the reflexive morpheme {et}, while active forms are not marked at all.\\n\\nNow free stems which may stand on their own can be assigned =0. However, some stems require verbal inflectional markers.\\n\\nVerbal Inflectional Markers\\n\\nWith respect to verbal inflexional markers (VIMs), there are various types of Semitic verbs: those which do not require a VIM (e.g. sing. 3rd masc. ), and those which require a VIM in the form of a prefix (e.g. perfect), suffix (e.g. some imperfect forms), or circumfix (e.g. other imperfect forms).\\n\\nEach VIM is lexically marked inter alia with two features: `type' which states whether it is a prefix or a suffix, and `circum' which denotes whether it is a circumfix. Rules 5-8 (Listing 8) handle this.\\n\\nInterfacing with a Syntactic Parser Performance\\n\\nTable 1 provides the results of analysing verbal classes. The test for each class represents analysing most of its inflexions. The test was executed on a Sparc ELC computer.\\n\\nBy constructing a corpus which consists only of the most frequent words, one can estimate the performance of analysing the corpus as follows,\\n\\nConclusion\\n\\nFor useful results, a Semitic morphological analyser needs to interact with a syntactic parser in order to resolve ambiguities. Most non-vocalised strings give more than one solution, and some inflectional forms are homographs even if fully vocalised (e.g. in Syriac imperfect verbs: sing. 3rd masc. = plural 1st common, and sing. 3rd fem. = sing. 2nd masc.). We mentioned earlier the possibility of using TAGs.\\n\\nBibliography\\n\\nAho, A. and Ullman, J. (1977). Principles of Compiler Design. Addison-Wesley.\\n\\nAntworth, E. (1990). PC-KIMMO: A two-Level Processor for Morphological Analysis. Occasional Publications in Academic Computing 16. Summer Institute of Linguistics, Dallas.\\n\\nBear, J. (1986). A morphological recognizer with syntactic and phonological rules. In COLING-86, pages 272-6.\\n\\nBeesley, K. (1990). Finite-state description of Arabic morphology. In Proceedings of the Second Cambridge Conference: Bilingual Computing in Arabic and English.\\n\\nBeesley, K. (1991). Computer analysis of Arabic morphology. In Comrie, B. and Eid, M., editors, Perspectives on Arabic Linguistics III: Papers from the Third Annual Symposium on Arabic Linguistics. Benjamins, Amsterdam.\\n\\nBeesley, K., Buckwalter, T., and Newton, S. (1989). Two-level finite-state analysis of Arabic morphology. In Proceedings of the Seminar on Bilingual Computing in Arabic and English. The Literary and Linguistic Computing Centre, Cambridge.\\n\\nBird, S. and Ellison, T. (1994). One-level phonology. Computational Linguistics, 20(1):55-90.\\n\\nCarter, D. (1995). Rapid development of morphological descriptions for full language processing systems. In EACL-95, pages 202-9.\\n\\nGoldsmith, J. (1976). Autosegmental Phonology. PhD thesis, MIT. Published as Autosegmental and Metrical Phonology, Oxford 1990.\\n\\nGrimley-Evans, E., Kiraz, G., and Pulman, S. (1996). Compiling a partition-based two-level formalism. In COLING-96. Forthcoming.\\n\\nJoshi, A. (1985). Tree-adjoining grammars: How much context sensitivity is required to provide reasonable structural descriptions. In Dowty, D., Karttunen, L., and Zwicky, A., editors, Natural Language Parsing. Cambridge University Press.\\n\\nKarttunen, L. (1983). Kimmo: A general morphological processor. Texas Linguistic Forum, 22:165-86.\\n\\nKarttunen, L. (1993). Finite-state lexicon compiler. Technical report, Palo Alto Research Center, Xerox Corporation.\\n\\nKarttunen, L. and Beesley, K. (1992). Two-level rule compiler. Technical report, Palo Alto Research Center, Xerox Corporation.\\n\\nKataja, L. and Koskenniemi, K. (1988). Finite state description of Semitic morphology. In COLING-88, volume 1, pages 313-15.\\n\\nKay, M. (1987).\\n\\nNonconcatenative finite\\n\\n\\n\\nstate morphology.\\n\\nIn EACL\\n\\n\\n\\n87, pages 2\\n\\n\\n\\n10.\\n\\nKiraz, G. (1994a). Automatic concordance generation of Syriac texts. In Lavenant, R., editor, VI Symposium Syriacum 1992, Orientalia Christiana Analecta 247, pages 461-75. Pontificio Institutum Studiorum Orientalium.\\n\\nKiraz, G. (1994b). Lexical Tools to the Syriac New Testament. JSOT Manuals 7. Sheffield Academic Press.\\n\\nKiraz, G. (1994c). Multi-tape two-level morphology: a case study in Semitic non-linear morphology. In COLING-94, volume 1, pages 180-6.\\n\\nKiraz, G. (1995). Introduction to Syriac Spirantization. Bar Hebraeus Verlag, The Netherlands.\\n\\nKiraz, G. (1996). Computational Approach to Non-Linear Morphology. PhD thesis, University of Cambridge.\\n\\nKnuth, D. (1973). The Art of Computer Programming, volume 3. Addison-Wesley.\\n\\nKornai, A. (1991).\\n\\nFormal Phonology.\\n\\nPhD thesis, Stanford University.\\n\\nKoskenniemi, K. (1983). Two-Level Morphology. PhD thesis, University of Helsinki.\\n\\nLavie, A., Itai, A., and Ornan, U. (1990). On the applicability of two level morphology to the inflection of Hebrew verbs. In Choueka, Y., editor, Literary and Linguistic Computing 1988: Proceedings of the 15th International Conference, pages 246-60.\\n\\nMcCarthy, J. (1981). A prosodic theory of nonconcatenative morphology. Linguistic Inquiry, 12(3):373-418.\\n\\nNarayanan, A. and Hashem, L. (1993). On abstract finite-state morphology. In EACL-93, pages 297-304.\\n\\nPulman, S. and Hepple, M. (1993). A feature-based formalism for two-level phonology: a description and implementation. Computer Speech and Language, 7:333-58.\\n\\nRitchie, G., Black, A., Russell, G., and Pulman, S. (1992). Computational Morphology: Practical Mechanisms for the English Lexicon. MIT Press, Cambridge Mass.\\n\\nRuessink, H. (1989). Two level formalisms. Technical Report 5, Utrecht Working Papers in NLP.\\n\\nShieber, S. (1986). An Introduction to Unification-Based Approaches to Grammar. CSLI Lecture Notes Number 4. Center for the Study of Language and Information, Stanford.\\n\\nSproat, R. (1992).\\n\\nMorphology and Computation.\\n\\nMIT Press, Cambridge Mass.\\n\\nWiebe, B. (1992). Modelling autosegmental phonology with multi-tape finite state transducers. Master's thesis, Simon Fraser University.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9604012.xml'}),\n",
       " Document(page_content=\"How much is enough? : Data requirements for statistical NLP\\n\\nFeeding training data to statistical representations of language has become a popular past-time for computational linguists, but our understanding of what constitutes a sufficient volume of data remains shadowy. For example, Brown et al. (1992) used over 500 million words of text to train their language model. Is this enough? Could devouring even more data further improve the accuracy of the model learnt? In this paper I explore a number of issues in the analysis of data requirements for statistical NLP systems. A framework for viewing such systems is proposed and a sample of existing works are compared within this framework. Finally, the first steps toward a theory of data requirements are made by establishing an upper bound on the expected error rate of a class of statistical language learners as a function of the volume of training data.\\n\\nIntroduction\\n\\nStatistical approaches to natural language processing are becoming increasingly popular, being applied to a wide variety of tasks. For example, Weischedel et al. (1993) explores part-of-speech tagging, parsing and acquisition of lexical frames. Nonetheless, all these tasks share some important characteristics, not the least of which is the requirement for a sizable corpus of training data. One question which has largely been ignored is how much data is enough? For example, given a limited body of training data, it is essential to know which statistical NLP methods are likely to be accurate before pursuing any one. Also, given a particular method, when will acquiring further training data cease to improve the system accuracy? Currently, the field is conspicuously lacking a general theory of data requirements for statistical NLP.\\n\\nStatistical NLP systems are designed to make choices; hopefully in an informed manner. To do this they use indicators, upon which their choices are conditioned. The purpose of computing statistics is to inductively establish the relationship between the indicators and the choices to be made. Consider for example a next word predictor which attempts to predict the next word on the basis of the preceding word. To do this it must have an understanding of the relationship  between the indicator (the preceding word) and the choice (the next word). It is possible to acquire this understanding by computing statistics over a large corpus, a process called training. Once trained, the system may then be applied to a new text and its accuracy evaluated. This paper is concerned with the dependence of a system's accuracy on the size of the training corpus. In the following section, the notions of indicators, choices and training data will be made more formal.\\n\\nStatistical Processors\\n\\nA Framework\\n\\nA statistical NLP system deals with a certain linguistic universe. Formally, there is a set of linguistic events\\n\\nfrom which every training example and every test instance will be drawn. In the next word predictor, this need only be the set of all pairs of words which may be adjacent in text.\\n\\nLet V be a finite set of values that we would like to assign to a given linguistic input. This defines the range of possible answers that the analyser can make. In the predictor, this is the set of words plus an end of sentence symbol. Let\\n\\nbe the random variable describing the distribution of values that linguistic events take on. We also require a set of indicators, B, to use in selecting a value for a given linguistic event. I will refer to each element of B as a bin. In the predictor, the set of bins is the set of words plus a start of sentence symbol. Let\\n\\nbe the random variable describing the distribution of bins into which linguistic events fall.\\n\\nThe task of the analyser is to choose which value is most likely given only the indicator. Therefore, it is a function\\n\\nThe task of the learning algorithm is to acquire this function by computing statistics on the training set.\\n\\nPutting these components together, we can define a Statistical Processor, S as a tuple\\n\\nB and V are finite sets, the bins and values respectively\\n\\nA is the trained analysis function\\n\\nAmongst all such statistical processors, there is a special class in which we are interested. Define a probabilistic analyser to be a statistical processor which computes a function\\n\\nsuch that\\n\\nfor all\\n\\nand then computes A as:\\n\\nThe problem of acquiring A is thus transformed into one of estimating the function p using the training corpus. Generally, p(b,v) is viewed as an estimate of the probability\\n\\n.\\n\\nTraining Data\\n\\nFormally, a training corpus, c, of m instances, is an element from\\n\\nwhere each pair (b,v) is sampled according to the random variables I and J from\\n\\n. For probabilistic analysers, there are a variety of methods by which an appropriate function p can be estimated from a corpus; one simple example being the Maximum Likelihood Estimate. Regardless of the learning algorithm used, each possible training corpus, c, results in the acquisition of some function, Pc. Our aim is to explore the dependence of the expected accuracy of Pc on the magnitude of m.\\n\\nSurprisingly, it is not always obvious how many training instances have been used to train a statistical method. It is not generally sufficient to report the size of the corpus in words. A system which collects word associations using a window of cooccurrence 10 words wide will find 819 instances in a 100 word corpus, while one collecting the objects of the preposition on from the same corpus, would most likely find only a few instances. Therefore, before any conclusions can be drawn about data requirements, the training corpus must be measured in terms of instances.\\n\\nEach of these instance falls into a particular bin by virtue of its associated indicator. In choosing the indicators, we have implicitly defined equivalence classes for instances. The statistical processor will treat every instance in a bin identically. Further, once the bins are chosen, the greater the number of training instances that fall into a bin, the greater our confidence in the statistical inference made by the processor for test cases in that bin. For instance, the next word predictor is more likely to be correct when the preceding word is common than when it is a rare word.\\n\\nIt is not always obvious how many bins a given statistical method employs. Often multiple indicators are used. For instance, a trigram tagger uses the tags of the two preceding words and the current word to choose a new tag. In this case,\\n\\nwhere T is the tagset and W is the vocabulary.\\n\\nThis example demonstrates an important point. By choosing to take into account the tags of two preceding words, the trigram tagger requires |T| times as many bins as a bigram tagger (where\\n\\n). With more bins, the trigram tagger is sensitive to a broader range of context and thus can in principle achieve a greater accuracy. However, because there are more bins, there are fewer training instances in each bin. Thus, statistical estimation will be less accurate. In practice, high accuracy requires at least a few training instances per bin. Thus increasing the number of indicators may actually decrease the overall accuracy.\\n\\nFor probabilistic analysers it is useful to define the number of slots, L, to be\\n\\n|B|(|V|-1), which is the number of independent parameters needed to define the function p.\\n\\nError Rates and Optimality\\n\\nFor any non-trivial general statistical processor the indicators used cannot perfectly represent the entire linguistic event space. Thus, in general there exist values\\n\\n,\\n\\nfor which\\n\\nboth\\n\\nand\\n\\nfor some\\n\\n. Suppose without loss of generality that\\n\\nA(b) = v1. The analyser will be in error with probability at least\\n\\n. This is the root of a rather difficult problem in statistical NLP because no matter how inaccurate a trained statistical processor is, the inaccuracy may be due to the imperfect representation of\\n\\nby B.\\n\\nProbabilistic analysers always select just one value for each bin, the one which maximises p.  Let\\n\\nThis is the probability of the analyser being in error on a randomly selected element of\\n\\n.\\n\\nLet\\n\\nbe any function which minimises the expected error rate and\\n\\n.\\n\\nGiven B and V,\\n\\nis the smallest possible expected error rate. Any probabilistic analyser which achieves an accuracy close to this is unlikely to benefit from further training data.\\n\\nUnless large volumes of manually annotated data exist, measuring the size of\\n\\nin any given statistical processor presents a difficult challenge. Hindle and Rooth (1993) have attempted a similar task using human subjects on the problem of prepositional phrase attachment. Subjects were given only the preposition and the preceding verb and noun and then were asked to select the attachment. This was precisely the task facing their statistical processor. The subjects could only perform the attachment correctly in around 86% of cases. If we assume that the subjects incorrectly analysed the remaining 14% of cases because these cases depended on knowledge of the wider context, then any statistical learning algorithm based only on these indicators cannot do better than 86%. Of course, if there is insufficient training data the system may do considerably worse.\\n\\nAssuming that human performance on the task accurately reflects the value of\\n\\nis the only means known at present to estimate the value of\\n\\n. Unfortunately, this approach is expensive to apply and makes a number of questionable psychological assumptions. For example, it assumes that humans can accurately reproduce parts of their language analysis behaviour on command. It may also suffer when representational aspects of the analysis task cannot be explained easily to experimental subjects. A worthwhile goal for future research is to establish a statistical method for estimating or bounding\\n\\nusing\\n\\nlanguage data.\\n\\nStatistical Learning\\n\\nExisting Methods\\n\\nIn this section, I show how a number of existing statistical NLP works fit into the framework, including a tagger, a sense disambiguator and three syntactic analysers. For each, I consider how the various elements of the general statistical processor are instantiated.\\n\\nWeischedel et al. (1993) uses (among other experiments) a trigram hidden Markov model to tag text for part of speech. The training data is four million words of the University of Pennsylvania Treebank, tagged with a set of 47 different tags. I shall regard B as consisting of the two previous tags (\\n\\n), while V is simply the tagset. The system also takes into account lexical tag frequencies (that is,\\n\\n). I will assume however that data sparseness does not affect the lexical tag frequency estimates. Since the trigram estimates and the lexical tag frequencies are combined as independent factors, ignoring the lexical component does not seem unreasonable. The situation is further complicated because probability is maximised over a sequence of words, rather than for a single word. The framework needs to be extended to capture these mechanisms, but for the moment the approximations I have made may be useful. Since every word in the corpus (bar the first two) is used for training, we have m=4 million and\\n\\nThe accuracy is reported to be around 97%, which is approximately the accuracy of human taggers using the whole context.\\n\\nYarowsky (1992) describes a sense disambiguation system which uses a 100 word window of cooccurrences. He uses a mutual information-like measure which combines the cooccurrence statistics for all words in each category of Roget's thesaurus. The result is a profile of contexts for a category which can be used to estimate how likely each category is within a certain context. Comparing the different possible categories for the word provides a broad sense discrimination. The training corpus is Grolier's encyclopedia which contains on the order of 10 million words. Each of these provides 100 training instances (every other word in the window), so\\n\\nbillion. Since the evidence from each word in the context is combined independently, it is reasonable to regard B as simply the set of distinct words in Grolier's. Again, further work is needed to make this approximation unnecessary. V is the set of Roget's categories (\\n\\n|V| = 1042), so assuming the vocabulary is around 100,000,\\n\\nmillion. The average accuracy reported is 92%.\\n\\nHindle and Rooth (1993) propose a system to syntactically disambiguate prepositional phrase attachments. Unambiguous examples of attachments are used to find lexical associations (a likelihood ratio) between prepositions and the nouns or verbs they attach to. They cyclically apply this technique, adding disambiguated attachments into the training set, until all the training data (ambiguous or not) has been used. This approach can be approximated by a probabilistic analyser. Each association value is ascribed to a pair (w,p) where w is a verb or noun and p is a preposition. Thus B is a product of two indicator spaces: the set of verbs and nouns and the set of prepositions. Assuming they used 10,000 nouns and verbs (5,000 of each) and 100 prepositions, |B| = 1 million. The analyser computes a probability for each of two possible attachments, nominal and verbal, so V is binary. The training set consists of 754,000 noun attachments and 468 thousand verb ones giving m = 1.22 million. The accuracy reported is close to 80%, while human subjects given the same indications could achieve 85-88% accuracy. If the latter figure reflects the optimal error rate, it appears there is still room for improvement by adding training data or changing the statistical measures.\\n\\nLauer (1994) describes a system for syntactically analysing compound nouns. Two-word compounds extracted from Grolier's encyclopedia were used to measure mutual information between every pair of thesaurus categories (using Roget's thesaurus) and the results used to select a bracketing for three-word compounds. Since an association value is computed for every pair of thesaurus categories, |B|is equal to\\n\\n. There are only two possible bracketings to choose from, so again V is binary. The training corpus consists of about 35,000 two-word compounds, giving\\n\\nm = 35,000 and\\n\\nmillion. The accuracy reported is 75%.\\n\\nResnik and Hearst (1993) aim to enhance Hindle and Rooth's (1993) work by incorporating information about the head noun of the prepositional phrase in question. Thus B is now a product of three spaces: the set of nouns and verbs, the set of prepositions and the set of nouns. To reduce the data requirement, a freely available on-line thesaurus, called WordNet is used (Beckwith et al., 1991). WordNet groups words into synsets, categories of synonymous words. These synsets are arranged in a taxonomy, so that every word is also provided with a list of hypernyms. The system then adds together the frequency counts for nouns within a synset, providing more data about each. This reduces the number of bins, since it is the synsets which are taken as indicators rather than individual words. If we assume roughly 1000 synsets, 1000 verbs and 100 prepositions, then\\n\\n|B| =  200 million. V is still binary. Their training corpus is an ``order of magnitude smaller than'' Hindle and Rooth's, so m is around 100,000. Unlike Hindle and Rooth's, their corpus is parsed, which should give better results. Interestingly, they combine evidence from large groups of synsets within WordNet's hypernym hierarchy using a t-test. This causes the effective number of synsets for nouns to be reduced, perhaps by as much as a factor of 10 (thus\\n\\nmillion). I will therefore assume that\\n\\nmillion. Even given the additional information about the head noun of the prepositional phrase, the accuracy reported fails to improve on that of Hindle and Rooth, being 78%. It is possible that insufficient training data is the cause of this shortfall.\\n\\nAn Important Trade\\n\\n\\n\\noff\\n\\nThe model formulated above and the empirical data presented support a number of qualitative inferences about the potential of systems given a fixed training set size. Because training data will always be limited, such reasoning is an important part of system design. Therefore before turning to some quantitative analyses, I will examine a few such inferences.\\n\\nThe most important of these is in regard to linguistic sophistication, that is the degree to which the system uses knowledge of the patterns of language. This kind of knowledge is extremely important, since it often allows just the right distinctions to be made. More simplistic systems will inevitably assign one choice to two different inputs because their linguistic knowledge fails to support a distinction. Therefore, it seems desirable to incorporate as much linguistic sophistication as possible. While this is a tempting direction to take for improving system performance, there is a barrier.\\n\\nConsider, for instance, the effect on data requirements of incorporating new indicators. Each indicator increases the number of distinctions which the system can make. For example, Resnik and Hearst (1993) take into account the object of the preposition. In doing so, they distinguish cases which Hindle and Rooth (1993) did not. As a result, the number of cases their system considers is substantially larger than those considered by Hindle and Rooth's. In terms of the framework, Resnik and Hearst have many more bins than Hindle and Rooth.\\n\\nIt is easy to see that incorporating a new indicator increases the number of bins combinatorially. The size of B is multiplied by the range of the new indicator. This results in the ratio m:Lfalling by the same factor, which, as I have argued above, can be detrimental to the overall accuracy.\\n\\nThe situation is worse still if the training set is not hand annotated. In this case, introducing the new indicator creates additional ambiguity in the training set since the value of the new indicator must be determined for each training example. This effectively decreases the number of training instances resulting in a further decrease in m:L.\\n\\nThus, linguistic sophistication presents a trade-off between accuracy and data sparseness. It is a balance between poor modeling of the language and insufficient data for accurate statistics. If we are to strike a satisfactory compromise, we need a strong theory of data requirements and ways to make more economic use of data.\\n\\nOne such method is termed conceptual association, as defined in Resnik and Hearst (1993). By collecting statistics based on concepts rather than individual words, the number of bins is usually reduced. The idea is to generalise findings about words to cover other words which have the same meaning. The advantages of this approach are extensively argued in Resnik (1993) and the method is used in Lauer (1994). While concepts can help, the ambiguity introduced (namely what concept does a given word belong to) may undermine the increased accuracy. Further work is needed to establish the effects on data requirements of employing this strategy.\\n\\nA novel extension to this approach that has not yet been employed, would be to collect statistics at various levels of granularity. Statistics computed on counts of individual words would provide fine sensitivity, while statistics computed on counts of a small set of semantic primitives (such as ANIMATE, ABSTRACT, etc.) would provide the coarsest evidence. As many levels as desired between these two extremes could be employed in this way. The level used to make each choice could then be selected according to the degree of confidence available at each level. If insufficient data has been seen to allow a confident selection at one level, a coarser grained level would be tried. Resnik and Hearst (1993) seem to be simulating this when they perform a t-test across all levels of the WordNet hierarchy.\\n\\nFirst Steps Towards a Theory\\n\\nA Simple Learning Scheme\\n\\nLet\\n\\n, the training instances in a corpus c that fall into bin b.\\n\\nLet\\n\\n, the frequency of the value v in the set of training instances, t.\\n\\nLet\\n\\n, the most common value for instances from a corpus c in a bin b. Where several values have equal frequencies, one should be chosen at random.\\n\\nDefine the learning algorithm such that:\\n\\nSince each bin has only one value with non-zero probability, V is effectively a binary set (either the instance has the non-zero value or it does not). Thus, L = |B|. Notice also that the value assigned highest probability by Pc is the one most frequently falling into the bin. That is,\\n\\n.\\n\\nTwo possible cases arise when the analyser is faced with making a decision on the basis of some indication, b. Either the corpus contains no occurrences of (b, v) for any value\\n\\n(Case A) or there is some training data which falls into the bin (Case B).\\n\\nEmpty Bins\\n\\nCase A arises when none of the training instances fall into the bin. Let pb denote\\n\\nNon\\n\\n\\n\\nempty Bins\\n\\nIn Case B, we have at least one instance in the corpus for the given bin. Let\\n\\nSkewed Bins\\n\\nAn obvious question is why systems such as Lauer (1994) and Resnik and Hearst (1993) work at all given that far less than one instance is expected for each slot. One possible answer is that different bins have widely differing frequencies. The system quickly learns about the most frequent cases at the expense of less frequent ones.\\n\\nThis can be modeled by considering the different distributions, pb defined for Case A above. In that analysis, the probability of encountering an empty bin was maximised over all possible distributions. However, if something is known about the distribution, in principle a tighter bound is possible. For example, suppose some fraction of the bins have very low probability. That is,\\n\\nsuch that\\n\\nfor some small c.\\n\\nLet\\n\\nand\\n\\n.\\n\\nNow:\\n\\nNow the second term is maximised when\\n\\nSo letting\\n\\n:\\n\\nSo knowing a pair of values c and\\n\\nis a useful, if primitive, means of lowering the upper bound on data requirements. Since the distribution of bins does not depend on the values we are seeking to learn, it should be possible to develop simple techniques for estimating values of c and\\n\\n.\\n\\nFuture Work\\n\\nA great deal of work remains to be done. I will mention only a few directions where the work begs to be extended. First, the mathematical model doesn't capture several aspects of existing models, such as maximising probabilities over sequences of words and combining evidence from multiple sources. Second, the simple learning algorithm presented differs from those used in practice in several ways. It would be useful to explore the relationship between the algorithm I have proposed and others in existing statistical methods (for example, the backing off method in Katz, 1987). Third, smoothing is frequently used to alleviate data sparseness (see Dagan et al., 1993), but the model does not include any means to represent the process of smoothing. Finally, almost all statistical NLP systems deal with some noise in the training data. This is especially important in systems like Yarowsky (1992) where training is unsupervised. The mathematical results need to be extended to reflect noisy training data and to support reasoning about the sensitivity of data requirements to noise.\\n\\nConclusion\\n\\nIn this paper I have indicated the lack of a general theory of data requirements within the field of statistical NLP. As a first step in the development of such a theory I have presented a framework for statistical NLP systems. I have shown how several prominent works in the field fit this model and demonstrated a number of mathematical results which support inferences about data requirements. I believe this represents a significant first step along the road to a better understanding of when and how statistical NLP methods can be applied.\\n\\nAcknowledgments\\n\\nWithout Mark Johnson's interest and collaboration, this paper would not exist. Many of the key ideas originate with him and I am indebted to him for his patience and attention. The work has also benefited from assistance from Richard Buckland, Robert Dale, Mark Dras, Mike Johnson and Steven Sommer. Financial support is gratefully acknowledged from the Australian Government and the Microsoft Institute.\\n\\nBibliography\\n\\n1 Beckwith, R., Fellbaum, C., Gross, D. and Miller, G. (1991) WordNet: A lexical database organized on psycholinguistic principles, in Zernik, Uri (ed.) Lexical Acquisition: Exploring On-line Resources to Build a Lexicon, Lawrence Erlbaum, pp211-32\\n\\n2 Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., Lai, J. C. and Mercer, R. L. (1992) An Estimate of an Upper Bound for the Entropy of English Computational Linguistics 18(1) pp31-40\\n\\n3 Dagan, I., Marcus, S. and Markovitch, S. (1993) Contextual Word Similarity and Estimation from Sparse DataProceedings of the 31st Annual Meeting of the Association for Computational Linguistics, Columbus, Ohio. see also cmp-lg/9405001\\n\\n4 Dras, Mark and Lauer, Mark (1993) Lexical Preference Estimation Incorporating Case Information Proceedings of the Natural Language Processing Workshop, Australian Joint Conference on Artificial Intelligence, Melbourne, Australia.\\n\\n5 Hindle, Don and Mats Rooth (1993) Structural Ambiguity and Lexical Relations Computational Linguistics Vol. 19(1), Special Issue on Using Large Corpora I, pp 103-20\\n\\n6 Katz, S. M. (1987) Estimation of probabilities from sparse data for the language model of a speech recognizer IEEE Transactions on Acoustics, Speech, and Signal Processing, Vol. 35(3)\\n\\n7 Lauer, Mark (1994) Conceptual Association for Compound Noun Analysis Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, Student Session, Las Cruces, New Mexico. cmp-lg/9409002\\n\\n8 Resnik, P. (1993) Selection and Information: A Class-Based Approach to Lexical Relationships PhD dissertation, University of Pennsylvania, Philadelphia, PA.\\n\\n9 Resnik, Philip and Marti Hearst (1993) Structural Ambiguity and Conceptual Relations Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, June 22, Ohio State University, pp 58-64\\n\\n10 Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L. and Palmucci, J. (1993) Coping with Ambiguity and Unknown Words through Probabilistic Models Computational Linguistics Vol. 19(2), Special Issue on Using Large Corpora II, pp 359-82\\n\\n11 Yarowsky, David  (1992) Word-Sense Disambiguation Using Statistical Models of Roget's Categories Trained on Large Corpora Proceedings of COLING-92, Nantes, France, pp 454-60\\n\\nFootnotes\\n\\nThis paper has been published in the Proceedings of the Second Conference of the Pacific Association for Computational Linguistics, Brisbane, Australia, 1995. Unless a more accurate statistical processor based on the same indicators already exists. Brackets indicate measured on different data and/or under different conditions. Reported in Resnik(1993). Reported in Dras and Lauer(1993). Some stemming is performed, so it is the number of stems in the vocabulary that we want. I have allowed all the training examples as instances, even though some are acquired by cyclic refinement.\", metadata={'source': '../data/raw/cmplg-xml/9509001.xml'}),\n",
       " Document(page_content=\"Combining Knowledge Sources to Reorder N-Best Speech Hypothesis Lists\\n\\nA simple and general method is described that can combine different knowledge sources to reorder N-best lists of hypotheses produced by a speech recognizer. The method is automatically trainable, acquiring information from both positive and negative examples. Experiments are described in which it was tested on a 1000-utterance sample of unseen ATIS data.\\n\\nIntroduction\\n\\nThe plausibility score originally assigned to H by the recognizer.\\n\\nThe sets of surface unigrams, bigrams and trigrams present in H.\\n\\nWhether or not H receives a well-formed syntactic/semantic analysis.\\n\\nIf so, properties of that analysis.\\n\\nCombining knowledge sources\\n\\nThis section describes how different knowledge sources (KSs) can be combined. We start by assuming the existence of a training corpus of N-best lists produced by the recognizer, each list tagged with a ``reference sentence'' that determines which (if any) of the hypotheses in it was correct. We analyse each hypothesis H in the corpus using a set of possible KSs, each of which associates some form of information with H.  Information can be of two different kinds. Some KSs may directly produce a number which can be viewed as a measure of H's plausibility. Typical examples are the score which the recognizer assigned to H, and the score for whether or not Hreceived a linguistic analysis (1 or 0 respectively). More commonly, however, the KS will produce a list of one or more ``linguistic items'' associated with H, for example surface N-grams in H or the grammar rules occurring in the best linguistic analysis of H, if there was one. A given linguistic item L is associated with a numerical score through a ``discrimination function'' (one function for each type of linguistic item), that summarizes the relative frequencies of occurrence of L in correct and incorrect hypotheses respectively. Discrimination functions are discussed in more detail shortly. The score assigned to H by a KS of this kind will be the sum of the discrimination scores for all the linguistic items it finds.\\n\\nThe most interesting role in the above is played by the discrimination functions. The intent is that linguistic items which tend to occur more frequently in correct hypotheses than incorrect ones will get positive scores; those which occur more frequently in incorrect hypotheses than correct ones will get negative scores. To take an example from the ATIS domain, the trigram a list of is frequently misrecognized by DECIPHER as a list the. Comparing the different hypotheses for various utterances, we discover that if we have two distinct hypotheses for the same utterance, one of which is correct and the other incorrect, and the hypotheses differ by one of them containing a list of while the other contains a list the, then the hypothesis containing a list of is nearly always the correct one. This justifies giving the trigram a list of a positive score, and the trigram a list the a negative one.\\n\\nWe now define formally the discrimination function dT for a given type T of linguistic item. We start by defining dT as a function on linguistic items. As stated above, it is then extended in a natural way to a function on hypotheses by defining dT(H) for a hypothesis H to be\\n\\n, where the sum is over all the linguistic items L of type T associated with H.\\n\\nU is an utterance,\\n\\nH1 and H2 are hypotheses for U exactly one of which is correct,\\n\\nL is a linguistic item of type T which is associated with exactly one of H1 and H2.\\n\\nd(g,b) ] 0 if g ] b\\n\\nd(g,b) =\\n\\n\\n\\nd(b,g) (and hence\\n\\nd(g,b) = 0 if g = b).\\n\\nd(g1,b) ] d(g2,b) if g1 ] g2\\n\\nThis formula is a symmetric, logarithmic transform of the function\\n\\n(g + 1)/(g + b + 2), which is the expected a posteriori probability that a new\\n\\n(U, H1, H2, L) 4-tuple will be a good occurrence, assuming that, prior to the quantities g and b being known, this probability has a uniform a priori distribution on the interval [0,1].\\n\\nExperiments\\n\\nExperimental set\\n\\n\\n\\nup\\n\\nExperiments were carried out by first dividing the corpus into five approximately equal pools, in such a way that sentences from any given speaker were never assigned to more than one pool. Each pool was then in turn held out as test data, and the other four used as training data. The fact that utterances from the same speaker never occurred both as test and training data turned out to have an important effect on the results, and is discussed in more detail later.\\n\\nKnowledge sources used\\n\\nResults\\n\\nOne point of experimental method is interesting enough to be worth a diversion. In earlier experiments, reported in the notebook version of the paper, we had not separated the data in such a way as to ensure that the speakers of the utterances in the test and training data were always disjoint. This led to results which were both better and also qualitatively different; the N-gram KSs made a much larger contribution, and appeared to dominate the linguistic KSs. This presumably shows that there are strong surface uniformities between utterances from at least some of the speakers, which the N-gram KSs can capture more easily than the linguistic ones. It is possible that the effect is an artifact of the data-collection methods, and is wholly or partially caused by users who repeat queries after system misrecognitions.\", metadata={'source': '../data/raw/cmplg-xml/9407010.xml'}),\n",
       " Document(page_content=\"GRAMPAL: A Morphological Processor for Spanish implemented in Prolog\\n\\nA model for the full treatment of Spanish inflection for verbs, nouns and adjectives is presented. This model is based on feature unification and it relies upon a lexicon of allomorphs both for stems and morphemes. Word forms are built by the concatenation of allomorphs by means of special contextual features. We make use of standard Definite Clause Grammars (DCG) included in most Prolog implementations, instead of the typical finite-state approach. This allows us to take advantage of the declarativity and bidirectionality of Logic Programming for NLP. The most salient feature of this approach is simplicity: A really straightforward rule and lexical components. We have developed a very simple model for complex phenomena. Declarativity, bidirectionality, consistency and completeness of the model are discussed: all and only correct word forms are analysed or generated, even alternative ones and gaps in paradigms are preserved. A Prolog implementation has been developed for both analysis and generation of Spanish word forms. It consists of only six DCG rules, because our lexicalist approach -i.e. most information is in the dictionary. Although it is quite efficient, the current implementation could be improved for analysis by using the non logical features of Prolog, especially in word segmentation and dictionary access. Keywords:  Applications of Logic Programming to NLP, Computational Morphology.\\n\\nIntroduction\\n\\nIt is well-known that the so-called non-concatenative processes are the most difficult single problem that morphological processors must deal with. Experience has shown that it is not easy for any approach. Unification-based morphology uses suppletion (i.e. alternative allomorphs for a lemma) and feature description as a general mechanism for handling those processes. Two-level morphology uses instead rules that match lexical representations (lemmas) with surface representations (actual spelling forms). The latter has been been claimed to be more elegant, but it is obvious that often the two-level model contains many rules needed for a very few cases.\\n\\nOn the other hand, our goal is to generate and recognize all (and only) well-formed inflected forms, and thus we do not accept ``missing forms'' for defective verbs (see below), but do accept duplicate but correct forms.\\n\\nMajor issues in Spanish computational morphology\\n\\nSpanish morphology is not a trivial subject. As an inflectional language, Spanish shows a great variety of morphological processes, particularly non-concatenative ones. We will try to summarize the outstanding problems which any morphological processor of Spanish has to deal with:\\n\\n2. The frequent irregularity of both verb stems and endings. Very common verbs, such as tener (to have), poner (to put), poder (to be able to), hacer (to do), etc., have up to 7 different stems: hac-er, hag-o, hic-e, ha-r, hiz-o, haz, hech-o. This example shows internal vowel modification triggered by different morphemes having the same external form: hag-o; hiz-o, hech-o (The first /-o/ is first person singular present indicative morpheme; the second /-o/ is third singular preterit indicative morpheme; and the third /-o/ is past participle morpheme - an irregular one, by the way). As well as these non-concatenative processes, there exist other, very common, kinds of internal variation, as illustrated by the following example.\\n\\n2,300 out of 7,600 verbs in our dictionary are classified as irregular, and 5,300 as regular -i.e. only one stem for all the forms as in am-ar -- am-o, etc. (to love).\\n\\n3. Gaps in some verb paradigms. In the so-called defective verbs some forms are missing or simply not used. For instance, meteorological verbs such as llover, nevar (to rain, to snow), etc. are conjugated only in third person singular. Other ones are more peculiar, like abolir (to abolish) that lacks first, second and third singular and third plural present indicative forms, all present subjunctive forms, and the second singular imperative form. In other verbs, the compound tenses are excluded from the paradigm, like in soler (to do usually).\\n\\n4. Duplicate past participles:  a number of verbs have two alternative forms, both correct, like impreso, imprimido (printed). In such cases, the analysis has to treat both.\\n\\n5. There exist some highly irregular verbs that can be handled only by including many of their forms directly in the lexicon (like ir (to go), ser (to be), etc).\\n\\n6. Nominal inflection can be of two major types: with grammatical gender (i.e. concatenating the gender morpheme to the stem) and with inherent gender (i.e. without gender morphemes). Most pronouns and quantifiers belong to the first class, but nouns and adjectives can be in any of the two classes, with a different distribution: 4% of the nouns have grammatical gender and 92% have inherent gender, while 70% of the adjectives are in the first group. Some nouns and adjectives present alternative correct forms for plural -e.g. for bamb (bamboo), bamb-s and, bamb-es.\\n\\n7. There is a small group (3%) of invariant nouns with the same form for singular and plural, e.g. crisis. On the other hand, 30% of the adjectives present the same form for masculine and feminine, e.g. azul (blue). There exist also singularia tantum, where only the singular form is used, like estrs (stress); and pluralia tantum, where only the plural form is allowed, e.g. matemticas, (mathematics).\\n\\n8. In contrast with verb morphology, nominal processes do not produce internal change in the stem caused by the addition of a gender or plural suffix, although there can be many allomorphs produced by spelling changes: luz, luc-es (light, lights).\\n\\nAll these phenomena suggest that there is no such a universal model (e.g. two-level, unification, or others) for (surface) morphology. Instead, we have approaches more suited for some processes than others. The computational morphologist must decide which is more appropriate for a particular language. We support the idea that unification and feature-based morphology is more adequate for languages, such as Spanish and other Latin languages, that have alternative stems triggered by specific suffixes, missing forms in the paradigm, and duplicate correct forms.\\n\\nThe model\\n\\nAs we stated before, our model relies on a context-free feature-based grammar, that is particularly well suited for the morpho-syntactic processes. For morpho-graphemics, our model depends on the storage -or computation- of all the possible allomorphs both for stems and endings. This feature permits that both analysis and synthesis be limited to morpheme concatenation, as the general and unique mechanism. This simplifies dramatically the rule component.\\n\\nWe present some examples of dictionary entries: two verbal ending entries (allomorphs) for the past participle morphemes and two allomorph stems for imprimir, compatible with those endings.\\n\\nWhere vm and vl stands for the values of the ``morphological category'' that we are using to drive the DCG rule invocation. All the dictionary entries are coded with a predicate that corresponds to its morphological category. The full inventory of such categories follows:\\n\\nw For complete inflected word forms. wl For words (nouns and adjectives) that can accept a number morpheme. vl For verb lexemes (stems). nl For nominal -nouns and adjectives- lexemes. vm For verb morphemes. ng For nominal gender morphemes. nn For nominal number morphemes.\\n\\nFor reference, and to check the meaning of the examples, a short self-description of the arguments of those predicates follows:\\n\\nWe have introduced some contextual atomic features that impose restrictions on the concatenation of morphemes through standard unification rules. Such features are never percolated up to the parent node of a rule. Multi-valued atomic features are permitted in the unification mechanism, being interpreted as a disjunction of atomic values. We represent this disjunction as a Prolog list. Disjunction of values is used only for contextual features (stem_type, suffix_type, conjugation, gender_type and number_type) just to improve storage efficiency, since this device is actually not needed if different entries were encoded in the lexicon.\\n\\nEach of the 49 inflected forms is  represented by a numeric code, and the additional value 100 is used as a shorthand for the disjunction of all of them (used for regular verbs; see the entry for imprim above). The contextual feature stem_type (stt) is used to identify the verb stem and ending corresponding to each form, and the contextual feature suffix_type (sut) distinguishes among several allomorphs of the inflectional morpheme by means of a set of values:\\n\\nSince this value set is much smaller than the stem_type set, we have chosen an alphabetic code. With the combination of both features, and the addition of a third feature conj (conjugation), we can state unequivocally which is the correct sequence of stem and ending for each case (see examples above, where imprim only matches ido for all features, and impres matches o, thus preventing ill-formed concatenations -for these morphemes- such as imprim-o or impres-ido).\\n\\nIn the same fashion, we have two special contextual features for the nominal inflection, nut (number_type) and get (gender_type), to identify the various allomorphs for the plural and gender morphemes, and associate them with the proper nominal stems. The following examples show those contextual features both in nominal morphemes and in nominal lexeme entries:\\n\\nThese entries allow the analysis/generation of the word forms presidente, presidenta, presidentes and presidentas for the lemma presidente; doctor, doctora, doctores and doctoras for doctor; and bamb, bambs and bambes for bamb.\\n\\nThe grammatical features (category, lemma, tense, mood, person, number and gender are the only features that are delivered to the w node, and from this level can be used by a syntactic DCG grammar.\\n\\nA unification-based system relies very much on the lexical side. It is needed a robust and large dictionary, properly coded. Additionally, our model depends on the accessibility of all possible allomorphs, so their storage is also necessary. Fortunately, there is no need for typing all of them by hand, since this would be an impractical, time consuming and error-prone task. Morpho-graphemics for Spanish is quite regular and we have formalized and implemented the automatic computation of the allomorphs of any verb from the infinitive form.\\n\\nThe grammar\\n\\nThe rule component of the model is quite small, because most of the information is in the lexicon. In particular, inflected verb forms are analysed or generated by two rules. Actually, only one rule is needed, but as we used the value 100 for the stt feature for regular verbs instead of a disjunction of all the possible stt values, we split the rule in two:\\n\\nNominal inflection is a bit more complicated, because of the combination of two inflectional morphemes (gender and number) in some cases. Our model needs the 4 rules shown to handle this. The first one is for singular words, when the stem has to be concatenated to a gender suffix (ni-o, ni-a); the second is for plural words, where an additional number suffix is added (nio-s) ; the third builds plurals from an allomorph stem and a plural morpheme (len / leon-es); and the fourth rule validates as words the singular forms (wl) obtained from the first rule without further concatenation:\\n\\nThe predicate member included in the procedural part of the DCG rule implements disjunction in atomic contextual features, although it could have been eliminated with a different encoding of the lexical entries.\\n\\nThe Processor\\n\\nThe grammar rules are stated using the DCG formalism included in most Prolog implementations, thus we have used the DCG interpreter both for parsing and generating word forms. Since the interpreter is supplied with morphemes included in the dictionary for its proper operation, a segmenter has to be included to provide the parser with candidate word segmentations. This is achieved by means of a non-deterministic predicate that finds all the possible segmentations of a word. This is one of the efficiency drawbacks of the current implementation of GRAMPAL.\\n\\nConclusions\\n\\nA Prolog prototype, GRAMPAL, was developed to intensively test the model, both as analyser and as generator. This processor implemented in Prolog has shown that logic programming can be used successfully to handle the Spanish inflectional morphology. We have also implemented a C version of GRAMPAL, but it needs separate components for analysis and generation, due to the lack of reversibility that Logic Programming has provided us with.\\n\\nThe model presented is based on two basic principles:\\n\\nEmpirical rigour: all and only correct forms are analysed and generated, whether regular or not; gaps in verb paradigms are observed; suppletive forms are considered valid, and so on. It is important to stress that GRAMPAL does not overgenerate or overanalyse.\\n\\nThe current dictionary has a considerable size: 43,000 lemma entries, including 24,400 nouns, 7,600 verbs, and 11,000 adjectives. The model could be used for derivative morphology and compounds as well, but this has not been done yet, since further linguistic analysis must be done to specify the features needed to permit derivatives and compounds.\\n\\nBibliography\\n\\nAho, A.V. ; Hopcroft, J.E. and Ullman, J.D. (1983). Tries. In Data Structures and Algorithms, ch. 5, sec. 3, pp. 163-169. Addison Wesley.\\n\\nAntworth, E. (1990). PC-KIMMO: A Two-level Processor for Morphological Analysis. Academic Computing Department, Summer Institute of Linguistics. Dallas, TX.\\n\\nBear, J. (1986). A Morphological Recogniser with Syntactic and Phonological Rules. Proceedings of the 11th International Conference on Computational Linguistics (COLING 86), pp. 272-276.\\n\\nGazdar, F. and Mellish, C.S. (1989). Natural Language Processing in Prolog. Addison Wesley.\\n\\nGoi, J.M. ; Gonzlez, J.C. and Lpez, J. (1994). A Framework for Lexical Representation. Technical Report UPM/DIT/LIA 5/94. Universidad Politcnica de Madrid, 1994.\\n\\nKoskenniemi, K. (1985). A general two-level computational model for word-form recognition and production. In Karlsson, F. (ed): Computational Morphosyntax, pp. 1-18. Dept. of Linguistics. University of Helsinki.\\n\\nMart, M.A. (1986). Un sistema de anlisis morfolgico por ordenador. Procesamiento del Lenguaje Natural, n. 4, pp. 104-110.\\n\\nMeya, M. (1986). Anlisis morfolgico como ayuda a la recuperacin de informacin. Procesamiento del Lenguaje Natural, n. 4, pp. 91-103.\\n\\nMoreno, A. (1991). Un modelo computacional basado en la unificacin para el anlisis y la generacin de la morfologa del espaol. Tesis Doctoral. Universidad Autnoma de Madrid, 1992.\\n\\nRitchie, G.; Pulman, S.; Black, A. and Russell, G. (1987). A Computational Framework for Lexical Description. Computational Linguistics, vol. 13, n. 3-4, pp. 290-307.\\n\\nTzoukermann, E. and Liberman, M. (1990). A Finite-State Morphological Processor for Spanish. Proceedings of the 13th International Conference on Computational Linguistics (COLING 90), vol. 3, pp. 277-281.\\n\\nTrost, H. (1990). The application of two-level morphology to non-concatenative German morphology. Proceedings of the 13th International Conference on Computational Linguistics (COLING 90), vol.2, pp. 371-376.\\n\\nFootnotes\\n\\nThis work has been partially supported by the Spanish Plan Nacional de I+D, under the Research Project An Architecture for para Natural Language Interfaces with User Modeling (TIC91-0217C02-01). See the next section for further discussion of the adequacy of the two-level model for Spanish, including defective forms (i.e. null forms in the conjugation) and alternative correct forms. The codes 83 and 86 stand for the courtesy imperative: imprima usted, impriman ustedes. These word forms are the same as the 53 and 56 ones. Actually, this number encodes a particular combination of person, number, tense and mood features. These are not the traditional ones, since they capture the problems arising in written language, such as diacritical marks, different surface letters for the same phoneme, etc. In these figures are neither included closed categories, nor allomorphs for verb and nominal morphemes.\", metadata={'source': '../data/raw/cmplg-xml/9507004.xml'}),\n",
       " Document(page_content=\"Similarity-Based Estimation of Word Cooccurrence Probabilities 1\\n\\nIn many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations ``eat a peach'' and ``eat a beach'' is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on ``most similar'' words. We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.\\n\\nIntroduction\\n\\nData sparseness is an inherent problem in statistical methods for natural language processing. Such methods use statistics on the relative frequencies of configurations of elements in a training corpus to evaluate alternative analyses or interpretations of new samples of text or speech. The most likely analysis will be taken to be the one that contains the most frequent configurations. The problem of data sparseness arises when analyses contain configurations that never occurred in the training corpus. Then it is not possible to estimate probabilities from observed frequencies, and some other estimation scheme has to be used.\\n\\nClass-based and similarity-based models provide an alternative to the independence assumption. In those models, the relationship between given words is modeled by analogy with other words that are in some sense similar to the given ones.\\n\\nWe applied our method to estimate unseen bigram probabilities for Wall Street Journal text and compared it to the standard back-off model. Testing on a held-out sample, the similarity model achieved a 20% reduction in perplexity for unseen bigrams. These constituted just 10.6% of the test sample, leading to an overall reduction in test-set perplexity of 2.4%. We also experimented with an application to language modeling for speech recognition, which yielded a statistically significant reduction in recognition error.\\n\\nThe remainder of the discussion is presented in terms of bigrams, but it is valid for other types of word cooccurrence as well.\\n\\nDiscounting and Redistribution\\n\\nMany low-probability bigrams will be missing from any finite sample. Yet, the aggregate probability of all these unseen bigrams is fairly high; any new sample is very likely to contain some.\\n\\nBecause of data sparseness, we cannot reliably use a maximum likelihood estimator (MLE) for bigram probabilities. The MLE for the probability of a bigram (w1,w2) is simply:\\n\\nwhere\\n\\nc(w1,w2) is the frequency of (w1,w2) in the training corpus and N is the total number of bigrams. However, this estimates the probability of any unseen bigram to be zero, which is clearly undesirable.\\n\\nA back-off model requires methods for (a) discounting the estimates of previously observed events to leave out some positive probability mass for unseen events, and (b) redistributing among the unseen events the probability mass freed by discounting. For bigrams the resulting estimator has the general form\\n\\nKatz uses the Good-Turing formula to replace the actual frequency\\n\\nc(w1,w2) of a bigram (or an event, in general) with a discounted frequency,\\n\\nc[\\n\\n\\n\\n](w1,w2),\\n\\ndefined by\\n\\nwhere nc is the number of different bigrams in the corpus that have frequency c. He then uses the discounted frequency in the conditional probability calculation for a bigram:\\n\\nThe Similarity Model\\n\\nThe KL distance is 0 when\\n\\nw1 = w'1, and it increases as the two distribution are less similar.\\n\\nW(w'1,w1) is defined as\\n\\nEvaluation\\n\\nThe bigram similarity model was also tested as a language model in speech recognition. The test data for this experiment were pruned word lattices for 403 WSJ closed-vocabulary test sentences. Arc scores in those lattices are sums of an acoustic score (negative log likelihood) and a language-model score, in this case the negative log probability provided by the baseline bigram model.\\n\\nFrom the given lattices, we constructed new lattices in which the arc scores were modified to use the similarity model instead of the baseline model. We compared the best sentence hypothesis in each original lattice and in the modified one, and counted the word disagreements in which one of the hypotheses is correct. There were a total of 96 such disagreements. The similarity model was correct in 64 cases, and the back-off model in 32. This advantage for the similarity model is statistically significant at the 0.01 level. The overall reduction in error rate is small, from 21.4% to 20.9%, because the number of disagreements is small compared with the overall number of errors in our current recognition setup.\\n\\nRelated Work\\n\\nIn cooccurrence smoothing, as in our method, a baseline model is combined with a similarity-based model that refines some of its probability estimates. The similarity model in cooccurrence smoothing is based on the intuition that the similarity between two words wand w' can be measured by the confusion probability PC(w'|w) that w' can be substituted for w in an arbitrary context in the training corpus. Given a baseline probability model P, which is taken to be the MLE, the confusion probability\\n\\nPC(w'1|w1) between conditioning words w'1 and w1 is defined as\\n\\nthe probability that w1 is followed by the same context words as w'1. Then the bigram estimate derived by cooccurrence smoothing is given by\\n\\nFurther Research\\n\\nConclusions\\n\\nSimilarity-based models suggest an appealing approach for dealing with data sparseness. Based on corpus statistics, they provide analogies between words that often agree with our linguistic and domain intuitions. In this paper we presented a new model that implements the similarity-based approach to provide estimates for the conditional probabilities of unseen word cooccurrences.\\n\\nOur method combines similarity-based estimates with Katz's back-off scheme, which is widely used for language modeling in speech recognition. Although the scheme was originally proposed as a preferred way of implementing the independence assumption, we suggest that it is also appropriate for implementing similarity-based models, as well as class-based models. It enables us to rely on direct maximum likelihood estimates when reliable statistics are available, and only otherwise resort to the estimates of an ``indirect'' model.\\n\\nAcknowledgments\\n\\nWe thank Slava Katz for discussions on the topic of this paper, Doug McIlroy for detailed comments, Doug Paul for help with his baseline back-off model, and Andre Ljolje and Michael Riley for providing the word lattices for our experiments.\\n\\nBibliography\\n\\nBlack, Ezra, Fred Jelinek, John Lafferty, David M. Magerman, David Mercer, and Salim Roukos. 1993. Towards history-based grammars: Using richer models for probabilistic parsing. In 30th Annual Meeting of the Association for Computational Linguistics, pages 31-37, Columbus, Ohio. Ohio State University, Association for Computational Linguistics, Morristown, New Jersey.\\n\\nBrown, Peter F., Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479.\\n\\nChurch, Kenneth W. and William A. Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5:19-54.\\n\\nDagan, Ido, Shaul Markus, and Shaul Markovitch. 1993. Contextual word similarity and estimation from sparse data. In 30th Annual Meeting of the Association for Computational Linguistics, pages 164-171, Columbus, Ohio. Ohio State University, Association for Computational Linguistics, Morristown, New Jersey.\\n\\nEssen, Ute and Volker Steinbiss. 1992. Coocurrence smoothing for stochastic language modeling. In Proceedings of ICASSP, volume I, pages 161-164. IEEE.\\n\\nGood, I. J. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3):237-264.\\n\\nGrishman, Ralph and John Sterling. 1993. Smoothing of automatically generated selectional constraints. In Human Language Technology, pages 254-259, San Francisco, California. Advanced Research Projects Agency, Software and Intelligent Systems Technology Office, Morgan Kaufmann.\\n\\nJelinek, Frederick, Robert L. Mercer, and Salim Roukos. 1992. Principles of lexical language modeling for speech recognition. In Sadaoki Furui and M. Mohan Sondhi, editors, Advances in Speech Signal Processing. Mercer Dekker, Inc., pages 651-699.\\n\\nKatz, Slava M. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speeech and Signal Processing, 35(3):400-401.\\n\\nLafferty, John, Daniel Sleator, and Davey Temperley. 1992. Grammatical trigrams: aa probabilistic model of link grammar. In Robert Goldman, editor, AAAI Fall Symposium on Probabilistic Approaches to Natural Language Processing, Cambridge, Massachusetts. American Association for Artificial Intelligence.\\n\\nPaul, Douglas B. 1991. Experience with a stack decoder-based HMM CSR and back-off n-gram language models. In Proceedings of the Speech and Natural Language Workshop, pages 284-288, Palo Alto, California, February. Defense Advanced Research Projects Agency, Information Science and Technology Office, Morgan Kaufmann.\\n\\nPereira, Fernando C. N., Naftali Z. Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In 30th Annual Meeting of the Association for Computational Linguistics, pages 183-190, Columbus, Ohio. Ohio State University, Association for Computational Linguistics, Morristown, New Jersey.\\n\\nRosenfeld, Ronald and Xuedong Huang. 1992. Improvements in stochastic language modeling. In DARPA Speech and Natural Language Workshop, pages 107-111, Harriman, New York, February. Morgan Kaufmann, San Mateo, California.\\n\\nSchabes, Yves. 1992. Stochastic lexicalized tree-adjoining grammars. In Proceeedings of the 14th International Conference on Computational Linguistics, Nantes, France.\\n\\nSugawara, K., M. Nishimura, K. Toshioka, M. Okochi, and T. Kaneko. 1985. Isolated word recognition using hidden Markov models. In Proceedings of ICASSP, pages 1-4, Tampa, Florida. IEEE.\\n\\nFootnotes\\n\\nThey also consider other definitions of confusion probability and smoothed probability estimate, but the one above yielded the best experimental results. For WSJ trigrams, only 58.6% of test set trigrams occur in 40M of words of training (Doug Paul, personal communication).\", metadata={'source': '../data/raw/cmplg-xml/9405001.xml'}),\n",
       " Document(page_content=\"Response Generation in Collaborative Negotiation1\\n\\nIn collaborative planning activities, since the agents are autonomous and heterogeneous, it is inevitable that conflicts arise in their beliefs during the planning process. In cases where such conflicts are relevant to the task at hand, the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs. This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution. Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise, and of selecting appropriate evidence to justify the need for such modification. Furthermore, by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions, our model can successfully handle embedded negotiation subdialogues.\\n\\nIntroduction\\n\\nThis paper presents a model for engaging in collaborative negotiation to resolve conflicts in agents' beliefs about domain knowledge. Our model 1) detects conflicts in beliefs and initiates a negotiation subdialogue only when the conflict is relevant to the current task, 2) selects the most effective aspect to address in its pursuit of conflict resolution when multiple conflicts exist, 3) selects appropriate evidence to justify the system's proposed modification of the user's beliefs, and 4) captures the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions, thus enabling the system to handle embedded negotiation subdialogues.\\n\\nRelated Work\\n\\nWebber and Joshi web_jos_coling82 have noted the importance of a cooperative system providing support for its responses. They identified strategies that a system can adopt in justifying its beliefs; however, they did not specify the criteria under which each of these strategies should be selected. Walker wal_coling94 described a method of determining when to include optional warrants to justify a claim based on factors such as communication cost, inference cost, and cost of memory retrieval. However, her model focuses on determining when to include informationally redundant utterances, whereas our model determines whether or not justification is needed for a claim to be convincing and, if so, selects appropriate evidence from the system's private beliefs to support the claim.\\n\\nFeatures of Collaborative Negotiation Response Generation in Collaborative Negotiation Evaluating Proposed Beliefs\\n\\nExample\\n\\nTo illustrate the evaluation of proposed beliefs, consider the following utterances: S:S: #2U:\\n\\nto (5) #1 I think Dr. Smith is teaching AI next semester.\\n\\nto (6) #2 Dr. Smith is not teaching AI.\\n\\nto (7) He is going on sabbatical next year.\\n\\nModifying Unaccepted Proposals Selecting the Focus of Modification Selecting Justification for a Claim\\n\\nTo convince the user of a belief, _bel, our system selects appropriate justification by identifying beliefs that could be used to support _bel and applying filtering heuristics to them. The system must first determine whether justification for _bel is needed by predicting whether or not merely informing the user of _bel will be sufficient to convince him of _bel. If so, no justification will be presented. If justification is predicted to be necessary, the system will first construct the justification chains that could be used to support _bel. For each piece of evidence that could be used to directly support _bel, the system first predicts whether the user will accept the evidence without justification. If the user is predicted not to accept a piece of evidence (evidi), the system will augment the evidence to be presented to the user by posting evidi as a mutual belief to be achieved, and selecting propositions that could serve as justification for it. This results in a recursive process that returns a chain of belief justifications that could be used to support _bel.\\n\\nExample\\n\\nThe system will try to establish the mutual beliefs as an attempt to satisfy the precondition of Modify-Node. This will cause the system to invoke Inform discourse actions to generate the following utterances:\\n\\nS:S:\\n\\n#2\\n\\nto (8) #1 Dr. Smith is not going on sabbatical next year.\\n\\nto (9) He postponed his sabbatical until 1997.\\n\\nIf the user accepts the system's utterances, thus satisfying the precondition that the conflict be resolved, Modify-Node can be performed and changes made to the original proposed beliefs. Otherwise, the user may propose modifications to the system's proposed modifications, resulting in an embedded negotiation subdialogue.\\n\\nConclusion\\n\\nThis paper has presented a computational strategy for engaging in collaborative negotiation to square away conflicts in agents' beliefs. The model captures features specific to collaborative negotiation. It also supports effective and efficient dialogues by identifying the focus of modification based on its predicted success in resolving the conflict about the top-level belief and by using heuristics motivated by research in social psychology to select a set of evidence to justify the proposed modification of beliefs. Furthermore, by capturing collaborative negotiation in a cycle of Propose-Evaluate-Modify actions, the evaluation and modification processes can be applied recursively to capture embedded negotiation subdialogues.\\n\\nAcknowledgments\\n\\nDiscussions with Candy Sidner, Stephanie Elzer, and Kathy McCoy have been very helpful in the development of this work. Comments from the anonymous reviewers have also been very useful in preparing the final version of this paper.\\n\\nBibliography\\n\\nJames Allen. 1991. Discourse structure in the TRAINS project. In Darpa Speech and Natural Language Workshop.\\n\\nLawrence Birnbaum, Margot Flowers, and Rod McGuire. 1980. Towards an AI model of argumentation. In Proceedings of the National Conference on Artificial Intelligence, pages 313-315.\\n\\nAlison Cawsey, Julia Galliers, Brian Logan, Steven Reece, and Karen Sparck Jones. 1993. Revising beliefs and intentions: A unified framework for agent interaction. In The Ninth Biennial Conference of the Society for the Study of Artificial Intelligence and Simulation of Behaviour, pages 130-139.\\n\\nJennifer Chu-Carroll and Sandra Carberry. 1994. A plan-based model for response generation in collaborative task-oriented dialogues. In Proceedings of the Twelfth National Conference on Artificial Intelligence, pages 799-805.\\n\\nJennifer Chu-Carroll and Sandra Carberry. 1995. Generating information-sharing subdialogues in expert-user consultation. In Proceedings of the 14th International Joint Conference on Artificial Intelligence. To appear.\\n\\nJennifer Chu-Carroll. 1995. A Plan-Based Model for Response Generation in Collaborative Consultation Dialogues. Ph.D. thesis, University of Delaware. Forthcoming.\\n\\nPaul R. Cohen. 1985. Heuristic Reasoning about Uncertainty: An Artificial Intelligence Approach. Pitman Publishing Company.\\n\\nRobin Cohen. 1987. Analyzing the structure of argumentative discourse. Computational Linguistcis, 13(1-2):11-24, January-June.\\n\\nColumbia University Transcripts. 1985. Transcripts derived from audiotape conversations made at Columbia University, New York, NY. Provided by Kathleen McKeown.\\n\\nJulia R. Galliers. 1992. Autonomous belief revision and communication. In Gardenfors, editor, Belief Revision. Cambridge University Press.\\n\\nH. Paul Grice. 1975. Logic and conversation. In Peter Cole and Jerry L. Morgan, editors, Syntax and Semantics 3: Speech Acts, pages 41-58. Academic Press, Inc., New York.\\n\\nBarbara J. Grosz and Candace L. Sidner. 1990. Plans for discourse. In Cohen, Morgan, and Pollack, editors, Intentions in Communication, chapter 20, pages 417-444. MIT Press.\\n\\nDale Hample. 1985. Refinements on the cognitive model of argument: Concreteness, involvement and group scores. The Western Journal of Speech Communication, 49:267-285.\\n\\nAravind K. Joshi. 1982. Mutual beliefs in question-answer systems. In N.V. Smith, editor, Mutual Knowledge, chapter 4, pages 181-197. Academic Press.\\n\\nLynn Lambert and Sandra Carberry. 1991. A tripartite plan-based model of dialogue. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 47-54.\\n\\nBrian Logan, Steven Reece, Alison Cawsey, Julia Galliers, and Karen Sparck Jones. 1994. Belief revision and dialogue management in information retrieval. Technical Report 339, University of Cambridge, Computer Laboratory.\\n\\nJoseph A. Luchok and James C. McCroskey. 1978. The effect of quality of evidence on attitude change and source credibility. The Southern Speech Communication Journal, 43:371-383.\\n\\nMark T. Maybury. 1993. Communicative acts for generating natural language arguments. In Proceedings of the National Conference on Artificial Intelligence, pages 357-364.\\n\\nKathleen R. McKeown. 1985. Text Generation : Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press.\\n\\nDonald D. Morley. 1987. Subjective message constructs: A theory of persuasion. Communication Monographs, 54:183-203.\\n\\nRichard E. Petty and John T. Cacioppo. 1984. The effects of involvement on responses to argument quantity and quality: Central and peripheral routes to persuasion. Journal of Personality and Social Psychology, 46(1):69-81.\\n\\nMartha E. Pollack. 1986. A model of plan inference that distinguishes between the beliefs of actors and observers. In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, pages 207-214.\\n\\nAlex Quilici. 1992. Arguing about planning alternatives. In Proceedings of the 14th International Conference on Computational Linguistics, pages 906-910.\\n\\nRachel Reichman. 1981. Modeling informal debates. In Proceedings of the 7th International Joint Conference on Artificial Intelligence, pages 19-24.\\n\\nJohn C. Reinard. 1988. The empirical study of the persuasive effects of evidence, the status after fifty years of research. Human Communication Research, 15(1):3-59.\\n\\nRodney A. Reynolds and Michael Burgoon. 1983. Belief processing, reasoning, and evidence. In Bostrom, editor, Communication Yearbook 7, chapter 4, pages 83-104. Sage Publications.\\n\\nCandace L. Sidner. 1992. Using discourse to negotiate in collaborative activity: An artificial language. In AAAI-92 Workshop: Cooperation Among Heterogeneous Intelligent Systems, pages 121-128.\\n\\nCandace L. Sidner. 1994. An artificial discourse language for collaborative negotiation. In Proceedings of the Twelfth National Conference on Artificial Intelligence, pages 814-819.\\n\\nSRI Transcripts. 1992. Transcripts derived from audiotape conversations made at SRI International, Menlo Park, CA. Prepared by Jacqueline Kowtko under the direction of Patti Price.\\n\\nKatia Sycara. 1989. Argumentation: Planning other agents' plans. In Proceedings of the 11th International Joint Conference on Artificial Intelligence, pages 517-523.\\n\\nMarilyn Walker and Steve Whittaker. 1990. Mixed initiative in dialogue: An investigation into discourse segmentation. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pages 70-78.\\n\\nMarilyn A. Walker. 1992. Redundancy in collaborative dialogue. In Proceedings of the 15th International Conference on Computational Linguistics, pages 345-351.\\n\\nMarilyn A. Walker. 1994. Discourse and deliberation: Testing a collaborative strategy. In Proceedings of the 15th International Conference on Computational Linguistics.\\n\\nBonnie Webber and Aravind Joshi. 1982. Taking the initiative in natural language data base interactions: Justifying why. In Proceedings of COLING-82, pages 413-418.\\n\\nSteve Whittaker and Phil Stenton. 1988. Cues and control in expert-client dialogues. In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, pages 123-130.\\n\\nRobert S. Wyer, Jr. 1970. Information redundancy, inconsistency, and novelty and their role in impression formation. Journal of Experimental Social Psychology, 6:111-127.\\n\\nR. Michael Young, Johanna D. Moore, and Martha E. Pollack. 1994. Towards a principled representation of discourse plans. In Proceedings of the Sixteenth Annual Meeting of the Cognitive Science Society, pages 946-951.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9505001.xml'}),\n",
       " Document(page_content=\"On Learning More Appropriate Selectional Restrictions\\n\\nWe present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora. It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes. Evaluation measures for the Selectional Restrictions learning task are discussed. Finally, an experimental evaluation of these variations is reported. Subject Areas: corpus-based language modeling, computational lexicography\\n\\nIntroduction\\n\\nThe basic technique for learning SRs\\n\\nDescription\\n\\nThe technique functionality can be summarized as:\\n\\nInput The training set, i.e. a list of complement co-occurrence triples, (verb-lemma, syntactic-relationship, noun-lemma) extracted from the corpus.\\n\\nPrevious knowledge used\\n\\nA semantic hierarchy (WordNet) where words are clustered in semantic classes, and semantic classes are organized hierarchically. Polysemous words are represented as instances of different classes.\\n\\nOutput\\n\\nA set of syntactic SRs, (verb-lemma, syntactic-relationship, semantic-class, weight). The final SRs must be mutually disjoint. SRs are weighted according to the statistical evidence found in the corpus.\\n\\nLearning process\\n\\n3 stages:\\n\\n1. Creation of the space of candidate classes. 2. Evaluation of the appropriateness of the candidates by means of a statistical measure. 3. Selection of the most appropriate subset in the candidate space to convey the SRs.\\n\\nAssoc(v,s,c)  =   p(c|v,s) I(v;c|s) \\\\nonumber \\\\\\\\ =  p(c|v,s) \\\\log \\\\frac{p(c|v,s)}{p(c|s) \\\\nonumber} \\\\end{eqnarray} -->\\n\\nThe two terms of Assoc try to capture different properties:\\n\\n1. Mutual information ratio, I(v;c|s), measures the strength of the statistical association between the given verb v and the candidate class c in the given syntactic position s.  It compares the prior distribution, p(c|s), with the posterior distribution, p(c|v,s).\\n\\n2. p(c|v,s) scales up the strength of the association by the frequency of the relationship.\\n\\nProbabilities are estimated by Maximum Likelihood Estimation, counting the relative frequency of events in the corpus. However, it is not obvious how to calculate class frequencies when the training corpus is not semantically tagged as is the case. Nevertheless, we take a simplistic approach and calculate them in the following manner:\\n\\nWhere w is a constant factor used to normalize the probabilities\\n\\nWhen creating the space of candidate classes (learning process, stage 1), we use a thresholding technique to ignore as much as possible the noise introduced in the training set. Specifically, we consider only those classes that have a higher number of occurrences than the threshold. The selection of the most appropriate classes (stage 3) is based on a global search through the candidates, in such a way that the final classes are mutually disjoint (not related by hyperonymy).\\n\\nEvaluation\\n\\na. The technique achieves a good coverage.\\n\\nb. Most of the classes acquired result from the accumulation of incorrect senses.\\n\\nc. No clear co-relation between Assoc and the manual diagnosis is found.\\n\\nd. A slight tendency to over-generalization exists due to incorrect senses.\\n\\nAlthough the performance of the presented technique seems to be quite good, we think that some of the detected flaws could possibly be addressed. Noise due to polysemy of the nouns involved seems to be the main obstacle for the practicality of the technique. It makes the association score prefer incorrect classes and jump on over-generalizations. In this paper we are interested in exploring various ways to make the technique more robust to noise, namely, (a) to experiment with variations of the association score, (b) to experiment with thresholding.\\n\\nVariations on the association statistical measure\\n\\nVariations on the prior probability\\n\\nWhen considering the prior probability, the more independent of the context it is the better to measure actual associations. A sensible modification of the measure would be to consider p(c) as the prior distribution:\\n\\nThe first advantage of Assoc' would come from this (information theoretical) relationship. Specifically, the Assoc' takes into account the preference (selection) of syntactic positions for particular classes. In intuitive terms, typical subjects (e.g. [person, individual, ...]) would be preferred (to atypical subjects as [suit_of_clothes]) as SRs on the subject in contrast to Assoc. The second advantage is that as long as the prior probabilities, p(c), involve simpler events than those used in Assoc, p(c|s), the estimation is easier and more accurate (ameliorating data sparseness).\\n\\nA subsequent modification would be to estimate the prior, p(c), from the counts of all the nouns appearing in the corpus independently of their syntactic positions (not restricted to be heads of verbal complements). In this way, the estimation of p(c) would be easier and more accurate.\\n\\nEstimating class probabilities from noun frequencies\\n\\nOther statistical measures to score SRs\\n\\nThe statistical measures used to detect associations on the distribution defined by two random variables X and Y work by measuring the deviation of the conditional distribution, P(X|Y), from the expected distribution if both variables were considered independent, i.e. the marginal distribution, P(X). If P(X) is a good approximation of P(X|Y), association measures should be low (near zero), otherwise deviating significantly from zero.\\n\\nEvaluation\\n\\nEvaluation methods of SRs\\n\\nHowever, a related and crucial issue is which linguistic tasks are used as a reference. SRs are useful for both lexicography and NLP. On the one hand, from the point of view of lexicography, the goal of evaluation would be to measure the quality of the SRs induced, (i.e., how well the resulting classes correspond to the nouns as they were used in the corpus). On the other hand, from the point of view of NLP, SRs should be evaluated on their utility (i.e., how much they help on performing the reference task).\\n\\nLexicography\\n\\n\\n\\noriented evaluation\\n\\nAs far as lexicography (quality) is concerned, we think the main criteria SRs acquired from corpora should meet are: (a) correct categorization -inferred classes should correspond to the correct senses of the words that are being generalized-, (b) appropriate generalization level and (c) good coverage -the majority of the noun occurrences in the corpus should be successfully generalized by the induced SRs.\\n\\nSome of the methods we could use for assessing experimentally the accomplishment of these criteria would be:\\n\\nQuantification of coverage It could be measured as the proportion of triples whose correct sense belongs to one of the SRs.\\n\\nNLP evaluation tasks\\n\\nExperimental results\\n\\nIn order to evaluate the different variants on the association score and the impact of thresholding we performed several experiments. In this section we analyze the results. As training set we used the 870,000 words of WSJ material provided in the ACL/DCI version of the Penn Treebank. The testing set consisted of 2,658 triples corresponding to four average common verbs in the Treebank: rise, report, seek and present. We only considered those triples that had been correctly extracted from the Treebank and whose noun had the correct sense included in WordNet (2,165 triples out of the 2,658, from now on, called the testing-sample).\\n\\nComparing different techniques\\n\\nAs far as the evaluation measures try to account for different phenomena the goodness of a particular technique should be quantified as a trade-off. Most of the results are very similar (differences are not statistically significative). Therefore we should be cautious when extrapolating the results. Some of the conclusions from the tables above are:\\n\\n3. All versions of Assoc (except the local normalization) get good results. Specially the two techniques that exploit a simpler prior distribution, which seem to improve the basic technique.\\n\\n4. log-likelihood and D seem to get slightly worse results than Assoc techniques, although the results are very similar.\\n\\nThresholding\\n\\nFinally, it seems that precision and abstraction ratios are in inverse co-relation (as precision grows, abstraction decreases). In terms of WSS, general classes may be performing better than classes that fit the data better. Nevertheless, this relationship should be further explored in future work.\\n\\nConclusions and future work\\n\\nIn this paper we have presented some variations affecting the association measure and thresholding on the basic technique for learning SRs from on-line corpora. We proposed some evaluation measures for the SRs learning task. Finally, experimental results on these variations were reported. We can conclude that some of these variations seem to improve the results obtained using the basic technique. However, although the technique still seems far from practical application to NLP tasks, it may be most useful for providing experimental insight to lexicographers. Future lines of research will mainly concentrate on improving the local normalization technique by solving the noun sense ambiguity. We have foreseen the application of the following techniques:\\n\\nSimple techniques to decide the best sense c given the target noun n using estimates of the n-grams: P(c), P(c|n), P(c|v,s) and\\n\\nP(c|v,s,n), obtained from supervised and un-supervised corpora.\\n\\nCombining the different n-grams by means of smoothing techniques.\\n\\nUsing the WordNet hierarchy as a source of backing-off knowledge, in such a way that if n-grams composed by c        aren't enough to decide the best sense (are equal to zero), the tri-grams of ancestor classes could be used instead.\\n\\nBibliography\\n\\nR. Basili, M.T. Pazienza, and P. Velardi. 1992. Computational lexicons: the neat examples and the odd exemplars. In Procs 3rd ANLP, Trento, Italy, April.\\n\\nK.W. Church and P. Hanks. 1990. Word association norms, mutual information and lexicography. Computational Linguistics, 16(1).\\n\\nT.M. Cover and J.A. Thomas, editors. 1991. Elements of Information Theory. John Wiley.\\n\\nA. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, 39(B):1-38.\\n\\nT. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61-74.\\n\\nW. Gale and K. W. Church. 1991. Identifying word correspondences in parallel texts. In DARPA Speech and Natural Language Workshop, Pacific Grove, California, February. .\\n\\nR. Grishman and J. Sterling. 1992. Acquisition of selectional patterns. In COLING, Nantes, France, march.\\n\\nG. Hirst. 1987. Semantic interpretation and the resolution of ambiguity. Cambridge University Press.\\n\\nB. Levin. 1992. Towards a lexical organization of English verbs. University of Chicago Press.\\n\\nG. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1991. Five papers on wordnet. International Journal of Lexicography.\\n\\nP. S. Resnik. 1992. Wordnet and distributional analysis: A class-based approach to lexical discovery. In AAAI Symposium on Probabilistic Approaches to NL, San Jose, CA.\\n\\nP. S. Resnik. 1993. Selection and Information: A Class-Based Approach to lexical relationships. Ph.D. thesis, Computer and Information Science Department, University of Pennsylvania.\\n\\nF. Ribas. 1994a. An experiment on learning appropriate selectional restrictions from a parsed corpus. In COLING, Kyoto, Japan, August.\\n\\nF. Ribas. 1994b. Learning more appropriate selectional restrictions. Technical report, ESPRIT BRA-7315 ACQUILEX-II WP.\\n\\nG. Whittemore, K. Ferrara, and H. Brunner. 1990. Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases. In Procs. ACL, Pennsylvania.\\n\\nG. K. Zipf. 1945. The meaning-frequency relationship of words. The Journal of General Psychology, 33:251-256.\\n\\n(Acquilex-II Working Papers can be obtained by sending a request to cide@cup.cam.uk)\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9502009.xml'}),\n",
       " Document(page_content=\"Compositionality for Presuppositions over Tableaux1\\n\\nTableaux originate as a decision method for a logical language. They can also be extended to obtain a structure that spells out all the information in a set of sentences in terms of truth value assignments to atomic formulas that appear in them. This approach is pursued here. Over such a structure, compositional rules are provided for obtaining the presuppositions of a logical statement from its atomic subformulas and their  presuppositions. The rules are based on classical logic semantics and they are shown to model the  behaviour of presuppositions observed in natural language sentences built with if ...then, and and or. The advantages of this method  over existing frameworks for presuppositions are discussed.\\n\\nThe Problem of Compositionality for Presuppositions\\n\\nThis work is concerned with the problems of adding the concept of presupposition to a logical language. Although presupposition originates as a natural language phenomenon, for the purposes of the present work sentences will be represented as propositions of a logical language. At this level of granularity, presupposition can simply be represented as a relation between sentences. For instance, the sentence The typewriter is working can be said to presuppose: There is a typewriter.\\n\\nPresupposition has the additional property (often used to characterize it) that the negation of a sentence has the same presuppositions as the sentence itself. For example, the sentence The typewriter is not working has the same presupposition as its positive counterpart given above (namely, that there is a typewriter). The reader is referred to the literature on presupposition for a wider analysis of the implications of this property .\\n\\nA formal language has the property of compositionality if it is possible to describe the meaning of a complex  expression of the language in terms of the meaning of its parts. It is considered a desired property for any formal language. When a logical statement is composed from propositions that presuppose other propositions, it should be possible to  describe the presuppositions of the resulting complex expression in terms of its parts and the presuppositions of its parts. If one takes the natural language connectives if ...then , and  ,  and or   to be related to material conditional, conjunction, and disjunction, natural language examples provide some clues as to what the behaviour of presupposition should be. Sentences (1), (2) and (3) presuppose There is a typewriter; while sentences (4), (5)  and (6) do not.\\n\\n(1) If the typewriter is blue then Sue will be happy.\\n\\n(2) If  you are in Sue's office then the typewriter is blue.\\n\\n(3) Either the typewriter is blue or the chair is.\\n\\n(4) If there is a typewriter then the typewriter is blue.\\n\\n(5) Either there isn't a typewriter or the typewriter is blue.\\n\\n(6) Either the typewriter is blue or there isn't a typewriter.\\n\\nThe behaviour of presuppositions of sentences of this form has traditionally been studied as part of the projection problem for presuppositions, which is concerned with describing the presuppositions of a sentence in terms of the presuppositions of its subordinate clauses. The constructions considered in the projection problem involve nested subordination (verbs of propositional attitude, factive verbs) beyond the natural language connectives treated here.\\n\\nPrevious Work\\n\\nThe framework presented in this work is compared with three other approaches.\\n\\nor\\n\\ncarries with it the assumption that the speaker does not know\\n\\n,\\n\\n,\\n\\n,\\n\\nor\\n\\n, that is, that he considers an information state where all of them are open possibilities. The consistency checks required for defaults must consider these implicatures. The method obscures the issue of how the compositionality for truth values and the compositionality for presupposition are related.\\n\\nThe Tableau Interpretation\\n\\nThe Basic Framework\\n\\nWhere language is simplified to a set of sentences so that the internal construction of each sentence does not play a role, the representation of presupposition can be  restricted to defining the ordered pairs of sentences for which this relationship holds. I assume that such a  relation of presupposing is given for the atomic formulas of the language. To make this information conspicuous without introducing too many definitions, I notate the fact that `\\n\\npresupposes\\n\\n' by writing each instance of\\n\\nas\\n\\n.\\n\\nIn terms of this notation, the behaviour of presupposition concerning negation has the following implication:\\n\\n(For simplicity, I leave out the parentheses in these cases from now on.)\\n\\nFor complex sentences the relation of presupposing  has to be worked out, ideally in a compositional way. This is achieved in the next section by defining rules that govern the compositionality. In order to obtain a simpler formulation of these rules, a specific representation of the logical structure of the connectives is required. This representation is described in the present section.\\n\\nThe logical connectives I am considering have their own definition of compositionality with respect to truth value. In the present framework, these definitions are represented as tableaux expansion rules.\\n\\nTableau expansion rules:\\n\\nClassical negation operates over the language wherever reasoning about negated sentences is required.\\n\\nCoverage Property\\n\\nThe definition of tableau expansion rules ensures that these tableau obey a special property.\\n\\nCoverage Property:\\n\\nIf one branch of a tableau holds a sentence\\n\\n, then every (open) branch of that tableau will hold either\\n\\nor\\n\\n.\\n\\nThis property can be seen to hold: 1) it applies to each one of the expansion rules, 2) the procedure for adding sentences to a tableau is defined in terms of adding the new sentence to every (open) branch (and then applying the expansion rules to it).\\n\\nA consequence of this property in terms of the semantics, is that each branch of the tableau contains a complete atomic truth-value assignment that makes the sentence true. The structures that result are equivalent to classical truth tables. A tableau formulation is retained in spite of this fact because it takes into account that lines of the truth table that become inconsistent as more information is added are dropped out of the reckoning (as an effect of branch closure).\\n\\nThe Formalism as a Decision Method for the Logic\\n\\nThe present framework differs from traditional tableaux only in the definition of\\n\\nexpansion rules.\\n\\nThe traditional definition of\\n\\nexpansion rules is not suitable for presupposition because it specifies the alternatives in the minimal form that preserves soundness and completeness of the logical calculus. This is done in order to simplify the computation of logical consequences. The information that is not specified explicitly as a result of this policy does not affect logical consequences, but it is relevant to presupposition behaviour as described by the compositionality rules given here.\\n\\nThe tableaux given here (presuppositional tableaux or PT) can be used as a decision method in the same way as traditional tableaux (semantic tableaux or ST). A simple way of showing this may be to show that PT tableau are equivalent to ST tableaux. Given that the definitions of tableau, tableau for a set of sentences, closure, proof and refutation are the same in the PT framework as in ST tableaux, the discussion concerns only the expansion rules. Of these, only\\n\\nrules\\n\\ndiffer from one framework to the other.\\n\\nIt is enough to show that under the circumstances\\n\\nunder which ST tableau for a\\n\\nformula becomes closed,\\n\\nthe PT tableau for the same\\n\\nformula also becomes closed\\n\\n(and viceversa).\\n\\nCompositionality of Presupposition over Logical Connectives Compositionality Rules for Presupposition\\n\\nOver the representation of the logical language given in the previous section, compositionality of presupposition can be defined.\\n\\nTwo issues need to be dealt with: 1) how the presuppositions of a branch are ascertained from the presuppositions of the atomic propositions in it, and 2) how the presuppositions of branches of  the same tableau interact.\\n\\nI notate a branch as\\n\\n.\\n\\nI use\\n\\nas shorthand for `the proposition\\n\\nappears in the branch\\n\\n. The notation for presupposition is extended to branches so that\\n\\nstands\\n\\nfor `the branch\\n\\npresupposes\\n\\n'.\\n\\nThe compositionality rules for presupposition do not take into account presuppositional information from branches of a tableau that are closed.\\n\\nRule 1\\n\\nFor an open branch\\n\\nsuch that\\n\\n,\\n\\nunless\\n\\ni)\\n\\n,\\n\\nor\\n\\nii)\\n\\n, or iii)  there is some\\n\\n.\\n\\nRule 2\\n\\nFor a tableau\\n\\nwith at least one\\n\\nopen branch\\n\\n,\\n\\niff\\n\\n.\\n\\nThe determination of presuppositions of a branch is handled in a simple way by considering that presupposition is weaker than assertion, so that it survives only in cases where it does not overlap or clash with any other information. Conditions 1.i and 1.ii  capture the intuitive observation that presupposition is only informative when it concerns a proposition that does not appear in any  literal already in the branch. Condition 1.iii allows the presuppositions of a branch to survive only when there is no conflict between them.\\n\\nThe interaction between presuppositions of different branches can be shown to be trivial in the present framework by application of the Coverage Property. By the Coverage Property, different branches wil have the same atomic propositions and they will differ only in that each atomic proposition may appear negated in one branch and appear unnegated in another. In terms of Rule 1, these differences would affect only\\n\\nand\\n\\nas possible origins of presuppositions and\\n\\nin conditions i) and ii). Because the relation of presupposing is given for atomic propositions, if\\n\\nin  one branch, it is not possible to have\\n\\nin a different branch. The results of applying rule 1 are the same when any of these propositions is exchanged for its negation. This can be used to show that if a tableau has a branch that presupposes a proposition, then every branch of the tableau will presuppose that proposition. Conversely, if a tableau has a (open)  branch that does not presuppose a proposition, then no branch of the tableau will presuppose that proposition.\\n\\nSimple Sentences\\n\\nI give the first few examples in detail to show how the rules operate.\\n\\nExample (1) is a case of behaviour of presuppositions originating in the antecedent of a conditional. Sentence (1) If the typewriter is blue then Sue will be happy corresponds to the following representation.\\n\\nBy application of rule 1, every one of the alternatives presupposes\\n\\n. None of  the conditions holds for any branch. By application of rule 2 to any one of them,\\n\\nis obtained as a presupposition of the compound. This matches expected behaviour.\\n\\nThe sentence (7) Bill regrets that there is no hot water left presupposes (8) There is no hot water left. The problem is to determine what the presuppositions are for sentence (9) If Mary has had a bath, then Bill regrets that there is no hot water left. Assume sentence (9) has the form\\n\\n. The representation for this sentence in this framework would be:\\n\\nApplication of the rules predicts a presupposition\\n\\nfor the sentence.\\n\\nExample (4) is a case of behaviour of presuppositions originating in the consequent of a conditional, in the particular case where the presupposition itself forms the antecedent. Sentence (4) If there is a typewriter then the typewriter is blue corresponds to the following representation.\\n\\nBy rule 1, none of the alternatives under the compound presuppose\\n\\n(condition 1.i holds for the first two columns, condition 1.ii holds for the third one). This agrees with the intuitive behaviour that had to be modelled.\\n\\nDisjunction  presented the most problems in update semantics attempts to model intuitive behaviour in terms of compositionality. Example (5) Either there isn't a typewriter or the typewriter is blue shows the case where the presuppositions of one of the disjuncts are not presuppositions of the disjunction. This sentence corresponds to a structure that is equivalent to that for the conditional of example (4), and gives no presupposition for the compound. This matches the required behaviour.\\n\\nUnlike update semantics methods, this approach can also handle the symmetrical version of the disjunct with no problems. For the sentence in example (6) Either the typewriter is blue or there isn't a typewriter the corresponding expansion contains the same literals in each branch but in a different order. Since order in the branch plays no role in the compositionality rules, the predictions are the same as for the previous case.\\n\\nThe method can also handle the case of disjunctions with contradictory presuppositions. Take for instance the sentence in example (10) Either Bill has started smoking or Bill has stopped smoking. This has the following representation:\\n\\nIn this case, the method  predicts (condition 1.iii)  no presupposition for any column. Application of rule 2 gives no presupposition for the whole compound.\\n\\nComplex Sentences\\n\\nComplex sentences are treated as follows. First the sentence is expanded into a tableaux by application of the expansion rules. Then the compositionality rules are applied to the resulting tableau  in order to obtain the presuppositions of the sentence.\\n\\nSentence (11) If John is married and he has children, then his children are at school can act as an example. Assuming a logical form for this sentence\\n\\n, the tableau for this sentence would be:\\n\\nThe presupposition rules applied to this tableau predict no presupposition. (Rule 1 fails for all branches).\\n\\nDiscourses\\n\\nWhen logical statements of the type considered so far are strung into a sequence of assertions, a discourse is obtained. An appropriate treatment of discourses is required to study the effect of context on presuppposition interpretation. A discourse is represented by the tableaux for the set of formulas in it.\\n\\n. A representation for the discourse  (12),(9) or\\n\\nwould be the following tableau:\\n\\nOver this representation it can be seen that the rules correctly predict no presupposition for the discourse as a whole, even though sentence (9) on its own did have a presupposition.\\n\\nTraditional Presuppositional Concepts in  Terms of Tableaux\\n\\nThe tableaux framework allows definition of some of the traditional concepts that surround presupposition.\\n\\nThe presupposition\\n\\nof a presuppositional sentence\\n\\nadded to a tableau\\n\\nis satisfied if the tableau\\n\\nis closed.\\n\\nThe presupposition\\n\\nof a presuppositional sentence\\n\\nadded to a tableau\\n\\nis canceled if the tableau\\n\\nis closed.\\n\\nThese two definitions correspond to the intuitive concepts of satisfaction and cancelation. However, it is clear that there will be cases when some branches of a tableau are closed by\\n\\nand\\n\\nsome by\\n\\n. These hybrid cases between satisfaction and cancelation escape the simpler analysis and give rise to the need for projection rules. In their simplest manifestation,  hybrid cases occur as the traditional  cases of problematic projection. These involve sentences (4), (5) and  (6) given above. More complex manifestations concern discourses where the effect of context plays a role in the interpretation of presupposition. The discourse constructed with sentences (12) and  (9) is an instance of these cases. In all these examples  it holds that for any of the tableau representations some branches of the tableau are closed by the presupposition involved and some by its negation. Under those circumstances, the traditional definitions of satisfaction and/or cancelation could not account for the resulting presuppositional behaviour.\\n\\nThe present framework achieves this by allowing presupposition to be blocked locally by either\\n\\nor\\n\\n(conditions 1.i and 1.ii ).\\n\\nCritical Analyisis Advantages over van der Sandt\\n\\nThe framework presented here is closely related to that proposed by van der Sandt. Both can be interpreted as  a  branching structure for a sentence based on the connective words that appear in it. The advantages of this framework are that 1) it makes explicit the semantics that are attributed to the connectives, and 2) logical consistency (both at sentence level and at branch level) and logical consequence are explicitly taken into account in the framework.\\n\\nAdvantages over Beaver\\n\\nThe behaviour of all the connectives (including disjunction) is described satisfactorily by the rules given. No specific rules for each connective are required. The compositionality rules rely on the semantics in general terms. As a result, the same compositionality rules may be applied to other connectives if their semantics can be represented in the same framework in a way that preserves the Coverage Property.\\n\\nThe general  approach that starts with Karttunen and leads to Beaver is criticized for not being able to justify their choice of method for obtaining presuppositions of compounds on grounds other than that it describes the behaviour. In the present framework the method is given simply by attributing a weak informative  status to presupposition (so that it is overridden whenever it overlaps or conflicts with explicit  information or other presuppositions).\\n\\nAdvantages  over Mercer\\n\\nIn the present framework the information that Mercer must introduce as pragmatic implicatures can be read off the representation. An expansion rule like:\\n\\ncaptures the idea that each of\\n\\n,\\n\\n,\\n\\nand\\n\\nis  valid in at least one of the possible alternatives given. This property of the present framework is a direct result of the insistence on taking the semantics - as given by the internal structure of the sentence in terms of connectives - into account.\\n\\nThe method followed by the compositional rules is quite close to Gazdar's method for computing presuppositions simply in terms of consistency with the context. However, it presents two major innovations: 1) it allows both satisfaction and cancelation (presuppositions disappear when inconsistent and/or when already present), and 2) it applies the procedure locally. The combination of these two innovations allows adequate treatment of hybrid cases.\\n\\nConclusions\\n\\nPresuppositional behaviour in sentences can be described in relation to the implicit logical structure represented by the appearance of natural language connectives if ...then , and  ,  and or   in the sentences to be interpreted.\\n\\nThe framework presented here provides a method for determining the behaviour of presuppositions of complex logical statements in a compositional manner.\\n\\nThe predictions of the proposed method match the observed behaviour in traditional examples.\\n\\nThe semantics used in the present framework are chosen to ensure that all the different valid alternatives implied by a sentence are listed explicitly in a tableau for that sentence, and all atomic formulas involved appear (either negated or not) in every branch (Coverage Property). These constraints on the semantics allow all the predictions of the framework to be explained in terms of two basic assumptions: presuppositions are only considered informative where they do not overlap with  or contradict asserted information, and where they do not conflict with other presuppositions.\\n\\nBibliography\\n\\nBeaver, David, `The Kynematics of Presupposition', in: H. Kamp (ed. ), Presupposition, Dyana Report R2.2.A, Part II, Centre for Cognitive Science, University of Edimburgh, August 1993.\\n\\nBeaver, David, What Comes First in Dynamic Semantics, Institute for Logic, Language and Computation, Amsterdam, 1993.\\n\\nFitting, Melvin, First Order Logic and Automated Theorem Proving,  Springer Verlag, New York, 1989.\\n\\nGazdar, Gerald, Pragmatics,  1979.\\n\\nGazdar, Gerald, `A Solution to the Projection Problem' in: Syntax and Semantics 11. Presupposition, 1979.\\n\\nGervs, Pablo,  Logical Considerations in the Interpretation of Presuppositional Sentences, PhD Thesis, Department of Computing, Imperial College of Science, Technology and Medicine, London, forthcoming.\\n\\nHeim, Irene, The Semantics of Definite and Indefinite Noun Phrases, PhD Thesis, University of Massachusetts, 1982.\\n\\nHeim, Irene, `On the projection problem for presuppositions', in: D. Flickinger et al (eds. ), Proceedings of the Second West Coast Conference on Formal Linguistics, 1983. Reprinted in:  S. Davis (ed. ), Pragmatics, OUP, New York, 1991.\\n\\nKarttunen, Lauri, `Presupposition of Compound Sentences', Linguistic Inquiry, Volume IV, Number 2 (Spring 1973).\\n\\nKarttunen, Lauri, `Presupposition and Linguistic Context', Theoretical Linguistics I, 1974.\\n\\nKarttunnen and Peters `Conventional Implicature', in: Syntax and Semantics 11. Presupposition, 1979.\\n\\nMercer, Robert Ernest, A Default Logic Approach to the Derivation of Natural Language Presuppositions,Technical Report 87-35, Department of Computer Science, University of British Columbia, Vancouver, B.C.,Canada.\\n\\nMercer, Robert, `Towards a Common Logical Semantics for Presuppositions and Entailment', Journal of Semantics, Volume 9, Number 3, 1992, pp 223-250.\\n\\nVan der Sandt,  Rob A., `Presupposition Projection as Anaphora Resolution', Journal of Semantics, 9, 1992,  pp333-377. Helm, London, 1988.\\n\\nFootnotes\\n\\nPresented in: Computational Logic for Natural Language Processing, (A Joint COMPULOG/ELSNET/EAGLES Workshop), April 3-5, 1995, Edinburgh. The most important consequence is that attempts to model presupposition as an entailment lead to  characterizations where all presuppositions are tautologies in the classical sense, which is an undesirable result. This is based on the argument that if\\n\\nand\\n\\nthen it must be the case that\\n\\n. Because presuppositions are observed to be informative in their actual use by language speakers, this way of modelling them is not useful. For\\n\\nand\\n\\n, the negated and the unnegated version have the same presupppositions; for\", metadata={'source': '../data/raw/cmplg-xml/9505014.xml'}),\n",
       " Document(page_content=\"Semantics of Complex Sentences in Japanese\\n\\nThe important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively. However if there can be relations between every pair of semantic roles, the amount of computation to identify the relations that hold in the given sentence is extremely large. In this paper, for semantics of Japanese complex sentence, we introduce new pragmatic roles called observer and motivated respectively to bridge semantic roles of subordinate and those of main clauses. By these new roles constraints on the relations among semantic/pragmatic roles are known to be almost local within subordinate or main clause. In other words, as for the semantics of the whole complex sentence, the only role we should deal with is a motivated.\\n\\nIntroduction\\n\\nOur aim is to formalize constraints that are needed to develop a parser based on unification grammar (called ``UG'' henceforth) so that our parser can deal with variety of types of sentences in Japanese. However just parsing syntactically is not enough for natural language understanding. One important and necessary task to be done, when a parser processes a discourse in Japanese, is the so called zero anaphora resolution. All of syntactic, semantic, and pragmatic constraints are to be involved to resolve zero anaphora. Of course, some of omitted pronouns are syntactically resolved. For instance, VP with suffix te is not regarded as a clause but a conjunct VP. Therefore the subject of the VP with te, which is possibly omitted from surface, should corefer with the subject of the sentence. One example is\\n\\n`Hanako felt  cold and closed the window.'\\n\\n1. `Since Hanako behaved like feeling cold, I closed the window.' 2. `Since I behaved like feeling cold, Hanako closed the window.'\\n\\nThen we shift our attention to more microscopic one, in which ,roughly speaking, the important part of semantics of complex sentence is formalized as relations among semantic roles that appear in the main clause or the subordinate clause. At the first glance, the constraints about these relations are not local in terms of main or subordinate clauses. In other words, semantic roles that appear in subordinate clause and semantic roles that appear in the main clause seem to be directly constrained by the constraints of complex sentence. However, looking more carefully, we find that the constraints of subordinate clause and the constraints of main clause are represented as local constraints by introducing the new notion of motivated which is characterized as a person who has enough reason to act as the main clause describes. More precisely, motivated is one of the pragmatic roles that appear in a subordinate clause, and the constraints in subordinate clause are stated as identity relations between motivated and other semantic/pragmatic roles appearing in subordinate clause. Therefore these constraints are local in subordinate clause. The constraints in main clause are stated as identity relations between motivated which comes from subordinate clause, and other semantic roles appearing in main clause. Therefore in understanding the main clause we don't have to be care about semantic/pragmatic roles in subordinate clause other than a motivated.\\n\\nThe next question is how to represent the semantics of complex sentence in feature structure( called FS henceforth ). For this, we should write down the constraints about these relations among semantic/pragmatic roles in a feature structure formalism. Due to the space limitation, in this paper we mainly pursue the constraints about semantic feature structures.\\n\\nHierarchical Structure of Complex Sentence\\n\\nIn Fig.1 , Sub-Clause and Conjunct mean subordinate clause and conjunctive particle respectively. Note that Fig.1 represents not only the hierarchical structure but also the word order of a complex sentence in Japanese. The structure is almost the same as Gunji's structure except for explicitly showing complex proposition, subordinate-clause and conjunctive-particle that are newly added to deal with complex sentences. Note that `Comment' appearing in `Sub-Clause' has the same structure as `Comment' appearing just below `Judgement'. That is to say, `Comment' is recursively defined. However, in practice, the more the level of depth of recursively appearing `Comment' is, the less comprehensible the sentence is.\\n\\nSubordinate Clause\\n\\nDefinition  1 (Observer) Observer is a person who directly observes or is indirectly informed the situation described by the proposition part. Therefore an observer has a certain evidence to be convinced that that situation actually happens.\\n\\nAs for an observer introduced by garu, one of the widely known consequence about the nature of subjective predicate is the following. In a sentence, if a subjective adjective is used without being followed by a verbal suffix garu, the experiencer of the subjective adjective should be the speaker of the sentence.\\n\\nThe next thing we should do about a newly introduced notion of observer is to make clear the way to deal with it in FS. First of all, in our FS, a semantic content:SEM is basically a soa (state of affair) form of situation semantics. However we use semantic role like ``agent'', ``patient'', ``experiencer'', and so on, as argument roles of soa. Since an observer observes the situation which is characterized by a soa, if we know that there exists an observer, the observed soa is embedded in observing situation, which, in turn, is embedded in the whole semantic content. In this sense, the observed soa's argument role is observed. But as far as we have no confusion, we omit role name `observed' henceforth. A typical schema of SEM of FS of this type is the following. Note that we use garu as a value of the relation feature meant by `rel.' The English gross of this relation garu is `observe.'\\n\\nIf the sentence finishes just after ``garu/gat-ta'', the important points are 1) an introduced observer is the speaker, and consequently 2) the experiencer cannot be the speaker. If a clause with ``garu/gat-ta''is a subordinate clause, the experiencer cannot be identified with a semantic role corresponding to the subject of main clause or higher clause.\\n\\nAs for category 2, subjective verbs like ``kurusimu''(feel sick) and ``kanasimu''(feel sadness) that describe subjective and/or emotional experience in verb form, are used. Like the case of garu, an observer who observers the experience can be introduced. However this observer is not obligatory. Therefore unlike the ``garu/gat-ta'' case, the experiencer also can be an obligatory semantic role of higher clause as well as the speaker.\\n\\nComplex Sentence\\n\\nFeature Structure\\n\\nwhere English grosses of relation name is the following: sime:`close', node:`because', samu-i:`feel cold'.\\n\\nDefinition  2 (Motivated) Motivated is a person who is affected by the situation described by the subordinate clause deeply enough to feel or act as the main clause describes.\\n\\nThe important and indispensable part of semantics of complex sentence is, roughly speaking, the relation between a subordinate clause and the main clause. But if you look more closely, this relation is actually the relations among semantic/pragmatic roles appearing in the subordinate clause and those appearing in the main clause. The newly introduced role of motivated gives the most important clue for this relation. Therefore, in the rest of this paper, our effort will be concentrated into whom a motivated refers to. More precisely, in FS, our main concerns are which semantic role in the SEM of subordinate clause the motivated can or cannot be unified with, and which semantic role in the SEM of main clause the motivated can or cannot be unified with.\\n\\nConstraints\\n\\nwhere `name' means a name of each constraint.\\n\\nIn the rest of this section we show the examples that exemplify these constraints.\\n\\nLook at the following pair of example.\\n\\nIn this case, a person whose wallet was stolen is not explicit but regarded as an affected. Another case having an affected is that a relational noun is the subject of transitive passive. Then a person who is in the relation expressed by the relational noun is thought to be affected by that situation ,too. Here we take `mother', `father', `daughter', `son', `supervisor', and so forth as a relational noun. A couple of example sentences are the following.\\n\\n`Since his henchman was attacked, the boss retaliated.'\\n\\n`Although his henchman was attacked, the boss didn't retaliate.'\\n\\nIn sum, with these constraints, a constraint satisfaction process in UG based parsing can be done locally and consequently very efficiently. In other words, primarily a constraint satisfaction process of a subordinate clause can be done within the analysis of subordinate clause, and that of the main clause can be done within it except for using motivated whose value has already been constrained in the subordinate clause.\\n\\nRelated Works and Conclusions\\n\\nAnalysis of case in which a directional auxiliary verb i.e. `yaru',`kureru' is used is left as the future problem. Finally, we implemented a Japanese language understanding system based on the theory we state in this paper, but due to the space limitation we will report the detail of implementation in other place in the near future.\\n\\nBibliography\\n\\nBrennan, S., M. Walker Friedman and C.Pollard (1987). A Centering Approach to Pronouns. 25th Annual Meeting of ACL, pp.155-162\\n\\nGunji, T.(1987). Japanese Phrase Structure Grammar. Reidel, Dordrecht\\n\\nGunji,T. (1989). Relevance of the Formalization of Phrase Structure Grammar to Mechanical Language Processing. Report of Tokutei-Kenkyu, Ministry of Education and Academy\\n\\nIida,M. and P.Sells(1988). Discourse Factors in the Binding of zibun. in Japanese Syntax (ed. W.Poser) CSLI, Stanford\\n\\nKameyama, M. (1988). Japanese Zero Pronominal Binding: Where Syntax and Discourse Meet. in Japanese Syntax (ed. W.Poser) CSLI, Stanford\\n\\nKatagiri,Y. (1991). Perspectivity and Japanese Reflexive `zibun'. in CSLI Lecture Notes No.26, Situation Theory and its Applications Vol.2, J.Barwise et al eds. pp.425-447\\n\\nKuno, S. (1973). The structure of the Japanese Language. Cambridge, MIT Press\\n\\nKuno,S.(1978). Danwa no Bunpou. Taishukan, Tokyo\\n\\nOhye,S.(1975). Nitieigo no Hikakukenkyu. Taishukan, Tokyo\\n\\nSaito,R.(1992). Shinjou Jutugo no Goyouronteki Bunseki (Pragmatic Analysis about Psychological Predicates). Nihongogaku, Vol.11, No.6, pp.110-116\\n\\nMikami,A.(1953). Gendai Gohou Josetu.\\n\\nKuroshio\\n\\n\\n\\nShuppan, Tokyo\\n\\nMinami,F.(1974). Gendai Nihongo no Kouzou. Taishukan, Tokyo\\n\\nPalmer, F.R.,(1986). Mood and Modality. Cambridge University Press,Cambridge\\n\\nSells, P. (1985). Lectures on Contemporary Syntactic Theories. CSLI Stanford\\n\\nTakubo,Y. (1987). Tougokouzou to Bunmyakujouhou (Syntactic Structure and Contextual Information. Nihongogaku 1987-5,Meiji-shoin,Tokyo\\n\\nTeramura,H.(1984). Nihongo no sintakusu to imi II `Japanese Syntax and Semantics II'. Kuroshio-Shuppan,Tokyo\\n\\nTeramura,H.(1990). Nihongo no sintakusu to imi III `Japanese Syntax and Semantics III'. Kuroshio-Shuppan,Tokyo\\n\\nTsuda,H.,Hasida,K.,Sirai,H. (1989). JPSG Parser on Constraint Logic Programming. 4th ACL European Chapter\\n\\nWalker,M.,M. Iida and S. Cote(1990). Centering in Japanese Discourse. COLING'90\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9405028.xml'}),\n",
       " Document(page_content=\"Incorporating ``Unconscious Reanalysis'' into an Incremental, Monotonic Parser\\n\\nThis paper describes the author's implementation of a parser aimed at reproducing,  in a computationally explicit system,  the constraints of a particular psycholinguistic model (Gorrell in press). In Gorrell's  model, ``unconscious'' garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation, but there is no discussion as to how the parser decides which relations to add. We model this decision as a search for a node in the tree at which an explicitly defined parsing operation,  tree-lowering may be applied. With reference to English and Japanese processing data, we show  the importance of this search for  empirical adequacy of the  psycholinguistic model.\\n\\nConscious and Unconscious Garden Paths\\n\\nCertain researchers in the psycholinguistic community (Pritchett (1992), Gorrell (in press)), have argued for a binary distinction between two distinct types of garden path sentences. Conscious garden paths, such as (1) below,  are locally ambiguous sentences which give rise to reanalysis that is both experimentally detectable and causes a conscious sensation of difficulty or ``surprise effect''. Unconscious garden paths, on the other hand, such as (2),  cause reanalysis which is experimentally detectable,  but which is generally  not ``noticed'' by the speaker or hearer.\\n\\n(1) While John was eating the ice cream melted.\\n\\n(2) John knows the truth hurts.\\n\\nThis binary distinction has often been used to motivate a two-level architecture in the human syntactic processing system, where  what we will call the ``core parser'' performs standard attachment, as well as being able to reanalyse in the easy cases (such as on reaching   hurts in (2)), but where the assistance of a higher level resolver  (to use Abney's terminology (1987, 1989)), is required to solve the difficult cases, (such as on reaching melted in (1)). This ``core parser'' has  been the subject of a number of computational implementations, including Marcus's deterministic parser (1980), Description theory (henceforth, D-theory) (Marcus et al (1983)), and  Abney's licensing based model (1987, 1989). It has also been the subject of a number of psycholinguistic studies on a more theoretical level (Pritchett (1992), Gorrell (in press)).\\n\\nThe implementation described in this paper is based on the most recent model, that of (Gorrell (in press)). This model is interesting in that  it does not allow the parser to employ delay tactics,  such as using a  lookahead buffer  (Marcus (1980), Marcus et al (1983)), or  waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987, 1989), Pritchett  (1992)). Instead, processing is guided by the principle of Incremental Licensing, which states that ``the parser attempts incrementally to satisfy the principles of grammar''. For the purposes of this implementation, I have interpreted this to  mean that each word must be attached into a fully-connected  phrase marker as it is found in the input. The psychological desirability of such a Full Attachment model has been argued for, especially with regard to the processing of head-final languages, where evidence has been found of pre-head structuring (Inoue  Fodor (1991), Frazier (1987)). Such models  have also been explored  computationally (Milward (1995), Crocker (1991)).\\n\\nD\\n\\n\\n\\ntheory and Gorrell's Model\\n\\nGorrell employs the D-theoretic device of building up a set of dominance and precedence relations between  nodes,  where the set is intended to be constrained by informational monotonicity, in that once asserted to the set, no relation may be deleted or overridden. Gorrell restricts this constraint to Primary structural relations (i.e. dominance and precedence), while   secondary relations (e.g. thematic and case dependencies)  are not so constrained. Recall (2), repeated below:\\n\\n(2) John knows the truth hurts.\\n\\nAt the point  where John knows the truth has been processed, a complete clause will have been built:\\n\\n(3)  [S [NP1 John] [VP [V knows] [NP2 the truth]]\\n\\nThe description will include the information that the verb    knows precedes NP2, and that the VP dominates    NP2.\\n\\n{..., prec(V,NP2), dom(VP, NP2), ...}\\n\\nHowever,  on the subsequent input of hurts, the structure can be reanalysed by asserting an extra clausal node (call it S2) dominating  NP2 (which will then become the embedded subject), but which is in turn dominated by the matrix VP. This can be achieved by adding the following structural relations to the tree description {prec(V,S2), dom(VP,S2), dom(S2,NP2)}\\n\\n(4). [S [NP1 John] [VP [V knows] [S2 [NP2the truth] [VP2 hurts]]]]\\n\\nSince the description before the processing of the disambiguating word   hurts is a subset of the final tree description, the monotonicity requirement is satisfied. Note in particular, that, because dominance is a transitive relation, and because of the inheritance condition on trees (a node inherits the precedence relations of its ancestors), the two statements  dom(VP,NP2) and prec(V,NP2) remain true after reanalysis.\\n\\nNote also that the model will correctly fail to reanalyse for sentence (1)  above, since the reanalysis will require the retraction of the domination relation between the VP of the adverbial clause and the NP the ice cream.\\n\\nImplementation\\n\\nAlthough Gorrell proposes a general principle to guide initial attachment decisions (Simplicity: No vacuous structure building), and specifies the conditions under which ``unconscious reanalysis'' may occur, the model leaves unspecified the problem  of how the system may be implemented. Of particular interest is the problem of how the parser decides which relations to add to the set at each point in time, especially at disambiguating points.\\n\\nLexical Representation\\n\\nThe basic framework on which the implementation is built is similar to Tree Adjoining Grammar (Joshi et al 1975). Each lexical category is associated with a set of structural relations,  which determine its lexical subtree. We call this set the subtree projection of that lexical category. For example,   the subtree projection for verbs in the English grammar is as follows, where Lex is a variable which will be instantiated to the actual verb found in the input.\\n\\n{dom(S,NP), dom(S,VP), dom(VP,V),\\n\\ndom(V,Lex), prec(NP,VP)}\\n\\nLexical categories are also associated with lists of left and right attachment sites. In the above case, NP, (which will correspond to the subject of the verb), will be unified with the left attachment site. If a transitive verb is found in the input, then the parser consults the verb's argument structure and creates a new right attachment site for an NP, asserting also that this new NP is dominated by VP and preceded by V.\\n\\nAttachment\\n\\nSimple attachment can be performed in two ways, which are defined below, where the term current tree description is intended to denote the the set of structural relations built up to that point in processing:\\n\\nIntuitively, left attachment may be thought of in terms of attaching the current tree description to the left corner of the projection of the new word, while right attachment corresponds to attaching the projection of the new word to the right corner of the current tree description. They are equivalent to Abney's   Attach-L and Attach respectively.\\n\\nTree Lowering\\n\\nWe will call this operation  ``tree-lowering''. Intuitively, the operation   finds a node on the current tree description which matches the left attachment site of the projection of the new word, and attaches it, while inserting the root of the new projection in its place. The result is that the node chosen is ``lowered'' or ``subordinated''.\\n\\nIn order to maintain structural coherence, the new word attached via tree-lowering must be preceded by all other words previously attached into  the description. We can guarantee this by requiring the lowered node to dominate the last word to be attached. We also need to ensure that, to avoid crossing branches, the lowered node does not dominate any unsaturated attachment sites (or ``dangling nodes'') We therefore define accessibility for tree-lowering as follows:\\n\\nDEFINITION Accessibility: Let N be a node in the current tree description. Let W be the last word to be attached into the tree. N is accessible iff N dominates W, and N does not dominate any unsaturated attachment sites.\\n\\nIt will  be noticed that tree-lowering is similar in spirit to the adjunction operation of Tree Adjoining Grammars (Joshi et al, 1975). The difference is that the foot and root nodes of an auxiliary tree in TAG, (corresponding to the ``lowered'' node and the node that replaces it respectively) must be of the same syntactic category, whereas, as we have seen in this example, in the model proposed here, the two nodes may be of different categories, so long as the resulting structure is licensed by the grammar.\\n\\nIn the case of example (2), at the point where the truth has been processed, the parser must find an accessible node which matches the category of the left attachment site of hurts (i.e. an NP). The only choice is NP2:\\n\\n(3)  [S [NP1 John] [VP [V knows] [NP2 the truth]]\\n\\nNow, all the local relations in which NP2 participates are found:\\n\\n{dom(VP,NP2), prec(V,NP2)}\\n\\nand NP2 is substituted with the root of the new projection, S2 to derive two new relations:\\n\\n{dom(VP,S2), prec(V,S2)}\\n\\nThese relations are found to be licensed, because the verb which   V dominates (``knows'') may subcategorise for a clause, so these new relations are added to the set. Now, adding the subtree projection of hurts to the set, and  unifying its left attachment site with NP2 results in the derived structure with NP2 ``subordinated'' into the lower clause.\\n\\n[S [NP1 John] [VP [V knows] [S2 [NP2 the truth] [VP2 hurts]]]]\\n\\nWith the tree-lowering operation so defined, the problem of finding which relations to add to the set at a disambiguating point reduces to a search for an accessible node at which to apply this operation. However, this implies that, if more than one such node exists, the parser must be given a preference for making the requisite decision. Consider the following sentence fragment, for example:\\n\\n(5) I know [NP1 the man who believes [NP2 the countess]]...\\n\\nIf the input subsequently continues with a verb, then we have a choice of two nodes for lowering, i.e. NP1 and NP2. Though no experimental work has been done on this type of sentence,  there seems to be an intuitive preference for the lower attachment site,   NP2. In (6), binding constraints force lowering to be applied at NP2, while in (7), it must be applied at NP1. Of the two, most native English speakers report (6) to be easier.\\n\\n(6) I know  the man who believes  the countess killed herself.\\n\\n(7) I know the man who believes the countess killed himself.\\n\\nNote also, that, on standard X-bar assumptions, the attachment of post-modifiers may be derived via lowering at an X' node. In this case, the lowered node and its replacement will be of the same syntactic category (like the root and foot node of a TAG auxiliary tree). Researchers have noted a general preference for low attachment of post-modifiers (this is accounted for by the principle of late closure (Frazier and Rayner, 1982)). This would suggest that a reasonable search strategy for English would be to search the set of accessible node in a bottom-up direction for English.\\n\\nThe algorithm is constructed in such a way that lowering is only attempted in cases where simple attachment fails. This means that arguments (which are incorporated via simple attachment) will be attached preferentially to adjuncts (which are incorporated via lowering). This captures the general preference for argument over adjunct attachment, which is accounted for by the principle of   Minimal attachment in Frazier and Rayner (1982), and by the principle of simplicity in Gorrell (in press).\\n\\nProcessing Japanese\\n\\nMain/subordinate clause ambiguity\\n\\nJapanese presents a challenge for any incremental parsing model because, typically, it is not possible to determine where an embedded clause begins. Consider the following example:\\n\\n(8)  John ga [i ronbun wo kaita] seitoi wo hometa. John NOM essay ACC wrote student ACC praised ``John praised the student who wrote the essay''\\n\\nUp to the first verb kaita (``wrote''), the string is interpretable as a full clause (without a gap), meaning ``John wrote an essay'', and the incremental parser builds the requisite structure. However, the appearance of the head noun seito (student) means that at least part of the preceding clause must be reinterpreted as a relative clause including a gap (note that there is no overt relative pronoun in Japanese). One way of looking at what is happening here is to see    the subject NP   John ga as being dissociated from the clause in which it is originally attached, and reattached into  the main clause. But looking at it from a different perspective, as Gorrell has noted (in press), one can see the subject NP as remaining in the main clause, and the constituent bracketed in (8), (ronbun wo kaita (``wrote an essay''))  as being lowered into the relative clause. If this is possible, then we would expect examples like (8) to be unconscious garden paths, and this does indeed seem to be reflected in the intuitive data (see Mazuka and Itoh (in press)). However, if we are to allow our parser to handle such examples, we must expand the definition of tree-lowering, since, in order to build a relative clause, we have to assert extra   material (including the empty subject and the new S node),  which is not justified solely by the lexical requirements of the disambiguating word, the head noun seito.\\n\\nMinimal Expulsion\\n\\nInoue (1991), describes a ``minimal expulsion strategy'', which predicts a preference, on reanalysis, towards expelling the minimum amount of material from the clause. In our terms, this means that (assuming a binary right-branching clause structure, with the verb in its right corner)  the node selected for lowering must be as high as possible. This means that the bottom-up search which we use for English will wrongly predict a Maximal expulsion strategy. In  cases such as (8), assuming the bottom-up search,  when a post-clausal noun has been  reached in the input, the parser starts its search  from the node immediately dominating the last word to be incorporated, (i.e. the verb of what will become the relative clause). This means that,  in cases such as (8), the first preference  will be to lower the verb (and therefore ``expel'' both  subject and object), whereas the human preference, (to lower the object and verb, and therefore expel only the subject) is the parser's second choice on the bottom-up search strategy.\\n\\nMazuka and Itoh (in press) note that examples where both subject and object must be expelled from the relative clause, as would be the first choice in a bottom-up search, often cause a conscious garden path effect. An example, adapted  from Mazuka and Itoh is the following:\\n\\n(9) Yamasita ga yuuzin wo [ i houmonsita] kaisyai de mikaketa. Yamasita NOM friend ACC   visited company LOC  saw\\n\\n``Yamasita saw his friend at the company he visited.''\\n\\nIn order to capture  the minimal expulsion strategy in this class  of Japanese examples, therefore, search  for the lowering node should be conducted top-down. We are currently investigating the consequences of changing the search strategy in this way.\\n\\nThe Problem of Retrospective Reanalysis\\n\\nHaving formulated the constraints of Gorrell's model in terms of the    accessibility  of a node for tree-lowering, we can see that the model can be falsified if we can find a  case where the relevant disambiguating information comes at a point in processing where the node  which is required  to be lowered   is no longer accessible. Consider the following pair of sentences:\\n\\n(10) I saw the man with the moustache.\\n\\n(11) I saw the man with the telescope.\\n\\nIt is familiar from the psycholinguistic literature that there is a preference for attaching the with phrase as an instrumental argument of the verb (as in (11), on the reading where the telescope is the instrument of seeing). On the assumption that saw selects for a PP instrumental argument, we can derive this preference in the present model via the preference to attach as an argument as opposed to an adjunct. However, since we are constrained by incrementality, we will have to make an  attachment decision for  the PP as soon as the preposition with is encountered, and it will be attached in the preferred reading as a sister of the verb. This means that, in cases such as  (10), where, on the globally acceptable reading,  the PP is an  adjunct of the NP the man, this attachment will have to be revised, and the PP retrospectively adjoined into the relevant N'node. However, once the preposition with has been attached, the required N' node will no longer be accessible, and a conscious garden path effect will be predicted, which, intuitively, does not occur. Note that there is no garden path effect even if the preposition is  separated from the disambiguating head noun by a series of adjectives:  (``I saw the man with the neat, quaint, old-fashioned moustache/telescope'').\\n\\nThe same result obtains if we abstract away from the particular implementational details of tree-lowering, and return to the abstract level at which  Gorrell states his model. Once the PP has been attached as an argument of the verb, it can never be reanalysed as the adjunct of the preceding NP, because the NP will precede the PP before reanalysis, and dominate it after reanalysis, which is against the  ``exclusivity condition'' on trees  (i.e. no two nodes may stand in both a dominance and a precedence relation).\\n\\nA similar problem concerns examples such as the following, from Gibson et al (1993):\\n\\n(12) the lamps near the paintings of the house [that was damaged in the flood].\\n\\n(13) the lamps near the painting of the houses [that was damaged in the flood].\\n\\n(14)  the lamp near the paintings of the houses [that was damaged in the flood].\\n\\nin the above,  Gibson et al have manipulated number agreement to force low (12), middle (13) and high (14) attachment of the bracketed relative clause. The results of their on- and off-line experiments show clearly that the low attachment (corresponding to 12) is easiest, but the middle attachment (corresponding to (13)) is most difficult. This behaviour cannot be captured whether we adopt  a bottom-up or a top-down search for tree-lowering. However, even if we can  incorporate the required preferences into the parser, the constraint of incrementality will force us to make the decision on encountering that. This means that, assuming we decide initially to attach low, but number agreement on was subsequently forces high attachment, as in (14),  then a conscious   garden path effect will be predicted, as lowering cannot derive the reanalysis. This is true on the abstract level as well, since there will be nodes in the description which precede the original low position of the relative clause, but are dominated by the subsequent high position of the relative clause. However, intuitively, of the above sentences, it is only (13) which causes the conscious garden path effect.\\n\\nConclusion\\n\\nThe current implementation shows that the success of an abstract model such as Gorrell's depends crucially on the  computational details of the processing algorithm used. The search for the lowering site is of particular importance. In the final section we have seen that the combination of informational monotonicity with the assumption of strict incrementality results in a system which is too constrained to capture all the processing data. Future research will be aimed at determining, firstly, how we can enrich the  information to which the search strategy is sensitive in order to provide a better match with human preferences, and secondly, which constraints should be relaxed in order to avoid the problem of undergeneration.\\n\\nReferences\\n\\nAbney, S. P. (1987): Licensing and Parsing. Proceedings of NELS 17 p.1-15, University of Massachusetts, Amherst\\n\\nAbney, S. P. (1989): A computational model of human parsing. Journal of Psycholinguistic Research 18 p.129-144\\n\\nCrocker, M. W. (1991): A Logical Model of Competence and Performance in the Human Sentence Processor. PhD thesis, Dept. of Artificial Intelligence, University of Edinburgh, Edinburgh, U.K.\\n\\nFrazier, L. (1987): Syntactic processing: Evidence from Dutch. In Natural Language and Linguistic Theory 5.4 p.519-559\\n\\nFrazier, L. and K. Rayner, (1982): Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. Cognitive Psychology 14 p.178-210\\n\\nGibson, E., N. Pearlmutter, E. Canesco-Gonzalez and Greg Hickok (1993): Cross-linguistic Attachment Preferences: Evidence from English and Spanish. (ms. submitted to Cognition)\\n\\nGibson, E. and  N. Pearlmutter, E. (1994): A corpus-based account of Psycholinguistic Constraints on Prepositional Phrase Attachment (in C. Clifton, L. Frazier and K. Rayner (eds) Perspectives on Sentence Processing New York: Lawrence Erlbaum\\n\\nGorrell, P. (in press): Syntax and Perception. to be published by Cambridge University Press\\n\\nInoue, A. (1991): A comparative study of parsing in English and Japanese. PhD thesis, University of Conneticut.\\n\\nInoue, A. and J.D. Fodor (in press): Information-paced parsing of Japanese. (to appear in Mazuka  Nagai (eds))\\n\\nJoshi, A.K., L.S. Levy, and M. Takahashi, (1975): Tree Adjunct grammars. Journal of Computer and System Sciences 10, p.136-163\\n\\nMarcus, M. (1980): A Theory of Syntactic Recognition for Natural Language Cambridge, MA: MIT Press\\n\\nMarcus, M., D. Hindle, and M. Fleck (1983): D-theory: Talking about talking about trees. Association for Computational Linguistics 21 p.129-136\\n\\nMazuka, R. and  K. Itoh  (in press): Can Japanese be led down the garden path? (to appear in Mazuka and Nagai)\\n\\nMazuka, R., and Nagai (eds) (to appear): Japanese Syntactic Processing Hillsdale, NJ: Lawrence Earlbaum\\n\\nMilward, D. (1995): Incremental Interpretation of Categorial Grammar. in Proceedings of EACL (this volume)\\n\\nMitchell, D.C.  Cuetos, F. (1991): The origins of parsing strategies. Conference proceedings: Current issues in natural language processing University of Texas at Austin, TX\\n\\nPartee, B., A. ter Meulen and R. E. Wall (1993): Mathematical methods in Linguistics Dordrecht: Kluwer Academic Publishers\\n\\nPritchett, B. L. (1992): Grammatical Competence and Parsing Performance. Chicago, IL: University of Chicago Press\\n\\nFootnotes\\n\\nThe work reported here was done very much in a collaborative spirit with my supervisor, Dr. Matthew Crocker, and thanks are due to him for innumerable suggestions and ideas. I would also like to thank the people who have offered insightful comments on this work,  in particular,   David Milward and Martin Pickering. The research was supported by ESRC grant R00429334338 In fact, Gorrell conjectures that, where there is insufficient grammatical information to postulate a structural relation between two constituents, such as in a sequence of two non-case marked NPs in an English centre-embedded construction, the parser may  hold these constituents unstructured in its memory (in press, p.212). However, for the purposes of this implementation, we have taken the most constrained position. Note that, since we do not deal with such constructions, none of the arguments presented here hinge on whether or not the parser may buffer material in this way. The original D-theory model did  not compute precedence relations, except between terminal nodes. See Partee et al (1993) for a description of the conditions on trees, with which all tree descriptions must  comply. It will be noticed that the reanalysis here involves a realignment of thematic  and, on GB assumptions, case dependencies. These are examples of what Gorrell calls secondary relations, which are not subject to the monotonicity requirement.\\n\\nNote that Abney's STEAL operation (1987, 1989) is  more powerful than tree-lowering, since it may change domination relations,  and thus will allow sentences such as (1), though it excludes reduced relative garden paths,  such as The horse raced past the barn fell. The original D-theory model (Marcus et al (1983)) is also more powerful, because it allows the right-most daughter of a node to be lowered under a sibling node. Note that the relations defining the original position of NP2, (i.e. dom(VP,NP2) and  prec(V,NP2)) are not subtracted from the set. Note that in Marcus et al (1983), since precedence relations were not computed for non-terminals, lowering into a predecessor was possible, thus (11) would cause no processing difficulty. However, presumably, their parser would overgenerate on examples such as the horse raced past the barn fell. Preliminary findings suggest that a similar preference rating is employed in (written) production as well as (reading) comprehension for these examples. This can be seen in  Gibson et al's (1994) study. This shows a the LOW ] HIGH ] MID ordering in the attachment of the final PP in NPs of the following form found in the Brown corpus:\\n\\nNP1 Prep  NP2 Prep NP3 PP\\n\\nOf 105 unambiguous PP adjunct attachments, 68% were low-attached, 26% high attached and 10% mid-attached. However, the question of whether the syntactic  structures people   preferentially use in production should correspond to the syntactic structures people preferentially assign to strings during comprehension is still very much an open issue, though see Mitchell and Cuetos (1991) for a view that the  experience of previous  input influences parsing decisions.\", metadata={'source': '../data/raw/cmplg-xml/9502035.xml'}),\n",
       " Document(page_content=\"A Tractable Extension of Linear Indexed Grammars Introduction\\n\\nUnification-based grammar formalisms can be viewed as generalizations of Context-Free Grammars (CFG) where the nonterminal symbols are replaced by an infinite domain of feature structures. Much of their popularity stems from the way in which syntactic generalization may be elegantly stated by means of constraints amongst features and their values. Unfortunately, the expressivity of these formalisms can have undesirable consequences for their processing. In naive implementations of unification grammar parsers, feature structures play the same role as nonterminals in standard context-free grammar parsers. Potentially large feature structures are stored at intermediate steps in the computation, so that the space requirements of the algorithm are expensive. Furthermore, the need to perform non-destructive unification means that a large proportion of the processing time is spent copying feature structures.\\n\\nFrom Stacks to Trees\\n\\nOur goal is to generalize the constraints inherent in LIG, to a formalism that manipulates feature structures rather than stacks. As a guiding heuristic we will avoid formalisms that generate tree sets with an unbounded number of unbounded, dependent branches. It appears that the structure-sharing techniques used with LIG cannot be generalized in a straightforward way to such formalisms.\\n\\nSuppose that we generalize LIG to allow the stack to be passed from the mother to two daughters. If this is done recursion can be used to produce an unbounded number of unbounded, dependent branches. An alternative is to allow an unbounded stack to be shared between two (or more) daughters but not with the mother. Thus, rules may mention more than one unbounded stack, but the stack associated with the mother is still associated with at most one daughter. We refer to this extension as Partially Linear Indexed Grammars (plig).\\n\\nTrees to Feature Structures\\n\\nFinally, we note that acyclic feature structures without re-entrancy can be viewed as trees with branches labelled by feature names and atomic values only found at leaf nodes (interior nodes being unlabelled). Based on this observation, we can consider the constraints we have formulated for the tree system pltg as constraints on a unification-based grammar formalism such as PATR. We will call this system Partially Linear PATR (plpatr). Having made the move from trees to feature structures, we consider the possibility of re-entrancy in plpatr.\\n\\nGenerative Capacity\\n\\nAs is the case for the tree sets of ig, LIG and Tree Adjoining Grammar, the tree sets generated by pltg have path sets that are context-free languages. In other words, the set of all strings labelling root to frontier paths of derivation trees is a context-free language. While the tree sets of LIG and Tree Adjoining Grammars have independent branches, pltg tree sets exhibit dependent branches, where the number of dependent branches in any tree is bounded by the grammar. Note that the number of dependent branches in the tree sets of ig is not bounded by the grammar (e.g., they generate sets of all full binary trees).\\n\\nTractable Recognition\\n\\nBefore describing how we adapt this technique to the case of plpatr we discuss the sense in which plpatr derivations exhibit a ``context-freeness'' property. The constraints on plpatr which we have identified in this paper ensure that these feature values can be manipulated independently of one another and that they behave in a stack-like way. As a consequence, the storage technique used effectively for LIG recognition may be generalized to the case of plpatr.\\n\\nHowever, this is not the end of the story. In contrast to lig, plpatr licenses structure sharing on the right hand side of productions. That is, partial linearity permits feature values to be shared between daughters where they are not also shared with the mother. But in that case, it appears that checking the applicability of a production at some point in a derivation must entail the comparison of structures of unbounded size. In fact, this is not so. The plpatr recognition algorithm employs a second array (called the compatibility array), which encodes information about the compatibility of derived feature structures. Tuples of compatible derived feature structures are stored in the compatibility array using exactly the same approach used to store feature structures in the main recognition array. The presence of a tuple in the compatibility array (the indices of which encode which input substrings are spanned) indicates the existence of derivations of compatible feature structures. Due to the ``context-freeness'' of plpatr, new entries can be added to the compatibility array in a bottom-up manner based on existing entries without the need to reconstruct complete feature structures.\\n\\nConclusions\\n\\nIn considering ways of extending , this paper has introduced the notion of partial linearity and shown how it can be manifested in the form of a constrained unification-based grammar formalism. We have explored examples of the kinds of tree sets and string languages that this system can generate. We have also briefly outlined the sense in which partial linearity gives rise to ``context-freeness'' in derivations and sketched how this can be exploited in order to obtain a tractable recognition algorithm.\\n\\nAcknowledgements\\n\\nWe thank Roger Evans, Gerald Gazdar, Aravind Joshi, Bernard Lang, Fernando Pereira, Mark Steedman and K. Vijay-Shanker for their help.\\n\\nBibliography\\n\\nMartin Emele. 1991. Unification with lazy non-redundant copying. In 29[th] meeting Assoc. Comput. Ling., pages 323--330, Berkeley, CA.\\n\\nG. Gazdar. 1988. Applicability of indexed grammars to natural languages. In U. Reyle and C. Rohrer, editors, Natural Language Parsing and Linguistic Theories, pages 69--94. D. Reidel, Dordrecht, Holland.\\n\\nDale Gerdemann. 1989. Using restrictions to optimize unification parsing. In International Workshop of Parsing Technologies, pages 8--17, Pittsburgh, PA.\\n\\nKurt Godden. 1990. Lazy unification. In 28[th] meeting Assoc. Comput. Ling., pages 180--187, Pittsburgh, PA.\\n\\nS. P. Harrison and T. M. Ellison. 1992. Restriction and termination in parsing with feature-theoretic grammars. Computational Linguistics, 18(4):519--531.\\n\\nL. Karttunen and M. Kay. 1985. Structure sharing with binary trees. In 23[th] meeting Assoc. Comput. Ling., pages 133--136.\\n\\nT. Kasami. 1965. An efficient recognition and syntax algorithm for context-free languages. Technical Report AF-CRL-65-758, Air Force Cambridge Research Laboratory, Bedford, MA.\\n\\nKiyoshi Kogure. 1990. Strategic lazy incremental copy graph unification. In 13[th] International Conference on Comput. Ling., pages 223--228, Helsinki.\\n\\nF. C. N. Pereira. 1985. A structure-sharing representation for unification-based grammar formalisms. In 23[th] meeting Assoc. Comput. Ling., pages 137--144.\\n\\nS. M. Shieber. 1984. The design of a computer language for linguistic information. In 10[th] International Conference on Comput. Ling., pages 363-366.\\n\\nS. M. Shieber. 1985. Using restriction to extend parsing algorithms for complex-feature-based formalisms. In 23[rd] meeting Assoc. Comput. Ling., pages 82-93.\\n\\nHideto Tomabechi. 1991. Quasi-destructive graph unification. In 29[th] meeting Assoc. Comput. Ling., pages 315--322, Berkeley, CA.\\n\\nK. Vijay-Shanker and D. J. Weir. 1993. Parsing some constrained grammar formalisms. Computational Linguistics, 19(4):591--636.\\n\\nK. Vijay-Shanker and D. J. Weir. 1994. The equivalence of four extensions of context-free grammars. Math. Syst. Theory, 27:511-546.\\n\\nK. Vijay-Shanker, D. J. Weir, and A. K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In 25[th] meeting Assoc. Comput. Ling., pages 104-111.\\n\\nDavid Wroblewski. 1987. Nondestructive graph unification. In 6[th] National Conference on Artificial Intelligence, pages 582--587, Seattle, WA.\\n\\nD. H. Younger. 1967. Recognition and parsing of context-free languages in time n[3]. Information and Control, 10(2):189-208.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9502021.xml'}),\n",
       " Document(page_content=\"D\\n\\n\\n\\nTree Grammars\\n\\nDTG are designed to share some of the advantages of TAG while overcoming some of its limitations. DTG involve two composition operations called subsertion and sister-adjunction. The most distinctive feature of DTG is that, unlike TAG, there is complete uniformity in the way that the two DTG operations relate lexical items: subsertion always corresponds to complementation and sister-adjunction to modification. Furthermore, DTG, unlike TAG, can provide a uniform analysis for wh-movement in English and Kashmiri, despite the fact that the wh element in Kashmiri appears in sentence-second position, and not sentence-initial position as in English.\\n\\nIntroduction\\n\\nDerivations and Dependencies\\n\\nIn defining DTG we have attempted to resolve these problems with the use of a single operation (that we call subsertion) for handling all complementation and a second operation (called sister-adjunction) for modification. Before discussion these operations further we consider a second problem with TAG that has implications for the design of these new composition operations (in particular, subsertion).\\n\\nProblematic Constructions for TAG\\n\\nThe DTG Approach\\n\\nDefinition of D\\n\\n\\n\\nTree Grammars\\n\\nA d-tree is a tree with two types of edges: domination edges (d-edges) and immediate domination edges (i-edges). D-edges and i-edges express domination and immediate domination relations between nodes. These relations are never rescinded when d-trees are composed. Thus, nodes separated by an i-edge will remain in a mother-daughter relationship throughout the derivation, whereas nodes separated by an d-edge can be equated or have a path of any length inserted between them during a derivation. D-edges and i-edges are not distributed arbitrarily in d-trees. For each internal node, either all of its daughters are linked by i-edges or it has a single daughter that is linked to it by a d-edge. Each node is labelled with a terminal symbol, a nonterminal symbol or the empty string. A d-tree containing n d-edges can be decomposed into n+1 components containing only i-edges.\\n\\nIn this section we have defined ``raw'' DTG. In a more refined version of the formalism we would associate (a single) finite-valued feature structure with each node. It is a matter of further research to determine to what extent SICs and SACs can be stated globally for a grammar, rather than being attached to d-edges/nodes. See the next section for a brief discussion of linguistic principles from which a grammar's SICs could be derived.\\n\\nLinguistic Examples\\n\\nGetting Dependencies Right: English\\n\\nWe add SICs to ensure that the projections are respected by components of other d-trees that may be inserted during a derivation. A SIC is associated with the d-edge between VP and S node in the seems d-tree to ensure that no node labelled S' can be inserted within it - i.e., it can not be filled by with a wh-moved element. In contrast, since both the subject and the object of to adore have been moved out of the projection of the verb, the path to these arguments do not carry any SIC at all.\\n\\nIn the above discussion, substitutability played a central role in ruling out the derivation. We observe in passing that the SIC associated to the d-edge in the seems d-tree also rules out this derivation. The derivation requires that the S node of seems be inserted into the S'/S d-edge of claims. However, we would have to stretch the edge over two components which are both ruled out by the SIC, since they violate the projection from seems to its S node. Thus, the derivation is excluded by the independently motivated SICs, which enforce the notion of projection. This raises the possibility that, in grammars that express certain linguistic principles, substitutability is not needed for ruling out derivations of this nature. We intend to examine this issue in future work.\\n\\nGetting Word Order Right: Kashmiri Recognition\\n\\nConclusion\\n\\nDTG, like other formalisms in the TAG family, is lexicalizable, but in addition, its derivations are themselves linguistically meaningful. In future work we intend to examine additional linguistic data, refining aspects of our definition as needed. We will also study the formal properties of DTG, and complete the design of the Earley style parser.\\n\\nAcknowledgements\\n\\nWe would like to thank Rakesh Bhatt for help with the Kashmiri data. We are also grateful to Tilman Becker, Gerald Gazdar, Aravind Joshi, Bob Kasper, Bill Keller, Tony Kroch, Klaus Netter and the ACL-95 referees. Rambow was supported by the North Atlantic Treaty Organization under a Grant awarded in 1993, while at TALANA, Universit Paris 7.\\n\\nBibliography\\n\\nT. Becker, A. Joshi,  O. Rambow. 1991. Long distance scrambling and tree adjoining grammars. In EACL-91, 21-26.\\n\\nR. Bhatt. 1994. Word order and case in Kashmiri. Ph.D. thesis, Univ. Illinois.\\n\\nT. Bleam. 1994. Clitic climbing in spanish: a GB perspective. In TAG+ Workshop, Tech. Rep. TALANA-RT-94-01, Universit Paris 7, 16-19.\\n\\nJ. Bresnan  R. Kaplan. 1982. Lexical-functional grammar: A formal system for grammatical representation. In J. Bresnan, ed., The Mental Representation of Grammatical Relations. MIT Press.\\n\\nR. Frank. 1992. Syntactic Locality and Tree Adjoining Grammar: Grammatical, Acquisition and Processing Perspectives. Ph.D. thesis, Dept. Comp. Inf. Sc., Univ. Pennsylvania.\\n\\nA. Joshi. 1987. An introduction to tree adjoining grammars. In A. Manaster-Ramer, ed., Mathematics of Language, 87-114.\\n\\nA. Joshi, L. Levy,  M. Takahashi. 1975. Tree adjunct grammars. J. Comput. Syst. Sci., 10(1):136-163.\\n\\nA. Joshi  Y. Schabes. 1991. Tree-adjoining grammars and lexicalized grammars. In M. Nivat  A. Podelski, eds., Definability and Recognizability of Sets of Trees.\\n\\nR. Kasper, B. Kiefer, K. Netter,  K. Vijay-Shanker 1995. Compilation of HPSG to TAG. In ACL-95.\\n\\nA. Kroch. 1987. Subjacency in a tree adjoining grammar. In A. Manaster-Ramer, ed., Mathematics of Language, 143-172.\\n\\nA. Kroch. 1989. Asymmetries in long distance extraction in a Tree Adjoining Grammar. In Mark Baltin  Anthony Kroch, editors, Alternative Conceptions of Phrase Structure, 66-98.\\n\\nA. Kroch  A. Joshi. 1986. Analyzing extraposition in a tree adjoining grammar. In G. Huck  A. Ojeda, eds., Syntax  Semantics: Discontinuous Constituents,  107-149.\\n\\nI. Mel'cuk. 1988. Dependency Syntax: Theory and Practice.\\n\\nO. Rambow. 1994. Formal and Computational Aspects of Natural Language Syntax. Ph.D. thesis, Dept. Comput. Inf. Sc., Univ. Pennsylvania.\\n\\nO. Rambow.\\n\\n1994.\\n\\nMultiset\\n\\n\\n\\nValued Linear Index Grammars.\\n\\nIn ACL\\n\\n\\n\\n94, 263\\n\\n\\n\\n270.\\n\\nO. Rambow  A. Joshi. 1992. A formal look at dependency grammars and phrase-structure grammars, with special consideration of word-order phenomena. In Intern. Workshop on The Meaning-Text Theory, Darmstadt. Arbeitspapiere der GMD 671, 47-66.\\n\\nB. Santorini  S. Mahootian. 1995. Codeswitching and the syntactic status of adnominal adjectives. Lingua, 95.\\n\\nY. Schabes  S. Shieber. 1994. An alternative conception of tree-adjoining derivation. Comput. Ling., 20(1):91-124.\\n\\nS. Shieber.\\n\\n1985.\\n\\nEvidence against the context\\n\\n\\n\\nfreeness of natural language.\\n\\nLing. Phil., 8:333\\n\\n\\n\\n343.\\n\\nK. Vijay-Shanker. 1987. A Study of Tree Adjoining Grammars. Ph.D. thesis, Dept. Comput. Inf. Sc., Univ. Pennsylvania.\\n\\nK. Vijay-Shanker. 1992. Using descriptions of trees in a tree adjoining grammar. Comput. Ling., 18(4):481-517.\\n\\nThe XTAG Research Group. 1995. A lexicalized tree adjoining grammar for English. Tech. Rep. IRCS Report 95-03, Univ. Pennsylvania.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9505028.xml'}),\n",
       " Document(page_content=\"Discourse Obligations in Dialogue Processing\\n\\nWe show that in modeling social interaction, particularly dialogue, the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief, goal, and intention and their mutual and shared counterparts. In particular, we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system.\\n\\nMotivation\\n\\nFor instance, consider one simple phenomena: a question is typically followed by an answer, or some explicit statement of an inability or refusal to answer. The intentional story account of this goes as follows. From the production of a question by Agent B, Agent A recognizes Agent B's goal to find out the answer, and she adopts a goal to tell B the answer in order to be co-operative. A then plans to achieve the goal, thereby generating the answer. This provides an elegant account in the simple case, but requires a strong assumption of co-operativeness. Agent A must adopt agent B's goals as her own. As a result, it does not explain why A says anything when she does not know the answer or when she is not predisposed to adopting B's goals.\\n\\nConsider a stranger approaching an agent and asking, ``Do you have the time?'' It is unlikely that there is a joint intention or shared plan, as they have never met before. From a purely strategic point of view, the agent may have no interest in whether the stranger's goals are met. Yet, typically agents will still respond in such situations.\\n\\nAs another example, consider a case in which the agent's goals are such that it prefers that an interrogating agent not find out the requested information. This might block the formation of an intention to inform, but what is it that inspires the agent to respond at all?\\n\\nAs these examples illustrate, an account of question answering must go beyond recognition of speaker intentions. Questions do more than just provide evidence of a speaker's goals, and something more than adoption of the goals of an interlocutor is involved in formulating a response to a question.\\n\\nThe problem with systems which impose co-operativity in the form of automatic goal adoption is that this makes it impossible to reason about cases in which one might want to violate these rules, especially in cases where the conversational co-operation might conflict with the agent's personal goals.\\n\\nWe are developing an alternate approach that takes a step back from the strong plan-based approach. By the strong plan-based account, we mean models where there is a set of personal goals which directly motivates all the behavior of the agent. While many of the intuitions underlying these approaches seems close to right, we claim it is a mistake to attempt to analyze this behavior as arising entirely from the agent's high-level goals.\\n\\nWe believe that people have a much more complex set of motivations for action. In particular, much of one's behavior arises from a sense of obligation to behave within limits set by the society that the agent is part of. A model based on obligations differs from an intention-based approach in that obligations are independent of shared plans and intention recognition. Rather, obligations are the result of rules by which an agent lives. Social interactions are enabled by their being a sufficient compatibility between the rules affecting the interacting agents. One responds to a question because this is a social behavior that is strongly encouraged as one grows up, and becomes instilled in the agent.\\n\\nSketch of Solution\\n\\nThe model we propose is that an agent's behavior is determined by a number of factors, including that agent's current goals in the domain, and a set of obligations that are induced by a set of social conventions. When planning, an agent considers both its goals and obligations in order to determine an action that addresses both to the extent possible. When prior intentions and obligations conflict, an agent generally will delay pursuit of its intentions in order to satisfy the obligations, although the agent may behave otherwise at the cost of violating its obligations. At any given time, an agent may have many obligations and many different goals, and planning involves a complex tradeoff between these different factors.\\n\\nReturning to the example about questions, when an agent is asked a question, this creates an obligation to respond. The agent does not have to adopt the goal of answering the question as one of her personal goals in order to explain the behavior. Rather it is a constraint on the actions that the agent may plan to do. In fact, the agent might have an explicit goal not to answer the question, yet still is obliged to offer a response (e.g., consider most politicians at press conferences). The planning task then is to satisfy the obligation of responding to the question, without revealing the answer if at all possible. In cases where the agent does not know the answer, the obligation to respond may be discharged by some explicit statement of her inability to give the answer.\\n\\nObligations and Discourse Obligations\\n\\nObligations represent what an agent should do, according to some set of norms. The notion of obligation has been studied for many centuries, and its formal aspects are examined using Deontic Logic. Our needs are fairly simple, and do not require an extensive survey of the complexities that arise in that literature. Still, the intuitions underlying that work will help to clarify what an obligation is. Generally, obligation is defined in terms of a modal operator often called permissible. An action is obligatory if it is not permissible not to do it. An action is forbidden if it is not permissible. An informal semantics of the operator can be given by positing a set of rules of behavior R. An action is obligatory if its occurrence logically follows from R, and forbidden if its non-occurrence logically follows from R. An action that might occur or not-occur according to R is neither obligatory nor forbidden.\\n\\nSpecific obligations arise from a variety of sources. In a conversational setting, an accepted offer or a promise will incur an obligation. Also, a command or request by the other party will bring about an obligation to perform the requested action. If the obligation is to say something then we call this a discourse obligation. Our model of obligation is very simple. We use a set of rules that encode discourse conventions. Whenever a new conversation act is determined to have been performed, then any future action that can be inferred from the conventional rules becomes an obligation. We use a simple forward chaining technique to introduce obligations.\\n\\nObligations and Behavior\\n\\nIn general, we will want to allow action based on obligations to supersede performance of intended actions. For instance, consider an agent with an intention to do something as soon as possible. If an obligation is imposed, it will still be possible to perform the intended action, but a well-behaved agent might need to delay performance until the obligation is dealt with. For example, if the intention is to perform a series of inform acts, and then a listener requests repair of one, a well-behaved agent will repair that inform before proceeding to initiate the next intended one.\\n\\nUsing Discourse Obligations in a Dialogue System\\n\\nThe dialogue manager is responsible for maintaining the flow of conversation and making sure that the conversational goals are met. For this system, the main goals are that an executable plan which meets the user's goals is constructed and agreed upon by both the system and the user and then that the plan is executed.\\n\\nRepresenting Mental Attitudes\\n\\nThe over-riding goal for the TRAINS domain is to construct and execute a plan that is shared between the two participants. This leads to other goals such as accepting proposals that the other agent has suggested, performing domain plan synthesis, proposing to the other agent plans that the domain plan reasoner has constructed, or executing a completed plan.\\n\\nThe Discourse Actor Algorithm\\n\\nIn designing an agent to control the behavior of the dialogue manager, we choose a reactive approach in which the system will not deliberate and add new intentions until after it has performed the actions which are already intended. As shown above, though, new obligations will need to be addressed before performing intended actions. The agent's deliberative behavior could thus be characterized in an abstract sense as:\\n\\nloop\\t\\t perceive world and update beliefs\\t\\tif \\t\\tsystem has obligations\\t\\tthen \\t\\taddress obligations\\t\\telse if \\t\\tsystem has performable intentions\\t\\tthen \\t\\t perform actions\\t\\telse  \\t\\tdeliberate on goals\\n\\nWhen deciding what to do next, the agent first considers obligations and decides how to update the intentional structure (add new goals or intentions) based on these obligations. Obligations might also lead directly to immediate action. If there are no obligations, then the agent will consider its intentions and perform any actions which it can to satisfy these intentions. If there are no performable intentions, then the system will deliberate on its overall goals and perhaps adopt some new intentions (which can then be performed on the next iteration).\\n\\nIn most cases, the actor will merely form the intention to produce the appropriate utterance, waiting for a chance, according to turn-taking conventions, to actually generate the utterance. In certain cases, though, such as a repair, the system will actually try to take control of the turn and produce an utterance immediately. For motivations other than obligations, the system adopts a fairly ``relaxed'' conversational style; it does not try to take the turn until given it by the user unless the user pauses long enough that the conversation starts to lag (lines 14-17). When the system does not have the turn, the conversational state will still be updated, but the actor will not try to deliberate or act.\\n\\nIf there are no intended conversation acts, the next thing the actor considers is the grounding situation (lines 7-8). The actor will try to make it mutually believed (or grounded) whether particular speech acts have been performed. This will involve acknowledging or repairing user utterances, as well as repairing and requesting acknowledgement of the system's own utterances. Generally, grounding is considered less urgent than acting based on communicative intentions, although some grounding acts will be performed on the basis of obligations which arise while interpreting prior utterances.\\n\\nIf all accessible utterances are grounded, the actor then considers the negotiation of domain beliefs and intentions (lines 9-10). The actor will try to work towards a shared domain plan, adding intentions to perform the appropriate speech acts to work towards this goal. This includes accepting, rejecting, or requesting retraction of user proposals, requesting acceptance of or retracting system proposals, and initiating new system proposals or counterproposals.\\n\\nThe actor will first look for User proposals which are not shared. If any of these are found, it will add an intention to accept the proposal, unless the proposal is deficient in some way (e.g., it will not help towards the goal or the system has already come up with a better alternative). In this latter case, the system will reject the user's proposal and present or argue for its own proposal. Next, the actor will look to see if any of its own proposals have not been accepted, requesting the user to accept them if they have been simply acknowledged, or retracting or reformulating them if they have already been rejected. Finally, the actor will check its private plans for any parts of the plan which have not yet been proposed. If it finds any here, it will adopt an intention to make a suggestion to the user.\\n\\nIf none of the more local conversational structure constraints described above require attention, then the actor will concern itself with its actual high-level goals. For the TRAINS system, this will include making calls to the domain plan reasoner and domain executor, which will often return material to update the system's private view of the plan and initiate its own new proposals. It is also at this point that the actor will take control of the conversation, pursuing its own objectives rather than responding to those of the user.\\n\\nFinally, if the system has no unmet goals that it can work towards achieving (line 13), it will hand the turn back to the user or try to end the conversation if it believes the user's goals have been met as well.\\n\\nExamples\\n\\nAfter interpreting utterance 1, the system first decides to acknowledge this utterance (lines 7-8 in the actor algorithm) - moving the suggestion from an unacknowledged to unaccepted - and then to accept the proposal (lines 9-10). Finally, the system acts on the intentions produced by these deliberations (lines 5-6) and produces the combined acknowledgement/acceptance of utterance 2. This acceptance makes the goal shared and also satisfies the first of the discourse goals, that of getting the domain goal to work on.\\n\\nThe system queries its domain knowledge base and decides that the user is correct here (there are, indeed, oranges at Corning), and so decides to meet this obligation (lines 2-3) by answering in the affirmative. This results in forming an intention to inform, which is then realized (along with the acknowledgement of the utterances) by the production of utterance 4.\\n\\nSimilar considerations hold for the system responses 6 and 8. The reasoning leading up to utterance 14 is similar to that leading to utterance 2. Here the user is suggesting domain actions to help lead to the goal, and the system, when it gets the turn, acknowledges and accepts this suggestion.\\n\\nUtterances 15-2=4, 15-5=7, and 15-8=10 are interpreted as requests because of the imperative surface structure. The discourse obligation to address the request is incurred only when the system decides to acknowledge the utterances and ground them. After the decision to acknowledge, the obligations are incurred, and the system then addresses the requests, deciding to accept them all, and adding intentions to perform an accept speech act, which is then produced as 16.\\n\\nAfter the user's assent, the system then checks its goals, and, having already come up with a suitable plan, executes this plan in the domain by sending the completed plan to the domain plan executor.\\n\\nThis example illustrates only a small fraction of the capabilities of the dialogue model. In this dialogue, the system needed only to follow the initiative of the user. However this architecture can handle varying degrees of initiative, while remaining responsive. The default behavior is to allow the user to maintain the initiative through the plan construction phase of the dialogue. If the user stops and asks for help, or even just gives up the initiative rather than continuing with further suggestions, the system will switch from plan recognition to plan elaboration and incrementally devise a plan to satisfy the goal (although this plan would probably not be quite the same as the plan constructed in this dialogue).\\n\\nWe can illustrate the system behaving more on the basis of goals than obligations with a modification of the previous example. Here, the user releases the turn back to the system after utterance 2, and the deliberation proceeds as follows: the system has no obligations, no communicative intentions, nothing is ungrounded, and there are no unaccepted proposals, so the system starts on its high-level goals. Given its goal to form a shared plan, and the fact that the current plan (consisting of the single abstract move-commodity action) is not executable, the actor will call the domain plan reasoner to elaborate the plan. This will return a list of augmentations to the plan which can be safely assumed (including a move-engine event which generates the move-commodity, given the conditions that the oranges are in a boxcar which is attached to the engine), as well as some choice point where one of several possibilities could be added (e.g., a choice of the particular engine or boxcar to use).\\n\\nAssuming that the user still has not taken the turn back, the system can now propose these new items to the user. The choice could be resolved in any of several ways: the domain executor could be queried for a preference based on prior experience, or the system could put the matter up to the user in the form of an alternative question, or it could make an arbitrary choice and just suggest one to the user.\\n\\nThe user will now be expected to acknowledge and react to these proposals. If the system does not get an acknowledgement, it will request acknowledgement the next time it considers the grounding situation. If the proposal is not accepted or rejected, the system can request an acceptance. If a proposal is rejected, the system can negotiate and offer a counterproposal or accept a counter proposal from the user.\\n\\nDiscussion\\n\\nWe have argued that obligations play an important role in accounting for the interactions in dialog. Obligations do not replace the plan-based model, but augment it. The resulting model more readily accounts for discourse behavior in adversarial situations and other situations where it is implausible that the agents adopt each others goals. The obligations encode learned social norms, and guide each agent's behavior without the need for intention recognition or the use of shared plans at the discourse level. While such complex intention recognition may be required in some complex interactions, it is not needed to handle the typical interactions of everyday discourse. Furthermore, there is no requirement for mutually-agreed upon rules that create obligations. Clearly, the more two agents agree on the rules, the smoother the interaction becomes, and some rules are clearly virtually universal. But each agent has its own set of individual rules, and we do not need to appeal to shared knowledge to account for local discourse behavior.\\n\\nWe have also argued that an architecture that uses obligations provides a much simpler implementation than the strong plan-based approaches. In particular, much of local discourse behavior can arise in a ``reactive manner'' without the need for complex planning. The other side of the coin, however, is a new set of problems that arise in planning actions that satisfy the multiple constraints that arise from the agent's personal goals and perceived obligations.\\n\\nThe model presented here allows naturally for a mixed-initiative conversation and varying levels of co-operativity. Following the initiative of the other can be seen as an obligation driven process, while leading the conversation will be goal driven. Representing both obligations and goals explicitly allows the system to naturally shift from one mode to the other. In a strongly co-operative domain, such as TRAINS, the system can subordinate working on its own goals to locally working on concerns of the user, without necessarily having to have any shared discourse plan. In less co-operative situations, the same architecture will allow a system to still adhere to the conversational conventions, but respond in different ways, perhaps rejecting proposals and refusing to answer questions.\\n\\nAcknowledgements\\n\\nThis material is based upon work supported by ONR/DARPA under grant number N00014-92-J-1512. We would like to thank the rest of the TRAINS group at the University of Rochester for providing a stimulating research environment and a context for implementing these ideas within an integrated system.\\n\\nBibliography\\n\\nAirenti, Gabriella; Bara, Bruno G.; and Colombetti, Marco 1993. Conversation and behavior games in the pragmatics of dialogue. Cognitive Science 17:197-256.\\n\\nAllen, James and Perrault, C. 1980. Analyzing intention in utterances. Artificial Intelligence 15(3):143-178.\\n\\nAllen, James F. and Schubert, Lenhart K. 1991. The TRAINS project. TRAINS Technical Note 91-1, Computer Science Dept. University of Rochester.\\n\\nBratman, Michael E.; Israel, David J.; and Pollack, Martha E. 1988. Plans and resource-bounded practical reasoning. Technical Report TR425R, SRI International. Appears in Computational Intelligence, Vol. 4, No. 4, 1988.\\n\\nCarberry, S. 1990. Plan Recognition in Natural Language Dialogue. The MIT Press, Cambridge, MA.\\n\\nClark, Herbert H. and Schaefer, Edward F. 1989. Contributing to discourse. Cognitive Science 13:259 - 94. also appears as Chapter 5 in [].\\n\\nCohen, Phillip R. and Levesque, Hector J. 1991. Confirmations and joint action. In Proceedings IJCAI-91. 951-957.\\n\\nCohen, Phillip R. and Perrault, C. R. 1979. Elements of a plan-based theory of speech acts. Cognitive Science 3(3):177-212.\\n\\nConte, Rosaria and Castelfranchi, Cristiano 1993. Norms as mental objects. from normative beliefs to normative goals. In Working Notes AAAI Spring Symposium on Reasoning about Mental States: Formal Theories and Applications. 40-47.\\n\\nFerguson, George  1994. Domain plan reasoning in TRAINS-93. Trains technical note, Computer Science Dept. University of Rochester.\\n\\nGross, Derek; Allen, James; and Traum, David 1993. The TRAINS 91 dialogues. TRAINS Technical Note 92-1, Computer Science Dept. University of Rochester.\\n\\nGrosz, Barbara and Sidner, Candice 1986. Attention, intention, and the structure of discourse. CL 12(3):175-204.\\n\\nGrosz, Barbara J. and Sidner, Candace L. 1990. Plans for discourse. In Cohen, P. R.; Morgan, J.; and Pollack, M. E., editors 1990,   Intentions in Communication. MIT Press.\\n\\nKowtko, J.; Isard, S.; and Doherty, G. 1991. Conversational games within dialogue. In Proceedings of the ESPRIT Workshop on Discourse Coherence.\\n\\nLitman, D. J. and Allen, J. F. 1987. A plan recognition model for subdialogues in conversation. Cognitive Science 11:163-200.\\n\\nMann, William C. 1988. Dialogue games: Conventions of human interaction. Argumentation 2:511-532.\\n\\nMcRoy, Susan 1993. Abductive Interpretation and Reinterpretation of Natural Language Utterances. Ph.D. Dissertation, University of Toronto. Reproduced as TR CSRI-288 Department of Computer Science, University of Toronto.\\n\\nSchegloff, E. A. and Sacks, H. 1973. Opening up closings. Semiotica 7:289-327.\\n\\nShoham, Yoav and Tennenholtz, Moshe 1992. On the synthesis of useful social laws for artificial agent societies. In Proceedings AAAI-92. 276-281.\\n\\nTraum, David R. and Hinkelman, Elizabeth A. 1992. Conversation acts in task-oriented spoken dialogue. Computational Intelligence 8(3):575-599. Special Issue on Non-literal language.\\n\\nTraum, David R.  1994. The TRAINS-93 dialogue manager. Trains technical note, Computer Science Dept. University of Rochester.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9407011.xml'}),\n",
       " Document(page_content=\"Syntactic Analyses for Parallel Grammars: Auxiliaries and Genitive NPs\\n\\nThis paper focuses on two disparate aspects of German syntax from the perspective of parallel grammar development. As part of a cooperative project, we present an innovative approach to auxiliaries and multiple genitive NPs in German. The LFG-based implementation presented here avoids unnessary structural complexity in the representation of auxiliaries by challenging the traditional analysis of auxiliaries as raising verbs. The approach developed for multiple genitive NPs provides a more abstract, language independent representation of genitives associated with nominalized verbs. Taken together, the two approaches represent a step towards providing uniformly applicable treatments for differing languages, thus lightening the burden for machine translation.\\n\\nIntroduction\\n\\nWithin the cooperative parallel grammar project  PARGRAM (IMS-Stuttgart, Xerox-Palo Alto, Xerox-Grenoble), the analysis and representation of structures in the grammars must be viewed from a more global perspective than that of the individual languages (German, English, French). One major goal of  PARGRAM is the development of broad coverage grammars which are also modular and easy to maintain. Another major goal is the construction of parallel analyses for sentences of the same type in German, English, and French. If this can be achieved, the problem faced by machine translation (MT) could be greatly reduced. Due to the recent development of a faster and more powerful version of the LFG (Lexical-Functional-Grammar) based Grammar Writer's Workbench (Kaplan and Maxwell 1993) at Xerox, the implementation of a linguistically adequate, broad coverage grammar appears viable. Given the flexible projection-based architecture of LFG (Dalrymple et al. 1995) and the MT approach presented in Kaplan et al. (1989), a robust MT system is already in place.\\n\\nIn this paper, we concentrate on two issues within the broader perspective of  PARGRAM: the treatment of auxiliaries and the transparent representation of multiple genitive NPs in German. These phenomena represent two areas for which generally accepted proposals exist, but whose implementation in the context of parallel grammar development throws up questions as to their wider, crosslinguistic, feasibility. With respect to auxiliaries, the standard raising approach that is usually adopted yields undesirable structural complexity and results in idiosyncratic, language particular analyses of the role of auxiliaries. With regard to genitive NPs, the standard analysis for German yields structures which are too ambiguous for a succesful application of machine translation. The following sections present a solution in that morphological wellformedness conditions are stated at a separate component, the morphology projection. Furthermore, a representation of argument structure is implemented that is related to, but not identical to the representation of grammatical functions. Language particular idiosyncratic requirements are thus separated out from the language universal information required for further semantic interpretation, or machine translation.\\n\\nThe Formalism\\n\\nThe architecture of LFG assumed here is the ``traditional'' architecture described in Bresnan (1982), as well as the newer advances within LFG (Dalrymple et al. , 1995). A grammar is viewed as a set of correspondences expressed in terms of projections from one level of representation to another. Two fundamental levels of representations within LFG are the c(onstitutent)-structure and the f(unctional)-structure. The c-structure encodes idiosyncratic phrase structural properties of a given language, while the f-structure provides a language universal representation of grammatical functions (e.g.,  SUBJect,  OBJect), complementation, tense, binding, etc. The correspondence between c-structure and f-structure is not onto or one-to-one, but many-to-one, allowing an abstraction over idiosyncratic c-structure properties of a language (e.g., discontinuous constituents).\\n\\nIn addition, several proposals exploring possible representations of a s(emantic)-structure have been made over the years (e.g. Halvorsen and Kaplan (1988), Dalrymple et al. (1993)). As the realization of a separate semantic component is only planned for the latter stages within  PARGRAM, no further discussion of possible formalisms will take place here. It should be noted, however, that rudimentary semantic information, such as argument structure information (lexical semantics), is encoded within the f-structures in order to facilitate transfer in some cases. A case in point is presented in the section on German genitive NPs.\\n\\nAuxiliaries -\\n\\n\\n\\na flat approach\\n\\nThe Received Wisdom\\n\\nAuxiliaries have given rise to lively debates concerning their exact syntactic status (e.g. Chomsky (1957), Ross (1967), Pullum and Wilson (1977), Akmajian et al. (1979), Gazdar et al. (1982)): are they simply main verbs with special properties, or should they instantiate a special category  AUX? Within current lexical approaches (Lexical-Functional-Grammar (LFG), Head-driven Phrase Structure Grammar (HPSG)), auxiliaries (e.g. have, be) and modals (e.g. must, should) are treated as raising verbs, which are marked as special in some way: in HPSG through an [ AUX: +] feature (Pollard and Sag 1994), in LFG (Bresnan 1982) by a difference in  PRED value. However, newer work within LFG (Bresnan 1995, T.H. King 1995) has been moving away from the raising approach towards an analysis where auxiliaries are elements which contribute to the clause only tense/aspect, agreement, or voice information, but not a subcategorization frame. This view is also in line with approaches within GB (Government-Binding), which see auxiliaries simply as possible instantiations of the functional category I (see also Halle and Marantz (1993)).\\n\\nThe ``traditional'' treatment of auxiliaries in both HPSG (Pollard and Sag 1994) and LFG has its roots in Ross's (1967) proposal to treat auxiliaries and modals on a par with main verbs. In particular, auxiliaries are treated as a subclass of raising verbs (e.g. Pollard and Sag (1994), Falk (1984)). For example, a simple sentence like (1) would correspond to the c-structure and f-structure shown in (2) and (3), respectively. Note that the level of embedding in the f-structure exactly mirrors the c-structure: each verbal element takes a complement.\\n\\nThe main reasons to treat auxiliaries as complement taking verbs in English are: 1) an account of VP-ellipsis, VP-topicalization, etc. follows immediately; 2) restrictions on the nature of the verbal complement (progressive, past participle, etc.) following the auxiliary can be stated straightforwardly (Pullum and Wilson (1977), Akmajian et al. (1979), Gazdar et al. (1982)). The latter point holds for German as well, and in fact, without some sort of a hierarchical structure, stating wellformedness conditions on a string of multiple auxiliaries becomes wellnigh impossible in light of the greater ordering possibilities granted by the flexible German word order. There are also major reasons, however, for not adopting this analysis: 1) linguistic adequacy; 2) unmotivated structural complexity; 3) non-parallel analyses for predicationally equivalent sentences. Consider the French equivalent of (-2) in (1).\\n\\nAs argued by Akmajian et al. (1979), crosslinguistic evidence indicates that elements bearing only tense, mood, or voice should belong to a distinct syntactic category. In many languages, like French or Japanese, the information carried by will (future), or have (perfect) is realized morphologically rather than periphrastically. The analysis in (0) thus effectively claims that there exists a deep difference in the predicational structure of auxiliaries like will and have and the French   aura. This is not desirable from a crosslinguistic point of view, nor is it helpful for MT.\\n\\nAlternative Implementation\\n\\nThe approach adopted here is a flat analysis of auxiliaries at f-structure ((1)).\\n\\nThe auxiliaries wird `will' and haben `have' now only contribute information as to the overall tense, but do not subcategorize for complements. Structural phenomena like VP-ellipsis, coordination, or topicalization can, however, still be accounted for in terms of an appropriate embedding at c-structure (cf. (-3)). The role of auxiliaries in natural language is now adequately modeled, in particular with respect to a more realistic treatment of tense (compare (-2) and (0)), as the French (-1) has essentially the same f-structure as (0).\\n\\nHowever, the flat f-structure in (0) provides no room for a statement of selectional requirements, allowing massive overgeneration (e.g. nothing blocks the presence of two haben in (-4)). Neither can the particular order of auxiliaries be regulated. Our solution takes advantage of LFG's flexible projection-based architecture by implementing a projection which models the hierarchical selectional requirements of auxiliaries, yet does not interfere with the subcategorizational properties of verbs, as would be the case under a raising analysis.\\n\\nIn LFG, the flexible word order of German is handled via   functional uncertainty, which characterizes long-distance dependencies without resorting to movement analyses (Netter (1988), Zaenen and Kaplan (1995)). As in (0), which illustrates our alternative solution, functional uncertainty is represented by the Kleene Star (*). The annotation on the NPs indicates that they could fulfill the role of any possible grammatical function (GF), e.g. SUBJ or  OBJ, and that the level of embedding ranges from zero to infinite. With every auxiliary subcategorizing for an  XCOMP, the two NPs could conceivably be arguments of three different verbs: wird, haben, or   gedreht. Thus, the greater structural complexity unnecessarily increases the search space for the determination of a verb's arguments. In (0), however, the m-structure is projected from the c-structure parallel to the f-structure through annotations similar to the usual f-structure annotations. Statements about ``morphological'' dependents ( DEP) are thus decoupled from functional uncertainty: the relation of NP arguments to their predicator now does not extend through various layers of artificial structural complexity ( XCOMPs). For VP-topicalization or extraposition an unbounded long-distance dependency must still be assumed. However, as the functional uncertainty path for auxiliaries is distributed only over the m-structure of the verb complex ((\\n\\n), and does not involve the resolution of the role of NP arguments, there are in fact differing paths of functional uncertainty involved. The dependencies between predicators and their arguments and auxiliaries and their dependents are thus neatly factored out. The m-structure corresponding to the matrix VP in (0) is (1). The desired flat f-structure resulting from the usual\\n\\nand\\n\\nannotations is as in (-1).\\n\\nLike the f-structure, the m-structure is an attribute-value matrix. It encodes language-specific information about idiosyncratic constraints on morphological forms. The m-structure is not derived from the f-structure. Rather, both representations are in simultaneous correspondence with the c-structure. The following (abbreviated) lexical entry exemplifies the pieces of information needed. The disjunctive lexical entry for wird `will' in (1) takes the various combinatory possibilities of auxiliaries and main verbs into account, and provides the appropriate tense feature. For example, it requires that the embedded  VFORM be  BASE, and that there be no passive involved for a simple future like   wird drehen.\\n\\nFeatures needed only to ensure language particular wellformedness are no longer unified into the f-structure, cluttering a representation that is meant to be language independent. In our analysis, only features needed for further semantic interpretation, MT, or for the expression of language universal syntactic generalizations are represented at f-structure. For example, morphologically encoded information like case, gender, or agreement is needed for statements as to binding, predicate-argument relations, or the determination of complex clause structures (given that agreement is generally clause-bounded), and is therefore represented at f-structure. Wellformedness conditions on adjective inflection or relative pronoun agreement, however, can now be stated on the m-structure as idiosyncratic, language particular information which can be ignored for purposes of MT or semantic interpretation.\\n\\nSyntactic Analyses for Parallel Grammars: Auxiliaries and Genitive NPs\\n\\nMiriam Butt  - Christian Fortmann  - Christian Rohrer Institut fr Maschinelle Sprachverarbeitung  Universitt Stuttgart Azenbergstr. 12  70174 Stuttgart, Germany {mutt|fortmann|rohrer}@ims.uni-stuttgart.de\\n\\nAbstract: This paper focuses on two disparate aspects of German syntax from the perspective of parallel grammar development. As part of a cooperative project, we present an innovative approach to auxiliaries and multiple genitive NPs in German. The LFG-based implementation presented here avoids unnessary structural complexity in the representation of auxiliaries by challenging the traditional analysis of auxiliaries as raising verbs. The approach developed for multiple genitive NPs provides a more abstract, language independent representation of genitives associated with nominalized verbs. Taken together, the two approaches represent a step towards providing uniformly applicable treatments for differing languages, thus lightening the burden for machine translation.\\n\\nIntroduction\\n\\nWithin the cooperative parallel grammar project  PARGRAM (IMS-Stuttgart, Xerox-Palo Alto, Xerox-Grenoble), the analysis and representation of structures in the grammars must be viewed from a more global perspective than that of the individual languages (German, English, French). One major goal of  PARGRAM is the development of broad coverage grammars which are also modular and easy to maintain. Another major goal is the construction of parallel analyses for sentences of the same type in German, English, and French. If this can be achieved, the problem faced by machine translation (MT) could be greatly reduced. Due to the recent development of a faster and more powerful version of the LFG (Lexical-Functional-Grammar) based Grammar Writer's Workbench (Kaplan and Maxwell 1993) at Xerox, the implementation of a linguistically adequate, broad coverage grammar appears viable. Given the flexible projection-based architecture of LFG (Dalrymple et al. 1995) and the MT approach presented in Kaplan et al. (1989), a robust MT system is already in place.\\n\\nIn this paper, we concentrate on two issues within the broader perspective of  PARGRAM: the treatment of auxiliaries and the transparent representation of multiple genitive NPs in German. These phenomena represent two areas for which generally accepted proposals exist, but whose implementation in the context of parallel grammar development throws up questions as to their wider, crosslinguistic, feasibility. With respect to auxiliaries, the standard raising approach that is usually adopted yields undesirable structural complexity and results in idiosyncratic, language particular analyses of the role of auxiliaries. With regard to genitive NPs, the standard analysis for German yields structures which are too ambiguous for a succesful application of machine translation. The following sections present a solution in that morphological wellformedness conditions are stated at a separate component, the morphology projection. Furthermore, a representation of argument structure is implemented that is related to, but not identical to the representation of grammatical functions. Language particular idiosyncratic requirements are thus separated out from the language universal information required for further semantic interpretation, or machine translation.\\n\\nThe Formalism\\n\\nThe architecture of LFG assumed here is the ``traditional'' architecture described in Bresnan (1982), as well as the newer advances within LFG (Dalrymple et al. , 1995). A grammar is viewed as a set of correspondences expressed in terms of projections from one level of representation to another. Two fundamental levels of representations within LFG are the c(onstitutent)-structure and the f(unctional)-structure. The c-structure encodes idiosyncratic phrase structural properties of a given language, while the f-structure provides a language universal representation of grammatical functions (e.g.,  SUBJect,  OBJect), complementation, tense, binding, etc. The correspondence between c-structure and f-structure is not onto or one-to-one, but many-to-one, allowing an abstraction over idiosyncratic c-structure properties of a language (e.g., discontinuous constituents).\\n\\nIn addition, several proposals exploring possible representations of a s(emantic)-structure have been made over the years (e.g. Halvorsen and Kaplan (1988), Dalrymple et al. (1993)). As the realization of a separate semantic component is only planned for the latter stages within  PARGRAM, no further discussion of possible formalisms will take place here. It should be noted, however, that rudimentary semantic information, such as argument structure information (lexical semantics), is encoded within the f-structures in order to facilitate transfer in some cases. A case in point is presented in the section on German genitive NPs.\\n\\nAuxiliaries -\\n\\n\\n\\na flat approach\\n\\nThe Received Wisdom\\n\\nAuxiliaries have given rise to lively debates concerning their exact syntactic status (e.g. Chomsky (1957), Ross (1967), Pullum and Wilson (1977), Akmajian et al. (1979), Gazdar et al. (1982)): are they simply main verbs with special properties, or should they instantiate a special category  AUX? Within current lexical approaches (Lexical-Functional-Grammar (LFG), Head-driven Phrase Structure Grammar (HPSG)), auxiliaries (e.g. have, be) and modals (e.g. must, should) are treated as raising verbs, which are marked as special in some way: in HPSG through an [ AUX: +] feature (Pollard and Sag 1994), in LFG (Bresnan 1982) by a difference in  PRED value. However, newer work within LFG (Bresnan 1995, T.H. King 1995) has been moving away from the raising approach towards an analysis where auxiliaries are elements which contribute to the clause only tense/aspect, agreement, or voice information, but not a subcategorization frame. This view is also in line with approaches within GB (Government-Binding), which see auxiliaries simply as possible instantiations of the functional category I (see also Halle and Marantz (1993)).\\n\\nThe ``traditional'' treatment of auxiliaries in both HPSG (Pollard and Sag 1994) and LFG has its roots in Ross's (1967) proposal to treat auxiliaries and modals on a par with main verbs. In particular, auxiliaries are treated as a subclass of raising verbs (e.g. Pollard and Sag (1994), Falk (1984)). For example, a simple sentence like (1) would correspond to the c-structure and f-structure shown in (2) and (3), respectively. Note that the level of embedding in the f-structure exactly mirrors the c-structure: each verbal element takes a complement.\\n\\nThe main reasons to treat auxiliaries as complement taking verbs in English are: 1) an account of VP-ellipsis, VP-topicalization, etc. follows immediately; 2) restrictions on the nature of the verbal complement (progressive, past participle, etc.) following the auxiliary can be stated straightforwardly (Pullum and Wilson (1977), Akmajian et al. (1979), Gazdar et al. (1982)). The latter point holds for German as well, and in fact, without some sort of a hierarchical structure, stating wellformedness conditions on a string of multiple auxiliaries becomes wellnigh impossible in light of the greater ordering possibilities granted by the flexible German word order. There are also major reasons, however, for not adopting this analysis: 1) linguistic adequacy; 2) unmotivated structural complexity; 3) non-parallel analyses for predicationally equivalent sentences. Consider the French equivalent of (-2) in (1).\\n\\nAs argued by Akmajian et al. (1979), crosslinguistic evidence indicates that elements bearing only tense, mood, or voice should belong to a distinct syntactic category. In many languages, like French or Japanese, the information carried by will (future), or have (perfect) is realized morphologically rather than periphrastically. The analysis in (0) thus effectively claims that there exists a deep difference in the predicational structure of auxiliaries like will and have and the French   aura. This is not desirable from a crosslinguistic point of view, nor is it helpful for MT.\\n\\nAlternative Implementation\\n\\nThe approach adopted here is a flat analysis of auxiliaries at f-structure ((1)).\\n\\nThe auxiliaries wird `will' and haben `have' now only contribute information as to the overall tense, but do not subcategorize for complements. Structural phenomena like VP-ellipsis, coordination, or topicalization can, however, still be accounted for in terms of an appropriate embedding at c-structure (cf. (-3)). The role of auxiliaries in natural language is now adequately modeled, in particular with respect to a more realistic treatment of tense (compare (-2) and (0)), as the French (-1) has essentially the same f-structure as (0).\\n\\nHowever, the flat f-structure in (0) provides no room for a statement of selectional requirements, allowing massive overgeneration (e.g. nothing blocks the presence of two haben in (-4)). Neither can the particular order of auxiliaries be regulated. Our solution takes advantage of LFG's flexible projection-based architecture by implementing a projection which models the hierarchical selectional requirements of auxiliaries, yet does not interfere with the subcategorizational properties of verbs, as would be the case under a raising analysis.\\n\\nIn LFG, the flexible word order of German is handled via   functional uncertainty, which characterizes long-distance dependencies without resorting to movement analyses (Netter (1988), Zaenen and Kaplan (1995)). As in (0), which illustrates our alternative solution, functional uncertainty is represented by the Kleene Star (*). The annotation on the NPs indicates that they could fulfill the role of any possible grammatical function (GF), e.g. SUBJ or  OBJ, and that the level of embedding ranges from zero to infinite. With every auxiliary subcategorizing for an  XCOMP, the two NPs could conceivably be arguments of three different verbs: wird, haben, or   gedreht. Thus, the greater structural complexity unnecessarily increases the search space for the determination of a verb's arguments. In (0), however, the m-structure is projected from the c-structure parallel to the f-structure through annotations similar to the usual f-structure annotations. Statements about ``morphological'' dependents ( DEP) are thus decoupled from functional uncertainty: the relation of NP arguments to their predicator now does not extend through various layers of artificial structural complexity ( XCOMPs). For VP-topicalization or extraposition an unbounded long-distance dependency must still be assumed. However, as the functional uncertainty path for auxiliaries is distributed only over the m-structure of the verb complex ((\\n\\n), and does not involve the resolution of the role of NP arguments, there are in fact differing paths of functional uncertainty involved. The dependencies between predicators and their arguments and auxiliaries and their dependents are thus neatly factored out. The m-structure corresponding to the matrix VP in (0) is (1). The desired flat f-structure resulting from the usual\\n\\nand\\n\\nannotations is as in (-1).\\n\\nLike the f-structure, the m-structure is an attribute-value matrix. It encodes language-specific information about idiosyncratic constraints on morphological forms. The m-structure is not derived from the f-structure. Rather, both representations are in simultaneous correspondence with the c-structure. The following (abbreviated) lexical entry exemplifies the pieces of information needed. The disjunctive lexical entry for wird `will' in (1) takes the various combinatory possibilities of auxiliaries and main verbs into account, and provides the appropriate tense feature. For example, it requires that the embedded  VFORM be  BASE, and that there be no passive involved for a simple future like   wird drehen.\\n\\nFeatures needed only to ensure language particular wellformedness are no longer unified into the f-structure, cluttering a representation that is meant to be language independent. In our analysis, only features needed for further semantic interpretation, MT, or for the expression of language universal syntactic generalizations are represented at f-structure. For example, morphologically encoded information like case, gender, or agreement is needed for statements as to binding, predicate-argument relations, or the determination of complex clause structures (given that agreement is generally clause-bounded), and is therefore represented at f-structure. Wellformedness conditions on adjective inflection or relative pronoun agreement, however, can now be stated on the m-structure as idiosyncratic, language particular information which can be ignored for purposes of MT or semantic interpretation.\\n\\nMultiple Genitive NPs\\n\\nThe differing surface realization of genitives within NPs in English (preverbal NPs, postverbal PPs), French (postverbal PPs), and German (preverbal NPs, postverbal PPs or NPs), poses a particular challenge for a parallel grammar development project like  PARGRAM. In this paper, we suggest a treatment of multiple genitive NPs which not only accounts for some restrictions on their distribution within German, but also allows a language independent (universal) representation, thus facilitating MT.\\n\\nIn general, the distribution of multiple NPs within NPs is an area of German syntax which has not received a satisfactory account to date (e.g., Pollard and Sag (1994), Bhatt (1990), Haider (1988)). In German, nouns generally have at most one genitive which may occur in a prenominal or postnominal position adjacent to the noun. Both kinds of genitives have the same morphological shape. However, nominalizations that are derived from a transitive verb allow for two genitives, one in the prenominal, the other in the postnominal position.\\n\\nThe function of a genitive is generally expressed as indicating a possessor:  POSS within LFG. However, in the case of two genitives, the assignment of two  POSS values violates the uniqueness-condition on f-structures and is furthermore insufficient to distinguish the two differing kinds of genitives. We therefore propose the utilization of two functions named  GEN1 and GEN2 in order to avoid association with any specific semantic role. Furthermore, as genitives in the NP are generally optional, they are taken to express no governed functions, i.e., they are not subcategorized for by the noun. So  GEN1 and  GEN2 are semantic functions in LFG on a par with, say, adjuncts. The NP rule for German then is (1).\\n\\nIf the head-noun is not derived from, say, a verb, the single genitive in either position is interpreted as a possessor. In case of a derived nominal, however, a genitive is interpreted according to the thematic roles assigned to the arguments of the verbal base. That means the functions GEN1 and sc gen2 have to be linked to the appropriate roles. Neither of the two functions is in principle restricted to any specific role. But if both genitives are present they must be interpreted according to a thematic role hierarchy.\\n\\nAs (1) shows, if only one genitive is present, its prenominal interpretation may be as agent or as patient. A postnominal (single) genitive is interpreted as agent if the head noun is derived from an intransitive, and as a patient/theme if derived from a transitive.\\n\\nHowever, if two genitives occur, as in (1), the prenominal genitive is restricted to an agent, and the postnominal one to patient. This restriction must be encoded at some level, but does not follow from the distiction between  GEN1 and  GEN2, which are functions that do not bear any semantic content on their own.\\n\\nTo obtain the correct linking, the argument structure of the verbal base must be available. Since MT is based on f-structures within PARGRAM, the argument structure has to be present at this level of representation. Nominalization is therefore implemented as a morphologically driven process (lexical rule) which eliminates  SUBJ and  OBJ from the verb's subcategorization frame and enters the verb's argument structure into the lexical entry of the noun. This yields the optionality of genitives while preserving the underlying semantics, as shown in (1). The association of  GEN1 and  GEN2 then is determined according to a hierarchical order of arguments (Bresnan, 1995).\\n\\nThis approach also provides a means of handling certain cases of categorial shift. For instance, in German temporal and conditional adjuncts may be realized as PPs dominating an NP headed by a deverbal noun. English does not have this option, but employs an adjunct-clause instead. Here, the  GEN1 and  GEN2 functions of the German f-structure have to be related correctly to the  SUBJ and OBJ functions of the English f-structure.\\n\\nHere the linking of the  GEN1 and  GEN2 functions to the appropriate thematic role in the German f-structure drives the transfer of these functions to the  SUBJ and  OBJ functions of the English f-structure.\", metadata={'source': '../data/raw/cmplg-xml/9604013.xml'}),\n",
       " Document(page_content=\"LEXICAL FUNCTIONS AND MACHINE TRANSLATION Description of the Problem\\n\\nCollocations present specific problems in translation, both in human and automatic contexts. If we take the construction heavy smoker in English and attempt to translate it into French and German, we find that a literal translation of heavy yields the wrong result, since the concept expressed by the adjective (something like `excessive') is translated by grand (large) in French and stark (strong) in German. We observe then that in some sense the adjectives stark, grand and heavy are equivalent in the collocational context, but that this is of course not typically the case in other contexts, cf grande boite, starke Schachtel and heavy box, where the adjectives could hardly be viewed as equivalent. It seems then that adjectives which are not literal translations of one another may share meaning properties specifically in the collocational context.\\n\\nHow then can we specify this special equivalence in the machine translation dictionary? The answer seems to lie in addressing the concept which underlies the union of adjective and noun in these three cases, i.e., intensification, and hence establish a single meaning representation for the adjectives which can be viewed as an interlingual pivot for translation.\\n\\nRepresentation\\n\\nThe use we make of lexical functions as interlingual representations, does not respect their original Mel'cukian interpretation. Furthermore, we have transferred them from their context in the Meaning-Text Theory to a different theoretical setting. We have embedded the concept in an HPSG-like grammar theory. In this section we review this operation. First we consider the features of Mel'cuk's treatment that we have wanted to preserve. Next we show how they have been imported into the HPSG framework.\\n\\nCollocations and LFs\\n\\nEach article in the ECD describes what is called a `lexeme': a word in some specific reading. In the lexical combinatorics zone, we find a list of the lexical functions that are relevant to this particular lexeme. Each lexical function is followed by one or more lexemes (the result or value of the function applied to the head word). The idea is that each combination of the argument with one of the values of the function forms a collocation in our terminology. The argument corresponds to the base and each value is a collocate. The following features of this representation are important to us.\\n\\nLexical functions are used to represent an important syntactico-semantic relation between the base and the collocate. The restricted combinatorial potential of the collocate lexeme is accounted for by listing it at each base with which it can occur.\\n\\nThe second of these characteristics points out that the collocational restriction is seen as a purely lexical, idiosyncratic one: all collocations are explicitly listed.\\n\\nOne other aspect of collocations which we have to deal with is the relation between the collocate lexeme and its freely occurring counterpart. Collocate lexemes often differ in some respects from their literal variants while sharing other properties. Mel'cuk deals with this by including in the ECD an entry for the free variant and putting the collocate-specific information in the entry for the base (with the result of the lexical functions). The full entry of the collocate is the result of taking the entry for the free variant and overwriting it with the information provided at the base.\\n\\nCollocations in HPSG\\n\\nThe three aspects of Mel'cuk's analysis we wanted to encode in HPSG were the following.\\n\\nCoding the base-collocate relation in the lexicon. Choosing the level at which lexical functions will be situated. Relating the collocate information to the free variant entry.\\n\\nWe have provided straightforward solutions to these problems. For the first problem we have taken over the ECD architecture rather directly, by creating a dedicated `collocates' field in the entry for bases which contains all the relevant collocates. As far as the second problem is concerned, the obvious place to put lexical functions is in the semantic representation provided by HPSG. There are various reasons for this. One is that LFs are used in the deep syntax level in Mel'cuk's model, a level oriented towards meaning. Another reason is that this level seems most appropriate to be used in transfer/translation and because we want to use lexical functions in transfer, this is where they should be. In contrast to the ECD, the meaning of the collocate is represented by the lexical function only.\\n\\nThe following is an example of the entry for criticism with the encoding of strong as a collocate. We use  SEM/SMALL>_IND as an abbreviation for the feature path  SEM.CONT.IND.\\n\\nJust as in the ECD the base contains a specific zone in which the collocates are listed. In our case, the feature ` COLLS' has a set of lexical entries as its value.\\n\\nThe collocate subentry only provides partial information. In fact, it provides only the information that is specific to the occurrence of strong in its combination with criticism. In this case only the semantics is given. We further assume that the lexicon also contains a `super-entry' which provides all the information that is shared by all the different occurrences of strong. This entry is where the variable $strong points to. Of course, other architectures that try to avoid redundant specification of information are equally possible. For instance if one assumes a mechanism of default unification, one can have $strong refer to the full entry describing `strong' in say its ordinary use, and have the values that are particular to the collocational strong overwrite the values provided in the ordinary entry, as in Mel'cuk's proposal.\\n\\nCollocations, Rules and Principles\\n\\nSo far, we have not specified in what way one gets from the lexical entries for the base and the collocate to the representation of the collocational expression.\\n\\nIn HPSG, the descriptions of complex expressions are constrained by principles. We will assume that collocations are subject to the same constraints. The ordinary rules of combination (combining adjectives and nouns, for instance) thus account for most of the properties of the collocational combination. However, we are still left with the typical `collocational restriction' which needs to be accounted for.\\n\\nWe have therefore added a principle which says that constructions that are analysed as collocations (indicated by the type  COLLOCATION) are either head-adjunct structure or head-complement structures with specific restrictions holding between the head and the adjunct or the head and the complement respectively. Let's consider the former case, illustrated by the heavy smoker example. The adjunct daughter will contain the adjective collocate. In such collocational constructions the collocate adjuncts have to be `licensed' by the noun or the head daughter. This is implemented by requiring that the collocates field ( COLLS) of the head daughter contains a reference to a lexical entry that is compatible with the adjunct daughter. In the literal reading of an expression such as heavy smoker, the phrase will not be analysed as a COLLOCATION and the principle does not apply.\\n\\nIssues in Translation\\n\\nThe project has tried to investigate the use of lexical functions as an interlingual device, i.e., one which is shared by the semantic representations of collocations in the language pairs.\\n\\nThe typing of a collocation with such a function opens up the way to a treatment of collocations inside a given language module and hence to a substantial reduction in the number of collocations explicitly handled in the multilingual transfer dictionary. The existence of a collocation function is established during analysis. This information is used to generate the correct translation in the target language. To illustrate, the English analysis module might analyse (1) as (2). The transfer module maps (2) onto (3) which is then synthesised by the French module to (4).\\n\\nThe example points out that the translation strategy is a mixture of transfer and interlingua. The bases are transferred but the representation of the collocate is shared between the source and the target representation. This treatment of collocations rests, among others, on the assumptions that there are only a limited number of lexical functions, that lexical functions can be assigned consistently, that all (or a significant number of) collocations realise a lexical function, that lexical functions are not restricted to particular languages, etc. In the following paragraph we present an outline of the translation process. Next, we discuss some of the problems which follow from our approach and we propose some ways to solve them.\\n\\nLexical Functions as Interlingua\\n\\nIt was assumed that the starting point for transfer is the semantic representation of the phrase. Using a semantic representation as input to transfer implies that we relate semantic values of words and phrases. For our purposes this is very satisfying since we will now be using the semantics of collocates instead of their orthography, in other words: we use lexical functions and abstract away from the particular realisation of a collocate in a particular language.\\n\\nWe now state the relation between the semantic representations of the source language and target language. The semantic relation between the phrase heavy smoker and its French counterpart can be made explicit in the following bilingual sign:\\n\\nTypically, the lexicon will contain a bilingual sign for each possible value of  RELN. Thus, for translating heavy smoker into grand fumeur we will need the obvious entry for smoker-fumeur plus the entry below:\\n\\nThe interlingual status of the lexical function is self-evident. Any occurrence of Magn will be left intact during transfer and it will be the generation component that ultimately assigns a monolingual lexical entry to the LF.\\n\\nProblems\\n\\nLexical Functions abstract away from certain nuances in meaning and from different syntactic realizations. We discuss some of the problems raised by this abstraction in this section.\\n\\nOvergenerality\\n\\nAn important problem stems from the interpretation of LFs implied by their use as an interlingua -- namely that the meaning of the collocate in some ways reduces to the meaning implied by the lexical function. This interpretation is trouble-free if we assume that LFs always deliver unique values; unfortunately cases to the contrary can be readily observed. An example attested from our corpus was the range of adverbial constructions possible with the verbal head oppose: adamantly, bitterly, consistently, steadfastly, strongly, vehemently, vigorously, deeply, resolutely, etc. The function Magn is an appropriate descriptor in all cases since each adverb functions as a typical intensifier in this context. However each adverb also denotes some other meaning aspect(s). The imprecision of LFs will mean that we have no means of distinguishing between the various intensifiers possible in the context of a given keyword, and hence will not have sufficient information to choose the most appropriate translation where, correspondingly, multiple possibilities exist in the target language. An important question here is how dramatic this loss of translation quality really is.\\n\\nBon(Const: lecture) = informative\\n\\nBon(Agent: lecture) = clear\\n\\nIn both cases the idea is that the precision of the lexical function is essentially enhanced by appealing to the semantic facets of its argument.\\n\\nSyntactic Divergences\\n\\nAnother issue that has to be raised concerns the translation of collocations into non-collocational constructions. If we are to maintain a consistent interlingual approach to the translation of these cases, we must extend our LF-based approach accordingly. We consider one case briefly.\\n\\nFurther examples exist where productive morphological processes (e.g., affixation) lead to the lexicalisation in one language of concepts that exist as syntagmatic constructs in another. Again, we suggest the use of merged LFs and corresponding mappings via lexical paraphrasing rules as a possible translation strategy in these cases.\\n\\nSummary and Conclusions\\n\\nIn this paper we have discussed how the lexicographical concept of lexical functions, introduced by Mel'cuk to describe collocations, can be used as an interlingual device in the machine translation of such structures. We have shown how the essentials of the ECD analysis can be embedded in the lexicon and grammar of a unification based theory of language.\\n\\nOur use of lexical functions as an interlingua assumes that the relevant aspects of the meaning of the collocate are fully captured by the LF. The LF therefore determines the accuracy of translations, which may be impoverished due to the generalised nature of basic LFs. We have suggested some ways in which LFs can be enriched with lexical semantic information to improve translation quality.\\n\\nThe interlingua level reflects what is semantically common to expressions which form translational equivalents. It abstracts away from specific syntactic realisations. Given that collocations may translate as non-collocations, we also have to provide a way to represent these expressions using lexical functions. We have provided an illustration on how to proceed in one such case.\\n\\nAcknowledgements\\n\\nWe would like to thank the following partners and colleagues: Susan Armstrong-Warwick, Laura Bloksma, Nicoletta Calzolari, R. Lee Humphreys, Simon Murison-Bowie and Andr Schenk.\\n\\nBibliography\\n\\nA. Abeill and Y. Schabes. Parsing idioms in lexicalized tags. In EACL/89, Manchester, 1989.\\n\\nP. Anick and J. Pustejovsky. An application of lexical semantics to knowledge acquisition from corpora. In Coling/90, Helsinki, 1990.\\n\\nK. Choueka, S.T. Klein, and E. Neuwitz. Automatic retrieval of frequent idiomatic and collocational expressions in a large corpus. ALLC Journal, pages 34-38, 1983.\\n\\nK. W. Church and P. Hanks. Word association norms, mutual information and lexicography. In ACL/89, Vancouver, 1989.\\n\\nU. Heid and S. Raab. Collocations in multilingual generation. In EACL/89, pages 130-136, Manchester, 1989.\\n\\nDirk Heylen. Collocations and the lexicalisation of semantic operations. Technical report, OTS, 1993.\\n\\nD. Heylen. Lexical functions and knowledge representation. In P. Saint-Dizier and E. Viegas, editors, Computational Lexical Semantics. CUP, to appear.\\n\\nB. Krenn and G. Erbach. Idioms and support verb constructions. In J. Nerbonne, K. Netter, and C. Pollard, editors, German Grammar in HPSG. CSLI Lecture Notes, To appear.\\n\\nS. W. McRoy. Using multiple knowledge sources for word sense discrimination. Computational Linguistics, 18(1):1-30, 1992.\\n\\nI.A. Mel'cuk and A.K. Zolkovsky. Sur la synthse smantique. T.A. Informations, 2:1-85, 1970.\\n\\nI.A. Mel'cuk and A.K. Zolkovsky. Explanatory Combinatorial Dictionary of Modern Russian. Wiener Slawistischer Almanach Sonderband 14, Vienna, 1984.\\n\\nI. A. Mel'cuk, N. Arbatchewsky-Jumarie, L. Elnitsky, L. Iordanskaja, and A. Lessard. Dictionnaire explicatif et combinatoire du franais contemporain. Les Presses de l'Universit de Montral, Montreal, 1984.\\n\\nS. Nirenburg, R. McCardell, E. Nyberg, S. Huffman, E. Kenschaft, and I. Nirenburg. Lexical realization in natural language generation. In Second International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages, Pittsburgh, 1988.\\n\\nC. Pollard and I. Sag. Information Based Syntax and Semantics. CSLI, Stanford, 1987.\\n\\nC. Pollard and I. Sag. Head driven phrase structure grammar. to appear.\\n\\nJ. Pustejovsky, S. Bergler, and P. Anick. Lexical semantic techniques for corpus analysis. Computational Linguistics, 19(2):331-358, 1992.\\n\\nJ. Pustejovsky.\\n\\nThe generative lexicon.\\n\\nComputational Linguistics, 17(4), 1991.\\n\\nFrank Smadja and Kathleen R. McKeown. Automatically extracting and representing collocations for language generation. In 28th Annual Meeting of the Association for Computational Linguistics, Pittsburgh, PA, 1990.\\n\\nF. Smadja. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143-177, 1993.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9410009.xml'}),\n",
       " Document(page_content=\"REFERENCE RESOLUTION USING SEMANTIC PATTERNS IN JAPANESE NEWSPAPER ARTICLES\\n\\nDISCUSSION\\n\\nThe second and third heuristic methods show high accuracy in finding the referents of dousha with ga and ha. This means that partial semantic parsing (in which key semantic information such as company name, human name, and title is marked) is sufficient for reference resolution of important referential expressions such as dousha in Japanese. Moreover, since the two modified methods are simple, they will be easily implemented by computationally inexpensive finite-state pattern matchers (Hobbs et al. 1992; Cowie et al. 1993). Therefore, they will be suitable for large scale text processing (Jacobs 1992; Chinchor et al. 1993). One important point to realize is that the second and third methods, although they are simple to implement, achieve something that is rather complicated and may be computationally expensive otherwise. For example, in order to find the correct referent of a given dousha, you may have to skip one entire paragraph and find the referent two paragraphs before, or you may have to choose the right company name from several possible company names which appear before the given dousha. The modified methods do this correctly most of the time without worrying about constructing sometimes complicated syntactic structures of the sentences in the search window for the possible referent.\\n\\nAnother important point is that the modified methods make good use of post-nominal particles, especially ha and ga. For example, if the referent is located two sentences or more before, then the referent (the company name) comes with ha almost all the time (35 out of 38 such cases for both dousha). It seems that if the referent of the dousha in consideration is more than a certain distance before, two sentences in this case, then the referent is marked with ha most of the time. Kitani also uses this ha or ga marked company names as key information in his reference resolution algorithm for dousha (Kitani 1994).\\n\\nCONCLUSION\\n\\nThe locations and contexts of the referents of dousha in Japanese Joint-Venture articles are determined by hand. Three heuristic methods are proposed and tested. The methods which use semantic information in the text and its patterns show high accuracy in finding the referents (96% for dousha with ga and 96% for dousha with ha for the unseen test data). The high success rates suggest that a semantic pattern-matching approach is not only a valid method but also an efficient method for reference resolution in the newspaper article domains. Since the Japanese language is highly case-inflected, case (particle) information is used effectively in these methods for reference resolution. How much one can do with semantic pattern matching for reference resolution of similar expressions such as ``the company'' or ``the Japanese company'' in English newspaper articles is a topic for future research.\\n\\nACKNOWLEDGEMENT\\n\\nI would like to thank the Tipster project group at the CRL for their inspiration and suggestions. I would also like to thank Dr. Yorick Wilks, Dr. John Barnden, Mr. Steve Helmreich, and Dr. Jim Cowie for their productive comments. The newspaper articles used in this study are from the Tipster Information Extraction project provided by ARPA.\\n\\nREFERENCES\\n\\nChinchor, N., L. Hirschman, and D. Lewis (1993). Evaluating Message Understanding Systems: An Analysis of the Third Message Understanding Conference (MUC-3). Computational Linguistics, 19(3), pp. 409-449. Cowie, J., T. Wakao, L. Guthrie, W. Jin, J. Pustejovsky, and S. Waterman (1993). The Diderot Information Extraction System. In the proceedings of The First Conference of the Pacific Association for Computational Linguistics (PACLING 93) Simon Fraser University, Vancouver, B.C. Canada, pp. 23-32. Jacobs, P.S. (1992). Introduction: Text Power and Intelligent Systems. In P.S. Jacobs Ed., Text-Based Intelligent Systems. Lawrence Erlbaum Associates, Hillsdale New Jersey, pp. 1-8. Hobbs, J., D. Appelt, M. Tyson, J. Bear, and D. Israel (1992). SRI International Description of the FASTUS System used for MUC-4. In the proceedings of Fourth Message Understanding Conference (MUC-4), Morgan Kaufmann Publishers, San Mateo, pp. 269-275. Kitani, T. (1994). Merging Information by Discourse Processing for Information Extraction. In the proceedings of the tenth IEEE Conference on Artificial Intelligence for Applications, pp. 168-173. Muraki, K., S. Doi, and S. Ando (1993). Context Analysis in Information Extraction System based on Keywords and Text Structure. In the proceedings of the 47th National Conference of Information Processing Society of Japan, 3-81. (In Japanese). Shibata, M., O. Tanaka, and J. Fukumoto (1990). Anaphora in Newspaper Editorials.\\n\\nIn the proceedings of the 40th National Conference of Information Processing Society of Japan, 5F-4. (In Japanese).\\n\\nFootnotes\\n\\nThis paper was written when the author was at the Computing Research Laboratory of New Mexico State University. The author has been at University of Sheffield since January 1994.\", metadata={'source': '../data/raw/cmplg-xml/9410021.xml'}),\n",
       " Document(page_content=\"FAST PARSING USING PRUNING AND GRAMMAR SPECIALIZATION\\n\\nWe show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning. These methods together give an order of magnitude increase in speed, and the coverage loss entailed by grammar specialization is reduced to approximately half that reported in previous work. Experiments described here suggest that the loss of coverage has been reduced to the point where it no longer causes significant performance degradation in the context of a real application.\\n\\nIntroduction\\n\\nSuppose that we have a general grammar for English, or some other natural language; by this, we mean a grammar which encodes most of the important constructions in the language, and which is intended to be applicable to a large range of different domains and applications. The basic question attacked in this paper is the following one: can such a grammar be concretely useful if we want to process input from a specific domain? In particular, how can a parser that uses a general grammar achieve a level of efficiency that is practically acceptable?\\n\\nThe central problem is simple to state. By the very nature of its construction, a general grammar allows a great many theoretically valid analyses of almost any non-trivial sentence. However, in the context of a specific domain, most of these will be extremely implausible, and can in practice be ignored. If we want efficient parsing, we want to be able to focus our search on only a small portion of the space of theoretically valid grammatical analyses.\\n\\nConstituent Pruning\\n\\nBefore both the phrasal and full parsing stages, the constituent table (henceforth, the chart) is pruned to remove edges that are relatively unlikely to contribute to correct analyses.\\n\\nFor example, after the string ``Show flight D L three one two'' is lexically analysed, edges for ``D'' and ``L'' as individual characters are pruned because another edge, derived from a lexical entry for ``D L'' as an airline code, is deemed far more plausible. Similarly, edges for ``one'' as a determiner and as a noun are pruned because, when flanked by two other numbers, ``one'' is far more likely to function as a number.\\n\\nPhrasal parsing then creates a number of new edges, including one for ``flight D L three one two'' as a noun phrase. This edge is deemed far more likely to serve as the basis for a correct full parse than any of the edges spanning substrings of this phrase; those edges, too, are therefore pruned. As a result, full parsing is very quick, and only one analysis (the correct one) is produced for the sentence. In the absence of pruning, processing takes over eight times as long and produces 37 analyses in total.\\n\\nThe pruning algorithm\\n\\nOur algorithm estimates the probability of correctness of each edge: that is, the probability that the edge will contribute to the correct full analysis of the sentence (assuming there is one), given certain lexical and/or syntactic information about it. Values on each criterion (selection of pieces of information) are derived from training corpora by maximum likelihood estimation followed by smoothing. That is, our estimate for the probability that an edge with property P is correct is (modulo smoothing) simply the number of times edges with property P occur in correct analyses in training divided by the number of times such edges are created during the analysis process in training.\\n\\nThe current criteria are:\\n\\nits tag (corresponding to its major category symbol plus, for a few categories, some additional distinctions derived from feature values);\\n\\nfor a lexical edge, its word or semantic word class (words with similar distributions, such as city names, are grouped into classes to overcome data sparseness); or for a phrasal edge, the name of the final (topmost) grammar rule that was used to create it;\\n\\nthe tag of a neighbouring edge immediately to its left. If there are several left neighbours, the one giving the highest probability is used.\\n\\nThe right bigram score: as above, but considering right neighbours.\\n\\nThe unigram score: the probability of correctness of an edge considering only the tree of grammar rules, with words or word classes at the leaves, that gave rise to it. For a lexical edge, this reduces to its word or word class, and its tag.\\n\\nOther criteria, such as trigrams and finer-grained tags, are obviously worth investigating, and could be applied straightforwardly within the framework described here.\\n\\nNext, account is taken of the connectivity of the chart. Each vertex of the chart is labelled with the score of the best path through the chart that visits that vertex. In accordance with the dependence assumption, the score of a path is defined as the minimum of the scores of its component edges. Then the score of each edge is recalculated to be the minimum of its existing score and the scores of its start and end vertices, on the grounds that a constituent, however intrinsically plausible, is not worth preserving if it does not occur on any plausible paths.\\n\\nFinally, a pruning threshold is calculated as the score of the best path through the chart multiplied by a certain fraction. For the first pruning phase we use 1/20, and for the second, 1/150, although performance is not very sensitive to this. Any constituents scoring less than the threshold are pruned out.\\n\\nRelation to other pruning methods\\n\\nAs the example above suggests, judicious pruning of the chart at appropriate points can greatly restrict the search space and speed up processing. Our method has points of similarity with some very recent work in Constraint Grammar and is an alternative to several other, related schemes.\\n\\nFirstly, a remarked earlier, it generalizes tagging: it not only adjudicates between possible labels for the same word, but can also use the existence of a constituent over one span of the chart as justification for pruning another constituent over another span, normally a subsumed one, as in the ``D L'' example. This is especially true in the second stage of pruning, when many constituents of different lengths have been created. Furthermore, it applies equally well to lattices, rather than strings, of words, and can take account of acoustic plausibility as well as syntactic considerations.\\n\\nGrammar specialization\\n\\nThere are two main parameters that can be adjusted in the EBL learning phase. Most simply, there is the size of the training corpus; a larger training corpus means a smaller loss of coverage due to grammar specialization. (Recall that grammar specialization in general trades coverage for speed). Secondly, there is the question of how to select the rule-chunks that will be turned into macro-rules. At one limit, the whole parse-tree for each training example is turned into a single rule, resulting in a specialized grammar all of whose derivations are completely ``flat''. These grammars can be parsed extremely quickly, but the coverage loss is in practice unacceptably high, even with very large training corpora. At the opposite extreme, each rule-chunk consists of a single rule-application; this yields a specialized grammar identical to the original one. The challenge is to find an intermediate solution, which specializes the grammar non-trivially without losing too much coverage.\\n\\nutterance ] utterance_unit ] imperative_VP ] NP ] {rel, VP_modifier} ] PP\\n\\nThe precise definition of the rule-chunking criteria is quite simple, and is reproduced in the appendix.\\n\\nNote that only the non-phrasal rules are used as input to the chunks from which the specialized grammar rules are constructed. This has two important advantages. Firstly, since all the phrasal rules are excluded from the specialization process, the coverage loss associated with missing combinations of phrasal rules is eliminated. As the experiments in the next section show, the resulting improvement is quite substantial. Secondly, and possibly even more importantly, the number of specialized rules produced by a given training corpus is approximately halved. The most immediate consequence is that much larger training corpora can be used before the specialized grammars produced become too large to be handled by the LR table compiler. If both phrasal and non-phrasal rules are used, we have been unable to compile tables for rules derived from training sets of over 6,000 examples (the process was killed after running for about six hours on a Sun Sparc 20/HS21, SpecINT92=131.2). Using only non-phrasal rules, compilation of the tables for a 15,000 example training set required less than two CPU-hours on the same machine.\\n\\nExperiments\\n\\nConclusions and further directions\\n\\nTable 2 indicates that EBL and pruning each make processing about three times faster; the combination of both gives a factor of about nine. In fact, as the detailed breakdown shows, even this underestimates the effect on the main parsing phase: when both pruning and EBL are operating, processing times for other components (morphology, pruning and preferences) become the dominant ones. As we have so far expended little effort on optimizing these phases of processing, it is reasonable to expect substantial further gains to be possible.\\n\\nEven more interestingly, Table 3 shows that real system performance, in terms of producing a good translation, is significantly improved by pruning, and is not degraded by grammar specialization. (The slight improvement in coverage with EBL on is not statistically significant). Our interpretation of these results is that the technical loss of grammar coverage due to the specialization and pruning processes is more than counterbalanced by two positive effects. Firstly, fewer utterances time out due to slow processing; secondly, the reduced space of possible analyses means that the problem of selecting between different possible analyses of a given utterance becomes easier.\\n\\nTo sum up, the methods presented here demonstrate that it is possible to use the combined pruning and grammar specialization method to speed up the whole analysis phase by nearly an order of magnitude, without incurring any real penalty in the form of reduced coverage. We find this an exciting and significant result, and are further continuing our research in this area during the coming year. In the last two paragraphs we sketch some ongoing work.\\n\\nThe second topic is a more radical departure, and can be viewed as an attempt to make interleaving of parsing and pruning the basic principle underlying the CLE's linguistic analysis process. Exploiting the ``stratified'' nature of the EBL-specialized grammar, we group the chunked rules by level, and apply them one level at a time, starting at the bottom. After each level, constituent pruning is used to eliminate unlikely constituents. The intent is to achieve a trainable robust parsing model, which can return a useful partial analysis when no single global analysis is found. An initial implementation exists, and is currently being tested; preliminary results here are also very positive. We expect to be able to report on this work more fully in the near future.\\n\\nAcknowledgements\\n\\nThe work reported in this paper was funded by Telia Research AB. We would like to thank Christer Samuelsson for making the LR compiler available to us, Martin Keegan for patiently judging the results of processing 16,000 ATIS utterances, and Steve Pulman and Christer Samuelsson for helpful comments.\\n\\nBibliography\\n\\nAgns, M-S., Alshawi, H., Bretan, I., Carter, D.M. Ceder, K., Collins, M., Crouch, R., Digalakis, V., Ekholm, B., Gambck, B., Kaja, J., Karlgren, J., Lyberg, B., Price, P., Pulman, S., Rayner, M., Samuelsson, C. and Svensson, T. 1994. Spoken Language Translator: First Year Report. SRI technical report CRC-043\\n\\nAlshawi, H. (ed.)\\n\\n1992.\\n\\nThe Core Language Engine.\\n\\nMIT Press.\\n\\nAndry, F., M. Gawron, J. Dowding, and R. Moore. 1994. A Tool for Collecting Domain Dependent Sortal Constraints From Corpora. Proc. COLING-94, Kyoto.\\n\\nBriscoe, Ted, and John Carroll. 1993. Generalized Probabilistic LR Parsing of Natural Language (Corpora) with Unification-Based Grammars. Computational Linguistics, 19:1, pp. 25-60.\\n\\nChurch, Ken. 1988. A stochastic parts program and noun phrase parser for unrestricted text. Proc. 1st ANLP, Austin, Tx., pp. 136-143.\\n\\nCutting, D., J. Kupiec, J. Pedersen and P. Sibun. 1992. A Practical Part-of-Speech Tagger Proc. 3rd ANLP, Trento, Italy, pp. 133-140.\\n\\nDeMarcken, C.G. 1990. Parsing the LOB Corpus Proc. 28th ACL, Pittsburgh, Pa., pp. 243-251\\n\\nDeRose, Steven. 1988. Grammatical Category Disambiguation by Statistical Optimization. Computational Linguistics 14, pp. 31-39\\n\\nGambck, Bjrn, and Manny Rayner. ``The Swedish Core Language Engine''. Proc. 3rd Nordic Conference on Text Comprehension in Man and Machine, Linkping, Sweden. Also SRI Technical Report CRC-025.\\n\\nGrishman, R., N. Nhan, E. Marsh and L. Hirschmann. 1984. Automated Determination of Sublanguage Usage. Proc. 22nd COLING, Stanford, pp. 96-100.\\n\\nvan Harmelen, Frank, and Alan Bundy. 1988. Explanation-Based Generalization = Partial Evaluation (Research Note) Artificial Intelligence 36, pp. 401-412.\\n\\nHemphill, C.T., J.J. Godfrey and G.R. Doddington. 1990. The ATIS Spoken Language Systems pilot corpus. Proc. DARPA Speech and Natural Language Workshop, Hidden Valley, Pa., pp. 96-101.\\n\\nKarlsson, F., A. Voutilainen, J. Heikkil and A. Anttila (eds). 1995. Constraint Grammar. Mouton de Gruyer, Berlin, New York.\\n\\nMcCord, M. 1993. Heuristics for Broad-Coverage Natural Language Parsing. Proc. 1st ARPA Workshop on Human Language Technology, Princeton, NJ. Morgan Kaufmann.\\n\\nMitchell, T., R. Keller, and S. Kedar-Cabelli. 1986. Explanation-Based Generalization: a Unifying View. Machine Learning 1:1, pp. 47-80.\\n\\nMurveit, H., Butzberger, J., Digalakis, V. and Weintraub, M. 1993. Large Vocabulary Dictation using SRI's DECIPHER(TM) Speech Recognition System: Progressive Search Techniques. Proc. Inter. Conf. on Acoust., Speech and Signal, Minneapolis, Mn.\\n\\nRayner, M. 1988. Applying Explanation-Based Generalization to Natural-Language Processing. Proc. the International Conference on Fifth Generation Computer Systems, Kyoto, pp. 1267-1274.\\n\\nRayner, M., Alshawi, H., Bretan, I., Carter, D.M., Digalakis, V., Gambck, B., Kaja, J., Karlgren, J., Lyberg, B., Price, P., Pulman, S. and Samuelsson, C. 1993. A Speech to Speech Translation System Built From Standard Components. Proc.1st ARPA workshop on Human Language Technology, Princeton, NJ. Morgan Kaufmann. Also SRI Technical Report CRC-031.\\n\\nRayner, M., D. Carter and P. Bouillon. 1996. Adapting the Core Language Engine to French and Spanish. Proc. NLP-IA, Moncton, New Brunswick. Also SRI Technical Report CRC-061.\\n\\nRayner, M., D. Carter, V. Digalakis and P. Price. 1994. Combining Knowledge Sources to Reorder N-Best Speech Hypothesis Lists. Proc.2nd ARPA workshop on Human Language Technology, Princeton, NJ., pp. 217-221. Morgan Kaufmann. Also SRI Technical Report CRC-044.\\n\\nRayner. M., and C. Samuelsson. 1990. Using Explanation-Based Learning to Increase Performance in a Large NL Query System. Proc. DARPA Speech and Natural Language Workshop, June 1990, pp. 251-256. Morgan Kaufmann.\\n\\nSamuelsson, C. 1994. Notes on LR Parser Design. Proc. COLING-94, Kyoto, pp. 386-390.\\n\\nSamuelsson, C. 1994. Grammar Specialization through Entropy Thresholds. Proc. ACL-94, Las Cruces, NM, pp. 188-195.\\n\\nSamuelsson, C., and M. Rayner. 1991. Quantitative Evaluation of Explanation-Based Learning as an Optimization Tool for a Large-Scale Natural Language System. Proc. 12th IJCAI, Sydney, pp. 609-615.\\n\\nWoods, W. 1985. Language Processing for Speech Understanding. Computer Speech Processing, W. Woods and F. Fallside (eds), Prentice-Hall International.\\n\\nYarowsky, D. 1994. Decision Lists for Lexical Ambiguity Resolution. Proc. ACL-94, Las Cruces, NM, pp. 88-95.\\n\\nAppendix: definition of the ``New'' chunking rules Footnotes\", metadata={'source': '../data/raw/cmplg-xml/9604017.xml'}),\n",
       " Document(page_content=\"Morphology with a Null\\n\\n\\n\\nInterface\\n\\nWe   present  an    integrated architecture   for word-level   and  sentence-level processing  in  a unification-based paradigm. The  core  of the system  is  a  CLP implementation  of  a unification  engine   for feature  structures  supporting relational values. In this framework   an HPSG-style grammar is implemented. Word-level   processing uses    X2MORF, a morphological  component based  on    an extended version    of  two-level  morphology. This component is tightly integrated with the grammar as a relation. The advantage of  this approach is that morphology  and syntax  are kept logically autonomous while at the   same time minimizing   interface problems.\\n\\nIntroduction\\n\\nA major reason for this is  the problem of interfacing morphology with syntax. Reflecting the current trend   in syntax towards lexicalism, unification-based systems use highly  structured feature structures as input. Translating the output of morphological components into such a representation has proved to be difficult. Reducing interface problems is therefore crucial to success.\\n\\nA  tight integration between word  and  sentence level processing also has linguistic advantages. The boundary between morphology and syntax is fuzzy. When processing written  text the units morphology  has to deal with are, in a  technical sense, not  words but character strings separated by delimiters. While these strings roughly correspond to the words of a  sentence there  are  problematic cases. In German, e.g., zu-infinitive or verbs with separable  prefixes are written as a single unit in some instances and separately in others.\\n\\nThe harder  problem is  the  integration  of morphophonology  which is traditionally  formalized in a  way  not easily translatable into  the feature formalism. We will show  how this  can be achieved by merging the word-level grammar of  X2MORF  into an HPSG-style grammar, and by adopting a relational view of its two-level rules.\\n\\nIn this paper we  assume basic familiarity with  unification-based NLP techniques and two-level morphology.\\n\\nIntegrating Morphology into HPSG\\n\\nHead-driven    Phrase   Structure   Grammar  (HPSG,   PS1, PS2) can be viewed as a mono-level but multi-stratal theory of  grammar, where  different strata  relate to  different  aspects of linguistic information,  but   are  represented uniformly  in  feature logics. As  such  it is well   suited as a  linguistic theory  for our enterprise.\\n\\nHPSG differentiates between three  strata-- PHON,  SYNSEM and   DTRS. Though morphology is  not considered in the standard approach, it suggests  itself to  be  included as a fourth  stratum by introducing a feature      MORPH into  the type  sign. Morphotactics are  easily  described  in  terms  of a   feature  based grammar. The    problem is how  to  deal  with  morphophonology. Two proposals have been made to overcome this problem.\\n\\nLike the  other approaches ours is  also  based on  HPSG. However, we employ  a different approach to integration. Our grammar is encoded using a   unification engine based   on constraint  logic  programming (CLP). Besides  conventional attribute-value descriptions this system allows  for the direct  representation of  more general relations,  as they are required  by HPSG. This  extension of the formalism is used for the integration of morphology. Thus  X2MORF  is treated as one special relation of  the grammar. As  a result, our approach is  more modular than the others. While being fully integrated morphology can still be viewed as an autonomous component leading  to a more flexible design.\\n\\nWe  will now  give an overview    of  X2MORF before  describing the integrated system and its implementation in detail.\\n\\nWord Level Processing -\\n\\n\\n\\nX2MORF\\n\\nThe  other extension  concerns      the two-level rules,  which    are supplemented   with a morphological filter   consisting  of a  feature structure. This is  important because  in  morphophonology only some rules are purely phonologically motivated. Others are triggered by a mixture of phonological and morphological facts. Such rules cannot be properly represented in the standard approach.\\n\\nImplementing HPSG in a CLP Framework\\n\\nIn   CLP the notion  of unification  is   replaced by the more general notion of constraint solving. Constraint solvers  may be embedded into a logic programming language either by writing a meta-interpreter or by making   use of  a  system  which   allows  for  the implementation of unification extensions.\\n\\nConstraints  imposed onto  feature structures   by  the principles  of grammar are  stated  in  a  conditional form where  the  antecedent is restricted to  contain only typing requirements. In order  to account for these conditional constraints  we  adopt  a  licensing  view:  Every node  of  a feature structure has to be licensed by all principles of grammar.\\n\\nEmbedding  X2MORF into the Feature System\\n\\nOriginally  X2MORF  was   realized  as  a   separate  morphological component interfaced  to  the  sentence analyzer/generator  only   via sequential   data transfer. In  the case  of   analysis, the feature structure representing  the word form was   transmitted to the parser. For generation,  X2MORF  expected  a  feature structure as    input reproducing   one or  more   word    forms. This  purely   sequential architecture was not satisfactory because of the problems mentioned in the introduction.\\n\\nTo obtain a correct  relationship  between surface and  lexical string every  transition  has  to  be  licensed   by a   morphological  rule. Transitions  not mentioned  by  rules are  handled  by a default rule. Instantiation of contexts may not   be done by the rules   themselves, since  this   would make it  impossible   to obtain negation   via the cut-operator. Instead,   it is handled  separately in  a backtrackable fashion.\\n\\nThe  integration of the  two-level relation into the general framework of  the  feature based sentence-level  and  word-level grammars is now performed by  adding this relation  as a principled constraint  at the appropriate level.\\n\\nIn a definite   clause style AVM  notation this  could  be  written as follows (given that morphology/3 is a wrapper  around the morphology relation given above,  starting with empty left context and hiding the nullified surface stream): Concatenation is delayed until the argument's  MSTRING is instantiated. Thus, infinite loops when concatenating are avoided.\\n\\nInteraction between  syntactic and morphological processes takes place at the  word level. The  application of  the two-level rules relating the surface string (i.e the  PHON-value of  the word) and the lexical-string (i.e. MORPHMSTRING) is also triggered here. This  interaction is  completely   neutral with  respect to the direction of  processing  due to   its relational  nature. Parsing  is performed by simply instantiating the  PHON value. Generation can be achieved when  MORPHMSTRING is present, which in turn is obtained by concatenating the   lexical strings of the msigns instantiated by the morph grammar.\\n\\nConclusion\\n\\nWe have presented a framework for  the tight integration of word level and sentence  level processing in a   unification-based paradigm. The system is   built upon a   unification engine   implemented  in a  CLP language supporting types and definite   relations as part of  feature descriptions. Using   this  extended   feature  formalism, which    is independently motivated   by requirements    of  standard HPSG,     a reimplementation of  X2MORF was  integrated  into the grammar as  a specialized relation.\\n\\nThis architecture has computational as  well as linguistic advantages. Integrating morphology and  morphophonology directly into  the grammar is in the  spirit of HPSG, which  views grammar as a relation between the phonological    (or  graphemic) form  of   an  utterance   and its syntactic/semantic   representation. This  way  the   treatment  of phenomena transcending  the boundary between  morphology and syntax is also made possible.\\n\\nOn the implementation side, the practical problems of interfacing two inherently different modules are eliminated. For applications this means that using a morphological component is made easy. Nevertheless, this tight integration still leaves morphology and syntax/semantics as autonomous components, enabling direct use of existing data sets describing morphophonology in terms of the two-level paradigm.\\n\\nBibliography\\n\\nAbramson  H.:  A Logic  Programming View  of Relational Morphology, in Proceedings of the 15th COLING, August 23-28, 1992, Vol.III, pp.850-854, 1992.\\n\\nBird S., Klein E.:  Enriching HPSG Phonology, University of Edinburgh, UK, Research Paper EUCCS/RP-56, 1993.\\n\\nCarpenter B., Pollard C., Franz A.: The Specification and Implementation of Constraint-Based Unification Grammars, Proceedings of 2[nd] IWPT, Cancun, Mexico, 143-153, 1991.\\n\\nCarpenter B.: The Logic of Typed Feature Structures, Cambridge University Press, Cambridge Tracts in Theoretical Computer Science 32, 1992.\\n\\nGoldsmith J.A. : Autosegmental and Metrical Phonology, Basil Blackwell, Oxford, 1990.\\n\\nHolzbaur C.: Metastructures vs. Attributed Variables in the Context of Extensible Unification, in Bruynooghe M. and Wirsing M.(eds. ), Programming Language Implementation and Logic Programming, Springer, LNCS 631, pp.260-268, 1992.\\n\\nJaffar J., Lassez J.L. : Constraint Logic Programming, in Proceedings 14th ACM POPL Conf., Munich, 1987.\\n\\nKrieger H.-U., Pirker H., Nerbonne J.: Feature-based Allomorphy, Proceedings of the 31st Annual Meeting of the ACL,  Columbus, Ohio, pp.140-147, 1993.\\n\\nMatiasek J., Heinz W.: A CLP Based Approach to HPSG, sterreichisches Forschungsinstitut fr Artificial Intelligence, Wien, TR-93-26, 1993.\\n\\nPollard C.J., Sag I.A. : Information-Based Syntax and Semantics, University of Chicago Press, Chicago, 1987.\\n\\nPollard,  C.J,  Sag I.A. : Head-Driven Phrase Structure Grammar, University of Chicago Press and CSLI Publications, in press.\\n\\ndi Sciullo A.-M., Williams E.: On the Definition of Word, MIT Press, Cambridge, MA, 1987.\\n\\nSproat R.: Morphology and Computation, MIT Press, Cambridge, MA, ACL-MIT Series in NLP, 1992.\\n\\nTrost H.: The Application of Two-Level Morphology to Non-Concatenative German Morphology, in Karlgren H.(ed. ), Proceedings of the 13th COLING, Helsinki, Finland, pp.371-376, 1990.\\n\\nTrost H.: X2MORF: A Morphological Component Based on Augmented Two-Level Morphology, in Proceedings of the 12th IJCAI, Morgan Kaufmann, San Mateo, CA, pp.1024-1030, 1991.\\n\\nTrost H.: Coping with Derivation in a Morphological Component, in 6th Conference of the European Chapter of the ACL, Utrecht, pp.368-376, 1993.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9407001.xml'}),\n",
       " Document(page_content=\"Specifying Intonation from Context for Speech Synthesis\\n\\nThis paper presents a theory and a computational implementation for generating prosodically appropriate synthetic speech in response to database queries. Proper distinctions of contrast and emphasis are expressed in an intonation contour that is synthesized by rule under the control of a grammar, a discourse model, and a knowledge base. The theory is based on Combinatory Categorial Grammar, a formalism which easily integrates the notions of syntactic constituency, semantics, prosodic phrasing and information structure. Results from our current implementation demonstrate the system's ability to generate a variety of intonational possibilities for a given sentence depending on the discourse context.\\n\\nIntroduction [Q:] I know that a  LEFT thoracostomy is needed for the  SIMPLE pneumothorax,\\n\\n[A:]\\n\\nCombinatory Prosody\\n\\ninto a range\\n\\nis written\\n\\n, while the corresponding leftward-combining functor is written\\n\\n.\\n\\nand\\n\\nmay themselves be function categories. For example, a transitive verb is a function from (object) NPs into predicates - that is, into functions from (subject) NPs into S, written as follows:\\n\\nWe also need the following two rules of functional application, where X and Y are variables over categories: FUNCTIONAL APPLICATION:\\n\\n. One way of writing such an interpreted category that is particularly convenient for translating into unification-based programming languages like Prolog is the following:\\n\\n) to functions\\n\\nfrom NPs (with interpretation\\n\\n) to Ss (with\\n\\ninterpretation\\n\\n). Constants in interpretations bear primes, variables do not, and there is a convention of left-associativity, so that\\n\\nrecommend' x y is\\n\\nequivalent to\\n\\n(recommend' x) y. CCG extends this strictly context-free categorial base in two respects. First, all arguments, such as NPs, bear only type-raised categories, such as\\n\\n. That is to say that the category of an NP, rather than being that of a simple argument, is that of a function over functions-over-such-arguments, namely verbs and the like. Similarly, all functions into such categories, such as determiners, are functions into the raised categories, such as\\n\\n. For example, subject NPs bear the following category in the full notation: traumaid :=\\n\\n[a.] You propose, and Traumaid recommends, lavage.\\n\\n[b.] The treatment that Traumaid recommends\\n\\n[Q:] I know that the surgeon recommends a left thoracotomy, but what does Traumaid recommend?\\n\\n[A:] ( T RAUMAID recommends) ( LA VAGE.) L+H* \\t\\t LH% \\t\\t  H* LL$\\n\\n[Theme:]\\n\\nS:recommend' z\\n\\n\\n\\ntraumaid'/NP:z\\n\\n[Rheme:]\\n\\nNP:\\n\\n\\n\\nlavage\\n\\nThis rule says that any sequence bearing the null tone can be regarded as an ``unmarked'' intermediate phrase theme.\\n\\nModeling Contrast\\n\\nThe Implementation\\n\\nTheme:\\n\\nResults\\n\\nConclusions\\n\\nThe results show that is possible to generate synthesized spoken responses with contextually appropriate intonational contours in a database query task. Many important problems remain, both because of the limited range of discourse-types and intonational tunes considered here, and because of the extreme oversimplification of the discourse model (particularly with respect to the ontology, or variety of types of discourse entities). Nevertheless, the system presented here has a number of properties that we believe augur well for its extension to richer varieties of discourse, including the types of monologues and commentaries that are more appropriate for the actual TraumAID domain. Foremost among these is the fact that the system and the underlying theory are entirely modular. That is, any of its components can be replaced without affecting any other component because each is entirely independent of the particular grammar defined by the lexicon and the particular knowledge base that the discourse concerns. It is only because CCG allows us to unify the structures implicated in syntax and semantics on the one hand, and intonation and discourse information on the other, that this modular structure can be so simply attained.\\n\\nAcknowledgments\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9407015.xml'}),\n",
       " Document(page_content=\"AN EXPERIMENT ON LEARNING APPROPRIATE SELECTIONAL RESTRICTIONS FROM A PARSED CORPUS1\\n\\nWe present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora. The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items. Some experimental results about the performance of the method are provided.\\n\\nINTRODUCTION\\n\\nThese last years there has been a common agreement in the natural language processing research community on the importance of having an extensive coverage of the surface lexical semantics of the domain to work with, (specially, typical contexts of use). This knowledge may be expressed at different levels of abstraction depending on the phenomena involved: selectional restrictions (SRs), lexical preferences, col-locations, etc. We are specially interested on SRs, which can be expressed as semantic type constraints that a word sense imposes on the words with which it combines in the process of semantic interpretation. SRs must include information on the syntactic position of the words that are being restricted semantically. For instance, one of the senses of the verb drink restricts its subject to be an animal and its object to be a liquid.\\n\\nTHE METHOD OF ACQUISITION\\n\\nSRs have been used to express semantic constraints holding in different syntactic and functional configurations. However, in this paper we focus only in selectional restrictions holding between verbs and their complements. The method can be easily exported to other configurations. We won't distinguish the SRs imposed by verbs on arguments and adjuncts. We believe that few adjuncts are going to provide enough evidence in the corpus for creating SRs. In the following paragraphs we describe the functional specification of the system.\\n\\nTraining set\\n\\nOutput\\n\\nThe result of the learning process is a set of syntactic SRs, (verb, syntactic relationship, semantic class). Semantic classes are represented extensionally as sets of nouns. SRs are only acquired if there are enough cases in the corpus as to gather statistical evidence. As long as distinct uses of the same verb can have different SRs, we permit to extract more than one class for the same syntactic position. Nevertheless, they must be mutually disjoint, i.e. not related by hyperonymy.\\n\\nPrevious knowledge used\\n\\nLearning process\\n\\nExtracting Co\\n\\n\\n\\noccurrence Triples\\n\\nIn any process of learning from examples the accuracy of the training set is the base for the system to make correct predictions. In our case, where the semantic classes are hypothesized not univoquely from the examples, accuracy becomes fundamental.\\n\\nHowever, if the co-occurrences were extracted from a corpus annotated with structural syntactic information (i.e., part of speech and ``skeletal'' trees), the results would have considerably higher degrees of accuracy and representativity. In this way, it would be easy to detect all the relationships between verb and complements, and few non-related co-occurrences would be extracted. The most serious objection to this approach is that the task of producing syntactic analyzed corpora is very expensive. Nevertheless, lately there has been a growing interest to produce skeletally analyzed corpora\\n\\nA parser, with some simple heuristics, would be enough to meet the requirements of representativeness and accuracy introduced above. On the other hand, it could be useful to represent the co-occurrence triples as holding between lemmas, in order to gather as much evidence as possible. A simple morphological analyzer that could get the lemma for a big percentage of the words appearing in the corpus would suffice.\\n\\nSemantic Knowledge Used\\n\\nClass appropriateness: the Association Score\\n\\nWhen trying to choose a measure of the appropriateness of a semantic class, we have to consider the features of the problem: (1) robustness in front of noise, and (2) conservatism in order to be able to generalize only from positive examples, without having the tendency to over-generalize.\\n\\nbe the sets of all verbs, nouns, syntactic positions, and possible noun classes, respectively. Given\\n\\nand\\n\\n, Association Score, Assoc, between v and c in a syntactic position s is defined to be\\n\\nWhere conditional probabilities are estimated by counting the number of observations of the joint event and dividing by the frequency of the given event, e.g.\\n\\nThe two terms of Assoc try to capture different properties of the SR expressed by the candidate class. Mutual information, I(v;c|s), measures the strength of the statistical association between the given verb v and the candidate class c in the given syntactic position s.  If there is a real relationship, then hopefully\\n\\n. On the other hand, the conditional probability, P(c|v,s), favors those classes that have more occurrences of nouns.\\n\\nSelecting the best classes\\n\\nThe existence of noise in the training set introduces classes in the candidate space that can't be considered as expressing SRs. A common technique used for ignoring as far as possible this noise is to consider only those events that have a higher number of occurrences than a certain threshold. However, some erroneous classes may persist because they exceed the threshold. However, if candidate classes were ordered by the significance of their Assoc with the verb, it is likely that less appropriate classes (introduced by noise) would be ranked in the last positions of the candidate list.\\n\\nThe algorithm to learn SRs is based in a search through all the classes with more instances in the training set than the given threshold. In different iterations over these candidate classes, two operations are performed: first, the class, c, having the best Assoc (best class), is extracted for the final result; and second, the remaining candidate classes are filtered from classes being hyper/hyponyms to the best class. This last step is made because the definitive classes must be mutually disjoint. The iterations are repeated until the candidate space has been run out.\\n\\nEXPERIMENTAL RESULTS\\n\\nIn order to experiment the methodology presented, we implemented a system in a Unix machine. The corpus used for extracting co-occurrence triples is a fragment of parsed material from the Penn Treebank Corpus (about 880,000 words and 35,000 sentences), consisting of articles of the Wall Street Journal, that has been tagged and parsed. We used Wordnet as the verb and noun lexicons for the lemmatizer, and also as the semantic taxonomy for clustering nouns in semantic classes. In this section we evaluate the performance of the methodology implemented: (1) looking at the performance of the techniques used for extracting triples, (2) considering the coverage of the WordNet taxonomy regarding the noun senses appearing in Treebank, and (3) analyzing the performance of the learning process.\\n\\nThe total number of co-occurrence triples extracted amounts to 190,766. Many of these triples (68,800, 36.1%) were discarded before the lemmatizing process because the surface NP head wasn't a noun. The remaining 121,966 triples were processed through the lemmatizer. 113,583 (93.1%) could be correctly mapped into their corresponding lemma form.\\n\\nIn addition, we analyzed manually the results obtained for a subset of the extracted triples, looking at the sentences in the corpus where they occurred. The subset contains 2,658 examples of four average common verbs in the Treebank: rise, report, seek and present (from now on, the testing sample). On the one hand, 235 (8.8%) of these triples were considered to be extracted erroneously because of the parser, and 51 (1.9%) because of the lemmatizer. Summarizing, 2,372 (89.2%) of the triples in the testing set were considered to be correctly extracted and lemmatized.\\n\\nWhen analyzing the coverage of WordNet taxonomy we considered two different ratios. On the one hand, how many of the noun occurrences have one or more senses included in the taxonomy: 113,583 of the 117,215 extracted triples (96.9%). On the other hand, how many of the noun occurrences in the testing sample have the correct sense introduced in the taxonomy: 2,165 of the 2,372 well-extracted triples (92.3%). These figures give a positive evaluation of the coverage of WordNet.\\n\\nIn order to evaluate the performance of the learning process we inspected manually the SRs acquired on the testing-sample, assessing if they corresponded to the actual SRs imposed. A first way of evaluation is by means of measuring precision and recall ratios in the testing sample. In our case, we define precision as the proportion of triples appearing in syntactic positions with acquired SRs, which effectively fulfill one of those SRs. Precision amounts to 79.2%. The remaining 20.8% triples didn't belong to any of the classes induced for their syntactic positions. Some of them because they didn't have the correct sense included in the WordNet taxonomy, and others because the correct class had not been induced because there wasn't enough evidence. On the other hand, we define recall as the proportion of triples which fulfill one of the SRs acquired for their corresponding syntactic positions. Recall amounts to 75.7%.\\n\\nOk The acquired SR is correct according to the noun senses contained in the corpus.\\n\\nAbs The best level for stating the SR is not the one induced, but a lower one. It happens because erroneous senses, metonymies, ..., accumulate evidence for the higher class.\\n\\nAbs Some of the SRs could be best gathered in a unique class. We didn't find any such case.\\n\\nSenses The class has cropped up because it accumulates enough evidence, provided by erroneous senses.\\n\\nNoise The class accumulates enough evidence provided by erroneously extracted triples.\\n\\nb. Although many of the classes acquired result from the accumulation of incorrect senses (73.3%), it seems that their size tends to be smaller than classes in other categories, as they only contain a 51.4% of the senses .\\n\\nd. The over-generalization seems to be produced because of little difference in the nouns included in the rival classes. Nevertheless this situation is rare.\\n\\ne. The impact of noise provided by  erroneous extraction of co-occurrence triples, in the acquisition of wrong semantic classes, seems to be very moderate.\\n\\nf. Since different verb senses occur in the corpus, the SRs acquired appear mixed.\\n\\nFURTHER WORK\\n\\nAlthough performance of the technique presented is pretty good, some of the detected problems could possibly be solved. Specifically, there are various ways to explore in order to reduce the problems stated in points b and c above:\\n\\n1. To measure the Assoc by means of Mutual Information between the pair v-s and c. In this way, the syntactic position also would provide information (statistical evidence) for measuring the most appropriate classes.\\n\\n3. To estimate the probabilities of classes, not directly from the frequencies of their noun members, but correcting this evidence by the number of senses of those nouns, e.g\\n\\nIn this way, the estimated function would be a probability distribution, and more interesting, nouns would provide evidence on the occurrence of their hyperonyms, inversely proportional to their degree of ambiguity.\\n\\n4. To collect a bigger number of examples for each verbal complement, projecting the complements in the internal arguments, using diathesis sub-categorization rules. Hopefully, Assoc would have a better performance if it was estimated on a bigger population. On the other hand, in this way it would be possible to detect the SRs holding on internal arguments.\\n\\nIn order to solve point d above, we have foreseen two possibilities:\\n\\n1. To take into consideration the statistical significance of the alternatives involved, before doing a generalization step, climbing upwards,\\n\\n2. To use the PPs that in the corpus are attached to other complements and not to the main verb as a source of ``implicit negative examples'', in such a way that they would constrain the over-generalization.\\n\\nBibliography\\n\\nR. Basili, M.T. Pazienza, and P. Velardi. Computational lexicons: the neat examples and the odd exemplars. In Proc. of the 3rd ANLP, 1992.\\n\\nK.W. Church, W. Gale, P. Hanks, and D. Hindle. Using statistics in lexical analysis. In U. Zernik, editor, Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon. Lawrence Erlbaum, 1991.\\n\\nK.W. Church and P. Hanks. Word association norms, mutual information and lexicography. Computational Linguistics, 16(1), 1990.\\n\\nT. Dunning. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1), 1993.\\n\\nG. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. Five papers on wordnet. Technical report, CSL, Princeton University, 1990.\\n\\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of english: the Penn Treebank. Computational Linguistics, 19(2), 1993.\\n\\nP. Resnik. Wordnet and distributional analysis: A class-based approach to lexical discovery. In Proc. of AAAI Workshop on Statistical Methods in NLP, 1992.\\n\\nF. Ribas. Learning more appropriate selectional restrictions. Technical report, ESPRIT BRA-7315 ACQUILEX-II Working Paper, August 1994.\\n\\nG. Whittemore, K. Ferrara, and H. Brunner. Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases. In Proc. of the 28th ACL, 1990.\\n\\nDavid Yarowsky. Word-sense disambiguation using statistical models of Roget's categories trained on large corpora. In Proceedings of COLING-92, Nantes, France, 1992.\\n\\nFootnotes\\n\\nthis total doesn't equal the number of triples in the testing sample because the same noun may belong to more than one class in the final SRs\", metadata={'source': '../data/raw/cmplg-xml/9409004.xml'}),\n",
       " Document(page_content='Tagset Reduction Without Information Loss\\n\\nA technique for reducing a tagset used for n-gram part-of-speech disambiguation is introduced and evaluated in an experiment. The technique ensures that all information that is provided by the original tagset can be restored from the reduced one. This is crucial, since we are interested in the linguistically motivated tags for part-of-speech disambiguation. The reduced tagset needs fewer parameters for its statistical model and allows more accurate parameter estimation. Additionally, there is a slight but not significant improvement of tagging accuracy.\\n\\nMotivation\\n\\nGenerally, the categories for part-of-speech tagging are linguistically motivated and do not reflect the probability distributions or co-occurrence probabilities of words belonging to that category. It is an implicit assumption for statistical part-of-speech tagging that words belonging to the same category have similar probability distributions. But this assumption does not hold in many of the cases.\\n\\nThere are two contradicting requirements. On the one hand, more tags mean that there is more information about a word at hand, on the other hand, the more tags, the severer the sparse-data problem is and the larger the corpora that are needed for training.\\n\\nThis paper presents a way to modify a given tagset, such that categories with similar distributions in a corpus are combined without losing information provided by the original tagset and without losing accuracy.\\n\\nClustering of Tags\\n\\nThe aim of the presented method is to reduce a tagset as much as possible by combining (clustering) two or more tags without losing information and without losing accuracy. The fewer tags we have, the less parameters have to be estimated and stored, and the less severe is the sparse data problem. Incoming text will be disambiguated with the new reduced tagset, but we ensure that the original tag is still uniquely identified by the new tag.\\n\\nThe basic idea is to exploit the fact that some of the categories have a very similar frequency distribution in a corpus. If we combine categories with similar distribution characteristics, there should be only a small change in the tagging result. The main change is that single tags are replaced by a cluster of tags, from which the original has to be identified. First experiments with tag clustering showed that, even for fully automatic identification of the original tag, tagging accuracy slightly increased when the reduced tagset was used. This might be a result of having more occurrences per tag for a smaller tagset, and probability estimates are preciser.\\n\\nUnique Identification of Original Tags\\n\\nA crucial property of the reduced tagset is that the original tag information can be restored from the new tag, since this is the information we are interested in. The property can be ensured if we place a constraint on the clustering of tags.\\n\\nTo ensure that there is such a unique function, we prohibit some of the possible combinations. A cluster is allowed if and only if there is no word in the lexicon which can have two or more of the original tags combined in one cluster. Formally, seeing tags as sets of words and clusters as sets of tags:\\n\\nIf this condition holds, then for all words w tagged with a cluster c, exactly one tag twc fulfills\\n\\nyielding\\n\\nforig(w, c) = twc.\\n\\nSo, the original tag can be restored any time and no information from the original tagset is lost.\\n\\nExample: Assume that no word in the lexicon can be both comparative (JJR) and superlative adjective (JJT). The categories are combined to {JJR,JJT}. When processing a text, the word easier is tagged as {JJR,JJT}. Since the lexicon states that easier can be of category JJR but not of category JJT, the original tag must be JJR.\\n\\nCriteria For Combining Tags\\n\\nThe are several criteria that can determine the quality of a particular clustering.\\n\\n1.\\n\\nCompare the trigram probabilities\\n\\np(B|Xi, A),\\n\\np(B|A, Xi), and\\n\\np(Xi|A,B), i = 1,2. Combine two tags X1 and X2, if these probabilities coincide to a certain extent. 2. Maximize the probability that the training corpus is generated by the HMM which is described by the trigram probabilities. 3. Maximize the tagging accuracy for a training corpus.\\n\\nCriterion (1) establishes the theoretical basis, while criteria (2) and (3) immediately show the benefit of a particular combination. A measure of similarity for (1) is currently under investigation. We chose (3) for our first experiments, since it was the easiest one to implement. The only additional effort is a separate, previously unused part of the training corpus for this purpose, the clustering part. We combine those tags into clusters which give the best results for tagging of the clustering part.\\n\\nThe Algorithm\\n\\nThe total number of potential clusterings grows exponential with the size of the tagset. Since we are interested in the reduction of large tagsets, a full search regarding all potential clusterings is not feasible. We compute the local maximum which can be found in polynomial time with a best-first search.\\n\\nApplication of Tag Clustering\\n\\nTwo standard trigram tagging procedures were performed as the baseline. Then clustering was performed on the same data and tagging was done with the reduced tagset. The reduced tagset was only internally used, the output of the tagger consisted of the original tagset for all experiments.\\n\\nThree parts are taken from the corpus. Part A consists of about 127,000 words, part B of about 10,000 words, and part C of about 10,000 words. The rest of the corpus, about 10,000 words, is not used for this experiment. All parts are mutually disjunct.\\n\\nClustering was applied in the next steps. In the third experiment, part A was used for trigram training, part B for clustering and part C for testing. In the fourth experiment, part A was used for trigram training, part C for clustering and part B for testing.\\n\\nThe baseline experiments used the clustering part for the normal training procedure to ensure that better performance in the clustering experiments is not due to information provided by the additional part.\\n\\nThe improvement in the tagging result is too small to be significant. However, the tagset is reduced, thus also reducing the number of parameters without losing accuracy. Experiments with larger texts and more permutations will be performed to get precise results for the improvement.\\n\\nConclusions\\n\\nWe have shown a method for reducing a tagset used for part-of-speech tagging without losing information given by the original tagset. In a first experiment, we were able to reduce a large tagset and needed fewer parameters for the n-gram model. Additionally, tagging accuracy slightly increased, but the improvement was not significant. Further investigation will focus on criteria for cluster selection. Can we use a similarity measure of probability distributions to identify optimal clusters? How far can we reduce the tagset without losing accuracy?\\n\\nBibliography\\n\\nKenneth Ward Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proc. Second Conference on Applied Natural Language Processing, pages 136-143, Austin, Texas, USA.\\n\\nDoug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the 3rd Conference on Applied Natural Language Processing (ACL), pages 133-140.\\n\\nR. G. Garside, G. N. Leech, and G. R. Sampson (eds.). 1987. The Computational Analysis of English. Longman.\\n\\nL. R. Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. In Proceedings of the IEEE, volume 77(2), pages 257-285.\\n\\nGeoffrey Sampson.\\n\\n1995.\\n\\nEnglish for the Computer.\\n\\nOxford University Press, Oxford.\\n\\nAndreas Stolcke and Stephen M. Omohundro. 1994. Best-first model merging for hidden markov model induction. Technical Report TR-94-003, International Computer Science Institute, Berkeley, California, USA.\\n\\nFootnotes', metadata={'source': '../data/raw/cmplg-xml/9505010.xml'}),\n",
       " Document(page_content=\"COMMON TOPICS AND COHERENT SITUATIONS: INTERPRETING ELLIPSIS IN THE CONTEXT OF DISCOURSE INFERENCE\\n\\nIt is claimed that a variety of facts concerning ellipsis, event reference, and interclausal coherence can be explained by two features of the linguistic form in question: (1) whether the form  leaves behind an empty constituent in the syntax, and (2) whether the form is anaphoric in the semantics. It is proposed that these features interact with one of two types of discourse inference, namely Common Topic inference and Coherent Situation inference. The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account.\\n\\nIntroduction\\n\\nEllipsis is pervasive in natural language, and hence has received much attention within both computational and theoretical linguistics. However, the conditions under which a representation of an utterance may serve as a suitable basis for interpreting subsequent elliptical forms remain poorly understood; specifically, past attempts to characterize these processes within a single traditional module of language processing (e.g., considering either syntax, semantics, or discourse in isolation) have failed to account for all of the data. In this paper, we claim that a variety of facts concerning ellipsis resolution, event reference, and interclausal coherence can be explained by the interaction between the syntactic and semantic properties of the form in question and the type of discourse inference operative in establishing the coherence of the antecedent and elided clauses.\\n\\nIn the next section, we introduce the facts concerning gapping, VP-ellipsis, and non-elliptical event reference that we seek to explain. In Section 3, we categorize elliptical and event referential forms according to two features: (1) whether the expression leaves behind an empty constituent in the syntax, and (2) whether the expression is anaphoric in the semantics. In Section 4 we describe two types of discourse inference, namely Common Topic inference and Coherent Situation inference, and make a specific proposal concerning the interface between these and the syntactic and semantic representations they utilize. In Section 5, we show how this proposal accounts for the data presented in Section 2. We contrast the account with relevant past work in Section 6, and conclude in Section 7.\\n\\nEllipsis and Interclausal Coherence\\n\\nVP-ellipsis is characterized by an initial source sentence, and a subsequent target sentence with a bare auxiliary indicating the elision of a verb phrase:\\n\\nBill became upset, and Hillary did too.\\n\\n# The decision was reversed by the FBI, and the ICC did too. [ reverse the decision ]\\n\\nIn March, four fireworks manufacturers asked that the decision be reversed, and on Monday the ICC did. [ reverse the decision ]\\n\\n# This letter provoked a response from Bush, and Clinton did too. [ respond ]\\n\\nThe decision was reversed by the FBI, and the ICC did it too. [ reverse the decision ] An adequate theory of ellipsis and event reference must account for this distinction.\\n\\nIn sum, the felicity of both gapping and VP-ellipsis appears to be dependent on the type of coherence relation extant between the source and target clauses. Pronominal event reference, on the other hand, appears not to display this dependence. We seek to account for these facts in the sections that follow.\\n\\nSyntax and Semantics of Ellipsis and Event Reference\\n\\nIn this section we characterize the forms being addressed in terms of two features: (1) whether the form leaves behind an empty constituent in the syntax, and (2) whether the form is anaphoric in the semantics. In subsequent sections, we show how the distinct mechanisms for recovering these types of missing information interact with two types of discourse inference to predict the phenomena noted in the previous section.\\n\\nDiscourse Inference\\n\\nTo be coherent, utterances within a discourse segment require more than is embodied in their individual syntactic and semantic representations alone; additional inter-utterance constraints must be met. Here we describe two types of inference used to enforce the constraints that are imposed by coherence relations. In each case, arguments to coherence relations take the form of semantic representations retrieved by way of their corresponding node(s) in the syntax; the operations performed on these representations are dictated by the nature of the constraints imposed. The two types of inference are distinguished by the level in the syntax from which these arguments are retrieved.\\n\\nCommon Topic Inference\\n\\nCoherent Situation Inference\\n\\nBill is a politician, and therefore he's dishonest. (Result) Bill is dishonest because he's a politician. (Explanation) Bill is a politician, but he's honest. (Violated Expectation) Bill is honest, even though he's a politician. (Denial of Preventer) Beyond what is asserted by the two clauses individually, understanding each of these sentences requires the presupposition that being a politician implies being dishonest. Inferring this is only reliant on the sentential-level semantics for the clauses as a whole; there are no p, ai, or bi to be independently identified. The same is true for what Hume called Contiguity relations (perhaps including Hobbs' Occasion and Figure-ground relations); for the purpose of this paper we will consider these as weaker cases of Cause or Effect.\\n\\nTo reiterate the crucial observation, Common Topic inference utilizes the syntactic structure in identifying the semantics for the sub-sentential constituents to serve as arguments to the coherence constraints. In contrast, Coherent Situation inference utilizes only the sentential-level semantic forms as is required for abducing a coherent situation. The question then arises as to what happens when constituents in the syntax for an utterance are empty. Given that the discourse inference mechanisms retrieve semantic forms through nodes in the syntax, this syntax will have to be recovered when a node being accessed is missing. Therefore, we posit that missing constituents are recovered as a by-product of Common Topic inference, to allow the parallel properties and entities serving as arguments to the coherence relation to be accessed from within the reconstructed structure. On the other hand, such copying is not triggered in Coherent Situation inference, since the arguments are retrieved only from the top-level sentence node, which is always present. In the next section, we show how this difference accounts for the data given in Section 2.\\n\\nApplying the Analysis\\n\\nIn previous sections, we have classified several elliptical and event referential forms as to whether they leave behind an empty constituent in the syntax and whether they are anaphoric  in the semantics. Empty constituents in the syntax are not in themselves referential, but are recovered during Common Topic inference. Anaphoric expressions in the semantics are independently referential and are resolved through purely semantic means regardless of the type of discourse inference. In this section we show how the phenomena presented in Section 2 follow from these properties.\\n\\nLocal Ellipsis\\n\\nBill became upset, and Hillary angry. This fact is predicted by our account in the following way. In the case of Common Topic constructions, the missing sentence in the target will be copied from the source, the sentential semantics may be derived, and the arguments to the coherence relations can be identified and reasoning carried out, predicting felicity. In the case of Coherent Situation relations, no such recovery of the syntax takes place. Since a gapped clause in and of itself has no sentence-level semantics, the gapping fails to be felicitous in these cases.\\n\\nThe stripping construction is similar to gapping except that there is only one bare constituent in the target (also generally receiving contrastive accent); unlike VP-ellipsis there is no stranded auxiliary. We therefore might predict that stripping is also acceptable in Common Topic constructions but not in Coherent Situation constructions, which appears to be the case:\\n\\nIn summary, gapping and related constructions are infelicitous in those cases where Coherent Situation inference is employed, as there is no mechanism for recovering the sentential semantics of the elided clause.\\n\\nVP\\n\\n\\n\\nEllipsis\\n\\n# The decision was reversed by the FBI, and the ICC did too. [ reverse the decision ]\\n\\nIn March, four fireworks manufacturers asked that the decision be reversed, and on Monday the ICC did. [ reverse the decision ] These facts are also predicted by our account. In the case of Common Topic constructions, a suitable syntactic antecedent must be reconstructed at the site of the empty VP node, with the result that the anaphoric expression takes on its accompanying semantics. Therefore, VP-ellipsis is predicted to require a suitable syntactic antecedent in these scenarios. In Coherent Situation constructions, the empty VP node is not reconstructed. In these cases the anaphoric expression is resolved on purely semantic grounds; therefore VP-ellipsis is only constrained to having a suitable semantic antecedent.\\n\\n? ? Clinton was introduced by John, but Mary didn't. [ introduce Clinton ]\\n\\n? ? This letter provoked a response from Bush, but Clinton didn't. [ respond ]  Clinton was to have been introduced by someone, but obviously nobody did. [ introduce Clinton ]\\n\\nThis letter deserves a response, but before you do, ... [ respond ]    To summarize thus far, the data presented in the earlier account as well as examples that conflict with that analysis are all predicted by the account given here.\\n\\nEvent Reference\\n\\n# The decision was reversed by the FBI, and the ICC did too. [ reverse the decision ] The decision was reversed by the FBI, and the ICC did it too. [ reverse the decision ] As stated earlier, forms such as do it are anaphoric, but leave no empty constituents in the syntax. Therefore, it follows under the present account that such reference is successful without regard to the type of discourse inference employed.\\n\\nRelationship to Past Work\\n\\nClinton was introduced by John because Mary had refused to, and Gore was too. [ introduced by John because Mary had refused to ] # Clinton was introduced by John because Mary had refused to, and Fred did too. [ introduced Clinton because Mary had refused to ] The current approach accounts for these cases.\\n\\nConclusion\\n\\nAcknowledgments\\n\\nThis work was supported in part by National Science Foundation Grant IRI-9009018, National Science Foundation Grant IRI-9350192, and a grant from the Xerox Corporation. I would like to thank Stuart Shieber, Barbara Grosz, Fernando Pereira, Mary Dalrymple, Candy Sidner, Gregory Ward, Arild Hestvik, Shalom Lappin, Christine Nakatani, Stanley Chen, Karen Lochbaum, and two anonymous reviewers for valuable discussions and comments on earlier drafts.\\n\\nBibliography\\n\\nNicholas Asher. 1993. Reference to Abstract Objects in Discourse. SLAP 50, Dordrecht, Kluwer.\\n\\nMary Dalrymple, Stuart M. Shieber, and Fernando Pereira. 1991. Ellipsis and higher-order unification. Linguistics and Philosophy, 14:399-452.\\n\\nM.A.K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman's, London. English Language Series, Title No. 9.\\n\\nJerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt, and Paul Martin. 1993. Interpretation as abduction. Artificial Intelligence, 63:69-142.\\n\\nJerry Hobbs.\\n\\n1990.\\n\\nLiterature and Cognition.\\n\\nCSLI Lecture Notes 21.\\n\\nDavid Hume. 1748. An Inquiry Concerning Human Understanding. The Liberal Arts Press, New York, 1955 edition.\\n\\nAndrew Kehler. 1993a. A discourse copying algorithm for ellipsis and anaphora resolution. In Proceedings of the Sixth Conference of the European Chapter of the Association for Computational Linguistics (EACL-93), pages 203-212, Utrecht, the Netherlands, April.\\n\\nAndrew Kehler. 1993b. The effect of establishing coherence in ellipsis and anaphora resolution. In Proceedings of the 31st Conference of the Association for Computational Linguistics (ACL-93), pages 62-69, Columbus, Ohio, June.\\n\\nAndrew Kehler. 1994. A discourse processing account of gapping and causal implicature. Manuscript presented at the Annual Meeting of the Linguistic Society of America, January.\\n\\nNancy Levin and Ellen Prince. 1982. Gapping and causal implicature. Presented at the Annual Meeting of the Linguistic Society of America.\\n\\nFernando Pereira.\\n\\n1990.\\n\\nCategorial semantics and scoping.\\n\\nComputational Linguistics, 16(1):1\\n\\n\\n\\n10.\\n\\nEllen Prince. 1986. On the syntactic marking of presupposed open propositions. In Papers from the Parasession on pragmatics and grammatical theory at the 22nd regional meeting of the Chicago Linguistics society, pages 208-222, Chicago, IL.\\n\\nHub Prst. 1992. On Discourse Structuring, VP Anaphora, and Gapping. Ph.D. thesis, University of Amsterdam.\\n\\nIvan Sag and Jorge Hankamer. 1984. Toward a theory of anaphoric processing. Linguistics and Philosophy, 7:325-345.\\n\\nIvan Sag.\\n\\n1976.\\n\\nDeletion and Logical Form.\\n\\nPh.D. thesis, MIT.\\n\\nRemko Scha and Livia Polanyi. 1988. An augmented context free grammar for discourse. In Proceedings of the International Conference on Computational Linguistics (COLING-88), pages 573-577, Budapest, August.\\n\\nMark Steedman.\\n\\n1990.\\n\\nGapping as constituent coordination.\\n\\nLinguistics and Philosophy, 13(2):207\\n\\n\\n\\n263.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9405010.xml'}),\n",
       " Document(page_content=\"DATR Theories and DATR Models Introduction\\n\\nDATR Theories\\n\\nValue descriptors are either atoms or inheritance descriptors, where an inheritance descriptor is further distinguished as either local (unquoted) or global (quoted). There is just one kind of local descriptor (node/path), but three kinds of global descriptor (node/path, path and node) .\\n\\nThere is a pragmatic distinction between definitional and extensional sentences akin to that drawn between the language used to define a database and that used to query it. DATR interpreters conventionally treat all extensional sentences as `goal' statements, and evaluate them as soon as they are encountered. Thus, it is not possible, in practice, to combine definitional and extensional sentences within a theory. Functionality for DATR theories, as defined above, is really a syntactic notion. However, it approximates a deeper, semantic requirement that the nodes should correspond to (partial) functions from paths to values.\\n\\nAn Overview of DATR\\n\\nIn each of the above cases, the theory provides an explicit statement about the value associated with the indicated path at the given node. As a result the default mechanism is effectively over-ridden.\\n\\nDATR Models\\n\\nDATR theories can be viewed semantically as collections of definitions of partial functions (`nodes' in DATR parlance) that map paths onto values. A model of a DATR theory is then an assignment of functions to node symbols that is consistent with the definitions of those nodes within the theory. This picture of DATR as a formalism for defining partial functions is complicated by two features of the language however. First, the meaning of a given node depends, in general, on the global context of interpretation, so that nodes do not correspond directly to mappings from paths to values, but rather to functions from contexts to such mappings. Second, it is necessary to provide an account of DATR's default mechanism. It will be convenient to present our account of the semantics of DATR in two stages.\\n\\nDATR Interpretations\\n\\nThat is, an interpretation is a model of a DATR theory just in case (for each global context) the function it associates with each node respects the definition of that node within the theory.\\n\\nImplicit Information and Default Models\\n\\nOn the other hand, there is no guarantee that a given model will also respect the following containment:\\n\\nConclusions\\n\\nThe work described in this paper fulfils one of the objectives of the DATR programme: to provide the language with an explicit, declarative semantics. We have presented a formal model of DATR as a language for defining partial functions and this model has been contrasted with an informal view of DATR as a language for representing inheritance hierarchies. The approach provides a transparent treatment of DATR's notion of (local and global) context and accounts for DATR's default mechanism by regarding value descriptors (semantically) as families of values indexed by paths.\\n\\nAcknowledgements\\n\\nThe author would like to thank Roger Evans, Gerald Gazdar, Bill Rounds and David Weir for helpful discussions on the work described in this paper.\\n\\nBibliography\\n\\nFrancois Andry, Norman Fraser, Scott McGlashan, Simon Thornton, and Nick Youd. 1992. Making DATR work for speech: lexicon compilation in SUNDIAL. Computational Linguistics, 18(3):245-267.\\n\\nGosse Bouma.\\n\\n1992.\\n\\nFeature structures and nonmonotonicity.\\n\\nComputational Linguistics, 18(2):183\\n\\n\\n\\n203.\\n\\nLynne Cahill and Roger Evans. 1990. An application of DATR: the TIC lexicon. In Proceedings of the 9th European Conference on Artificial Intelligence, pages 120-125.\\n\\nLynne Cahill. 1993. Morphonology in the lexicon. In Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics, pages 87-96.\\n\\nLynne Cahill. 1994. An inheritance-based lexicon for message understanding systems. In Proceedings of the 4th ACL Conference on Applied Natural Language Processing, pages 211-212.\\n\\nBob Carpenter. 1993. Skeptical and credulous default unification with applications to templates and inheritance. In Ted Briscoe, Valeria de Paiva, and Ann Copestake, editors,   Inheritance, Defaults and the Lexicon, pages 13-37. Cambridge University Press, Cambridge.\\n\\nGreville Corbett and Norman Fraser. 1993. Network morphology: a DATR account of Russian nominal inflection. Journal of Linguistics, 29:113-142.\\n\\nRoger Evans and Gerald Gazdar. 1989a. Inference in DATR. In Proceedings of the 4th Conference of the European Chapter of the Association for Computational Linguistics, pages 66-71.\\n\\nRoger Evans and Gerald Gazdar. 1989b. The semantics of DATR. In Proceedings of AISB-89, pages 79-87.\\n\\nRoger Evans, Gerald Gazdar, and David Weir. 1994. Using default inheritance to describe LTAG. In 3e Colloque International sur les Grammaires d'Arbres Adjoints (TAG+3), pages 79-87.\\n\\nRoger Evans, Gerald Gazdar, and David Weir. 1995. Encoding lexicalized tree adjoining grammars with a nonmonotonic inheritance hierarchy. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.\\n\\nGerald Gazdar. 1992. Paradigm function morphology in DATR. In Lynne Cahill and Richard Coates, editors, Sussex Papers in General and Computational Linguistics, number CSRP 239 in Cognitive Science Research Papers, pages 45-53. University of Sussex, Brighton.\\n\\nDafydd Gibbon and Doris Bleiching. 1991. An ILEX model for German compound stress in DATR. In Proceedings of the FORWISS-ASL Workshop on Prosody in Man-Machine Communication.\\n\\nJames Kilbury. 1992. Pardigm-based derivational morphology. In Guenther Goerz, editor, Proceedings of KONVENS 92, pages 159-168. Springer, Berlin.\\n\\nAdam Kilgariff. 1993. Inheriting verb alternations. In Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics, pages 213-221.\\n\\nHagen Langer. 1994. Reverse queries in DATR. In Proceedings of the 15th International Conference on Computational Linguistics, volume II, pages 1089-1095, Kyoto.\\n\\nGraham Russell, Afzal Ballim, John Carroll, and Susan Warwick-Armstrong. 1992. A practical approach to multiple default inheritance for unification-based lexicons. Computational Linguistics, 18(2):311-337.\\n\\nMark Young and Bill Rounds. 1993. A logical semantics for nonmonotonic sorts. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 209-215.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9505004.xml'}),\n",
       " Document(page_content='Cooperative Error Handling and Shallow Processing\\n\\nThis paper is concerned with the detection and correction of sub-sentential English text errors. Previous spelling programs, unless restricted to a very small set of words, have operated as post-processors. And to date, grammar checkers and other programs which deal with ill-formed input usually step directly from spelling considerations to a full-scale parse, assuming a complete sentence. Work described below is aimed at evaluating the effectiveness of shallow (sub-sentential) processing and the feasibility of cooperative error checking, through building and testing appropriately an error-processing system. A system under construction is outlined which incorporates morphological checks (using new two-level error rules) over a directed letter graph, tag positional trigrams and partial parsing. Intended testing is discussed.\\n\\nIntroduction\\n\\nUnless a keyboard user is particularly proficient, a frustrating amount of time is usually spent backtracking to pick up mis-typed or otherwise mistaken input. Work described in this paper started from an idea of an error processor that would sit on top of an editor, detecting/correcting errors just after entry, while the user continued with further text, relieved from tedious backtracking. Hence `co-operative\\' error processing. But if a program is to catch such errors very soon after they are entered, it will have to operate with less than the complete sentence.\\n\\nWork underway focuses on shallow processing: how far error detection and correction can proceed when the system purview is set to a stretch of text which does not admit complete sentential analysis. To date, grammar checkers and other programs which deal with illformed input usually step directly from spelling considerations to a full-scale sentence parse. However treating the sentence as a basic unit loses meaning when the `sentence\\' is incomplete or illformed. Shallow processing is also interesting because it should be cheaper and faster than a complete analysis of the whole sentence.\\n\\nTo investigate issues involved in shallow processing and cooperative error handling, the pet (processing errors in text) system is being built. The focus is on these two issues; no attempt is being made to produce a complete product . Pet operates over a shifting window of text (it can be attached simply and asynchronously to the Emacs editor). One word in this purview is in focus at a time. Pet will give one of three responses to this word; it will accept the word, suggest a correction, or indicate that it found an error it couldn\\'t correct. Below follow an outline and discussion of the (linguistic) components of pet and discussion of testing and evaluation of the system.\\n\\nPet System\\n\\nMorphological Processing  Spelling Checking\\n\\nThe word in focus is first passed through a two-level morphological analysis stage, based on an adaption of (Pulman, 1991). Two purposes are served here: checking the word is lexical (i.e. in the lexicon or a permissible inflection of a word in the lexicon) and collecting the possible categories, which are represented as sets of feature specifications (Grover, 1993).\\n\\nThis morphological lookup operates over a character trie which has been compressed into a (directed) graph. Common endings are shared and category information is stored on the first unique transition. The advantages of this compression are that (1) a word/morpheme is recognised (and category affixation rules (Grover, 1993) checked) as soon as the initial letters allow uniqueness, rather than at the end of the word, and (2) there is an immense saving of space. There was a reduction of over half the transitions on the trie formed from the Alvey lexicon.\\n\\nIf the word is unknown, the system reconsiders analysis from the point where it broke down with the added possibility of an error rule. There are currently four error rules, corresponding to the four Damerau transformations: omission, insertion, transposition, substitution (Damerau, 1964) - considered in that order (Pollock, 1983). The error rules are in two level format and integrate seamlessly into morphological analysis.\\n\\nTesting and Evaluation\\n\\nWith the aim of evaluating the effectiveness of shallow processing, tests will be carried out to see what proportion of different types of errors can be dealt with elegantly, adequately and/or efficiently. Under examination will be the number of errors missed/caught and wrongly/rightly corrected. Different components and configurations of the system will be compared, for example the error rules v. p.b.t.\\'s. Parameters of the system will be varied, for example the breadth of the purview, the position of the purview focus, the number of correction candidates and the timing of their generation. Will shallow processing miss too many of the errors cooperative error processing is aimed at?\\n\\nThere are two significant difficulties with collecting test data. The central difficulty is finding a representative sample of genuine errors by native speakers, in context, with the correct version of the text attached. Apart from anything else, `representative\\' is hard to decide - spectrum of errors or distribution of errors ? Secondly, any corpus of text usually contains only those errors that were left undetected in the text. Cooperative processing deals with errors that one backtracks to catch; if not a different class or range, these at least might have a different distribution of error types.\\n\\nThe ideal data would be records of peoples\\' keystrokes when interacting with an editor while creating or editing a piece of text. This would allow one measure of the (linguistic) feasibility of cooperative error processing: the effectiveness of shallow processing over errors revealed by the keystroke-record data. There does not appear to be an English source of this kind, so it is planned to compile one.\\n\\nFor comparison, a variety of other data has been collected. Preliminary tests used generated errors, from a program that produces random Damerau slips according to an observed distribution (Pollock, 1983), using confusion matrices where appropriate (Kernighan, 1990). Assembled data includes the Birkbeck corpus (Mitton, 1986) and multifarious misspelling lists (without context). Suggestions have been made to look for low frequency words in corpora and news/mail archives, and to the Longmans learner corpus (not native speakers).\\n\\nAcknowledgements\\n\\nThanks to all who offered advice on finding data, and to Doug McIlroy, Sue Blackwell and Neil Rowe for sending me their misspelling lists.\\n\\nThis work is supported by a British Telecom Scholarship, administered by the Cambridge Commonwealth Trust in conjunction with the Foreign and Commonwealth Office.\\n\\nBibliography\\n\\nHiyan Alshawi. 1992. The Core Language Engine. Cambridge, Massachusetts: The MIT Press.\\n\\nFred J. Damerau. 1964. ``A Technique for Computer Detection and Correction of Spelling Errors\",\\n\\nRoger Garside, Geoffrey Leech and Geoffrey Sampson, eds. 1987. The Computational Analysis of English. Longman. Commun. ACM, 7(3):171-176.\\n\\nClaire Grover, John Carroll and Ted Briscoe. 1993. ``The Alvey Natural Language Tools Grammar (4th Release)\", Tech. Rep. 284, Computer Lab, University of Cambridge.\\n\\nMark D. Kernighan, Kenneth W. Church and William A. Gale. 1990. ``A Spelling Correction Program Based on a Noisy Channel Model\", Proc. Coling-90, pp 205-210.\\n\\nRoger Mitton, ed. 1986. A Collection of Computer-Readable Corpora of English Spelling Errors (ver. 2). Birkbeck College, University of London.\\n\\nJoseph J. Pollock and Antonio Zamora. 1983. ``Collection and Characterization of Spelling Errors in Scientific and Scholarly Text\", J. Am. Soc. Inf. Sci., 34(1):51-58.\\n\\nStephen G. Pulman and Mark R. Hepple. 1993. ``A feature-based formalism for two-level phonology: a description and implementation\", Computer Speech and Language, 7(4):333-358.\\n\\nEdward M. Riseman and Allen R. Hanson. 1974. ``A Contextual Postprocessing System for Error Correction Using Binary n-Grams\", IEEE Trans. Comput., C-23(5):480-493.\\n\\nFootnotes\\n\\nIn particular, there are many HCI issues associated with such a system, which are beyond the scope of this paper.', metadata={'source': '../data/raw/cmplg-xml/9502031.xml'}),\n",
       " Document(page_content=\"Analysis of Japanese Compound Nounsusing Collocational Information\\n\\nAnalyzing compound nouns is one of the crucial issues for natural language processing systems, in particular for those systems that aim at a wide coverage of domains. In this paper, we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus. An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters. The accuracy of this method is about 80%.\\n\\nIntroduction\\n\\nAnalyzing compound nouns is one of the crucial issues for natural language processing systems, in particular for those systems that aim at a wide coverage of domains. Registering all compound nouns in a dictionary is an impractical approach, since we can create a new compound noun by combining nouns. Therefore, a mechanism to analyze the structure of a compound noun from the individual nouns is necessary.\\n\\nIn order to identify structures of a compound noun, we must first find a set of words that compose the compound noun. This task is trivial for languages such as English, where words are separated by spaces. The situation is worse, however, in Japanese where no spaces are placed between words. The process to identify word boundaries is usually called segmentation. In processing languages such as Japanese, ambiguities in segmentation should be resolved at the same time as analyzing structure. For instance, the Japanese compound noun ``SinGataKansetuZei''(new indirect tax), produces 16(=2[4]) segementations possibilities for this case. (By consulting a Japanese dictionary, we would filter out some.) In this case, we have two remaining possibilities: ``Sin (new)/Gata (type)/Kansetu (indirect)/Zei(tax)'' and ``SinGata (new)/Kansetu (indirect)/ Zei (tax).'' We must choose the correct segmentation, ``SinGata (new)/Kansetu (indirect)/Zei (tax)'' and analyze structure.\\n\\nSegmentation of Japanese is difficult only when using syntactic knowledge. Therefore, we could not always expect a sequence of correctly segmented words as an input to structure analysis. The information of structures is also expected to improve segmentation accuracy.\\n\\nextract collocations of nouns from a corpus of four kanzi character words\\n\\nreplace each noun in the collocations with thesaurus categories, to obtain the collocations of thesaurus categories\\n\\ncount occurrence frequencies for each collocational pattern of thesaurus categories\\n\\nCollocational Information for Analyzing Compound Nouns\\n\\nThis section describes procedures to acquire collocational information for analyzing compound nouns from a corpus of four kanzi character words. What we need is occurrence frequencies of all word collocations. It is not realistic, however, to collect all word collocations. We use collocations from thesaurus categories that are word abstractions.\\n\\nCollecting Word Collocations\\n\\nIn Japanese, kanzi character sequences longer than three are usually compound nouns, This tendency is confirmed by comparing the occurrence frequencies of kanzi character words in texts and those headwords in dictionaries. We investigated the tendency by using sample texts from newspaper articles and encyclopedias, and Bunrui Goi Hyou (BGH for short), which is a standard Japanese thesaurus. The sample texts include about 220,000 sentences. We found that three character words and longer represent 4% in the thesaurus, but 71% in the sample texts. Therefore a collection of four kanzi character words would be used as a corpus of compound nouns.\\n\\nFour kanzi character sequences are useful to extract binary relations of nouns, because dividing a four kanzi character sequence in the middle often gives correct segmentation. Our preliminary investigation shows that the accuracy of the above heuristics is 96 % (961/1000).\\n\\nAssigning Thesaurus Categories\\n\\nFortunately, there are few words that are assigned multiple categories in BGH. Therefore, we use method (1). Word collocations containing words with multiple categories represent about 1/3 of the corpus. If we used other thesauruses, which assign multiple categories to more words, we would need to use method (2), (3), or (4).\\n\\nCounting Occurrence of Category Collocations\\n\\nAfter assigning the thesaurus categories to words, we count occurrence frequencies of category collocations as follows: 1. collect word collocations, at this time we collect only patterns of word collocations, but we do not care about occurrence frequencies of the patterns 2. replace thesaurus categories with words to produce category collocation patterns 3. count the number of category collocation patterns Note: we do not care about frequencies of word collocations prior to replacing words with thesaurus categories.\\n\\nAlgorithm\\n\\nThe analysis consists of three steps: 1. enumerate possible segmentations of an input compound noun by consulting headwords of the thesaurus (BGH) 2. assign thesaurus categories to all words 3. calculate the preferences of every structure of the compound noun according to the frequencies of category collocations\\n\\nWe assume that a structure of a compound noun can be expressed by a binary tree. We also assume that the category of the right branch of a (sub)tree represents the category of the (sub)tree itself. This assumption exsists because Japanese is a head-final language, a modifier is on the left of its modifiee. With these assumptions, a preference value of a structure is calculated by recursive function p as follows:\\n\\nwhere function l and r return the left and right subtree of the tree respectively, cat returns thesaurus categories of the argument. If the argument of cat is a tree, cat returns the category of the rightmost leaf of the tree. Function cv returns an associativity measure of two categories, which is calculated from the frequency of category collocation described in the previous section. We would use two measures for cv:\\n\\nP(cat1, cat2) returns the relative frequency of collation cat1, which appears on the left side and cat2, which appears on the right.\\n\\nProbability:\\n\\nModified mutual information statistics (MIS):\\n\\nExperiments\\n\\nData and Analysis\\n\\nWe extract kanzi character sequences from newspaper editorials and columns and encyclopedia text, which has no overlap with the training corpus: 954 compound nouns consisting of four kanzi characters, 710 compound nouns consisting of five kanzi characters, and 786 compound nouns consisting of six kanzi characters are manually extracted from the set of the above kanzi character sequences. These three collections of compound nouns are used for test data.\\n\\nResults and Discussions\\n\\n'' means that the correct answer was not obtained because the heuristics is segmentation filtered out from the correct segmentation. The first row shows the percentage of cases where the correct answer is uniquely identified, no tie. The rows, denoted ``\\n\\n'', shows the percentage of correct answers in the n-th rank.\\n\\nshows the percentage of correct answers ranked lower or equal to 4th place.\\n\\nRegardless, more than 90% of the correct answers are within the second rank. The probabilistic measure cv1 provides better accuracy than the mutual information measure cv2 for five kanzi character compound nouns, but the result is reversed for six kanzi character compound nouns. The results for four kanzi character words are almost equal. In order to judge which measure is better, we need further experiments with longer words.\\n\\nWe could not obtain correct segmentation for 11 out of 954 cases for four kanzi character words, 39 out of 710 cases for five kanzi character words, and 15 out of 787 cases for six kanzi character words. Therefore, the accuracy of segmentation candidates are 99%(943/954), 94.5% (671/710) and 98.1% (772/787) respectively. Segmentation failure is due to words missing from the dictionary and the heuristics we adopted.\\n\\nAs mentioned in Section 1, it is difficult to correct segmentation by using only syntactic knowledge. We used the heuristics to reduce ambiguities in segmentation, but ambiguities may remain. In these experiments, there are 75 cases where ambiguities can not be solved by the heuristics. There are 11 such cases for four kanzi character words, 35 such cases for five kanzi character words, and 29 cases for six kanzi character words. For such cases, the correct segmentation can be uniquely identified by applying the structure analysis for 7, 19, and 17 cases, and the correct structure can be uniquely identified for 7, 10, and 8 cases for all collections of test data by using cv1. On the other hand, 4, 18, and 21 cases correctly segmented and 7, 11, and 8 cases correctly analyzed their structures for all collections by using cv2.\\n\\n) shows sums of distances between modifiers and modifiee contained in the structure. The distance is measured based on the number of words between a modifier and a modifiee. For instance, the distance is one, if a modifier and a modifiee are immediately adjacent.\\n\\nwhere d is the distance between two words and q(d) is the probability when two words of said distance is d and have a modification relation.\\n\\nWe redifined cv by taking this tendency as the formula that follows:\\n\\nWe assumed that the thesaurus category of a tree be represented by the category of its right branch subtree because Japanese is a head-final language. However, when a right subtree is a word such as suffixes, this assumption does not always hold true. Since our ultimate aim is to analyze semantic structures of compound nouns, then dealing with only the grammatical head is not enough. We should take semantic heads into consideration. In order to do so, however, we need knowledge to judge which subtree represents the semantic features of the tree. This knowledge may be extracted from corpora and machine readable dictionaries.\\n\\nA certain class of Japanese nouns (called Sahen meisi) may behave like verbs. Actually, we can make verbs from these nouns by adding a special verb ``-suru.'' These nouns have case frames just like ordinary verbs. With compound nouns including such nouns, we could use case frames and selectional restrictions to analyze structures. This process would be almost the same as analyzing ordinary sentences.\\n\\nConcluding Remarks\\n\\nWe propose a method to analyze Japanese compound nouns using collocational information and a thesaurus. We also describe a method to acquire the collocational information from a corpus of four kanzi character words. The method to acquire collocational information is dependent on the Japanese character, but the method to calculate preferences of structures si applicable to any language with compound nouns.\\n\\nThe experiments show that when the method analyzes compound nouns with an average length 4.9, it produces an accuracy rate of about 83%.\\n\\nincorporate other syntactic information, such as affixes knowledge\\n\\nuse another semantic information as well as thesauruses, such as selectional restriction\\n\\napply this method to disambiguate other syntactic structures such as dependency relations\\n\\nBibliography\\n\\nK. W. Church, W. Gale P. Hanks, and D. Hindle. Using statistics in lexical analysis. In Lexcal Acquisitin, chapter 6. Lawrence Erlbaum Associates, 1991.\\n\\nJ. Cowie, J. A. Guthrie, and L. Guthrie. Lexical disambiguation using simulated annealing. In COLING p310, 1992.\\n\\nT. Fujisaki, F. Jelinek, J. Cocke, and E. Black T. Nishino. A probabilistic parsing method for sentences disambiguation. In Current Issues in Parsing Thchnology, chapter 10. Kluwer Academic Publishers, 1991.\\n\\nR. Grishman and J. Sterling. Acquisition of selectional patterns. In COLING p658, 1992.\\n\\nD. Hindle and M. Rooth. Structual ambiguity and lexocal relations. In ACL p229, 1991.\\n\\nM. E. Lesk. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In ACM SIGDOC, 1986.\\n\\nH. Maruyama and S. Ogino. A statistical property of Japanese phrase-to-phrase modifications. Mathematical Linguistics, Vol. 18, No. 7, pp. 348-352, 1992.\\n\\nY. Tanaka. Acquisition of knowledge for natural language ;the four kanji character sequencies (in japanese). In National Conference of Information Processing Society of Japan, 1992.\\n\\nJ. Veronis and N. M. Ide. Word sense disambiguation with very large neural networks extracted from machine readable dictionaries. In COLING p389, 1990.\\n\\nD. Yarowsky. Word-sense disamibiguation using stastistical models of roget's categories trained on large corpora. In COLING p454, 1992.\\n\\nFootnotes\\n\\nThis paper was presented at COLING'94 at Kyoto, August 1994 Here ``/'' denotes a boundary of words.\", metadata={'source': '../data/raw/cmplg-xml/9412008.xml'}),\n",
       " Document(page_content=\"A Robust Parser Based on Syntactic Information Introduction\\n\\nI am sure this is what he means. This is, I am sure, what he means. The progress of machine does not stop even a day. Not even a day does the progress of machine stop.\\n\\nAbove examples show that people are used to write same meaningful sentences differently. In addition, people are prone to mistakes in writing sentences. So, the bulk of written sentences are open to the extragrammaticality.\\n\\nThe same jealousy can breed confusion, however, in the absence of any authorization bill this year.\\n\\n( (S (NP The/dt (ADJP same/jj) jealousy/nn) can/md (VP breed/vb (NP confusion/nn) ,/, however/rb ,/, (PP in/in (NP (NP the/dt absence/nn) (PP of/in (NP any/dt authorization/nn bill/nn)) (NP this/dt year/nn))))) ./.)\\n\\nA robust parser is one that can analyze these extragrammatical sentences without failure. However, if we try to preserve robustness by adding such rules whenever we encounter an extragrammatical sentence, the rulebase will grow up rapidly, and thus processing and maintaining the excessive number of rules will become inefficient and impractical. Therefore, extragrammatical sentences should be handled by some recovery mechanism(s) rather than by a set of additional rules.\\n\\nIn this paper, we present a robust parser with a recovery mechanism. We extend the general algorithm for least-errors recognition to adopt it as the recovery mechanism in our robust parser. Because our robust parser handle extragrammatical sentences with this syntactic information oriented recovery mechanism, it can be independent of a particular system or particular domain. Also, we present the heuristics to reduce the number of edges so that we can upgrade the performance of our parser.\\n\\nThis paper is organized as follows : We first review a general algorithm for least-errors recognition. Then we present the extension of this algorithm, and the heuristics adopted by the robust parser. Next, we describe the implementation of the system and the result of the experiment of parsing real sentences. Finally, we make conclusion with future direction.\\n\\nAlgorithm and Heuristics\\n\\nGeneral algorithm for least\\n\\n\\n\\nerrors recognition\\n\\nThe algorithm works as follows : A procedure SCAN is carried out for each state in S(i). SCAN checks various correspondences of input token t(i) against terminal symbols in RHS of rules. Once SCAN is done, COMPLETER substitutes all final states of S(i) into all other analyses which can use them as components.\\n\\nperfect match : If c(p,j) = t(i) then add (p, j+1, f, e) to S(i+1) if possible.\\n\\nCOMPLETER COMPLETER handles substitution of final states in S(i) like that of original Earley's algorithm. Each final state means the recognition of a nonterminal.\\n\\nExtension of least\\n\\n\\n\\nerrors recognition algorithm\\n\\nThe extended least-errors recognition algorithm can handle not only terminal errors but also nonterminal errors.\\n\\nHeuristics\\n\\nThe robust parser using the extended least-errors recognition algorithm overgenerates many error-hypothesis edges during parsing process. To cope with this problem, we adjust error values according to the following heuristics. Edges with more error values are regarded as less important ones, so that those edges are processed later than those of less error values.\\n\\nBy these heuristics, our robust parser can process only plausible edges first, instead of processing all generated edges at the same time, so that  we can enhance the performance of the robust parser and result in the great reduction in the number of resultant trees.\\n\\nImplementation and Evaluation\\n\\nThe robust parser\\n\\nExperimental result\\n\\nTo show usefulness of the robust parser proposed in this paper, we made some experiments.\\n\\nRule We can derive 4,958 rules and their frequencies out of 14,137 sentences in the Penn treebank tree-tagged corpus, the Wall Street Journal. The average frequency of each rule is 48 times in the corpus. Of these rules, we remove rules which occurs fewer times than the average frequency in the corpus, and then only 192 rules are left. These removed rules are almost for peculiar sentences and the left rules are very general rules. We can show that our robust parser can compensate for lack of rules using only 192 rules with the recovery mechanism and heuristics.\\n\\nTest set First,  1,000 sentences are selected randomly from the WSJ corpus, which we have referred to in proposing the robust parser. Of these sentences, 410 are failed in normal parsing, and are processed again by the robust parser. To show the validity of these heuristics, we compare the result of the robust parser using  heuristics  with one not using heuristics. Second, to show the adaptability of our robust parser, same experiments are carried out on 1,000 sentences from the ATIS corpus in Penn treebank, which we haven't referred to when we propose the robust parser. Among 1,000 sentences from the ATIS, 465 sentences are processed by the robust parser after the failure of the normal parsing.\\n\\nConclusion\\n\\nOur short term goal is to propose an automatic method that can learn parameter values of heuristics by analyzing the corpus. We expect that automatically learned values of parameters can upgrade the performance of the parser.\\n\\nAcknowledgement\\n\\nThis work was supported(in part) by Korea Science and Engineering Foundation(KOSEF) through Center for Artificial Intelligence Research(CAIR), the Engineering Research Center(ERC) of Excellence Program.\\n\\nBibliography\\n\\n[Black, 1991] E. Black et al. A Procedure for quantitatively comparing the syntactic coverage of English grammars. Proceedings of Fourth  DARPA Speech and Natural Language Workshop, pp. 306-311, 1991.\\n\\n[Carbonell and Hayes, 1983] J. G. Carbonell and P. J. Hayes. Recovery Strategies for Parsing Extragrammatical Language. American Journal of Computational Linguistics, vol. 9, no. 3-4, pp. 123-146, 1983.\\n\\n[Hayes and Carbonell, 1981] P. Hayes and J. Carbonell. Multi-strategy Construction-Specific Parsing for Flexible Data Base Query Update. Proceedings of the 7th International Joint Conference on Artificial Intelligence, pp. 432-439, 1981.\\n\\n[Hayes and Mouradian, 1981] P. J. Hayes and G. V. Mouradian. Flexible Parsing. American Journal of Computational Linguistics, vol. 7, no. 4, pp. 232-242, 1981.\\n\\n[Hendrix, 1977] G. Hendrix. Human Engineering for Applied Natural Language Processing. Proceedings of the 5th International Joint Conference on Artificial Intelligence, pp. 183-191, 1977.\\n\\n[Kwasny and Sondheimer, 1981] S. Kwasny and N. Sondheimer. Relaxation Techniques for Parsing Grammatically Ill-Formed Input in Natural Language Understanding Systems. American Journal of Computational Linguistics, vol. 7, no. 2, pp. 99-108, 1981.\\n\\n[Lyon, 1974] G. Lyon. Syntax-Directed Least-Errors Analysis for Context-Free Languages. Communications of the ACM, vol. 17, no. 1, pp. 3-14, 1974.\\n\\n[Marcus, 1991] M. P. Marcus. Building very Large natural language corpora : the Penn Treebank, 1991.\\n\\n[Mellish, 1989] C. S. Mellish. Some Chart-Based Techniques for Parsing Ill-Formed Input. Association for Computational Linguistics, pp. 102-109, 1989.\\n\\n[Schank  et al. , 1980] R. C. Schank, M. Lebowitz and L. Brinbaum. An Intergrated Understander. American Journal of Computational Linguistics, vol. 6, no. 1, pp. 13-30, 1980.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9502024.xml'}),\n",
       " Document(page_content=\"A Plan-Based Model for Response Generation in Collaborative Task-Oriented Dialogues1\\n\\nThis paper presents a plan-based architecture for response generation in collaborative consultation dialogues, with emphasis on cases in which the system (consultant) and user (executing agent) disagree. Our work contributes to an overall system for collaborative problem-solving by providing a plan-based framework that captures the Propose-Evaluate-Modify cycle of collaboration, and by allowing the system to initiate subdialogues to negotiate proposed additions to the shared plan and to provide support for its claims. In addition, our system handles in a unified manner the negotiation of proposed domain actions, proposed problem-solving actions, and beliefs proposed by discourse actions. Furthermore, it captures cooperative responses within the collaborative framework and accounts for why questions are sometimes never answered.\\n\\nIntroduction\\n\\nIn this paper, we present a plan-based architecture for response generation in collaborative consultation dialogues, with emphasis on cases in which the system and the user disagree. The model treats utterances as proposals open for negotiation and only incorporates a proposal into the shared plan under construction if both agents believe the proposal to be appropriate. If the system does not accept a user proposal, the system attempts to modify it, and natural language utterances are generated as a part of this process. Since the system's utterances are also treated as proposals, a recursive negotiation process can ensue. This response generation architecture has been implemented in a prototype system for a university advisement domain.\\n\\nModeling Collaboration\\n\\nWe adopt a plan-based mechanism because it is general and easily extendable, allows the same declarative knowledge about collaborative problem-solving to be used both in generation and understanding, and allows the recursive nature of our theory to be represented by recursive meta-plans. This paper focuses on one component of our model, the arbitrator, which performs the Evaluate and Modify actions in the Propose-Evaluate-Modify cycle of collaboration.\\n\\nThe Arbitration Process\\n\\nA proposal consists of a chain of actions for addition to the shared plan. The arbitrator evaluates a proposal and determines whether or not to accept it, and if not, modifies the original proposal to a form that will potentially be accepted by both agents. The arbitrator has two subcomponents, the evaluator and the modifier, and has access to a library of generic recipes for performing actions.\\n\\nThe Evaluator\\n\\nThe processes for detecting conflicts and better alternatives start at the top-level proposed action, and are interleaved because we intend for the system to address the highest-level action disagreed upon by the agents. This is because it is meaningless to suggest, for example, a better alternative to an action when one believes that its parent action is infeasible.\\n\\nDetecting Conflicts About Plan Validity Detecting Sub-Optimal Solutions The Ranking Advisor\\n\\nThe ranking advisor's task is to determine how best the parameters of an action can be instantiated, based on the user's preferences. For each object that can instantiate a parameter of an action (such as CS621 instantiating _course in Take-Course(UserA,_course)), the evaluator provides the ranking advisor with the values of its attributes (e.g., Difficulty(CS621,difficult)) and the user's preferences for the values of these attributes (e.g., prefers(UserA, Difficulty(_course,moderate), Take-Course, weak)).\\n\\nTwo factors should be considered when ranking the candidate instantiations: the strength of the preference and the closeness of the match. The strength of a preference indicates the weight that should be assigned to the preference. The closeness of the match (exact, strong, weak, or none) measures how well the actual and the preferred values of an attribute match. It is measured based on the distance between the two values where the unit of measurement differs depending on the type of the attribute. For example, for attributes with discrete values (difficulty of a course can be very-difficult, difficult, moderate, easy, or very-easy), the match between difficult and moderate will be strong, while that between difficult and easy will be weak. The closeness of the match must be modeled in order to capture the fact that if the user prefers difficult courses, a moderate course will be considered preferable to an easy one, even though neither of them exactly satisfies the user's preference.\\n\\nExample\\n\\nThe Modifier\\n\\nIn order to retain as much of the original proposal as possible when modifying a proposal, Modify-Relation has two specializations: Remove-Node and Alter-Node. The former is selected if the action itself is inappropriate, and will cause the action to be removed from the dialogue model. The latter is chosen if a parameter is inappropriately instantiated, in which case the action will remain in the dialogue model and the problematic parameter will be left uninstantiated.\\n\\nExample of Correcting an Invalid Proposal\\n\\n#1U: #2S: (4) #1 I want to satisfy my seminar course requirement. (5) Who is teaching AI?\\n\\nThe evaluation process, which determines whether or not to accept the proposal, starts at the top-level proposed domain action, Satisfy-Seminar-Course(U,CS). Suppose the system believes that Satisfy-Seminar-Course(U,CS) contributes to Get-Masters(U,CS), that U can perform Satisfy-Seminar-Course(U,CS), and that there is no better alternative to the instantiation of Satisfy-Seminar-Course. The evaluator then checks its child action Take-Course(U,AI). The system's recipe library indicates that Take-Course(U,AI) does not contribute to Satisfy-Seminar-Course(U,CS), since it believes that AI is not a seminar course, causing the proposal to be rejected.\\n\\n#1S: #2 (6) #1 Taking AI does not contribute to satisfying the seminar course requirement. (7) AI is not a seminar course.\\n\\nIf the user accepts the system's utterances, thus satisfying the precondition that the conflict be resolved, Modify-Relation can be performed and changes made to the dialogue model. In this example, the proposal is rejected due to an inappropriate instantiation of the parameter _course; thus Modify-Relation will select Alter-Node as a specialization to replace all instances of AI in the dialogue model with a variable. This variable can be reinstantiated by Insert-Correction, the second subaction of Correct-Relation.\\n\\nAssuming that the system and the user encounter no further conflict in reinstantiating the variable, the arbitration process at the meta-level is completed and the original dialogue is returned to. The proposed additions now consist of actions agreed upon by both agents and will therefore be incorporated into the existing model. Notice that our model separates the negotiation subdialogue (captured at the meta level) from the original dialogue while allowing the same plan-based mechanism to be used at both levels. It also accounts for why the user's original question about the instructor of AI is never answered -- a conflict was detected that made the question superfluous. Thus certain situations in which questions fail to be answered can be accounted for by the collaborative process rather than being viewed as a violation of cooperative behaviour.\\n\\nExample of Suggesting Better Alternatives\\n\\n#1U: #2 (8) #1 I want to satisfy my theory course requirement. (9) Who is teaching CS621?\\n\\n#1S: #2 (10) #1 CS601 is a better alternative than CS621. (11) CS601 meets at 2pm and involves formal languages and grammar.\\n\\nThe Belief Level\\n\\nWe showed how our arbitrator detects and resolves conflicts at the domain level. Our goal, however, is to develop a mechanism that can handle negotiations at the domain, problem-solving, and discourse levels in a uniform fashion. The process can be successfully applied to the problem-solving level because both the domain and problem-solving levels represent actions that the agents propose to do (at a later point in time for the domain level and at the current time for the problem-solving level); however, the discourse level actions are actions that are currently being executed, instead of proposed for execution. This causes problems for the modification process, as illustrated by the following example.\\n\\n#1U: #2S: (12) #1 I want to take AI. (13) Dr. Brown is teaching AI, (14) since he is a full professor.\\n\\nIn order to preserve the representation of the discourse level, and to handle the kind of conflict shown in the previous example, we expand the dialogue model to include a belief level. The belief level captures domain-related beliefs proposed by discourse actions as well as the relationship amongst them. For instance, an Inform action proposes a mutual belief (MB) of a proposition and an Obtain-Info-Ref action proposes that both agents come to know the referent (Mknowref) of a parameter. Thus, information captured at the belief level consists not of actions, as in the other three levels, but of beliefs that are to be achieved, and belief relationships, such as support, attack, etc.\\n\\nDiscourse Level Example Revisited\\n\\nThe evaluation process starts at the proposed domain level. Suppose that the system believes that both Take-Course(U,AI) and Build-Plan(U,S,Take-Course(U,AI)) can be performed. However, an examination of the proposed belief level causes the proposal to be rejected because the system does not believe that Dr. Brown being a full professor supports the fact that he teaches AI. Thus, Correct-Relation is selected as the specialization of Modify-Proposal in order to resolve the conflict regarding this supports relationship. Again in order to satisfy the precondition of modifying the proposal, the system invokes the Inform action which would generate the following utterance:\\n\\n#1S: #2 (15) #1 Dr. Brown being a full professor does not provide support for him teaching AI.\\n\\nThus, with the addition of the belief level, the arbitrator is able to capture the process of evaluating and modifying proposals in a uniform fashion at the domain, problem-solving, and belief levels. An additional advantage of the belief level is that it captures the beliefs conveyed by the discourse level, instead of how they are conveyed (by an Inform action, by expressing doubt, etc. ).\\n\\nRelated Work\\n\\nConclusions and Future Work\\n\\nThis paper has presented a plan-based system that captures collaborative response generation in a Propose-Evaluate-Modify cycle. Our system can initiate subdialogues to negotiate implicitly proposed additions to the shared plan, can appropriately respond to user queries that are motivated by ill-formed or suboptimal solutions, and handles in a unified manner the negotiation of proposed domain actions, proposed problem-solving actions, and beliefs proposed by discourse actions. In addition, our system captures cooperative responses within an overall collaborative framework that allows for negotiation and accounts for why questions are sometimes never answered (even in the most cooperative of environments).\\n\\nAcknowledgments\\n\\nThe authors would like to thank Stephanie Elzer for her comments on earlier drafts of this paper.\\n\\nBibliography\\n\\nAllen, J. 1991. Discourse structure in the TRAINS project. In Darpa Speech and Natural Language Workshop.\\n\\nBratman, M. 1990. What is intention? In Cohen, P.; Morgan, J.; and Pollack, M., eds., Intentions in Communication. chapter 2,  15-31.\\n\\nCawsey, A.\\n\\n1993.\\n\\nPlanning interactive explanations.\\n\\nInternational Journal of Man\\n\\n\\n\\nMachine Studies  169\\n\\n\\n\\n199.\\n\\nEdmonds, P. 1993. A computational model of collaboration on reference in direction-giving dialogues. Technical Report CSRI-289, Univ. of Toronto.\\n\\nEller, R., and Carberry, S. 1992. A meta-rule approach to flexible plan recognition in dialogue. User Modeling and User-Adapted Interaction 2:27-53.\\n\\nElzer, S.; Chu, J.; and Carberry, S. 1994. Recognizing and utilizing user preferences in collaborative consultation dialogues. In Progress.\\n\\nGrosz, B., and Sidner, C. 1990. Plans for discourse. In Cohen, P.; Morgan, J.; and Pollack, M., eds., Intentions in Communication. chapter 20,  417-444.\\n\\nGuinn, C., and Biermann, A. 1993. Conflict resolution in collaborative discourse. In Proceedings of the IJCAI-93 Workshop:Computational Models of Conflict Management in Cooperative Problem Solving,  84-88.\\n\\nHeeman, P., and Hirst, G. 1992. Collaborating on referring expressions. Technical Report 435, Univ. of Rochester.\\n\\nHeeman, P. 1993. Speech actions and mental states in task-oriented dialogues. In AAAI 1993 Spring Symposium on Reasoning About Mental States: Formal Theories and Applications.\\n\\nJoshi, A.; Webber, B.; and Weischedel, R. 1984. Living up to expectations: Computing expert responses. In Proceedings of the AAAI,  169-175.\\n\\nJoshi, A. 1982. Mutual beliefs in question-answer systems. In Smith, N., ed., Mutual Knowledge. chapter 4,  181-197.\\n\\nLambert, L., and Carberry, S. 1991. A tripartite plan-based model of dialogue. In Proceedings of the ACL,  47-54.\\n\\nLambert, L., and Carberry, S. 1992. Modeling negotiation dialogues. In Proceedings of the ACL,  193-200.\\n\\nLitman, D., and Allen, J. 1987. A plan recognition model for subdialogues in conversation. Cognitive Science 11:163-200.\\n\\nLochbaum, K. 1991. An algorithm for plan recognition in collaborative discourse. In Proceedings of the ACL,  33-38.\\n\\nMaybury, M. 1992. Communicative acts for explanation generation. International Journal of Man-Machine Studies 37:135-172.\\n\\nMaybury, M. 1993. Communicative acts for generating natural language arguments. In Proceedings of the AAAI,  357-364.\\n\\nMoore, J., and Paris, C. 1993. Planning text for advisory dialogues: Capturing intentional, rhetorical and attentional information. Computational Linguistics 19(4):651-694.\\n\\nPollack, M. 1986. A model of plan inference that distinguishes between the beliefs of actors and observers. In Proceedings of the ACL,  207-214.\\n\\nQuilici, A. 1991. The Correction Machine: A computer Model of Recognizing and Producing Belief Justifications in Argumentative Dialogs. Ph.D. Dissertation, UCLA.\\n\\nReed, S.\\n\\n1982.\\n\\nCognition: Theory and Applications.\\n\\nchapter 14,  337\\n\\n\\n\\n365.\\n\\nSidner, C. 1992. Using discourse to negotiate in collaborative activity: An artificial language. In AAAI-92 Workshop: Cooperation Among Heterogeneous Intelligent Systems,  121-128.\\n\\nSycara, K. 1989. Argumentation: Planning other agents' plans. In Proceedings of the IJCAI,  517-523.\\n\\nvan Beek, P. 1987. A model for generating better explanations. In Proceedings of the ACL,  215-220.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9405011.xml'}),\n",
       " Document(page_content=\"Evaluation of Semantic Clusters\\n\\nSemantic clusters of a domain form an important feature that can be useful for performing syntactic and semantic disambiguation. Several attempts have been made to extract the semantic clusters of a domain by probabilistic or taxonomic techniques. However, not much progress has been made in evaluating the obtained semantic clusters. This paper focuses on an evaluation mechanism that can be used to evaluate semantic clusters produced by a system against those provided by human experts.\\n\\nIntroduction\\n\\nThe Need\\n\\nAlthough there has been a lot of work done in extracting semantic classes of a given domain, relatively little attention has been paid to the task of evaluating the generated classes. In the absence of an evaluation scheme, the only way to decide if the semantic classes produced by a system are ``reasonable'' or not is by having an expert analyze them by inspection. Such informal evaluations make it very difficult to compare one set of classes against another and are also not very reliable estimates of the quality of a set of classes. It is clear that a formal evaluation scheme would be of great help.\\n\\nHatzivassiloglou and McKeown (1993) cluster adjectives into partitions and present an interesting evaluation to compare the generated adjective classes against those provided by an expert. Their evaluation scheme bases the comparison between two classes on the presence or absence of pairs of words in them. Their approach involves filling in a YES-NO contingency table based on whether a pair of words (adjectives, in their case) is classified in the same class by the human expert and by the system. This method works very well for partitions. However, if it is used to evaluate sets of classes where the classes may be potentially overlapping, their technique yields a weaker measure since the same word pair could possibly be present in more than one class.\\n\\nAn ideal scheme used to evaluate semantic classes should be able to handle overlapping classes (as opposed to partitions) as well as hierarchies. The technique proposed by Hatzivassiloglou and McKeown does not do a good job of evaluating either of these. In this paper, we present an evaluation methodology which makes it possible to properly evaluate overlapping classes. Our scheme is also capable of incorporating hierarchies provided by an expert into the evaluation, but still lacks the ability to compare hierarchies against hierarchies.\\n\\nIn the discussion that follows, the word ``clustering'' is used to refer to the set of classes that may be either provided by an expert or generated by the system, and the word ``class'' is used to refer to a single class in the clustering.\\n\\nEvaluation Approach\\n\\nAs mentioned above, we intend to be able to compare a clustering generated by a system against one provided by an expert. Since a word can occur in more than one class, it is important to find some kind of mapping between the classes generated by the system and the classes given by the expert. Such a mapping tells us which class in the system's clustering maps to which one in the expert's clustering, and an overall comparison of the clusterings is based on the comparison of the mutually mapping classes.\\n\\nThe three main steps in the evaluation process are the acquisition of ``correct'' classes from domain experts, mapping the experts' clustering to that generated by the system, and generating an overall measure that represents the system's performance when compared against the expert.\\n\\nKnowledge Acquisition from Experts\\n\\nThe objective of this step is to get human experts to undertake the same task that the system performs, i.e., classifying a set of words into several potentially overlapping classes. The classes produced by a system are later compared to these ``correct'' classifications provided by the expert.\\n\\nMapping Algorithm\\n\\nIn order to determine pairwise mappings between the clustering generated by the system and one provided by an expert, a table of F-measures is constructed, with a row for each class generated by the system, and a column for every class provided by the expert. Note that since the expert actually provides a hierarchy, there is one column corresponding to every individual class and subclass provided by the expert. This allows the system's classes to map to a class at any level in the expert's hierarchy. This table gives an estimate of how well each class generated by the system maps to the ones provided by the expert.\\n\\nThe algorithm used to compute the actual mappings from the F-measure table is briefly described here. In each row of the table, mark the cell with the highest F-measure as a potential mapping. In general, conflicts arise when more than one class generated by the system maps to a given class provided by the expert. In other words, whenever a column in the table has more than one cell marked as a potential mapping, a conflict is said to exist. To resolve a conflict, one of the system classes must be re-mapped. The heuristic used here is that the class for which such a re-mapping results in minimal loss of F-measure is the one that must be re-mapped. Several such conflicts may exist, and re-mapping may lead to further conflicts. The mapping algorithm iteratively searches for conflicts and resolves them till no more conflicts exist. Note also that a system class may map to an expert class only if the F-measure between them exceeds a certain threshold value. This ensures that a certain degree of similarity must exist between two classes for them to map to each other. We have used a threshold value of 0.20. This value is obtained purely by observations made on the F-measures between different pairs of classes with varying degrees of similarity.\\n\\nComputation of the Overall F-measure Results and Discussion\\n\\nIt is our belief that the evaluation scheme presented in this paper is useful for comparing different clusterings produced by the same system or those produced by different systems against one provided by an expert. The resulting precision, recall, and F-measure should not be treated as a kind of ``gold standard'' to represent the quality of these classes in some absolute sense. It has been our experience that, as semantic clustering is a highly subjective task, evaluating a given clustering against different experts may yield numbers that vary considerably. However, when different clusterings generated by a system are compared against the same expert (or the same set of experts), such relative comparisons are useful.\\n\\nThe evaluation scheme presented here still suffers from one major limitation -- it is not capable of evaluating a hierarchy generated by a system against one provided by an expert. Such evaluations get complicated because of the restriction of one-to-one mapping. More work definitely needs to be done in this area.\\n\\nBibliography\\n\\nRajeev Agarwal. 1995. Semantic feature extraction from technical texts with limited human intervention. Ph.D. thesis, Mississippi State University, May.\\n\\nRoberto Basili, Maria Pazienza, and Paola Velardi. 1994. The noisy channel and the braying donkey. In Proceedings of the ACL Balancing Act Workshop, pages 21-28, Las Cruces, New Mexico, July.\\n\\nNancy Chincor. 1992. MUC-4 evaluation metrics. In Proceedings of the Fourth Message Understanding Conference (MUC-4).\\n\\nRalph Grishman and John Sterling. 1993. Smoothing of automatically generated selectional constraints. In Proceedings of the ARPA Workshop on Human Language Technology. Morgan Kaufmann Publishers, Inc., March.\\n\\nVasileios Hatzivassiloglou and Kathleen R. McKeown. 1993. Towards the automatic identification of adjectival scales: Clustering adjectives according to meaning. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 172-82.\\n\\nFrancois-Michel Lang and Lynette Hirschman. 1988. Improved portability and parsing through interactive acquisition of semantic information. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 49-57, February.\\n\\nJames Pustejovsky. 1992. The acquisition of lexical semantic knowledge from large corpora. In Proceedings of the Speech and Natural Language Workshop, pages 243-48, Harriman, N.Y., February.\\n\\nLisa Rau, Paul Jacobs, and Uri Zernik. 1989. Information extraction and text summarization using linguistic knowledge acquisition. Information Processing and Management, 25(4):419-28.\\n\\nPhilip Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania, December. (Institute for Research in Cognitive Science report IRCS-93-42).\\n\\nFootnotes\\n\\nThe author is currently at Texas Instruments and all inquiries should be addressed to rajeev@csc.ti.com. The author is currently at Texas Instruments and all inquiries should be addressed to rajeev@csc.ti.com. The author is currently at Texas Instruments and all inquiries should be addressed to rajeev@csc.ti.com.\", metadata={'source': '../data/raw/cmplg-xml/9505011.xml'}),\n",
       " Document(page_content=\"Algorithms for Analysing the Temporal Structure of Discourse1 2\\n\\nWe describe a method for analysing the temporal structure of a discourse which takes into account the effects of tense, aspect, temporal adverbials and rhetorical structure and which minimises unnecessary ambiguity in the temporal structure. It is part of a discourse grammar implemented in Carpenter's  ALE formalism. The method for building up the temporal structure of the discourse combines constraints and preferences: we use constraints to reduce the number of possible structures, exploiting the  HPSG type hierarchy and unification for this purpose; and we apply preferences to choose between the remaining options using a temporal centering mechanism. We end by recommending that an underspecified representation of the structure using these techniques be used to avoid generating the temporal/rhetorical structure until higher-level information can be used to disambiguate.\\n\\nIntroduction\\n\\nConstraints on narrative continuations\\n\\nThe problem for practical systems is twofold:  we could assume that in the case of narrative the Kamp/Hinrichs/Partee algorithm is the default, but each time the default is applied we would need to check all our available world knowledge to see whether there isn't a world knowledge postulate which might be overriding this assumption. Clearly this would make the processing of text a very expensive operation.\\n\\nBecause of considerations like these, our aim in the implementation work was to treat tense, aspect, cue words and rhetorical relations as mutually constraining, with more specific information such as explicit cue words having higher priority than less specific information such as tense. The main advantage of this approach is that it reduces temporal structure ambiguity without having to rely on detailed world knowledge postulates.\\n\\nAn  HPSG implementation of a discourse grammar\\n\\nIn our  ALE implementation, a  contains the following slots for temporal information:\\n\\nCUE/SMALL>_WORD: Cues to rhetorical structure, e.g., ``because.'' V/SMALL>_AND/SMALL>_NP/SMALL>_LIST: Contains content words found in this DCU, and is used to compare the content words  of  the  current    with those in previous threads, in order to rate the semantic ``closeness'' of the to each thread. SEM/SMALL>_ASPECT: Contains the semantic aspect (event, state, activity). We have extended the Penn  Carpenter implementation of the grammar so that semantic aspect is calculated compositionally (and stored here). RHET/SMALL>_RELN: The relation between this  DCU and a previous one. Lexical items and phrases such as cue words (stored in  CUE/SMALL>_WORD) affect the value of this slot. TEMP/SMALL>_CENTER: Used for temporal centering; Keeps track of the thread currently being followed (since there is a preference for continuing the current thread) and all the threads that have been constructed so far in the discourse. FWD/SMALL>_CENTER: Existing threads BKWD/SMALL>_CENTER: The thread currently being followed CLOSED/SMALL>_THREADS: Threads no longer available for continuation TEMP/SMALL>_EXPR/SMALL>_RELNS: Stores the semantic interpretation of temporal expressions associated with this . TEMP/SMALL>_RELNS: Stores the temporal relations between the eventualities in the discourse. TEMPFOC: The most recent event in the current thread which a subsequent eventuality may elaborate upon (same-event), overlap, come just_after or precede.\\n\\nTENASP: Keeps track of the tense and syntactic aspect of the  (if the  is simple). TENSE: past, pres, fut ASPECT: simple, perf, prog, perf_prog\\n\\nThe algorithm\\n\\nFor reasons of space it is difficult to give examples of the sign-based output of the grammar, or of the  rules, so we will restrict ourselves here to a summary of the algorithm and to a very limited rendition of the system output. The algorithm used for calculating the temporal structure of a discourse can be summarised as follows. It consists of two parts, the constraint-based portion and the preference-based portion:\\n\\nCharts such as Table 1 provide the observations we use to fill in the value of RHET/SMALL>_RELN. Those observations are summarised below. In what follows, the event variable associated with i is ei and the  TEMPFOC of e1is the most recent event/activity processed, possibly e1 itself:\\n\\n2 describes a state, or\\n\\n1 describes a state and 2 describes an activity.\\n\\n2 describes a simple tense event, or\\n\\n1 describes a complex tense clause and 2 describes a complex tense event, or\\n\\n1 describes an event and 2 describes an atelic or a simple tense state, or\\n\\n1 describes a state and 2 describes a simple tense activity.\\n\\n2 describes an event, or\\n\\n1 doesn't describe an activity and 2 describes a past perfect stative.\\n\\n1 describes an event, or\\n\\n1 describes an activity and 2 describes an atelic, or\\n\\n1 and 2 describe states and either 2describes a simple tense state or 1 describes a complex tense state.\\n\\nAn underspecified representation\\n\\nBy using constraints and preferences, we can considerably reduce the amount of ambiguity in the temporal/rhetorical structure of a discourse. However, explicit cues to rhetorical and temporal relations are not always available, and these cases result in more ambiguity than is desirable when processing large discourses.\\n\\nConsider, however, that instead of generating all the possible temporal/rhetorical structures, we could use the information available to fill in the most restrictive type possible in the type hierarchy of temporal/rhetorical relations shown in Figure 1. We can then avoid generating the structures until higher-level information can be applied to complete the disambiguation process.\\n\\nConclusion\\n\\nWe presented a brief description of an algorithm for determining the temporal structure of discourse. The algorithm is part of an HPSG-style discourse grammar implemented in Carpenter's  ALE formalism. Its novel features are that it treats tense, aspect, temporal adverbials and rhetorical relations as mutually constraining; it postulates less ambiguity than current temporal structuring algorithms do; and it uses semantic closeness and other preference techniques rather than full-fledged world knowledge postulates to determine preferences over remaining ambiguities. We also recommended using an underspecified representation of temporal/rhetorical structure to avoid generating all solutions until higher-level knowledge can aid in reducing ambiguity.\\n\\nBibliography\\n\\nBarbara J. Grosz, Aravind Joshi, and Scott Weinstein. 1983. Providing a unified account of definite noun phrases in discourse. In the proceedings of the 21st Annual Meeting of the Association for Computational Linguistics, pages 44-50.\\n\\nErhard W. Hinrichs.\\n\\n1981.\\n\\nTemporale anaphora in englischen.\\n\\nStaatsExamen thesis, Universitt Tubingen.\\n\\nJanet Hitzeman, Claire Grover, and Marc Moens. 1994. The implementation of the temporal portion of the discourse grammar. Deliverable D.2.Temporal, LRE 61-062, University of Edinburgh, December.\\n\\nMegumi Kameyama, Rebecca Passonneau, and Massimo Poesio. 1993. Temporal centering. In the proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 70-77, Columbus, OH.\\n\\nHans Kamp. 1979. Events, instant and temporal reference. In R. Bauerle, U. Egli, and A. von Stechow, editors, Semantics from Different Points of View, pages 376-417, Springer-Verlag.\\n\\nAndrew Kehler. 1994. Temporal relations: Reference or discourse coherence? In the proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 319-321, June.\\n\\nAlex Lascarides and Nicholas Asher. 1991. Discourse relations and defeasible knowledge. In the proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 55-63, University of California at Berkeley.\\n\\nMarc Moens. 1987. Tense, Aspect and Temporal Reference. Ph.D. thesis, University of Edinburgh.\\n\\nJ. Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17(1):21-48.\\n\\nTerence Parsons. 1990. Events in the semantics of English: A Study in Subatomic Semantics. Massachusetts Institute of Technology.\\n\\nBarbara Hall Partee.\\n\\n1984.\\n\\nNominal and temporal anaphora.\\n\\nLinguistics and Philosophy, 7:243\\n\\n\\n\\n286.\\n\\nMassimo Poesio. 1994. Discourse Interpretation and the Scope of Operators. Ph.D. thesis, University of Rochester, Department of Computer Science, Rochester, NY.\\n\\nCarl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press and CSLI Publications.\\n\\nHub Prst, Remko Scha and Martin van den Berg. 1994. Discourse grammar and verb phrase anaphora. Linguistics and Philosophy, 17:261-327.\\n\\nRemko Scha and Livia Polanyi. 1988. An augmented context free grammar for discourse. In Proceedings of the 12th Conference on Computational Linguistics, pages 573-577, Prague, August.\\n\\nCandace L. Sidner. 1983. Focusing in the comprehension of definite anaphora. In M. Brady and R. Berwick, editors, Computational Models of Discourse. MIT Press, Cambridge, MA.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9502018.xml'}),\n",
       " Document(page_content=\"Planning Argumentative Texts\\n\\nThis paper presents PROVERB a text planner for argumentative texts. PROVERBs main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way. The former splits the task of presenting a particular proof into subtasks of presenting subproofs. The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus.\\n\\nIntroduction\\n\\nOur computational model can therefore be viewed as the first serious attempt at a comprehensive computational model that produces adequate argumentative texts from ND style proofs. The main aim is to show how existing text planning techniques can be adapted for this particular application. To test its feasibility, this computational model is implemented in a system called PROVERB.\\n\\nContext of Our Research The Framework of the Macroplanner\\n\\nThe macroplanner of PROVERB elaborates on communicative goals, selects and orders pieces of information to fulfill these goals. The output is an ordered sequence of proof communicative act intentions (PCAs). PCAs can be viewed as speech acts in our domain of application.\\n\\nPlanning Framework\\n\\nPROVERB combines the two above mentioned presentation modes by encoding communication knowledge for both top-down planning and bottom-up presentation in form of operators in a uniform planning framework. Since top-down presentation operators embody explicit communicative norms, they are given a higher priority. A bottom-up presentation is chosen only when no top-down presentation operator applies. The overall planning framework is realized by the function Present. Taking as input a subproof, Present repeatedly executes a basic planning cycle until the input subproof is conveyed. Each cycle carries out one presentation operator, where Present always tries first to choose and apply a top-down operator. If impossible, a bottom-up operator will be chosen. The function   Present is first called with the entire proof as the presentation task. The execution of a top-down presentation operator may generate subtasks by calling it recursively. The discourse produced by each call to Present forms an attentional unit (compare the subsection below).\\n\\nThe Discourse Model and the Attentional Hierarchy\\n\\nThe discourse carried out so far is recorded in a discourse model. Rather than recording the semantic objects and their properties, our discourse model consists basically of the part of the input proof tree which has already been conveyed. The discourse model is also segmented into an attentional hierarchy, where subproofs posted by a top-down presentation operators as subtasks constitute attentional units. The following are some notions useful for the formulation of the presentation operators:\\n\\nTask is the subproof in the input proof whose presentation is the current task.\\n\\nLocal focus is the intermediate conclusion last presented, while the semantic objects involved in the local focus are called the focal centers.\\n\\nProof Communicative Acts\\n\\nPCAs are the primitive actions planned during the macroplanning to achieve communicative goals. Like speech acts, PCAs can be defined in terms of the communicative goals they fulfill as well as their possible verbalizations. Based on an analysis of proofs in mathematical textbooks, each PCA has as goal a combination of the following subgoals:\\n\\n1. Conveying a step of the derivation. The simplest PCA is the operator Derive. Instantiated as below:\\n\\ndepending on the reference choices, a possible verbalization is given as following:\\n\\n``Because a is an element of S1 and S1 is a subset of S2, according to the definition of subset, a is an element of S2.''\\n\\n2. Updates of the global attentional structure. These PCAs sometimes also convey a partial plan for the further presentation. Effects of this group of PCAs include: creating new attentional units, setting up partially premises and the goal of a new unit, closing the current unit, or reallocating the attention of the reader from one attentional unit to another. The PCA\\n\\ncreates two attentional units with A and B as the assumptions, and Formula as the goal by producing the verbalization:\\n\\n``To prove Formula, let us consider the two cases by assuming A and B.''\\n\\nStructure of the Planning Operators Proof: a proof schema, which characterizes the syntactical structure of a proof segment for which this operator is designed. It plays the role of the goal slot in the traditional planning framework.\\n\\nApplicability Condition: a predicate.\\n\\nActs: a procedure which essentially carries out a sequence of presentation acts. They are either primitive PCAs, or are recursive calls to the procedure Present for subproofs.\\n\\nFeatures: a list of features which helps to select the best of a set of applicable operators.\\n\\nTop\\n\\n\\n\\nDown Planning\\n\\nThis section elaborates on the communicative norms concerning how a proof to be presented can be split into subproofs, as well as how the hierarchically-structured subproofs can be mapped onto some linear order for presentation. In contrast with operators employed in RST-based planners that split goals according to the rhetorical structures, our operators encode standard schemata for presenting proofs, which contain subgoals. The top-down presentation operators are roughly divided into two categories:\\n\\nschemata-based operators encoding complex schemata for the presentation of proofs of a specific pattern (twelve of them are currently integrated in PROVERB),\\n\\ngeneral operators embodying general presentation norms, concerning splitting proofs and ordering subgoals.\", metadata={'source': '../data/raw/cmplg-xml/9410032.xml'}),\n",
       " Document(page_content=\"A framework for lexical representation\\n\\nIn this paper we present a unification-based lexical platform designed for highly inflected languages (like Roman ones). A formalism is proposed for encoding a lemma-based lexical source, well suited for linguistic generalizations. From this source, we automatically generate an allomorph indexed dictionary, adequate for efficient processing. A set of software tools have been implemented around this formalism: access libraries, morphological processors, etc.\\n\\nMotivation and features\\n\\nWe summarize here some features of our representation language:\\n\\nExpressiveness:  All the information needed in the lexical database can be expressed, and structure can be imposed on it. Linguistic generalizations are captured by grouping related entries in lemmas, or by using the mechanism of  information inheritance.\\n\\nThe information related to a lemma is structured in a tree-shaped feature bundle attached to it. This tree structure is, we think, powerful enough to represent the relevant information, so the more general structure of a directed acyclic graph was discarded. This decision has proved to be right for morphology related information or low level syntactic features.\\n\\nVersatility:  Different implemented  applications  have different lexical interfaces, that are designed in a programming language dependent way. In our approach translation to other representation formats and languages is easily done in a non-ambiguous way.\\n\\nEconomy of expression:  The syntactic overhead needed to structure the information has been reduced to a minimum, without compromising neither the expressive ability nor the non-ambiguity of the syntax of the formalism.\\n\\nThis feature is in permanent conflict with readability, although the latter is not strongly degraded, since source lexical databases are intended to be liable to edition by hand with any text editor.\\n\\nNon redundancy:  Redundant information is kept to a minimum by exploiting the default inheritance devices and the notational abbreviations included, such as value disjunction.\\n\\nInfluence of the Spanish Morphology in the Design\\n\\nThe Spanish language strongly relies on morphology for word formation. This is particularly true for verbs, which have a different word form for each different combination of mood, tense, person and number. For that reason a majority of Spanish verbs have as much as 53 different simple (word) forms.\\n\\nNouns and adjectives (nominals) have also different forms, depending of the combination chosen of gender and number, so a maximum of four different word forms are needed for these part-of-speech lemmas.\\n\\nSo, for any serious natural language processing application built for Spanish, an account of morphology is needed, or at least of inflectional morphology. This is true not only for reducing the size of the lexicon to a manageable level, but also for capturing the linguistic fact that different entries (word forms) are strongly related.\\n\\nMorphological processing is constrained to morpheme concatenation, so its allomorphic variants have to be stored or computed.\\n\\nThe model follows a Graphical Word criterion, that only considers relevant its written form. This criterion requires that additional allomorphs be necessary in some cases, because of diacritical marks, or different surface realization of the same phoneme in different contexts.\\n\\nFeature unification is the information combining device used to select the relevant allomorphic variants to be concatenated. Allomorphs have an attached feature structure DAG, and two or more of them are concatenated only if their feature structures are validated by context-free word formation rules.\\n\\nModels for verbs and nominals are described in order to capture some interesting and well founded linguistic generalizations. These models capture regularities in the inflectional behavior of the Spanish verbs and nouns.\\n\\nSome lexicalized forms are included for very irregular word forms that can not be included in any of the proposed models.\\n\\nThis design leads towards  considering two different lexical levels:\\n\\nSource Lexical Base: This level captures linguistic generalizations by merging related allomorph entries, by considering classes of lemmas or by specifying rules to compute different allomorphic variants. Inflectional morphemes -that constitute a closed group- are also included, as well as the set of lexicalized word forms. This level concerns to agents editing the database.\\n\\nObject Dictionary: This level is related to the computer processing of the lexical knowledge included in the Lexical Base. To facilitate such process, the lexical entries are expanded to different allomorphs that constitute the key entries in this level. It is automatically derived from the Source Lexical Base.\\n\\nAn entry, at any of the levels, is composed of a name or label, that constitutes the access key for that entry, and an attached feature structure. We use the term EN (Entry Name) for the label and ES (Entry Structure) for the attached feature structure.\\n\\nMost entries in the Source Lexical Base are lemmas grouping related word forms. As each one can have different surface realizations, the relevant allomorphs are included as values of particular features in the ES attached to the label.\\n\\nIn the Object Dictionary, the roles of lemma identifiers and allomorphs are different, since for each entry, the EN is one of the allomorphic variants, and the lemma label is kept as the value of a particular feature included in its attached ES.\\n\\nThis paper is concerned with the representation language selected for the Source Lexical Level, that also includes rules for expressing how the mapping  to the Object Dictionary will be achieved. The representational issues for the latter are considered implementation details and will not be considered here.\\n\\nThe Language\\n\\nAs we stated above, each entry in this Source Lexical Base, is composed of an EN, or label, and a ES, or feature structure, restricted to be a tree. The ES has a number of labeled features, that can have an atomic value -a label assigned to that feature-, or a structured one -another feature structure. As a particular case, a string of characters can be encoded as an atomic value for a feature. Value assignment to a feature is achieved by an equation in the form:\\n\\nwhere p is a sequence of one or more blank space separated labels that constitute a path for accessing the feature from the root of the tree. The vi are the atomic values that this particular feature can have. Only one value is permitted if it is of character string type. Paths in the left hand side of the equations are the mechanism provided to define a tree-shaped feature structure, and the multiple-valued features are provided as a notational shorthand for disjunction.\\n\\nThe Source Lexical Base is split into sections, each one headed by a special keyword. An include facility is also provided in order to promote physical division of the Lexical Base into different computer files. The sections that can appear in the Source Lexical Base are reviewed below.\\n\\nMorphemes and words\\n\\nThe morphemes section is intended for the inclusion of inflectional morphemes with a grammatical function. These morphemes usually convey grammatical information such mood, aspect, person and tense (verbs), or gender and number (nouns or adjectives). The entries in this section will pass almost unchanged to the Object Dictionary upon compilation. One example is provided for the verbal ending -bamos, with agreement, tense and mood features, as well as a concatenating category (concat) imposed by the morphological rules, and two features (stt, sut) that restrict possible concatenations for that morpheme:\\n\\nThe words section is intended for lexicalized words that are included as is in the Source Lexical Base. The more frequent clients of this section are very irregular words, usually with an auxiliary function. The entries in this section will also  pass almost unchanged to the Object Dictionary. The section is provided to physically separate morphemes from words, although the behavior of the entries in both sections will be almost the same when compiling the Object Dictionary.\\n\\nClasses\\n\\nInformation inheritance has been widely used in Artificial Intelligence, as an element of some knowledge representation mechanisms, as well as a limited reasoning device. Our language defines classes as bundles of feature-value sets that can be inherited by any particular entry defined to be a member of a class. The entries belonging to a particular class inherit all the feature-value pairs present in their parent class. Inheritance is overridden for those feature-value pairs explicitly stated in the entry. This mechanism (default inheritance) provides a convenient and natural way to express regularities and exceptions. Classes can be members of other classes if desired, so it is possible to build complex inheritance hierarchies that group and optimize the information organization. Multiple inheritance is also allowed, so a priority schema has been adopted to avoid conflicts.\\n\\nA class definition is a label (EN) and a feature structure (ES) attached to it. If the class defined is a member of a set of other classes, these are listed in parenthesis after the EN. This is true for entries in other sections also (words, morphemes and lemmas).\\n\\nAllomorphy rules are usually stated in a class definition. Rule invocation, however, is made when a particular child entry from that class is processed. The EN of such entry acts as the argument for the rule. As an example we show partially one of the verbal models we are using:\\n\\nLemmas\\n\\nA lemma is a grouping of related entries that share common information. Each lemma will be expanded to different entries when the Object Dictionary is compiled. For our purposes a lemma groups the allomorphs needed to build all the inflected forms, not all the possible surface realizations -for verbs, where around 53 word forms are possible, a maximum of eight allomorphs are encoded.\\n\\nFor regular inflection, entries can be very short if the inheritance mechanism is used. For very irregular lemmas, the entry is usually longer, because all the information must be provided inside. We show a very simple example, that extensively uses the inheritance mechanism:\\n\\nAllomorphy Rules\\n\\nAllomorphy rules are declared in a separate section, and are designed to build particular allomorphs for a given lemma entry. Rule invocation is usually done in a class definition, although it can be done in a particular entry in the lemmas section. The examples that illustrated above the classes section had two rule invocations. These always happen as the value for a particular feature, and the value returned by the rule is assigned to such feature. Rule invocation is made by name, preceding it with the special character $. A special identity allomorphy rule invocation is provided as the token $$. As it was said before, any allomorphy rule invocation takes as argument the relevant EN for the entry under consideration, and if invocation takes place in a class definition the argument is the EN of the entry that inherits that feature.\\n\\nRule application is a pattern-matching process. The argument of the rule is matched sequentially against the left hand side of each production in the rule. When a match succeds, the relevant right hand side is returned. If there is no successful match, the rule fails and neither value is returned nor assigned to the feature that invoqued the rule. Patterns in the left hand side of the rule are a sequence of characters and variables -that represent a regular-expression pattern declared in a rule header. When the argument is succesfully matched against the left had side of the current production, variable instantiation takes place. If the right hand side of the production contains that variable, its instantiated value is used to compute the returned value.\\n\\nFormally, a rule contains a name, followed by local variable declarations and one or more productions, whose left and right sides are separated by the special token -]. Variable declarations are assignments -enclosed in brackets- of regular expressions to the variable identifier (some single alphabetic character). Regular expressions are the standard ones of the UNIX operating system, so they will not be discussed here. Variable invocation in the productions are preceded by the special character $.\\n\\nThe following example shows the rule that appeared in an example above. It computes an allomorph from an infinitive form when it finishes in -e Cir, being  C any consonant, by changing e to i and deleting the ir ending. This example has only one production.\\n\\nType Checking\\n\\nThis particular section has been designed to providee some kind of type checking. Open and closed features have to be declared here. For closed features all the possible values have to be declared too. We will show an example:\\n\\nstem is an open feature that can take any atomic value (including character strings), while pers is a closed one, and its legal values are only 1, 2 or 3. agr is a closed feature that can take only a feature structure as its value, and some restrictions are declared over its possible feature components: gender and number, or number and person.\\n\\nThis section is of special interest for consistency checking over the whole source lexical base, for detecting misspellings of feature names and values, and as reference for lexicographer editors. It will be used also by some tools to improve the efficiency of the deliverable products, as closed feature values can be coded since they form a finite set.\\n\\nObject Dictionary generation\\n\\nIn this section of the Source Lexical Base, rules are given for building the Object Dictionary. Each rule is a sequence of tree manipulating operators that can be used to modify the tree structure, filter out or add some branches to it.\\n\\nThe section specifies a set of rules for each of the sections  containing lexical entries (lemmas, words and morphemes). From the point of view of Object Dictionary construction all these three sections are equivalent, and it is because of these rules that they behave differently.\\n\\nThis section is split in three subsections, each one headed by one of the labels LEXEMES, MORPHEMES or WORDS. The rules in each subsection will be applied to the entries defined in the relevant section of the Source Lexical Base. For each rule succesfully applied, a new entry will be generated in the Object Dictionary.\\n\\nEach rule consist of a sequence of equations. The left hand side refers to the entry generated in the Object Dictionary and the right hand side to the entry under consideration in the Source Lexical Base. The special tokens $$ and @ refer to the EN and to the ES respectively. All rules must have a value assigned to $$ and to @, and the rule is successful if an effective value is assigned to $$ at runtime (assigned values might not exist).\\n\\nTree branches can be accessed by path from @, and assignments to non-existing branches are considered tree augmenting. Deleting a branch is done by specifying an incomplete copy: in the right hand side of an equation, after a subtree specification, a sequence of paths to eliminate is written into parenthesis, preceding each one with a minus (-) sign. Rules are invoqued sequentially, and non-monotonically: an equation can override a value assigned by previous equations.\\n\\nFor each possible allomorph that can be included in a lemma entry, a rule should be included in this section. We have not considered iteration to enhance this tree manipulating language to cope with an indeterminate number of allomorphs, because we have always found a manageable number of them.\\n\\nThe example shows that the entries in the words and morphemes sections are just copied. For the lemmas the same set of operations is repeated for each of the possible allomorphs (we show just the relevant rule for the third one). The allomorph is converted to the EN and the older EN becomes the feature lex of the target ES. Some deleting is also done:\\n\\nConclusions\\n\\nBut this framework would be useless if it were computationally intractable. A set of software tools has beed designed around it, setting-up the basis of our lexical platform. Extensive work is being carried out at our site to develop a loosely coupled, highly modular environment that allows to integrate this set of tools. Among them we will cite efficient dictionary access libraries, conversion tools between source and object formats (this includes regexp rule interpretation, multiple inheritance management, etc. ), and morphological analyser and generator.\\n\\nBibliography\\n\\nP-E International (1993). ALEP-1 version 1.1 User's Guide. Preliminary Documentation. Comission of of the European Communities.\\n\\nBriscoe, Ted. (1991). Lexical Issues in Natural Language Processing. In Natural Language and Speech, pp. 39-68. Esprit Basic Research Series Symposium. Springer Verlag.\\n\\nHausser, Roland,\\n\\n(1991).\\n\\nPrinciples of Computational Morphology.\\n\\nUnreferenced report distributed electronically.\\n\\nGoi, Jos M.; Gonzlez, Jos C. and Lpez, Jess. (1993). Especificacin del Formalismo Lxico para ARIES. UPM-DIT-GSI Internal Report.\\n\\nMoreno, Antonio. (1991). Un Modelo Computacional Basado en la Unificacin para el Anlisis y la Generacin de la Morfologa del Espaol. PhD. Thesis.\\n\\nMoreno, Antonio; Goi, Jos M.; Gonzlez, Jos C. and Olmedo, Cristina. (1994). A Morphological Model and Processor for Spanish. UPM-DIT-GSI Internal Report.\\n\\nRitchie, Graeme D.; Pulman, Stephen G.; Black, Alan W. and Russell, Graham J. (1987). A Computational Framework for Lexical Description. Computational Linguistics, vol. 13, n. 3-4, pp.290-307.\\n\\nRussell, Graham J.; Carroll, John  and Warwick-Armstrong, Susan. (1991). Multiple Default Inheritance in a Unification-Based Lexicon. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pp.215-221.\\n\\nShieber, Stuart M. (1986). An Introduction to Unification-Based Approaches to Grammar. CSLI Lecture Notes. Center for the Study of Language and Information. Stanford University.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9507002.xml'}),\n",
       " Document(page_content='PARSING A FLEXIBLE WORD ORDER LANGUAGE\\n\\nA logic formalism is presented which increases the expressive power of the ID/LP format of GPSG by enlarging the inventory of ordering relations and extending the domain of their application to non-siblings. This allows a concise, modular and declarative statement of intricate word order regularities.\\n\\nIntroduction\\n\\nNatural languages exhibit significant word order (WO) variation and intricate ordering rules. Despite the fact that specific languages show less variation and complexity in such rules (e. g. those characterized by either fixed, or totally free, WO), the vast majority of world languages lie somewhere in-between these two extremes (e. g. Steele 1981). Importantly, even the proclaimed examples of rigid WO languages (English) exhibit variation, whereas those with proclaimed total scrambling (Warlpiri; cf. Hale 1981) show restrictions (Kashket 1987). Therefore, we need general grammar formalism, capable of processing \"flexible\" WO (i.e. complex WO regularities, including both extremes).\\n\\nThere seem to be a number of requirements that such a formalism should (try to) fulfil (e. g. Pericliev and Grigorov 1992). Among these stand out the formalism\\'s:\\n\\n(i) Expressive power, i. e. capability of (reasonably) handling complex WO phenomena, or \"flexible\" WO.\\n\\n(ii) Linguistic felicity, i. e. capability of stating concisely and declaratively WO rules in a way maximally approximating linguistic parlance in similar situations.\\n\\n(iii) Modularity, i. e. the separation of constituency rules from the rules pertaining to the linearization of these constituents (for there may be many, and diverse, reasons for wanting linearization (and constituency) rules easily modifiable, incl. the transparency of WO statements, the imprecision of our current knowledge of ordering rules or the wish to tailor a system to a domain with specific WO).\\n\\n(iv) Reversibility, i. e. the ability of a system to be used for both parsing and generation (the reason being that, even if the system is originally intended for a parser, complex WO rules may be conveniently tested in the generation mode; in this sense it is not incidental that e. g. Kay Karttunen 1984 have first constructed a generator, and used it as a tool in testing the (WO) rules of their grammar, and only then have converted it into a parser).\\n\\nIn this paper, we present a logic-based formalism which attempts to satisfy the above requirements. A review shows that most previous approaches to WO within the logic grammars paradigm (Dahl  Abramson 1990) have not been satisfactory. Definite Clause Grammar, DCG, (Pereira  Warren 1980), with their CF-style rules, are not modular (in the sense above), so will have to specify explicitly each ordering of constituents in a separate rule, which results in an intolerably great number of rules in parsing a free WO language (e. g. for 5 constituents, which may freely permute, the number of rules is 5! = 120). Other approaches center around the notion of a \"gap\" (or \"skip\"). In Gapping Grammar (GG), for instance (Dahl  Abramson 1984, esp. Dahl 1984), where a rule with a gap may be viewed as a meta-rule, standing for a set of CF rules, free WO is more economically expressed, however, due the unnaturalness of expressing permutations by gaps, GGs generally are clumsy for expressing flexible WO, WO is not declaratively and modularly expressed, and GGs cannot be used for generation (being besides not efficiently implementable). Another powerful formalism, Contextual Discontinuous Grammar (Saint-Dizier 1988), which overcomes the GGs problems with generative capacity, is also far from being transparent and declarative in expressing WO (e. g. rules with fixed WO are transformed into free order ones by introducing special rules, containing symbols with no linguistic motivation, etc. ).\\n\\nProblems for the ID/LP format\\n\\nIn the Immediate Dominance/Linear Precedence (ID/LP) format of GPSG (Gazdar  Pullum 1981, Gazdar et al. 1985), where the information, concerning constituency (= immediate dominance) and linear order, is separated, WO rules are concisely, declaratively and modularly expressed over the domain of local-trees (i. e. trees of depth 1). E. g. the ID rule A\\n\\nB, C, D, if no linearization restrictions are declared,stands for the mother node expanded into its siblings appearing in any order;declaring the restriction { D [ C } e. g., it stands for the CFG rules { A\\n\\nB D C, A\\n\\nD B C and\\n\\nA\\n\\nD C B }.\\n\\nIt is important to note that in GPSG the linear precedence rules stated for a pair of sibling constituents should be valid for the whole set of grammar rules in which these constituents occur, and not just for some specific rule (this \"global\" empirical constraint on WO is called the Exhaustive Constant Partial Ordering (ECPO) property).\\n\\nHowever, there are problems with ECPO. They may be illustrated with a simple example from Bulgarian. Consider a grammar describing sentences with a reflexive verb and a reflexive particle (the NP-subject and the adverb being optional), responsible for expressions whose English equivalent is e. g. \"(Ivan) shaved himself (yesterday)\".\\n\\n(1)\\n\\nS\\n\\nNP, VP\\n\\n(2)\\n\\nS\\n\\nVP   % omitted subject\\n\\n(3)\\n\\nVP\\n\\nV[refl], Part[refl], Adv\\n\\n(4)\\n\\nVP\\n\\nV[refl], Part[refl]   % omitted adverb\\n\\nFirst, assume we derive a sentence, applying rules (2) and (3). (5a-b) are the only acceptable linearizations of the sister constituents in (3).\\n\\n(5a)\\n\\n(5b) (meaning: (Someone) shaved himself yesterday) LP rules however cannot enforce exactly these orderings because the CFG, corresponding to (5a-b), viz. (6) A\\n\\nB C D\\n\\nA\\n\\nD C B is non-ECPO. Thus, fixing any ordering between any two constituents in (3) will, of necessity, block at least one of the correct orderings (5a-b); alternatively, sanctioning no WO restriction will result in overgeneration, admitting, besides the grammatical (5a-b), 4 ungrammatical permutations. This inability to impose an arbitrary ordering on siblings we will call the ordering-problem of ID/LP grammars.\\n\\nNow assume we derive a sentence, applying rules (1) and (4). The ordering of the siblings, reflexive verb and particle, in (4) now depends on the order of nodes NP and VP higher up in the tree in rule (1):  if NP precedes VP in (1), then the reflexive particle must precede the verb in (4), otherwise it should follow it.\\n\\n(7a)\\n\\n(7b) (meaning: Ivan shaved himself) Again we are in trouble since LP rules cannot impose orderings among non-siblings, their domain of application being just siblings. This we call the domain-problem of ID/LP grammars. It is essential to note that the domain-problem may not be remedied (even if we are inclined to sacrifice linguistic intuitions) by \"flattening\" the tree, e. g. collapsing rules (1) and (4) into (8) S\\n\\nNP, V[refl], Part[refl] Escaping the second problem, thrusts us into the first:  we now cannot properly order the siblings, the CFG, corresponding to (7a-b), being the non-ECPO (6).\\n\\nSporadic counter-evidence for ECPO grammars has been found for some languages like English (the verb-particle construction, Sag 1987, Pollard and Sag 1987), German (complex fronting, Uszkoreit 1985, Engelkamp et al. 1992) and Finnish (the adverb myos \\'also, too\\' Zwicky and Nevis 1986). Bulgarian offers massive counter-evidence (Pericliev 1992b); one major example, the Bulgarian clitic system, we discuss in Section 4.\\n\\nThe formalism\\n\\nEFOG (Extended Flexible word Order Grammar) extends the expressive power of the ID/LP format. First, EFOG introduces further WO restrictions in addition to precedence (enabling it to avoid the ordering-problem), and, second, the formalism extends the domain of application of these WO restrictions (in order to handle the domain-problem).\\n\\nIn the immediate dominance part of rules EFOG has two types of constituents: non-contiguous (notated:  #Node) and contiguous (notated just: Node), where Node is some node. Informally, a contiguous node shows that its daughters form a contiguous sequence, whereas a non-contiguous one allows its daughters to be interspersed among the sisters of this non-contiguous node. E. g. in EFOG notation (using a double arrow for ID rules, small case letters for constants and upper case ones for variables), the grammar of the Latin sentence: Puella bona puerum parvum amat (good girl loves small boy), grammatical in all its 120 permutations and, besides, having discontinuity in the noun phrases, we capture with the following structured EFOG rules with no WO restrictions: s\\n\\n#np(nom), #vp.\\n\\nnp(Case)\\n\\nadj(Case), noun(Case).\\n\\nvp\\n\\nverb, #np(acc). accompanied by the dictionary rules: verb\\n\\n[amat].\\n\\nadj(nom)\\n\\n[bona].\\n\\nadj(acc)\\n\\n[parvum].\\n\\nnoun(nom)\\n\\n[puella].\\n\\nnoun(acc)\\n\\n[puerum]. The non-contiguous nodes allow us to impose an ordering (or to intersperse, as in the above case) all their daughter nodes without having to sacrifice the natural constituencies. It will be clear that this extension of the domain of LP rules (which can go any depth we like), besides ordering between non-siblings, allows an elegant treatment of discontinuities.\\n\\nprecedes (e. g. a [ b)\\n\\nimmediately precedes (a [[ b) (we also maintain the notation, ]and ]], for (immediately) follows; see commentary below)\\n\\nis adjacent (a [] b).\\n\\nis positioned first/last (e. g. first(a, Node), where Node is a node; e. g. first(a, s) designates that a is sentence-initial.\\n\\nConjunction (notated: and)\\n\\nDisjunction (or)\\n\\nNegation (not)\\n\\nImplication (if, e. g. (b ]] a) if (a [ c) )\\n\\nEquivalence (iff, e. g. (b ]] a) iff (a [ c) )\\n\\nIfthenelse (ifthenelse)\\n\\nOur WO restriction language is, of course, partly logically redundant (e. g. immediately precedence may be expressed through precedence and adjacency, and so is the case with the last two of the operators, etc.). However, what is logically is not necessarily psychologically equivalent, and our goal has been to maintain a linguist-friendly notation (cf. requirement (ii) of Section 1). To take just one example, we have \\'after\\' in addition to \\'before\\', since linguists normally speak of precedence of dependent with respect to head word, not vice versa, and hence will use both expressions in respective situations (surely it is not by chance that NLs also have both words).\\n\\nAs a simple example of the ordering possibilities of EFOG, consider the WO Universal 20 (of Greenberg and Hawkins) to the effect that NPs comprising dem(onstrative), num(eral), adj(ective) and noun can appear in that order, or in its mirror-image. We can write a \"universal\" rule enforcing adjacent permutations of all constituents as follows: np\\n\\ndem, num, adj, noun. lp: dem [] num and num [] adj and adj [] noun.\\n\\nBulgarian clitics\\n\\nBulgarian clitics fall into different categories: (1) nominals (short accusative pronouns:  me \"me\", te \"you\", etc. ; short dative pronouns:  mi \"to me\", ti \"to you\", etc. ); (2) verbs (the present tense forms of \"to be\" sam \"am\", si \"(you) are\", etc. ); (3) adjectives (short possessive pronouns:  mi \"my\", ti \"your\", etc. ; short reflexive pronoun:  si \"one\\'s own\"); (4) particles (interrogative li \"do\", reflexive se \"myself/yourself...\", the negative ne \"no(t)\", etc.). They have the distribution of the specific categories they belong to, but show diverse, and quite complex orderings, varying in accordance with the positions of their siblings/non-siblings as well as the position of other clitics appearing in the sentence. In effect, their ordering as a rule cannot be correctly stated in the standard ID/LP format.\\n\\nBy way of illustration, below we present the EFOG version (simplified for expository reasons) of the grammar (1-4) from Section 2 to get the flavour of how we handle the problems mentioned there. The ID rules are as follows (note that the non-contiguous node #vp allows its daughters v(refl), part(refl), and adv to be ordered with respect to np): (1\\') s\\n\\nnp, #vp.\\n\\n(2\\')\\n\\ns\\n\\nvp. % omitted subject (3\\') vp\\n\\nv(refl), part(refl), adv.\\n\\n(4\\')\\n\\nvp\\n\\nv(refl), part(refl).\\n\\n% omitted adverb\\n\\nnp\\n\\n[ivan].\\n\\nv(refl)\\n\\n[brasna].\\n\\npart(refl)\\n\\n[se].\\n\\nadv\\n\\n[vcera].\\n\\nThe WO of v(refl) and part(refl) is as follows. First, the reflexive particle never occurs sentence-initially (information we cannot express in ID/LP); in EFOG we express this as: lp: not(first(part(refl),s)). Secondly, we use the default rule \\'ifthenelse\\' to declare the regularity that the particle in question immediately precedes the verb, unless when the verb occurs sentence-initially, in which case the particle immediately follows it (which is of course also inexpressible in ID/LP): lp: ifthenelse(first(v(refl),s), v(refl) [[ part(refl), part(refl) [[ v(refl)).\\n\\nThese two straightforward LP rules thus are all we need to get exactly the linearizations we want:  those of (5a-b) and (7a-b), as well as all and the only other correct expressions derivable from the ID grammar. These LP rules are also interesting in that they express the overall behaviour of a number of other proclitically behaving clitics (as e.g. those with nominal and verbal nature; see above).\\n\\nBecause of space limitations we cannot enter into further details here. Suffice it to say that EFOG was tested successfully in the description of this very complicated domain  as well as in some other hard ordering problems in Bulgarian.\\n\\nConclusion\\n\\nLogic grammars have generally failed to handle flexible WO in a satisfactory way. We have described a formalism which allows the grammar-writer to express complex WO rules in a language (including discontinuity) in a concise, modular and natural way. EFOG extends the expressive power of the ID/LP format in both allowing complex LP rules and extending their domain of application.\\n\\nEFOG is based on a previous version of the formalism, called FOG (Pericliev and Grigorov 1992), also seeking to overcome the difficulties with the ID/LP format. FOG however looked for different solutions to the problems (e. g. using LP rules attached to each specific ID rule, rather than global ones, which unnecessarily proliferated the LP part of the grammar; or employing flattening rather than having non-contiguous grammar symbols to the same effect). EFOG is also related to FO-TAG (Becker et al. 1991) and the HPSG approach (Engelkamp et al. 1992, Oliva 1992) in extending the domain of applicability of LP rules. A comparisson with these formalisms is beyond the scope of this study; we may only mention here that our inventory of LP relations is larger, and unlike e. g. the latter approach we do not confine to binary branching trees.\\n\\nReferences\\n\\nBecker T., A. Joshi and O. Rambow (1991). Long-distance scrambling and TAG. Fifth Conference of the EACL, Berlin, pp. 21-26. Dahl, V. (1984). More on Gapping Grammars. Proc. of the Intern. Conf. on 5th Generation Computer Systems, ICOT, pp. 669-677. Dahl, V. and H. Abramson (1984). On Gapping Grammars. Proc. 2nd Intern. Conf. on Logic Programming, Uppsala, pp. 77-88. Dahl, V. and H. Abramson (1990). Logic Grammars. Springer. Engelkamp, J., G. Erbach and H. Uszkoreit (1992). Handling linear precedence constraints by unification. Annual Meeting of the ACL. Gazdar, G. and G. Pullum (1981). Subcategorization, constituent order and the notion of \"head\". M. Moortgat et al. (eds.) The Scope of Lexical Rules, Dordrecht, Holland, pp. 107-123. Gazdar, G., E. Klein, G. Pullum and I. Sag (1985). Generalized Phrase Structure Grammar. Harvard, Cambr., Mass. Hale, K. (1983). Warlpiri and the grammar of non-configurational languages. Natural Language and Linguistic Theory, 1, pp. 5-49. Kashket, M. (1987). A GB-based parser for Warlpiri, a free-word order language. MIT AI Laboratory. Kay, M. and L. Karttunen (1984). Parsing a free word order language. D. Dowty et al. (eds.) Natural Language Parsing. The Cambridge ACL series. Oliva, K. (1992). Word order constraints in binary branching syntactic structures. University of Saarland Report (appearing also in COLING\\'92). Pereira, F.C.N. and D.H.D. Warren (1980). Definite Clause Grammars for Natural Language Analysis.\\n\\nArtificial Intelligence, v.13, pp. 231-278. Pericliev, V. (1986). Non-projective con-structions in Bulgarian. 2nd World Congress of Bulgaristics, Sofia, pp. 271-280 (in Bulgarian). Pericliev, V. and I. Ilarionov (1986). Testing the projectivity hypothesis. COLING\\'86, Bonn, pp. 56- 58. Pericliev, V. (1992a). A referent grammar treatment of some problems in the Bulgarian nominal phrase. Studia Linguistica, Stockholm, pp. 49-62. Pericliev, V. (1992b). The ID/LP format:  counter-evidence from Bulgarian, (ms). Pericliev, V. and A. Grigorov (1992). Extending Definite Clause Grammar to handle flexible word order. B. du Boulay et al. (eds.) Artificial Intelligence V, North Holland, pp. 161-170. Pollard C., I. Sag (1987). Information-Based Syntax and Semantics. Vol. 1:  Fundamentals. CSLI Lecture Notes No. 13, Stanford, CA. Sag, I. (1987). Grammatical hierarchy and linear precedence. Syntax and Semantics, v.20, pp. 303- 339. Saint-Dizier, P. (1988). Contextual Discon-tinuous Grammars. Natural Language Understanding and Logic Programming, II, North Holland, pp. 29-43. Steele, S. (1981). Word order variation:  a typological study. G. Greenberg (ed.) Universals of Language, v.4, Stanford. Uszkoreit, H. (1985). Linear precedence in discontinuous constituents: complex fronting in German. SRI International, Technical Note 371. Zwicky, A. (1986). Immediate precedence in GPSG. OSU WPL 32, pp. 133-138.\\n\\nFootnotes\\n\\nThis often results in discontinuities (or non-projectivities). For an automated way of discovering and a description of such constructs in Bulgarian, cf. Pericliev and Ilarionov 1986, and Pericliev 1986. For the difficulties in handling the adjectival clitics in pure DCG, cf. Pericliev 1992a.', metadata={'source': '../data/raw/cmplg-xml/9505007.xml'}),\n",
       " Document(page_content=\"COLLABORATION ON REFERENCE TO OBJECTS THAT ARE NOT MUTUALLY KNOWN\\n\\nIn conversation, a person sometimes has to refer to an object that is not previously known to the other participant. We present a plan-based model of how agents collaborate on reference of this sort. In making a reference, an agent uses the most salient attributes of the referent. In understanding a reference, an agent determines his confidence in its adequacy as a means of identifying the referent. To collaborate, the agents use judgment, suggestion, and elaboration moves to refashion an inadequate referring expression.\\n\\nIntroduction\\n\\nIn conversation, a person sometimes has to refer to an object that is not previously known to the other participant. One particular situation in which this arises is in giving directions. For example: A B funny Go straight ahead until you get to a funny-looking building. The hearer has to understand the reference well enough that when he later reaches the building, he will recognize it as the intended referent.\\n\\nCollaboration on reference\\n\\nClark and Wilkes-Gibbs developed the following process model to explain their findings. To initiate the process, speaker A presents an initial version of a referring expression on which speaker B passes judgment. B can either accept it, reject it, or postpone his decision until later. If B rejects or postpones, then the expression must be refashioned by either A or B.  Refashionings are accomplished in three main ways:  repairing the expression by correcting speech errors, expanding the expression by adding more qualifications, or replacing part or all of the expression with new qualifications. Each judgment/refashioning pair operates on the current referring expression, replacing it with a new one. This process continues until the expression, kept in the participants' common ground, is mutually accepted.\\n\\nThis excerpt from Clark and Wilkes-Gibbs's data illustrates rejection (line 2), replacement (line 2), and acceptance (lines 3 and 4): A B clark Okay, and the next one is the person that looks like they're carrying something and it's sticking out to the left. It looks like a hat that's upside down. The guy that's pointing to the left again? Yeah, pointing to the left, that's it! [laughs] Okay.\\n\\nIf the recipient finds the initial referring expression plan invalid, then the agents will collaborate in its repair. Heeman and Hirst used plan repair techniques to refashion an expression, and used discourse plans, or meta-plans, to communicate the changes to it. Thus, a collaborative dialogue is modeled in terms of the evolution of the referring plan.\\n\\nFirst, an agent must communicate that she has not understood a plan. Depending on how the referring plan constrains the choice of referent, she constructs an instance of either reject-plan or postpone-plan, whose resulting surface speech actions are s-reject and s-postpone respectively. Next, one agent or the other must refashion the referring expression plan in the context of the judgment by either replacing some of its actions (by using replace-plan) or by adding new actions to it (by using expand-plan). The result of both plans is the surface speech action s-actions.\\n\\nBecause the model can play the role of both the initiator and the recipient, and because it can perform both plan construction and inference, two copies of the model can converse with one another, acting alternately as speaker and hearer. Acting as hearer, one copy of the system performs plan inference on each set of surface speech actions that it observes, and updates the state of the collaboration. It then switches roles to become the speaker, and looks for a goal to adopt, and constructs a plan that achieves it. After responding with the surface actions of the plan, it updates the state of the collaboration, presupposing that the other copy will accept the plan. The system repeats the process until it can find no more goals to adopt, at which time it switches back to being the hearer and waits for a response from the other copy.\\n\\nConfidence and salience\\n\\nThe basis of our model is that the hearer can accept a referring expression plan if (1) the plan contains a description that is useful for making an identification plan that the hearer can execute to identify the referent, and (2) the hearer is confident that the identification plan is adequate.\\n\\nIn our model, each agent associates a numeric confidence value with each of the attributes in the referring expression, and by composing these, computes a level of confidence in the adequacy of the complete referring expression plan that can be interpreted as ranging from low confidence to high confidence. The present composition function is simple addition, but one could envision more complex systems to compute confidence, such as an algebra of confidence or a non-numeric system. If the overall confidence value exceeds some set value, the agent's confidence threshold, then the agent believes the plan is adequate. That is, if the agent is the initiator, she believes that the other will be able to understand the reference; if the agent is the other, he believes that he has understood the reference.\\n\\nEach agent has his own beliefs about salience. It is the difference in their beliefs that leads to the necessity for collaboration on reference. Ideally, the initiator should construct referring expressions with the recipients' (believed) beliefs about salience in mind, but we have chosen to avoid this complexity by making the simplifying assumption that the initiator is an expert (and thus knows best what is salient).\\n\\nPlans for referring\\n\\nAn agent uses his salience hierarchy for two related purposes: the first to determine what is salient in a particular situation, and the second to determine the adequacy of a description. So, the hierarchy is accessed during both plan construction and plan inference.\\n\\nDuring plan inference, the salience hierarchy is used when evaluating a recognized plan. The mental actions in the intermediate plans determine the confidence values of each attribute (from the hearer's salience hierarchy), and add them up. The final constraint in the plan makes sure that the hearer's confidence threshold is exceeded. Thus, judging the adequacy of a referring expression plan falls out of the regular plan evaluation process. If the final constraint does not hold, then the invalidity is noted so that the plan can be operated on appropriately by the discourse plans.\\n\\nSuggestion and elaboration\\n\\nIf the recipient is not confident in the adequacy of the plan, he uses an instance of postpone-plan to inform the initiator that he is not confident of its adequacy, thereby causing the initiator to raise her own confidence threshold. Now, although he cannot refashion the expression himself, he does have the ability to help the initiator by suggesting a good way to expand it; suggestion is a conversational move in which an agent suggests a new attribute that he deems would increase his confidence in the expression's adequacy if the expression were expanded to include the attribute. Continuing with the example, if the hearer were not confident about the adequacy of the funny-looking building, he might suggest that the initiator use height (as well as architectural style), by asking Is it tall?. From this suggestion the initiator might expand her expression to the tall funny-looking building. So, in our sense, a suggestion is an illocutionary act of questioning; along with actually suggesting a way to expand a plan, the agent is asking whether or not the referent has the suggested attribute.\\n\\nTo decide what suggestion to make, the agent uses an instance of suggest-expand-plan, which has a mental action in its decomposition that chooses the attribute that he believes is the most salient that has not been used already. The result of the plan is the surface speech action, s-suggest, that communicates the suggestion.\\n\\nHowever, only the initiator of the referring expression can actually elaborate a referring expression, because only she has the knowledge to do so. Depending on whether the hearer of the expression makes a suggestion or not, the initiator has two options when elaborating a plan. If no suggestion was made, then she can expand the plan according to her own beliefs about the referent's attributes and their salience. On the other hand, if a suggestion was made, she could instead attempt to expand the plan by affirming or denying the attribute suggested. If possible, she should use the suggestion to elaborate the plan, thus avoiding unwanted conversational implicature, but its use may not be enough to make the plan adequate.\\n\\nThe decomposition of expand-plan calls the plan constructor with the goal of constructing a modifiers schema and with the suggested attribute as input--in a sense, continuing the construction of the initial referring plan. The plan constructor attempts to find a plan with the surface speech actions for the suggested attribute in its yield, but this might not be possible. In any case, the speaker constructs an expansion that will make the plan adequate according to her beliefs.\\n\\nThe response to a suggestion depends, obviously, on whether or not the suggestion was used to expand the plan. The speaker can (1) affirm that the plan was expanded with the suggestion by using the s-affirm speech act; (2) affirm that the suggestion was used, along with additional attributes that weren't suggested, by using s-affirm and s-actions; or (3) deny the suggestion with s-deny, and inform the other by s-actions as to how the plan was expanded.\\n\\nBy repeatedly using the postponement, elaboration, and suggestion moves, the two agents collaborate through discourse on refashioning the referring expression until they mutually believe that the recipient is confident that it is adequate.\\n\\nExample\\n\\nConclusion\\n\\nWhen an agent refers to a particular object that is not previously known to another agent, she has the intention that the agent be able to identify the object (when it is possible to do so) by means of the referring expression. Because of the inevitable differences in their beliefs about the world--specifically about what is salient--the agents may have to collaborate to make the expression adequate.\\n\\nWe have implemented a computational plan-based model that accounts for the collaborative nature of reference in the domain of interactive direction-giving. An agent constructs a referring expression plan by using the referent's most salient features. An agent understands a reference once he is confident in the adequacy of its (inferred) plan as a means of identifying the referent. To collaborate, the agents use judgment, suggestion, and elaboration moves to refashion the referring expression until they mutually believe that the recipient has understood.\\n\\nAcknowledgments\\n\\nOur work is supported by the University of Toronto and by the Natural Sciences and Engineering Research Council of Canada. We are grateful to Peter Heeman, Graeme Hirst, and Jeffrey Siskind for many helpful discussions.\\n\\nBibliography\\n\\nDouglas E. Appelt\\n\\n(1985a).\\n\\nPlanning English referring expressions.\\n\\nArtificial Intelligence, 26(1):1\\n\\n\\n\\n33.\\n\\nDouglas E. Appelt (1985b). Some pragmatic issues in the planning of definite and indefinite noun phrases. In Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics, pages 198-203.\\n\\nDouglas E. Appelt and Amichai Kronfeld (1987). A computational model of referring. In Proceedings of the Tenth International Joint Conference on Artificial Intelligence (IJCAI-87), pages 640-647.\\n\\nHerbert H. Clark and Deanna Wilkes-Gibbs (1986). Referring as a collaborative process. Cognition, 22:1-39. Reprinted in Cohen, P. R., Morgan, J., and Pollack, M. E., editors. (1990). Intentions in Communication. MIT Press. pages 463-493.\\n\\nPhilip R. Cohen (1981). The need for referent identification as a planned action. In Proceedings of the Seventh International Joint Conference on Artificial Intelligence (IJCAI-81), pages 31-36.\\n\\nJames Raymond Davis (1989). Back Seat Driver: Voice Assisted Automobile Navigation. Ph.D. thesis, Massachusetts Institute of Technology.\\n\\nAnn S. Devlin (1976). The ``small town'' cognitive map: Adjusting to a new environment. In G.T. Moore and R.G. Golledge, editors, Environmental Knowing: Theories, Research and Methods. Dowden, Hutchinson and Ross.\\n\\nPhilip G. Edmonds (1993). A computational model of collaboration on reference in direction-giving dialogues. M.Sc. thesis, published as technical report CSRI-289, Department of Computer Science, University of Toronto.\\n\\nPeter A. Heeman and Graeme Hirst (1992). Collaborating on referring expressions. Technical Report TR 435, Computer Science Dept., Univ. of Rochester, Rochester, New York.\\n\\nKevin Lynch (1960). The Image of the City. MIT Press.\\n\\nMartha Pollack (1990). Plans as complex mental attitudes. In P. R. Cohen, J. Morgan, and M. E. Pollack, editors,   Intentions in Communication, pages 77-103. MIT Press.\\n\\nGeorge Psathas (1991). The structure of direction-giving in interaction. In Deirdre Boden and Don H. Zimmerman, editors, Talk and Social Structure, pages 195-216. Polity Press.\\n\\nEhud Reiter and Robert Dale (1992). A fast algorithm for the generation of referring expressions. In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), pages 232-238.\\n\\nJohn. R. Searle (1969). Speech Acts: An Essay in the Philosophy of Language. Cambridge University Press.\\n\\nFootnotes\\n\\nThese models assume that all agents have identical beliefs, which is clearly insufficient for modeling collaborative dialogue. Given information about salience, we could construct such a hierarchy, but we do not presume that it would be easy to know what is salient. In Heeman and Hirst's model, an attribute has to be mutually believed to be used. Here, mutual belief is not possible because the hearer has no knowledge of the referent, but mutual belief is an intended effect of using this plan. Recall that she raised her confidence threshold as a result of the hearer's postponement move, so now she must meet the new threshold.\", metadata={'source': '../data/raw/cmplg-xml/9405013.xml'}),\n",
       " Document(page_content=\"An NLP Approach to a Specific Type of Texts: Car Accident Reports\\n\\nThe work reported here is the result of a study done within a larger project on the ``Semantics of Natural Languages'' viewed from the field of Artificial Intelligence and Computational Linguistics. In this project, we have chosen a corpus of insurance claim reports. These texts deal with a relatively circumscribed domain, that of road traffic, thereby limiting the extra-linguistic knowledge necessary to understand them. Moreover, these texts present a number of very specific characteristics, insofar as they are written in a quasi-institutional setting which imposes many constraints on their production. We first determine what these constraints are in order to then show how they provide the writer with the means to create as succint a text as possible, and in a symmetric way, how they provide the reader with the means to interpret the text and to distinguish between its factual and argumentative aspects.\\n\\nCharacteristics of the texts\\n\\n(1) Text Parameters A. the text involves at least two participants, generally two vehicles, one of which is the author's;\\n\\nB. the text is obligatorily short, at most one paragraph;\\n\\nC. by definition, the text is a narration in which an accident takes place;\\n\\nD. the text is sent to the author's insurance company.\\n\\nBeyond these four parameters which are determined by the nature of the reports, we also find in this text presuppositions due to the particular domain involved, the ``road'' domain. This domain-specific knowledge, which is part of the more general context C, is called here K.  K concerns vehicles, vehicle motions, traffic rules, the usual behavior of drivers and pedestrians, and also some elements of ``naive'' geometry.\\n\\nParameter D has a special bearing in so far as the writers know that the insurance agents must pass a judgement on their behavior and will determine their share of responsibility in the accident. Necessarily, the authors of those reports, while supposedly describing in an impartial way the different events which have occurred, will attempt to lessen their responsability. The texts thus present many instances of argumentative devices, whose usage forms part of the more general knowledge of the language conventions, LC. In a symmetric way, the reader, i.e. the insurance agent, must untangle the factual description from the argumentative presentation of the events.\\n\\n(2) The Writer's Problem: The writer W knows the factual content P corresponding to the circumstances of the accident and wants to convey it through a text T.  W must then choose a T such that (a) it will allow a reader R to rediscover P, and (b) it will minimize W's responsibility.\\n\\n(3) The Reader's Problem: The reader R knows the language conventions LC and a part of the context C.  R must then determine (a) the factual content P of the text T and (b) the argumentation presented by its writer W.\\n\\nThese two symmetrical tasks are thus both composed of a factual and an argumentative part. These two parts also coincide with the two goals we can define for an NLP approach to understanding and processing these texts. At the first level, we try to extract from the text the objective content corresponding to a factual analysis in order to recreate the event:  ``What happened? What real world events concerning the motions of these vehicles or the scene geometry actually occurred?''\\n\\nAt the second level, we take into account the nature and intent of the text. Our problem is then to uncover the argumentative devices used by the writer and to determine how they can be used by the reader, and later by our system, in interpreting the texts.\\n\\nFactual content of the texts\\n\\nParameter A\\n\\nParameter A (the fact that car accidents usually involve two participants, most often two vehicles) is used to infer the identity of some entities in the texts, or to establish coreference between two entities.\\n\\nThere is a specific naming convention in French insurance claim reports for the vehicles involved in an accident:  claimants must refer to their own vehicle as ``A'' and to their opponent's as ``B''. This convention, although it is part of the shared knowledge about LC, is not always followed, and indeed it seems to be a burden for W.  The reason is probably that a stereotypical description using only labels ``A'' and ``B'' for the vehicles involved sounds very neutral, such as could have been made by any independant observer of the accident, while in fact W was directly involved in the accident, and is thus personnally implicated, as a person endowed with awareness and intentionality. Thus, the authors often do not seem to be able to choose between a narrative style using the first person (``I'') and a descriptive style using the third person (``vehicle A''). Most of the texts are not homogeneous in this respect and combine the two styles, as if there was a struggle, probably unconscious, between a spontaneous narration of the different events and a stereotyped description using the convention.\\n\\n(4) Vehicle A waiting and stopped at the Pont de Levallois lights. Vehicle B arrived and hit  my left side mirror with its right side mirror.\\n\\n(5) b. Vehicle B wanted to turn right\\n\\nb. Being momentarily stopped in the right lane on Boulevard des Italiens, I had switched my blinker on; I was at a stop and getting ready to change lanes. Vehicle B coming from my left squeezed too close to me and damaged the whole left front side. (T7)\\n\\n(6) b. One of the cars in front of me opened its right front door (T3)\\n\\nb. My husband had entered the intersection when Mr. X's car hit the front of the vehicle.\\n\\n(7)\\n\\nb.\\n\\nmy bumper (T11)\\n\\nb. Je roulais (I was driving, literally I was rolling)\\n\\nin I had switched on my blinker, the referent of I is the driver;\\n\\nbeing stopped can be understood with I referring to the driver as well as to the vehicle;\\n\\nin the last sentence, the word me must refer to the vehicle:  from this text, R would never conclude that the driver's left cheek had been bruised.\\n\\nParameter B\\n\\nWhen setting to the task of writing such a report, the writer knows parameter B, the constraint that only about a paragraph may be used to relate the accident. At the same time, W must not forget any important information whose absence would prevent R from discovering the correct content P.\\n\\n(8) the road on which the intense traffic is going one-way in two lanes; (T5)\\n\\nThis constraint on the choice of information to give, which we call ``W's selection problem'' and which we will later exploit to infer some argumentative points, is part of the wider language conventions LC and constitutes a ``meta-knowledge'', essential for the success of communication.\\n\\nBecause of ``W's selection problem'', W will generally mention an event or an entity only in case its presence cannot be deduced from K or from other types of shared background knowledge, and only in case an explicit reference is absolutely necessary to understand the text. From this, it follows that the number of entities introduced in the text will be kept to a minimum. We can schematize this ``Minimality Assumption'' as follows:\\n\\n(9) Minimality Assumption: W's selection problem + Maxim of Quantity -] minimal number of entities introduced\\n\\n(10) I was driving on the right hand side of the road when a vehicle arriving in front of me in the curve was completely thrown off course. Keeping as close as possible to the right, I wasn't able to avoid the car which was coming with great speed. (T8)\\n\\nThe coreference is allowed first by the use of two compatible terms: indeed a car is a particular type of vehicle. This fact can be extracted from a hierarchy of concepts which is part of the background knowledge K (a car is the most typical kind of vehicle). Secondly, this coreference is licensed by the use of the definite article, which allows the inference that the entity has already been mentioned. Finally, it is confirmed by the ``Minimality Assumption'', which prevents the introduction of a third vehicle which would not play any role in the scene.\\n\\nParameter C\\n\\n(11) We were in Saint-Ouen, I was surprised by the person who braked in front of me, not being able to change lanes, and the road being wet, I couldn't stop completely in time (T15)\\n\\nWe can see here the effect of Parameter C:  since these texts are accident reports, the series of events they relate must by default contain an accident. The interpretation of the texts often requires the reconstruction of an impact between the two vehicles, otherwise the incident which is described would not warrant the existence of the report.\\n\\nArgumentation\\n\\nSo far, we have examined the texts from a purely factual point of view. Now, we take into account the argumentative aspect of these texts. Indeed, the authors know that these few lines, meant for their insurance company, may contribute to the final decision about their share of legal and financial liability. So, they will try to minimize their own responsibility. There are two ways W can justify his own behavior and make excuses for it:\\n\\nA. trying to push the blame onto his opponent by accusing him of an abnormal behavior;\\n\\nB. contrasting what was expected and what happened in reality, by invoking unforeseable circumstances.\\n\\nWith either strategy, W must first show that he has done everything that was required in the given circumstances and will always try to appear as innocent as possible.\\n\\nStrategy A: Blaming the other driver\\n\\n(12) The driver of vehicle B passed me on the right. (T11)\\n\\n(13) I was driving in my vehicle A in the right lane reserved for vehicles going straight ahead. Vehicle B was driving in the left lane reserved for vehicles going left (ground markings with arrows). It cut back in on my vehicle. (T12)\\n\\n(14) e. the latter turned left, forcing me to steer left to avoid it. (T2)\\n\\ne. According to the witness who was following me, the driver of vehicle B was doing a slalom between the cars. (T11)\\n\\ne. A vehicle with full white headlights blinding us struck us with great speed in the back of the vehicle, taking us into a series of barrel rolls before the vehicle stopped in a ditch. (T10)\\n\\ne. Vehicle B coming from my left, I find myself at the intersection, at moderate speed, about 40 km/h, when vehicle B hits my vehicle, and denies me the right-of-way from the right. (T4)\\n\\ne. at that moment vehicle B passed me with great speed (T9)\\n\\nStrategy B: Blaming unforeseable circumstances\\n\\nHere, the indications are mostly at the lexical level (e.g. tre surpris ``to be surprised'') and make frequent use of the negation.\\n\\n(15) b. I was surprised by the person who braked in front of me, not being able to change lanes, and the road being wet (T15)\\n\\nb. I didn't expect that a driver would wish to pass me for there weren't two lanes marked on the portion of the road where I was stopped. (T5)\\n\\nThe use of negation is also a favorite clue to indicate an opposition between what should have happened and what actually occurred.\\n\\n(16) b. I wasn't able to avoid the car which was coming with great speed. (T8)\\n\\nb. and the road being wet, I wasn't able to stop completely in time. (T15)\\n\\n(17) on impact, and because of the slippery pavement, my vehicle skids, and hits the metal railing around a tree, whence a second front impact. (T4)\\n\\n(18) I was driving at about 45 km/h in a small one-way street where cars were parked on both sides. Popping suddenly on my right coming out of a private building garage, Mrs.Glorieux's vehicle was at a very short distance from my vehicle; passage being impossible: surprised, I immediately put the brakes on but the impact was unavoidable. (T14)\\n\\nMorover, the authors may choose to describe only that part of reality which is in their favor, and the reader must thus be able to reconstruct the items that were left out (intentionally or not).\\n\\n(19) c. but I hit the second car which hadn't yet gone through the stop-sign. (T1)\\n\\nc. I wasn't able to avoid the car which was coming with great speed (T8)\\n\\nc. at that moment vehicle B passed me with great speed (T9)\\n\\nResolving Ambiguity and Drawing Inferences\\n\\nThese texts provide a number of examples of clearcut ambiguity between two situations A and B, which an argumentative type of justification helps resolve. The question that allows resolving the ambiguity is: ``What advantage would there be for W in implying situation A? or in implying situation B?''. We go in more details into some examples.\\n\\nLexical Ambiguity\\n\\n(20) Je roulais sur la partie droite de la chausse (T8)\\n\\nI was driving on the (right-hand side)/(straight portion) of the road\\n\\nHere, even though the whole text can also be interpreted with the droite/straight meaning, the droite/right interpretation is more plausible. However, only an argumentative type of reasoning can lead R to prefer the latter.\\n\\nSince the fact that in France one drives on the right is well-known, in specifying that he was driving on the right side of the road, W violates the Maxim of Quantity (i.e. not to say anything superfluous) and therefore must be taken as intending to convey some other information. In this case, it must be in order to assert that his behavior was conforming to the ``Rules of the Road'', which is a pertinent fact to mention. Here, informational redundancy by itself carries some information which allows inference.\\n\\nWe can thus formulate the following rule:\\n\\n(21) In case of ambiguity, prefer the interpretation which allows R to infer a ``correct behavior'' on W's part.\\n\\nTime Reference Ambiguity\\n\\n(22) Being momentarily stopped in the right lane on Boulevard des Italiens, I had switched my blinker on; I was at a stop and getting ready to change lanes. Vehicle B coming from my left squeezed too close to me and damaged the whole left front side. (T7)\\n\\nThe pluperfect implies that the process being talked about is perceived with another past event as a point of reference, which may not yet have been mentioned (the situation is exactly parallel in both French and English). Here, two different referential situations can be envisaged, with two different consequences:\\n\\nIf the accident itself is chosen as the point of reference, switching the blinker on signals a future change of lanes. It must therefore be the left blinker. This conclusion requires geometrical reasoning:  ``If X is stopped in the right lane and if X wants to change lanes, X can only go left''.\\n\\nIf the time of stopping is chosen as the point of reference, switching the blinker on is prior to the time of stopping and thus signals it. It must then be the right blinker, since the vehicle is in the right lane.\\n\\nAction or Intention?\\n\\nSometimes, the problem for R is to determine whether an action presented as an intended future event has remained at a purely intentional level or whether actions have been taken to attain it. For instance, when the intended action belongs to a script with sequential steps, the question arises whether some of the preparatory actions belonging to the script have already been accomplished.\\n\\nIn the inchoative to be about to interpretation, the action of ``switching the blinker on'' is an event independent of ``changing lanes''; in the agentive to prepare for interpretation, that same action corresponds to one of the preparatory acts. But more crucially, in the agentive interpretation, W may already have started changing lanes and then probably would be at fault, while in the inchoative reading, W would still be stopped and would be innocent.\\n\\n(23) Wanting to pass a hauler with its right blinker on, the latter turned left, forcing me to steer left to avoid it. The car skidded on the wet pavement and struck a sidewalk then a fence straight ahead. The truck driver had indeed switched on his left blinker, but the trailer was inverting the signal to the right. Not having touched me, the driver declared himself unconcerned by the situation and refused to draw a report. Having left my car to call a mechanic, I came back to find it with the right back door bashed in with no note left by the guilty party. (T2)\\n\\n(24) I was stopped at the intersection wishing to take the road on which the intense traffic is going one-way in two lanes; as the last vehicle of the flow was coming, I wanted to enter the second lane, leaving the first one free for it. The moment I started, I heard the shock in the back; I wasn't expecting a driver would wish to pass me for there weren't two lanes marked on the portion of the road where I was stopped. (T5)\\n\\nConclusion\\n\\nIn this paper we have tried to show the importance of situational, cultural and textual presuppositions from the point of view of both the writer W and the reader R.  As this work constitutes a first step in the study of natural language semantics in the context of an NLP project, the approach adopted here is an attempt to automate the process of understanding these texts and deriving inferences from them. Crucial issues for NLP are how to define and describe the different types of knowledge involved in the processes of writing and reading texts, and how to establish rules that mimic the reasoning involved in these activities.\\n\\nHere, we take advantage of the specificity of the texts - the authors narrate events leading to a car accident while trying to lessen their responsability - to circumscribe the type of knowledge required and to give some rules of interpretation, valid for this type of text, in this type of context. We have determined four parameters, and two types of knowledge necessary for both the production and the interpretation of these reports. Two of these parameters (A and C) and K belong to the factual domain, while the other two parameters (B and D) and LC pertain to discourse.\\n\\nFor clarity of exposition, we have distinguished these two types of characteristics in our texts by examining first the factual content of the texts and then their argumentative aspect, but it is not always easy to separate them and we can also ask whether there actually can be a purely factual reading of a text that would not take into account discourse and argumentation phenomena.\\n\\nIn any case, even if such a reading existed, it would be insufficient to account for the inferences that the reader can and must make from the textual data in order to reconstruct the events described by the text and to determine each participant's role in it. We have shown for instance that inferences based on argumentation could often help the reader clarify the text or choose between several interpretations. We find here the well-known difficulty of precisely defining the border between semantics and pragmatics.\\n\\nIt would be interesting to analyze the two corresponding texts by the two opponents reporting the same accident in order to establish which part of the information is objectively factual and shared by both texts, and which part of the information is argumentatively biased, thus better distinguishing the subjective part of both discourses. The omission of information, which was mentioned as one of the argumentative devices on the part of W and as a basis for inference on the part of R, would then become an even more important factor in the analysis. Very few such pairs of texts are available, but in the continuation of this project, we may try to do some further work based on these.\\n\\nLastly, we have shown that some inferences rely on assessing the relevance or the quality/quantity of the information given. This assessment itself refers to a norm which is shared by the community of speakers and thus belongs to LC. However, it remains extremely difficult to define this norm in advance and this type of inference, though crucial for language understanding, still appears beyond what is currently possible in NLP.\\n\\nBibliography\\n\\nBarwise, J. (1988). ``On the circonstancial relation between meaning and content'' in The situation in logic. CSLI n017, pp59-76.\\n\\nDucrot, O. (1972). Dire et ne pas dire. Principes de smantique linguistique. Paris: Hermann\\n\\nEstival, D.  F. Gayral (to appear) ``Contexte et Infrence''. Journe ATALA:  Inter-PRC Smantique des Langues Naturelles, Paris, November 1993.  to appear in Traitement Automatique des Langues,  vol.35.\\n\\nGayral, F. (1992). Smantique du langage naturel et profondeur variable : une premire approche. Thse de l'universit Paris-Nord, LIPN, Villetaneuse.\\n\\nGrice, H.P. (1975). `` Logic and conversation'', in Syntax and Semantics. New York: Academic Press. pp.41-58.\\n\\nRapport d'activit du groupe de travail inter-PRC (IA et CHM). (1990). Universit de Nancy.\\n\\nKerbrat-Orecchioni, C. (1986). L'nonciation de la subjectivit dans le langage. Paris:  A. Colin.\\n\\nPustejovsky, J. (1989). ``Type Coercion and Selection''. Proceedings of WCCFL VIII, Vancouver.\\n\\nSchank, R.C. R.P. Abelson (1977). Scripts, Plans, Goals and Understanding. Lawrence Erlbaum Associates, Hillsdale, New Jersey.\\n\\nFootnotes\\n\\nThe absence of information is as meaningful as its presence, but is much harder to assess, since in order to bring it to light, we need a basis for comparison. In this context, the comparison could be provided by the opponent's accident report; although difficult to achieve, this would constitute an interesting study. See [] for a more detailed presentation and more examples of this type of inferences permitted in our texts. If the adjective droite means straight, its opposite is then courbe/curved, if it means right, the opposite is then gauche/left. The semantic interpretation of the verb itself can also help in making the choice, see next section.\", metadata={'source': '../data/raw/cmplg-xml/9502032.xml'}),\n",
       " Document(page_content=\"An Algorithm to Co-Ordinate Anaphora Resolution and PPS Disambiguation Process\\n\\nBoth anaphora resolution and prepositional phrase (PP) attachment are the most frequent ambiguities in natural language processing. Several methods have been proposed to deal with each phenomenon separately, however none of proposed systems has considered the way of dealing both phenomena. We tackle this issue here, proposing an algorithm to co-ordinate the treatment of these two problems efficiently, i.e., the aim is also to exploit at each step all the results that each component can provide.\\n\\nIntroduction\\n\\nSeveral methods have been proposed to deal with anaphora resolution and prepositional phrase (PP) attachment phenomenon and separately, so that the literature is very abundant : for PPs see e.g., (Frazier and Fodor, 1979; Hobbs, 1990; Wilks and Huang, 1985), and for anaphora see e.g., (Carter, 1986; Reinhart, 1983; Sidner, 1983). However none of these methods has considered the way of dealing both phenomena in the same concrete system.\\n\\nWe propose in this paper an algorithm that deals with both phenomena, in the same analyser. The anaphora module pertains to the recent methods, uses a set of resolution rules based on the focusing approach, see (Sidner, 1983). These rules are applied to the conceptual representation and their output is a set of candidate antecedents. Concerning the PPs, unattached prepositions involve empty or unfilled roles in the Conceptual Structures (CSs), expressed in a frame-based language (Zarri, 1992). The disambiguation procedure aims at filling the empty roles using attachment rules.\\n\\nThis work was accomplished in the context of COBALT project (LRE 61-011 ), dealing with financial news. A detailed discussion about both procedures of anaphora resolution and PP attachment is largely developed in (Azzam, 1994).\\n\\nThe algorithm\\n\\nTwo of the main principles of the algorithm are :\\n\\na) The algorithm is applied on the text sentence by sentence, i.e. the ambiguities of the previous sentences have already been considered (resolved or not).\\n\\nb) The anaphora procedure skips the resolution of a given anaphor when this anaphor is preceded by an unattached preposition. This is because the resolution rules may have an empty role as a parameter, due to this unattached preposition. The resolution of the anaphor is then postponed to the second phase of anaphora resolution.\\n\\nThe proposed procedure is based on successive calls to the anaphora module and to the PP attachment module. The output of each call is a set of CSs that represent the intermediate results exchanged between each call and on which both modules operate in turn. The aim is to fill the unfilled roles in the CSs, due to anaphora or unattached PPs. To summarize the algorithm is:\\n\\n1) Apply the anaphora module first.\\n\\n2) Apply the PP attachment procedure.\\n\\n3) If some anaphora are left unresolved, apply the anaphora module again.\\n\\n4) If there are still unattached PPs, apply the attachment procedure again.\\n\\n5) Repeat (3) and (4), until all PPs and anaphors are treated.\\n\\nThe order in which the two modules are called is based on efficiency deduced from statistical data performed on  COBALT corpuses.\\n\\nThree main cases are faced by the algorithm :\\n\\na) When the anaphor occurs before a given preposition in the sentence, its resolution does not depend on where the preposition is to be attached (except for cataphors that are quite rare). In this case the anaphora module can be applied before the attachment procedure.\\n\\nThe example 1 below shows that the resolution of the anaphoric pronoun that must be performed first and that the PP starting with of be attached later.\\n\\n(1) The sale of Credito was first proposed last August and that of BCI late last year.\\n\\nb) When the anaphor occurs after one or several unattached prepositions, it could be an intra-sentential anaphor (i.e. referring to an entity in the same sentence), then its resolution may depend on one of the previous prepositional phrases. In this case, the resolution of the anaphora is postponed to a next call of the anaphora module according to principle b) stated above.\\n\\nc) When the anaphor is included in a PP (particular case of b), PP attachment rules need semantic information about the ``object'' of the PP; when it is a pronoun, no semantic information is available, so that the attachment rules can not be applied. The anaphoric pronouns have to be resolved first, so as to determine what semantic class they refer to ; the PP attachment procedure can then be applied. When a sequence contains more than two such PPs, i.e., with anaphors as objects, the length of a cycle is more than 4.\\n\\nAn example\\n\\n(2) UPHB shares have been suspended since October 29 at the firm's request following a surge in its share price on a takeover rumour.\\n\\nThe pronoun its can not be resolved by the anaphora resolution\\n\\nmodule because it is preceded by unattached PPs ; its resolution is\\n\\nskipped.\\n\\nThe PP attachment procedure is then called to determine the\\n\\nattachment of since and at while the object of the\\n\\nin PP comprises an\\n\\nanaphoric pronoun its (case c) and the on PP is\\n\\npreceded by its. The attachment of both PPs is then skipped.\\n\\nThe anaphora module is called again to resolve the anaphoric pronoun\\n\\nits, which is possible, in this example, since the previous PPs\\n\\nhave\\n\\nbeen attached and there is no anaphors before.\\n\\nFinally, the PP attachment procedure has to be called again for the\\n\\nin and on PPs.\\n\\nNotice that even if each module is called several times, there is no redundancy in the processing. The algorithm should be considered as the splitting of both anaphora resolution and PP attachment procedures into several phases and not as the repetition of each procedure.\\n\\nConclusion\\n\\nThe objective was to emphasise more than it has been done until now, the fact that PP attachment and anaphora resolution could interact in the same system in order to produce a complete conceptual analysis, instead of slowing down each other. The algorithm we proposed in this paper, is independent of the used approaches in both anaphora and attachment modules. It concerns rather the way of managing the interaction between the two modules.\\n\\nOur actual work addresses more the problems inside each module. The attachment module has been implemented at 99%. Presently we are working on the extension of the anaphora module particularly to deal also with the anaphoric definite noun phrases.\\n\\nReferences\\n\\nAzzam, S. 1994. CLAM COBALT conceptual analyser (COBALT Tech. Report Del6.2). CRIL Ingénierie.\\n\\nCarter, D. 1987. Interpreting Anaphors in natural language Texts. Chichester: Ellis Horwood.\\n\\nFrazier, L. and Fodor, J. 1979. The sausage machine: A New Two-Stage Parsing Model, Cognition, 6.\\n\\nHobbs, J.R., and Bear, J. 1990. Two Principles of Parse Reference in Proceedings of the 13th International Conference on Computational Linguistics - COLING/90, vol. 3, Karlgren, H., ed. Helsinki:  University  Press.\\n\\nReinhart, T. 1983. Anaphora and Semantic Interpretation. London : Croom Helm.\\n\\nSidner, C.L. 1983. Focusing for Interpretation of pronouns. American Journal of Computational Linguistics, 7,  217-231.\\n\\nWilks, Y., Huang, X., and Fass, D. 1985. Syntax, Preference and Right Attachment,  IJCAI.\\n\\nZarri, G.P. 1992. The descriptive component of hybrid knowledge representation language, In: Semantic networks in Artificial Intelligence, Lehmann, F., ed. Oxford: Pergamon Press.\", metadata={'source': '../data/raw/cmplg-xml/9502033.xml'}),\n",
       " Document(page_content=\"A Symbolic and Surgical Acquisition of Termsthrough Variation\\n\\nTerminological acquisition is an important issue in learning for NLP due to the constant terminological renewal through technological changes. Terms play a key role in several NLP-activities such as machine translation, automatic indexing or text understanding. In opposition to classical once-and-for-all approaches, we propose an incremental process for terminological enrichment which operates on existing reference lists and large corpora. Candidate terms are acquired by extracting variants of reference terms through FASTR, a unification-based partial parser. As acquisition is performed within specific morpho-syntactic contexts (coordinations, insertions or permutations of compounds), rich conceptual links are learned together with candidate terms. A clustering of terms related through coordination yields classes of conceptually close terms while graphs resulting from insertions denote generic/specific relations. A graceful degradation of the volume of acquisition on partial initial lists confirms the robustness of the method to incomplete data.\\n\\nAims\\n\\nAs terms mirror the concepts of the domain to which they belong, a constant knowledge evolution leads to a constant term renewal. Thus terminological acquisition is a necessary companion to NLP, specifically when dealing with technical texts.\\n\\nAcquired terms must be merged with the initial ones with consideration of eventual variants.\\n\\nAcquired terms are neither conceptually nor linguistically related to the original ones.\\n\\nThe set of original terms is ignored although it could be a useful source of knowledge for acquisition.\\n\\nIt is possible to conceive a finer approach to term acquisition by considering the local variants of terms within corpora. As term variants generally involve more than one term, their extraction can fruitfully exploit existing lists of terms in a process of non massive incremental acquisition. For example, if viral hepatitis is a known term, viral and autoimmune hepatitis is a variant of this term (a coordination) which displays autoimmune hepatitis as a candidate term. Moreover, this coordination indicates a strong closeness between the interpretation of both terms which can be associated to a link within a thesaurus. Henceforth, potential terms acquired through acquisition techniques will be called candidate terms. The decision whether to include a candidate term into a terminology is outside the scope of our work.\\n\\nAcquiring with a Concern for Prior Knowledge\\n\\nTools for acquiring terms generally operate on large corpora using various techniques to detect term occurrences. There are mainly two families of tools for term acquisition : statistical measures and NLP symbolic techniques.\\n\\nUpdating Rather Than Acquiring\\n\\nIs it realistic to suppose that lists of terms exist for technical domains ? The ever-growing mass of electronic documents calls for tools for accessing these data which have to make extensive use of term lists as sources of indexes. For this purpose, and for other activities related to textual databases, more and more thesauri exist. Some of them, such as the Unified Medical Language System meta-thesaurus, carry conceptual and/or linguistic information about the terms they contain. In our experiment we have used the [Pascal] terminological list composed of 71,623 multi-domain terms without conceptual links, provided by the documentation center INIST/CNRS.\\n\\nA Micro-syntax for an Accurate Extraction\\n\\n(1)\\n\\nAt a higher level, a set of meta-rules operates on the term rules and produces new rules describing potential variations. Each meta-rule is dedicated to a specific term structure and to a specific type of variation. For the sake of clarity, meta-rules are divided into two sets - meta-rules for two-word terms and meta-rules for three-word terms - and each set is subdivided into three subsets - meta-rules for coordination, insertion and permutation. Meta-rules for terms of four words or more are ignored because they produce very few variants (approximately 1% of the variants). Meta-rule (3) applies to rule (3) and yields a new rule (3) :\\n\\n(3)\\n\\n(3)\\n\\nCoordination. The candidate term is the term coordinated with the original one.\\n\\nInsertion. The candidate term is the term which has replaced the head of the original term through substitution.\\n\\nPermutation. In a permutation of a 2-word term, the argument of the original term is shifted from the left of the head to its right and is transformed into a prepositional phrase. The candidate term is the noun phrase inside this prepositional phrase. This definition is extended to terms of 3 words or more where one of the arguments is permuted.\\n\\nAcquiring Conceptual Classes\\n\\nCoordination\\n\\nFor its classification with other related words through head coordination.\\n\\nFor the definition of its subsenses depending on its arguments through argument coordination.\\n\\nMoreover, the spatial organization of the graph outlines the central role played by normal control and disease control. These two terms are the most generic ones. Their root position in this acyclic graph (except for the two symmetric links) mirrors the linguistic fact that an argument coordination between two terms tends to place first the most generic argument and then the most specific one. Thus, although placed at a similar conceptual level in the taxonomy, these terms are ordered from the most generic to the most specific along the coordination links. This two-level observation reveals that linguistic clues, when precisely observed, are good indications of the conceptual organization.\\n\\nInsertion\\n\\nThe meta-rules accounting for insertions insert one or more words inside a term string. The following meta-rule (4) denotes an insertion of one word inside a two-word term :\\n\\n(4)\\n\\nThe resulting structure is ambiguous depending on whether the leftmost word of the term is still an argument of the head noun in the variation (e.g. [inflammatory [bowel disease]]) or an argument of the inserted word (e.g. [[sunflower seed] oil]). The second structure is quite rare and does not correspond to a genuine variant of the original term because it has a different argument structure. However, most of these possibly incorrect variants are correct. It happens every time when the reference term (here sunflower oil) corresponds to an elided denomination of the variant which is in fact the reference term. In this case, the non-ambiguity of the elided form relies on pragmatic knowledge, because everyone knows that the seed is the part of the sunflower used to make sunflower oil.\\n\\nIncrementality and Robustness\\n\\nConclusion and Future Work\\n\\nThis study has proposed a novel approach to terminological acquisition that differs from the two main trends in this domain : morpho-syntactic filtering or statistical extraction. The main feature of our approach is accounting for existing lists of terms by observing their variants and yielding conceptual links as well as candidate terms. As long as they are accessible through morpho-syntactic dependencies in a corpus, these links can be used to  automatically construct parts of the taxonomy representing the knowledge in this domain. Among the applications of this method are lexical acquisition, thesaurus discovery and technological survey. More generally, terminological enrichment is necessary for NLP activities dealing with technical sublanguages because their efficiency and their quality depend on the completeness of their lexicons of terms and compounds.\\n\\nBibliography\\n\\nBasili, R.; Pazienza, M. T.; and Velardi, P. 1993. Acquisition of selectional patterns in sublanguages. Machine Translation 8:175-201.\\n\\nBourigault, D. 1993. An endogeneous corpus-based method for structural noun phrase disambiguation. In Proceedings, 6th European Chapter of the Association for Computational Linguistics (EACL'93),  81-86.\\n\\nChurch, K. W., and Hanks, P. 1989. Word association norms, mutual information and lexicography. In Proceedings, 27th Annual Meeting of the Association for Computational Linguistics (ACL'89),  76-83.\\n\\nDaille, B. 1994. Study and implementation of combined techniques for automatic extraction of terminology. In Proceedings, The Balancing Act : Combining Symbolic and Statistical Approaches to Language, Workshop at the 32nd Annual Meeting of the Association for Computational Linguistics.\\n\\nEnguehard, C. 1994. Acquisition of terminology from colloquial texts. In Proceedings, Computational Linguistics for Speech and Handwriting Recognition (CLSHR).\\n\\nGrefenstette, G. 1994. Explorations in Automatic Thesaurus Discovery. Dordrecht, The Netherlands: Kluwer Academic Publisher.\\n\\nJacquemin, C. 1994. Recycling terms into a partial parser. In Proceedings, 4th Conference on Applied Natural Language Processing (ANLP'94),  113-118.\\n\\nLewis, D. D., and Croft, W. B. 1990. Term clustering of syntactic phrasess. In Proceedings, 13th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'90),  385-404.\\n\\nResnik, P. 1993. Selection and Information : A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania, Institute for Research in Cognitive Science.\\n\\nShieber, S. N. 1986. An Introduction to Unification-Based Approaches to Grammar. CSLI Lecture Notes 4. Stanford, CA: CSLI.\\n\\nSmadja, F. 1993. Xtract : An overview. Computer and the Humanities 26:399-413.\\n\\nFootnotes\\n\\nMatched control is a partial term with a missing noun argument which is not ruled out by our acquisition process. With a proper acquisition, this term would not appear as a candidate and the links issuing from this term would issue from one of the correct terms tex2html_wrap_inline$$Nountex2html_wrap_inline$$matched control. Among these 71,623 terms, only 12,717 are found in the [Medic] corpus under their basic form or one of its correct variants.\", metadata={'source': '../data/raw/cmplg-xml/9505012.xml'}),\n",
       " Document(page_content='Treating Coordination with Datalog Grammars\\n\\nIntroduction\\n\\nIn their most common implementations, logic grammars resort to list representations of the strings being analyzed or synthesized. This list-based implementation results in several deficiencies in logic grammars, while other deficiencies are inherited from Prolog. Datalog grammars were born in order to address all these deficiencies, namely: an infinite Herbrand Universe, non-termination, unnecessary recomputation, structure creation on the heap, bottleneck for multi-threaded execution due to the use of (sequential) list data structures, and inability to work directly on files.\\n\\nIn Datalog grammars, a given CF grammar is automatically translated into an assertional representation, first proposed by Robert Kowalski, which is largely equivalent to the list-based one but which ensures, under appropriate evaluation mechanisms such as OLDT resolution, that the termination and complexity properties of the original CF-grammar are preserved. We have moreover shown that, in restricted but useful cases, this can be achieved even in the presence of extra arguments.\\n\\nCoordination has long been a  difficult problem both in linguistics and in language processing. The difficulty lies in that any two constituents can be coordinated (even of different kind), and in that often some substring that is explicit in one of the conjuncts is missing in the other. For instance, Wood\\'s example:\\n\\nJohn drove the car through and completely demolished a window.\\n\\nexhibits a missing object ( \\'\\'a window\") in the first conjunct, and a missing subject (\\'\\'John\") in the second. Moreover, in representing these coordinated sentences, say in some logical form, we must take care of not requantifying \"a window\" when we reconstitute its meaning at the missing point: the window driven through must be equated with the demolished one.\\n\\nWhile humans have in general no trouble reconstituting these missing elements and attaching the right semantics to them, it is a challenge to efficiently spell out for a machine the regularities found in coordination phenomena.\\n\\nIn this article we show how we can extend the incremental evaluation implementation of Datalog grammars in order to automatically extend a grammar which has no rules for coordination with a meta-grammatical treatment which allows us to parse coordinated sentences.\\n\\nBackground\\n\\nAssertional Representation\\n\\nIn DLGs, a call to analyze \"the martian disappeared\",  for instance, compiles into:\\n\\nwhile lexical rules compile into forms that  use these representations accordingly, e.g. :\\n\\nOther grammar rules translate just as in Definite Clause Grammars, the standard Prolog grammar formalism.\\n\\nIncremental evaluation\\n\\nIn order to increase efficiency, one possible implementation for for DLGs exploits the incremental Datalog technique of generating and maintaining data bottom-up. Using the well-known semi-naive evaluation algorithm, we begin with the set of axioms and obtain the theorems of the first \"layer\" by applying the derivation rules; then we take these theorems as new starting point, to derive the theorems of the second layer , and so on. Generally to derive the theorems of the next \"layer\", at least one theorem produced at the previous stage must be used. This process terminates when no more new theorems can be generated.\\n\\nCoordination\\n\\nA X conj Y B\\n\\na process is triggered in which backing up is done in the parse history in order to parse Y parallel to X, and B is parsed by merger with the state interrupted by the conjunction.\\n\\nThus, in Wood\\'s example we would have:\\n\\nThe reconstructed phrase should then be A X B and A Y B, with the warning already made re. requantification.\\n\\nWe next modify this treatment and express it through DLG constraints to be intertwined with the incremental evaluation of a DLG grammar. We shall then discuss more recent views on parsing parallel structures, and extend our treatment by adapting some of these ideas into our DLG framework.\\n\\nTreating coordination through DLGs plus constraints\\n\\nOur idea for a Datalog treatment of coordination is also, as in the work reviewed in the last section, based on the assumption that a string containing a conjunction contains around that conjunction two constituents which are being coordinated. But instead of identifying four substrings A, B, X and Y, we simply assume that there are two coordinating constituents, V and W, surrounding the conjunction, which must in general be of the same category and have parallel parses. Thus any missing elements in either V or W can be reconstructed from the other. We also adopt the heuristics that closer scoped coordinations will be attempted before larger scoped ones. Thus in Wood\\'s example, \\'\\'vp conj vp\" is tried before \\'\\'sent conj sent\".\\n\\nThus in that example, \\'\\'John\" would parse as the subject noun phrase of a sentence with a complex verb phrase. Therefore we have\\n\\nBecause the conjunction is reached before the first verb phrase is finished parsing (\"through\" analyses as a preposition introducing a prepositional phrase- i.e., expecting a noun phrase to follow), the unfulfilled expectation of a noun phrase is postponed until it can be equated with a noun phrase in W.\\n\\nNotice that what we mean by V and W having parallel parses is not that they must necessarily follow the  same structure to the last details, but that their structures must complement each other so that missing constituents in one may be reconstructed from the other. We further assume, for the purposes of this article, that they both must have the same root (in this case, a verb phrase root),  although this assumption is not necessary in general.\\n\\nAnother thing to notice is that, whereas in the first analysis of Wood\\'s example we end up with two conjoined sentences, in the analysis just proposed we end up with a sentence having a verb phrase which decomposes into two conjoined verb phrases. Linguistically speaking, it is arguable whether one analysis is preferable over the other one. But computationally speaking, the second analysis allows us to apply our meta-grammatical treatment of coordination to sentences for which the first analysis would fail. An example is\\n\\nJean mange une pomme rouge et une verte.\\n\\nThis sentence cannot be split into A X conj B Y to reconstitute an unreduced structure following the first analysis. On the other hand, using the second analysis, we can postulate\\n\\nV = une pomme rouge, W = une verte\\n\\nand require that W follow a structure parallel to that of V. This then allows us to reconstitute the missing noun in W.\\n\\nWe now describe our proposed extension of incrementally implemented Datalog grammars in an intuitive manner, using the above example. We assume a simple French grammar with rules such as\\n\\nOur grammar includes no rules for coordination (but does, of course, recognize conjunctions as such).\\n\\nLet us recall that, in a Datalog grammar, our input string would be represented as:\\n\\nThe idea is simply to check, at every step of the incremental derivation of the theorems, whether a theorem conj(N,M) has been derived. As soon as one is, a constraint is added to the effect that, in some subsequent step of the incremental derivation of theorems, a constituent of category Cat must be found between some point Z and the point N, such that the same category stretches between M and some later point P; and that finding them implies that the string between Z and P must also have category Cat.\\n\\nThis constraint can be noted:\\n\\nAs soon as one of these predictions is fulfilled (e.g. when we have found a noun phrase \\'\\'une pomme rouge\" between Z=2 and N=5), we can further specify the other prediction to follow the same structure as that of the found noun phrase, which will allow us to reconstruct any missing elements.\\n\\nNotice that backtracking can occur. For instance, the machine will first postulate that the conjoined categories must be \\'\\'adjective\", and that Z=4 (this would be a good guess for the sentence: \\'\\'Jean mange une pomme rouge et verte\"). But in our sample sentence, this first try will fail to find an adjective starting at point M=6, so backtracking would undo the bindings and suspend the constraint until other suitable candidates for \\'\\'Cat\" and \\'\\'Z\" are derived.\\n\\nWe next present a step-by-step follow up for the example given. Sequences of theorems derived are noted T1, T2, etc. ; whereas sets of constraints are noted C1,C2,etc.\\n\\ntries Z=4 and fails. So the constraint suspends until something else of the form Cat(Z,5) appears.\\n\\ntries Z=2, and uses top-down prediction to find a (possibly incomplete) noun phrase stretching from point 6 to some point P, e.g. through the rule:\\n\\nsucceeds with  substitutions X=7, Y=7,P=8\\n\\nNotice that, at the point in which the constraint succeeds with  substitutions X=7, Y=7, if the grammar included arguments for semantic representation, the semantic representations for the two nouns would be unified, given that one of them is missing (as shown by the fact that its starting point, 7, is the same as its ending point). We shall later give a full example involving semantic representations.\\n\\nRelated work on ellipsis\\n\\na pairing of constituents ... and their parts, such that each pair contains two semantically and structurally similar objects\\n\\nresults in the priority union:\\n\\nThus, the implicit constituent in the second sentence is reconstituted from the first by using a generally applicable procedure on the representations of the parallel structures.\\n\\ncalculus semantic representations, and used higher order unification.\\n\\nFor instance, in their example: Dan likes golf, and George does too.\\n\\nthey identify the antecedent or source as the complete structure (\\'\\'Dan likes golf\"), whereas the target clause (\\'\\'George does too\") is either missing, or contains only vestiges of, material found overtly in the source.\\n\\nTheir analysis of such structures consists of:\\n\\na) determining the parallel structure of source and target;\\n\\nb) determining which are parallel elements in source and target (e.g., \\'\\'Dan\" and \\'\\'George\" are parallel elements in the example);\\n\\nwhere s1 through sn are the interpretations of the parallel elements of the source, and s is the interpretation of the source itself. Only solutions which do not contain a primary occurrence of the parallel elements are considered (occurrences are primary if they arise directly from the parallel elements, as opposed to those arising for instance from a pronoun).\\n\\nIn the example,\\n\\nis solved by equating P with\\n\\nx. likes(x,golf)\\n\\ngiven that the other possible solution,\\n\\nx. likes(dan,golf) contains a primary occurrence of the parallel element, \\'\\'dan\", and must therefore be discarded.\\n\\nd) applying the property on the representation of the target, e.g.\\n\\nP(george)= [\\n\\nx.likes(x,golf)] george = likes(george,golf)\\n\\ne) conjoining the meanings of the source and of the target thus completed, e.g. :\\n\\nHowever, both methods share the following limitations:\\n\\nb) both approaches stress semantic parallelism, while pointing out that this is not sufficient in all cases\\n\\nBy examining ellipsis in the context of coordinated structures, which are parallel by definition, and by using extended DLGs, we provide a method in which parallel structures are detected and resolved through syntactic and semantic criteria, and which can be applied to either grammars using different semantic representations- feature structure,\\n\\nOur semantico\\n\\n\\n\\nsyntactic treatment of parallelism\\n\\nLet us now consider the  string\\n\\nJohn drove the car through and demolished a window 0    1     2   3   4       5    6         7 8      9\\n\\nwhere we have indicated the connections as numbers in between the words.\\n\\nWe  use the following grammar:\\n\\nT1 would contain the \\'D\\' connections for this sentence, and T2 adds:\\n\\n{name(john,0,1), verb2(X,Y,Z, drove_through(X,Y,Z),1,2),\\n\\ndet(Y,R,Sc,the(Y,R,Sc),2,3), noun(Y,car(Y),3,4),\\n\\nprep(through,4,5), conj(and,5,6), verb1(X1,Y1,\\n\\ndemolished(X1,Y1),6,7), det(W,R1,Sc1,a(W,R1,Sc1),7,8),\\n\\nnoun(V,window(V),8,9)}\\n\\nAt this point, a constraint to find points P and Q such that Cat(...,P,5) is parallel to Cat(...,6,Q) is generated (upon whose finding, something of the form Cat(..., P,Q) will be added to the set of theorems ), and this constraint suspends until the following new theorems have been derived:\\n\\nWe can now postulate Cat= vp and use top-down prediction to derive a (possibly incomplete) vp ending at point 5. When trying rule\\n\\nNow we can build an abstract NP by abstracting over the Scope argument of the source, and postulating an empty surface string (i.e., by equating the start and end points of the string):\\n\\nWe now unify the abstracted NP with the target NP to obtain the resolved target NP:\\n\\nwhich in turn completes the target vp:\\n\\nThe constraint now reads:\\n\\nNow we need to conjoin the parallel structures. This is done by what we call c-unification: unify the parts in the parallel terms which are unifiable, and conjoin  those that are not(i.e., the parallel elements), with the exception of the last two arguments, which are generating from the two pairs of last arguments P1-P2 and P2+1-P3 of the parallel structures, as P1 and P3 . We obtain:\\n\\nAfter this theorem\\'s addition, the sent rule can apply to derive\\n\\nDiscussion\\n\\nSecondly, our notion of   abstraction, which relies on converting into a variable those parts of a semantic representation which are contributed by the constituent that contains it, can be adapted to suit other semantic representations, provided that we can identify for them which part of the semantic representation each rule for a constituent contributes to the overall representation. This is not an unreasonable expectation for compositionally defined semantics.\\n\\nIn the third place, we should note that our analysis allows for the source clause to not necessarily be the first one- again as the example we just examined shows, we can have structures in which the incomplete substructure does not  antecede  the complete one. Thus our analysis can handle more cases than those in previous related work.\\n\\nNote that some special cases allow to use unification between isomorphical objects to obtain the proper quantification. By slightly modifying the grammar as\\n\\nwe can handle directly phrases like:\\n\\nClearly this works only for a class of particular constraints exhibiting strong isomorphism in the constructed meaning. For instance, noun groups of the form np1 and np2 and np3do have this property.\\n\\nWe must note, however, that in some cases we will need to complement our analysis with a further phase which we shall call \\'\\'reshaping\". Take for instance the sentence \\'\\'Each man and each woman ate an apple\". Since both parallel structures are complete, we do not need to perform abstraction and c-unification, but we do need to reshape the result of the analysis through distribution, thus converting\\n\\ninto\\n\\nIn this sentence, the conclusion which holds if Fred drinks BUT SAM DOES NOT, does not hold if both Fred and Sam drink. The implicit information that the first conclusion holds only if the premiss of the second sentence does not hold must be inferred. Using our approach, we could use the re-shaping phase to deal with cases such as this one, in which the presence of words such as \\'\\'too\" would trigger the generation of the full reading. A sentence of the form\\n\\nwould generate a representation such as\\n\\nwhich after reshaping would become:\\n\\nAcknowledgement\\n\\nThis research was supported by NSERC Research grants 31-611024 and OGP0107411, and by NSERC, CSS and SFU PRG Infrastructure and Equipment grant given to the Logic and Functional Programming Laboratory at SFU, in whose facilities part of this work was developed. We are also grateful to the Centre for Systems Science, LCCR and the School of Computing Sciences at Simon Fraser University for the use of their facilities. Paul Tarau also thanks for support from the FESR of the Universit de Moncton.\\n\\nBibliography\\n\\nN. Asher. Reference to Abstract Objects in Discourse, volume 50 of   Studies in Linguistics and Philosophy. Kluwer, 1992.\\n\\nJ. H. R. Calder. An Interpretation of Paradigmatic Morphology. PhD thesis, University of Edinburgh, 1990.\\n\\nV. Dahl and M. McCord. Treating coordination in logic grammars. American Journal of Computational Linguistics, 9:69-91, 1983.\\n\\nV. Dahl, P. Tarau, and Y. N. Huang. Datalog grammars. In Proc. 1994 Joint Conference on Declarative Programming, Peniscola, Spain, September 1994.\\n\\nM. Darlymple, S. Shieber, and F. Pereira. Ellipsis and higher-order unification. Linguistics and Philosophy, 14(4):399-452, 1991.\\n\\nC. Grover, C. Brew, S. Manandhar, and M. Moens. Priority union and generalization in discourse grammars. In Proc. 32nd ACL Conference, New Mexico, 1994.\\n\\nJ. Hodas. Specifying Filler-Gap Dependency Parsers in a Linear Logic Programming Language. In K. Apt, editor, Proc. 1992 Joint International Conference and Symposium on Logic Programming, pages 622-636. MIT Press, 1992.\\n\\nG. Huet. A unification algorithm for typed lambda-calculus. Theoretical Computer Science, 1:27-57, 1975.\\n\\nH. Prust. On Discourse Structuring, Verb Phrase Anaphora and Gapping. PhD thesis, Universiteit van Amsterdam, 1992.\\n\\nD. Srivastava and R. Ramakrishnan. Pushing Constraint Selections. The Journal of Logic Programming, 16:361-414, 1993.\\n\\nP. Tarau.\\n\\nBinProlog 3.30 User Guide.\\n\\nTechnical Report 95\\n\\n\\n\\n1, Dpartement d\\'Informatique, Universit\\n\\nde Moncton, Feb. 1995.\\n\\nftp://clement.info.umoncton.ca/BinProlog.\\n\\nW. Woods. An experimental parsing system for transition network grammars. In R. Rustin, editor, Natural Language Processing, pages 145-149. Algorithmic Press, New York, 1973.\\n\\nFootnotes', metadata={'source': '../data/raw/cmplg-xml/9505006.xml'}),\n",
       " Document(page_content=\"Robust Processing of Natural Language\\n\\nPrevious approaches to robustness in natural language processing usually treat deviant input by relaxing grammatical constraints whenever a successful analysis cannot be provided by ``normal'' means. This schema implies, that error detection always comes prior to error handling, a behaviour which hardly can compete with its human model, where many erroneous situations are treated without even noticing them. The paper analyses the necessary preconditions for achieving a higher degree of robustness in natural language processing and suggests a quite different approach based on a procedure for structural disambiguation. It not only offers the possibility to cope with robustness issues in a more natural way but eventually might be suited to accommodate quite different aspects of robust behaviour within a single framework.\\n\\nRobustness in Natural Language Processing\\n\\nThe notion of robustness in natural language processing is a rather broad one and lacks a precise definition. Usually, it is taken to describe a kind of monotonic behaviour, which should be guaranteed whenever a system is exposed to some sort of non-standard input data: A comparatively small deviation from a predefined ideal should lead to no or only minor disturbances in the system's response, whereas a total failure might only be accepted for sufficiently distorted input.\\n\\nUnder this informal notion robustness may well be interpreted as a system's indifference to a wide range of external disruptive factors including\\n\\nthe inherent uncertainty of real world input, e.g. speech or hand writing,\\n\\nnoisy environments,\\n\\nthe variance between speakers, for instance idiolectal, dialectal or sociolectal,\\n\\n``erroneous'' input with respect to some normative standard,\\n\\nan insufficient competence of the processing system, if e.g. exposed to a non-native language or new terminology,\\n\\nhighly varying speech rates and\\n\\nresource limitations due to the parallel execution of several mental activities.\\n\\nOne of the most impressive features of human language processing is the ability to retain its basic capabilities even if it is exposed to a combination of adverse factors. Technical solutions, on the other hand, are likely to have serious problems if confronted with only a single type of distortion, apart from the fundamental difficulties to supply the desired monotonic behaviour at all.\\n\\nAccordingly, problems of robustness in NLP have almost never been considered from a unifying perspective so far. A number of very specific techniques for some of those different aspects has been developed, which hardly can be related to each other.\\n\\nrobust stochastic modelling techniques which are able to capture generalizations across the individual variety and\\n\\nsophisticated search procedures which select among huge amounts of competing recognition hypotheses by comparing probability estimations for signal segments of increasing length.\\n\\nTraditionally the notion of robustness has been strongly connected to the processing of ill-formed input, where ill-formedness can be defined both, in terms of human standards of grammaticality or in terms of unexpected input. Most of the work has been concerned with the problem from a purely syntactic point of view and usually relied on two basic techniques: error anticipation and constraint relaxation.\\n\\nstereotypical spelling mistakes (\\n\\n\\n\\ncomittee,\\n\\n\\n\\nrigth, etc. ),\\n\\nSince both, error anticipation and constraint relaxation considerably enlarge the generative capacity of the original grammar they will lead to spurious ambiguities and serious search problems. This restricts their application to a kind of post mortem analysis. Only if a failure of the standard analysis procedure indicates the presence of non-standard input, error rules or relaxation techniques are activated to integrate the fragmentary results obtained so far.\\n\\nEven a superficial comparison with human processing principles shows the fundamental deficit of these approaches. A human reader or listener accepts ill-formed input to a wide degree, often without noticing an error at all. This is particularly true if strong expectations concerning the content of the utterance are involved or if heavy time constraints restrict the processing depth.\\n\\nObviously, there is a fundamental parallelism between robustness issues and time considerations, which syntactically oriented solutions lack so far. Robustness in human language processing does not amount to an additional effort, but instead facilitates both, insensitivity to ill-formed input as well as a flexible adaptation to temporal restrictions.\\n\\nThis basic pattern is much better modelled by semantically oriented approaches based on the slot-and-filler-principle. Here, highly domain specific expectations are coded by means of frame-like structures and checked against the input for satisfaction. The schema can be successfully extended to a kind of skimming understanding bringing together the question of robustness against syntactically ill-formed input and some simple considerations concerning resource limitations.\\n\\nThis advantage of a semantically guided analysis, however, is won by the cost of excluding another important robustness feature, namely the ability to cope with unexpected input (e.g. a change of topic beyond the narrow limitations of the domain or the violation of selectional restrictions in metaphorical expressions).\\n\\nObservations from Human Language Processing the autonomy between parallel lines of processing which embodies redundancy and allows to compensate partial insufficiencies and\\n\\nthe interactive nature of informational exchange which allows to relate partial structures on different levels of granularity.\\n\\nParallel and autonomous structures in language processing have not only evolved between syntactic and semantic aspects of language. They can be observed equally well at the level of speech comprehension where auditory (hearing) and visual (lip-reading) clues are usually combined to achieve a reliable recognition result. Again, both systems - in principle - are able to work independently, but synergy occurs if both are activated concurrently.\\n\\nA second group of observations related to the question of robustness concerns the expectation-driven nature of human language understanding. Here, expectations come to play at two different dimensions:\\n\\nSyntactic, semantic and pragmatic predictions about future input derived from previous parts of the utterance or dialogue.\\n\\nExpectations exchanged between parallel and autonomous processing structures for syntax and semantics.\\n\\nAlthough the importance of predictions for robustness is beyond question, here the second type of expectations shall be examined as a matter of priority, since they are expected to establish the attempted informational coupling between parallel processing units. As the simple examples above have shown, no predefined direction for this exchange of information can be assumed. Certain syntactic constructions may trigger specific semantic interpretations, a view which is strongly supported by the traditional perspective on the relation between syntax and semantics. In the opposite direction, semantic relations, e.g. derived from background knowledge, can not only be used to disambiguate between preestablished syntactic readings, but moreover are able to actively propose suitable syntactic structures. This bidirectionality of interaction seems to be of great importance for the ability to provide the mutual compensation necessary to treat deviant constructions of different kind.\\n\\nOf course, the expectation-based nature of natural language processing cannot guarantee a failure-proof performance under all circumstances. There certainly are situations in which strong expectations may override even sensory data. Such a situation can easily be studied in everyday conversation whenever e.g. pragmatic expectations are predominant. A similar problem occurs in experimental settings using intentionally desynchronised video input, where lip reading information sometimes overrides even the auditory stimulus. The problem is witnessed as well by the difficulties usually encountered in proof-reading one's own text: Extremely strong expectations concerning the content usually cause minor mistakes to be passed unnoticed.\\n\\nDisambiguation by Constraint Propagation autonomy guarantees a fall-back behaviour for failures of a single module\\n\\nexpectancy\\n\\n\\n\\noriented analysis facilitates the\\n\\ninformational exchange and\\n\\npreference-based processing guides the analysis towards a promising interpretation and establishes a loose coupling between modules.\\n\\nThe initial set of labels is successively reduced by applying compatibility and surface ordering constraints until a unique interpretation has been reached or the set of available constraints is exhausted. In the latter case, a total disambiguation cannot be achieved by purely syntactic means, as in the following attachment example:\\n\\nIn contrast to traditional grammars of the phrase structure type which license well-formed structures according to their rule system, constraint grammar rather happens to be an eliminative approach. Instead of imposing a normative description on the input data it takes them as starting point and tries to find a plausible interpretation for them.\\n\\nThough, a closer inspection of the kind of robustness feature introduced by the eliminative mode of operation reveals that its nature is quite accidental so far. Which types of deviation can be tolerated indeed, strongly depends on the rather arbitrary sequence of constraint applications. This shortcoming seems to be closely connected to the fact that both formalisms lack the notion of preference so far and therefore do not have the possibility to model the ``quality'' of a constraint. Hence, adding a preference-based selection strategy will be one of the most pressing needs for further improvement. Such an extension will be proposed in section 5. Before we turn to this topic section 4 introduces a modular representation schema along the traditional syntax-semantics distinction. It supports the desired functional autonomy as well as a highly interactive exchange of expectations between the two layers.\\n\\nRepresentation Layers a syntactic layer relating word forms according to functional surface structure notions e.g ( SUBJECT-OF, DIR-OBJECT-OF, PREP-MODIFIER-OF, etc.) and using constraints on ordering, agreement, valency, and valency saturation to select among competing structural configuration and\\n\\na semantic layer building sentence structures by means of thematic roles (like  AGENT-OF, INSTRUMENT-OF, TIME-OF, etc.) thereby relying upon the argument structure of semantic predicates and their corresponding selectional restrictions.\\n\\nAdhering to the principle of autonomy both layers are designed in a way which allows them to propagate constraints in a completely independent manner. Each modifier is specified for two possibly different modifiees and no cross-reference between the layers has been used so far.\\n\\nIn order to finally mediate the interaction between layers, a set of mapping constraints has to be provided which sets up bidirectional correspondences\\n\\nIt should have become obvious that the selectional restrictions as well as the mapping constraints at best can be taken to stand for a preferential interpretation. They surely are much to rigid to be sensibly used within a framework of strict reasoning.\\n\\nSemantic constraints need not be restricted to linguistically motivated (i.e. universally valid) ones. In particular, domain-specific restrictions play a crucial role in semantic disambiguation and should urgently be incorporated whenever possible. Here the semantic layer offers a convenient interface to a knowledge representation component which (on demand) can contribute constraints from e.g. specialized ontologies, referential instantiations or temporal reasoning.\\n\\nWeakening Constraints\\n\\nSo far, one of the most striking shortcomings has been the strictly binary nature of constraint satisfaction. Not surprisingly, it turned out to be most inappropriate within the area of semantic modelling where hardly a constraint can be formulated without restricting oneself to a particular, preferential reading.\\n\\nIn what follows, preferences are not modelled in the usual direct manner by emphasizing particular well-formed interpretations but rather indirectly by putting a penalty on all remaining alternatives which violate a constraint. For this purpose each constraint gets a penalty factor pf assigned reducing the confidence score in negative cases. Penalty factors may range from zero to one where pf=0  specifies a strict constraint in the classical sense and 0[pf[1  indicates a soft constraint accepting contradictory cases   with a confidence value proportional to pf Obviously, a value of one is meaningless because it neutralizes the constraint. Penalty factors are combined multiplicatively, i.e. compatibility matrices within the constraint satisfaction problem no longer contain binary categories but confidence scores also ranging from zero (for impossible combinations) up to one (for combinations not even violating a single constraint).\\n\\nThe indirect treatment of preference by penalty factors offers a consistent extension to the basic paradigm of constraint satisfaction. It does not sacrifice the eliminative nature of constraints but simply softens it. Inappropriate readings are excluded only if they violate strict constraints. In all other cases they are downgraded to a certain degree.\\n\\nIn particular, the penalty-based approach helps to tackle some normalization problems otherwise inherently connected with the constraint satisfaction approach: Most modifying relations (or combinations of them) will pass a constraint simply because it is irrelevant for that particular configuration. An increase of goodness estimates for these cases would yield a highly undesirable, since unjustified reinforcement.\\n\\nBy assigning the penalty factors pf(sy1)=pf(sy4)=0 to the constraints sy1 and sy4 from section 3 both are declared to be strict ones, a fact obviously being valid for the toy-size sample grammar which does not take into account coordinative structures. Using pf(sy2)=0.1 the agreement condition is treated as a rather strong one which allows exceptions only occasionally. pf(sy3)=0.3 on the other hand results in a much more permissive constraint justified by the fact that sy2 is meant to exclude ungrammatical utterances but sy3 only to disfavour a marked ordering.\\n\\nbeing close to the global minimum for all modification relations, combined with\\n\\nan as possible as high contrast to alternative relations and\\n\\na low contrast between all the confidence scores supporting the relation in question.\\n\\nFor experimental purposes a selection procedure based on the sum of quadratic errors for setting scores to zero has been used. Hence, structural interpretations violating a high number of rather strong constraints are pruned first.\\n\\nThe interpretation is retained even if its semantic support is neutralized as in the following utterance, containing a twofold type shift.\\n\\nIt switches to the alternative interpretation only in the case of combined syntactic distortions\\n\\nwhich, if desired, could be taken as a headline-style utterance, syntactic evidence will gain the upper hand against the violation of two selectional constraints. This interpretation, however, happens to be a rather fragile one and breaks immediately under arbitrary syntactic variation.\\n\\nSince the selection procedure operates on a global assessment of local structural configurations it cannot guarantee to find an optimal and globally consistent interpretation. The partially local mode of operation, on the other hand, can be expected to provide a quite natural explanation for human garden-path phenomena. Within the framework of preference-based disambiguation they turn out to be a special case of contradictory situations which manifest themselves as expectation violations: The consequences of a pruning decision may not coincide with local confidence estimations elsewhere in the constraint network.\\n\\nExpectation violations not necessarily do indicate an erroneous situation. They are frequently used as a speaker's intentionally chosen means to attract the attention of the audience. This happens for instance by deviating from an unmarked ordering to emphasize a topicalized constituent (c.f. (3b)) or by otherwise producing unexpected utterances.\\n\\ninternal difficulties (e.g. due to early commitment strategies in garden path situations) might offer the possibility to initiate a reanalysis and\\n\\nexternal reasons (e.g. ill-formed input) can be used to track down the error to find a possible remedy for it.\\n\\nPreference\\n\\n\\n\\nbased Reasoning\\n\\nEliminating implausible interpretations by locally pruning less favoured modification relations represents only one, though fundamental method for the disambiguation of natural language utterances. By selecting among modifying relations according to negative evidence from maximally dispreferred hypotheses, the technique fits quite well into the constraint satisfaction approach and achieves its robust behaviour by avoiding extremely risky decisions on a locally topmost reading. Taking this as a starting point the basic way of reasoning can well be complemented by a second propagation principle based on preference-induced constraints. These are activated only in situations where enough positive evidence can be derived from almost uniquely determined preferences. Since the existence of convincing preferences in realistic disambiguation tasks represents rather the exception than the rule the nature of this propagation principle is secondary.\\n\\nConclusion The approach departs from a predefined sequential arrangement of modules in favour of a strictly symmetrical architecture consisting of autonomous components for syntax and semantics.\\n\\nIt allows to treat syntactic ill-formedness and semantic deviations by providing a mechanism for mutual compensation. Syntactically anomalous utterances can be understood as long as there is enough semantic and/or pragmatic evidence. In order to communicate novel or unusual content a sufficiently high degree of syntactic support is required.\\n\\nInsufficient modelling information on any one of the processing layers might well result in the selection of an odd interpretation but will not cause the language processing unit to break down entirely.\\n\\nRobustness is not an add-on feature of an otherwise temperamental procedure but falls out from the basic properties of the processing mechanism.\\n\\nBibliography\\n\\nM. E. Catt. Intelligent diagnosis of ungrammaticality in computer-assisted language instruction. Technical Report CSRI-218, Computer Systems Research Institute, University of Toronto, 1988.\\n\\nG. Erbach. Towards a theory of degrees of grammaticality. Report 34, Universitt des Saarlandes, Computerlinguistik, Saarbrcken, 1993.\\n\\nC. J. Fillmore, P. Kay, and M. C. O'Connor. Regularity and idiomaticity in grammatical constructions: The case of let alone. Language, 64:501-538, 1988.\\n\\nK. I. Forster. Levels of processing and the structure of the of the language processor. In W. E. Cooper and E. C. T. Walker, eds., Sentence Processing: Psycholinguistic studies presented to Merrill Garret, p. 27-85. Lawrence Erlbaum, Hillsdale, NJ, 1979.\\n\\nL. Frazier. Theories of sentence processing. In J. L. Garfield, ed., Modularity in Knowledge Representation and Natural Language Understanding, p. 291-307. MIT-Press, Cambridge MA, 1987.\\n\\nS. Goeser. Chart parsing of robust grammars. In Proc. 14th Int. Conf. on Computational Linguistics, Coling '92, p. 120-126, Nancy, France, 1992.\\n\\nG. Hanrieder and G. Grz. Robust parsing of spoken dialogue using contextual knowledge and recognition probabilities. In Proc. of the ESCA Workshop on Spoken Dialogue Systems, Denmark, 1995.\\n\\nM. P. Harper and R. A. Helzerman. Managing multiple knowledge sources in constraint-based parsing of spoken language. Technical Report EE-94-16, School of Electrical Engineering, Purdue University, West Lafayette, 1994.\\n\\nM. P. Harper, L. H. Jamieson, C. B. Zoltowski, L. L. McPheters, and R. A. Helzerman. Semantics and constraint parsing. In Proc. of the Int. Conf. on Acoustics, Speech, and Signal Processing, ICASSP-92, p. II-63-66, 1992.\\n\\nA. Hauenstein and H. Weber. An investigation of tightly coupled time synchronous speech language interfaces using a unification grammar. Verbmobil Report 9, 1994.\\n\\nJ. K. Holbrook, K. P. Eiselt, and K. Mahesh. A unified process model of syntactic and semantic error recovery in sentence understanding. In Proc. of the 14th Annual Conf. of the Cognitive Science Society, p. 195-200, Bloomington, IN, 1992.\\n\\nR. S. Jackendoff.\\n\\nSemantics and Cognition.\\n\\nMIT Press, Cambridge, 1983.\\n\\nD. Jurafsky. A cognitive model of sentence interpretation: The construction grammar approach. Technical Report TR-93-077, International Computer Science Institute, Berkeley, 1993.\\n\\nF. Karlsson. Designing a parser for unrestricted text. In F. Karlsson, A. Voutilainen, J. Heikkil, and A. Anttila, eds., Constraint Grammar - A Language-Independent System for Parsing Unrestricted Text, p. 1-40. Mouton de Gruyter, Berlin, New York, 1995.\\n\\nF. Karlsson, A. Voutilainen, J. Heikkil, and A. Anttila, eds. Constraint Grammar - A Language-Independent System for Parsing Unrestricted Text. Mouton de Gruyter, Berlin, New York, 1995.\\n\\nL. Konieczny, B. Hemforth, and N. Voelker. The impact of context and semantic bias on constituent attachment in reading. In J. J. Quantz and B. Schmitz, eds., Ambiguity and Strategies of Disambiguation, p. 105-126. KIT-Report 120, Berlin, 1994.\\n\\nI. Kudo, H. Koshino, M. Chung, and T. Morimoto. Schema method: A framework for correcting grammatically ill-formed input. In Proc. 12th Int. Conf. on Computational Linguistics, Coling '88, p. 341-347, Budapest, 1988.\\n\\nK. Mahesh and K. P. Eiselt. Uniform representations for syntax-semantic arbitration. In Proc. of the 16th Annual Conf. of the Cognitive Science Society, p. 589-594, 1994.\\n\\nW. Marslen-Wilson. Functional parallelism in spoken word-recognition. Cognition, 25:71-102, 1987.\\n\\nW. Marslen-Wilson and L. K. Tyler. Against modularity. In J. L. Garfield, ed., Modularity in Knowledge Representation and Natural Language Understanding, p. 37-62. MIT-Press, Cambridge, MA, 1987.\\n\\nH. Maruyama. Structural disambiguation with constraint propagation. In Proc. 28th Annual Meeting of the ACL, p. 31-38, 1990.\\n\\nW. Menzel. Parsing of spoken language under time constraints. In T. Cohn, ed., Proc. 11th Europ. Conf. on Artificial Intelligence, p. 560-564, Amsterdam, 1994.\\n\\nC. Morgenstern.\\n\\nAlle Galgenlieder.\\n\\nCassirer, Berlin, 1933.\\n\\nC. Pollard and I. A. Sag. Head-Driven Phrase Structure Grammar. The University of Chicago Press, Chicago, 1994.\\n\\nM. Stede. The search for robustness in natural language understanding. Artificial Intelligence Review, 6(4):383-414, 1992.\\n\\nH. Uszkoreit. Strategies for adding control information to declarative grammars. Computerlinguistik Report 10, Universitt des Saarlandes, Saarbrcken, 1991.\\n\\nS. Wermter and V. Weber. Learning Fault-tolerant Speech Parsing with SCREEN. In Proc. of the 12th Nat. Conf. on Artificial Intelligence, p. 670-675, Seattle, 1994.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9507003.xml'}),\n",
       " Document(page_content=\"DEFAULT HANDLING IN INCREMENTAL GENERATION\\n\\nNatural language generation must work with insufficient input. Underspecifications can be caused by shortcomings of the component  providing the input or by the preliminary state of incrementally given input. The paper aims to escape from such dead-end situations by making assumptions. We discuss global aspects of default handling. Two problem classes for defaults in the incremental syntactic generator VM-GEN are presented to substantiate our discussion.\\n\\nMOTIVATION\\n\\nNatural Language Generation, i.e., the process of building an adequate utterance for some given content, is by nature a decision-making problem (Appelt, 1985). Internal decisions are made on the basis of the specified input. Unfortunately, input information can be insufficient in two respects:\\n\\nIf the input structure for generation is provided by another AI-system, global problems in producing sufficient input information for the generator may occur, e.g., because of translation mismatches in machine translation (Kameyama, 1991). In this case, the generator either has to use a default or formulate a request for clarification in order to be able to continue its processing, i.e., to produce an utterance. During simultaneous interpretation requests are rather unusual. Here defaults allow for a standalone handling of the problem. For example, problems during speech recognition of automatic interpretation can lead to results like ``the (man/men) will come to the hotel tomorrow''. If the system is not able to give a preference for one of the alternatives, e.g., by evaluating context information, the generator has to choose a probable number value on its own to complete verbalization.\\n\\nFurthermore, for incremental generation, the input information is produced and handed over step by step, so that it can be temporarily incomplete -- although as a whole it may become sufficient. This behaviour of a generator is motivated by psycholinguistic observations which show that people start speaking before all necessary linguistic material has been chosen (e.g., articulating a noun phrase before the dominating verb is selected). As a consequence of underspecification, incremental generation is essentially based on working with defaults. Elements are uttered before the processing or input  consumption has been finished. (Kitano, 1990) gives an example for defaults in the context of simultaneous interpretation: In Japanese, negation is specified at the end of the sentence while in English, it has to be specified in front of the finite verb. Therefore, during Japanese-English translation, where analysis, transfer, and generation are performed in a parallel and incremental way, the system has to commit, e.g., positive value before knowing the actual polarity.\\n\\nGenerally speaking, default handling specifies how processing, i.e., further decision-making, can continue without  sufficient input information. So, one can compare default handling with advice to the system. For reasons of uncertainty of assumptions, incremental systems with this facility must be able to repair the default decision when the assumption turns out to be wrong by information given later. Catching on to the above example, there can be a negation specifier given at the end of the Japanese input sentence which cannot be simply integrated into the output sentence because the finite verb has already been uttered. In this case, the output has to be repaired, e.g., by repeating parts of the utterance: ``I will be able to meet you ... oops ...I won't be able to meet you at the hotel this evening.''\\n\\nIn the following sections, we argue for the appropriateness of processing-conforming default handling. Basically, the processing-conforming mode  makes the overall system homogeneous because the combination of default-caused processing and input-licensed processing requires no specific description. The homogeneity becomes especially helpful in the  case where the input verifies the default assumption  rendering unnecessary any recomputation. For the opposite case where the default must be withdrawn we have to mark all defaults. Even more homogeneity is introduced to an incremental system if the default descriptions are given in terms of input specifications. This representation allows for easy checking the coincidence between a chosen default and input given later.\\n\\nThe content of this paper can be summarized as follows. Section 2 provides a general description for defaults in generation emphasizing the specific requirements in an incremental system. After identifying the conditions under which defaults are triggered (section 2.1), the application of a default (section 2.2) and the definition of its description (section 2.3) is outlined. The crucial case of removing defaults not coinciding with newly arriving input in an incremental system is discussed in section 2.4.\\n\\nIn section 3, this mechanism is applied to the incremental sentence generator VM-GEN. In the beginning of the section, the basic design of the system is outlined. Later on, default handling is included and exemplified for two general cases.\\n\\nIn the final section we summarize the main results of the paper. Furthermore, we discuss how default handling can be adapted to multilingual generation, as required by the speech-to-speech translation system VERBMOBIL (Block et al., 1992).\\n\\nGENERAL DISCUSSION OF DEFAULTS\\n\\nIn the literature of non-incremental generation, the need for defaults is hardly ever taken into account. The common point of view restricts the input to be sufficient for generation (see, e.g., the Text Structure by (Meteer, 1990) for a syntactic generator). In incremental generation, most authors agree on the necessity of using defaults (see, e.g., (De Smedt, 1990; Kitano, 1990; Ward, 1991)). Nevertheless, they do not in sufficient depth answer the question of how to guide the processes of default handling and repair within a generator. This problem is the starting-point for the following considerations.\\n\\nWe assume that generation is a decision-making process with the aim of producing a plausible utterance based on  given information. As mentioned in section 1, there are cases where this process stops (caused by underspecification of the input) before finishing its output.\\n\\nWe define a module named default handler which tries to resume the process by giving advice to it, i.e., by making assumptions about the missing input specification. With respect to this task it is discussed 1. in which situations defaults are applied (see section 2.1), 2. how default handling is integrated into a system (see section 2.2), 3. how the knowledge for default handling is described (see section 2.3), and 4. how assumptions are cancelled when they turn out to be inconsistent with newly arriving input (see section 2.4). In incremental generation, as mentioned in section 1, interleaved input consumption and output production causes specific default situations. An incremental processing scheme allows for an increase of efficiency and flexibility, e.g., by making the analysis and generation processes of a system for simultaneous interpretation overlap in time. There are two competing goals of incremental generation for spoken output, that must be taken into account when estimating the usefulness of defaults:\\n\\nFluency: Long hesitations should be avoided  during the production of an utterance, in order to be acceptable to the hearer.\\n\\nReliability: Errors in an utterance may cause misunderstanding. In most cases, errors should be recovered by appropriate self-corrections. Excessive use of self-corrections or erroneous expressions should be avoided because they decrease intelligibility of the utterance.\\n\\nObviously there is a trade-off between fluency and reliability: maximal reliability requires `secure' decisions and therefore leads to output delay. On the other hand, maximal fluency necessitates the use of assumptions and repair, respectively.\\n\\nWhen to Trigger Default Handling\\n\\nWe define as default situation the situation where a generation system has not yet finished the utterance but at the same time has consumed all given input and is not able to continue processing. In non-incremental generation, this corresponds to the fact that  the input lacks necessary information, because the entire input is assumed to be given at one time (e.g., the undecidable number value of the example described in section 1). Thus, default handling should be triggered immediately.\\n\\nIn incremental generation, however, the system may get a new piece of information later on that enables it to continue processing (e.g., the specification of a negation value + as outlined in the example in section 1). Therefore, possible alternatives are either to wait for the next input or to trigger default handling. The former violates the fluency goal, the latter may violate the reliability goal. We propose the explicit use of time-limits for delay intervals.\\n\\nFurthermore, the certainty of a default is described by a value. As soon as a  default situation is identified, the certainty of the default is checked to see whether it exceeds a predefined threshold that determines the degree of fluency/reliability.\\n\\nEach application of a default decreases the global certainty of the system's state. Consequently, there should be a limit for the maximal number of defaults applicable to the same sentence.\\n\\nHow to Integrate Default Handling\\n\\nBasically, there are two strategies to integrate default handling into ongoing processing.\\n\\nDefaults may be handled in a way that differs from the `normal' processing of the system, e.g., as short-cuts. One advantage can be an efficient handling of defaults. Furthermore, the designer of the default component is completely free in deciding about the realization of defaults in the system. A disadvantage is the difficulty of providing consistency between default-caused and input-licensed processing.\\n\\nAlternatively, the ongoing processing can deal with the default values in an ordinary manner (processing-conforming default handling). This may be less efficient but guarantees consistency during processing, especially in case of a replacement by an input-licensed value. For incremental generation, the system has to provide repair facilities in any case. So, they can also be used for non-monotonic modifications of default-caused results. We take this option in order to make the overall system homogeneous.\\n\\nHow to Describe Defaults\\n\\nThe knowledge source that is used for  default handling  should provide the most plausible actions for a default situation. We represent the knowledge as a set of heuristic rules called default descriptions. A default description defines a set of operations that  should be carried out in a certain situation where the generation process can not be continued. A default description has the following form:\\n\\nThe set of default preconditions defines tests that are applied to the given situation in order to find out whether the corresponding default body can be activated. They include tests for the existence of  particular information, tests for the structure under creation and tests for the state of processing. The default body describes how to continue processing with defaults in an adequate way. For incremental systems, we propose to express the body as a specification of input increments. An important prerequisite is that the size of increments is defined flexibly enough to cope with varying amounts of information. Obviously, an important advantage of this  approach is homogeneity of the overall system. Especially, the homogeneous representation of default-caused and input-licensed structures is the easiest and most direct way to test coincidences or contradictions between default-specified and input-caused values. In section 3, this approach is outlined by different examples. For non-incremental systems, an operational approach is preferable since there is no way to consume additional input increments, presupposing that the input has been considered as a whole before a default situation occurs.\\n\\nIf several default preconditions are applicable, the certainty values for default descriptions are examined to find which provides the system with the most plausible action.\\n\\nThe individual default descriptions should take into account the global constraints for processing stated in the knowledge sources of the system. For example, the assumption of nominative case for a German NP complement can regularly be made only once for the same verb. For reasons of homogeneity, the default description should at least be compatible with the specifications of the knowledge used for basic processing. In order to guarantee consistency, default descriptions should merely contain what is orthogonal to the basic knowledge sources.\\n\\nHow to Cancel Defaults\\n\\nThe repair of false assumptions is a crucial point of default handling in the context of  incremental processing because the default information does not remain locally but can cause further decisions of the system. Contrarily, for non-incremental input there will be no value given that can contradict default values.\\n\\nAs a first step of repair,  inconsistencies between  input-provided  and default-caused values are identified by simply matching the values. Then effects of the respective defaults are withdrawn introducing the input-provided values into the system. Generally, a decision during generation influences other decisions all over the system. Thus the effect of a default body may be propagated through the entire system (e.g., choosing a construction of main clause with causal subordinate clause influences the choice of syntactic realizations).\\n\\nRoughly speaking, withdrawing a default assumption can be realized by backtracking to the earlier state of the system where the default had been introduced or by non-monotonic changes to the current state of the system. The disadvantage of backtracking is that partial results are thrown away which could be reused during further processing. Non-monotonic changes preserve these results. In this framework, cancelling defaults requires the system to identify which results are caused by default handling. Dependency links between the immediate result of a default body and results of the influenced decisions allow for this identification. The disadvantage of non-monotonic changes is the complexity of computation, e.g., supported by a truth maintenance system. When designing an incremental system, simple backtracking is ruled out because the part of the sentence uttered cannot be withdrawn after it has been perceived by the addressee of the message.\\n\\nSo, we end up with a processing-conforming default handler for generation realizing repair by non-monotonic changes.\\n\\nEXAMPLES OF DEFAULTS IN VM-GEN\\n\\nThe adaptation of our general discussion of default handling to the system VM-GEN not only provides concrete examples for the reader but also shows that a homogeneous combination of default handling, regular processing, and utterance repair is possible.\\n\\nThe syntactic generator VM-GEN is a further development of TAG-GEN (Kilger, 1994) within the framework of VERBMOBIL, a speech-to-speech translation system. Its usefulness for simultaneous interpretation results from its incremental and parallel style of processing. VM-GEN is able to consume input increments of varying size. These increments describe lexical items or semantic relations between them. Single input increments are handed over to objects of a distributed parallel system, each of which tries to verbalize the structure that results from the corresponding input increment. VM-GEN uses an extension of Tree Adjoining Grammars (TAGs, cf. (Joshi, 1985)) as its syntactic representation formalism that is not only adequate for the description of natural language but also supports incremental generation (Kilger and Finkler, 1994).\\n\\nIn the following, we introduce examples for default processing triggered during the German inflection process in VM-GEN to substantiate the global statements made in section 2. Inflection uses some syntactic properties of an element to compute its morphological form. This information has partly to be specified in the input (e.g., the number for a noun) and is partly inherited from other elements (e.g., the number for a verb or the case for a noun). The two reasons for missing information necessitate different methods of treatment which nevertheless both can uniformly be integrated into regular processing.\\n\\nIf information of the first type is missing (e.g., because of problems during analysis, see section 1), an assumption can be made locally by simulating the respective part of the input. The default for missing number information in VM-GEN would look as follows:\\n\\nThe set of default preconditions is applied to all objects (OBJ) of VM-GEN in order to test the kind of underspecification (`number' in the example). The default body introduces a new value (sg) by creating an input increment as a default. The test for coincidence with the input-licensed value is realized by a comparison in the objects of VM-GEN. There is a unique association of input increments and objects of VM-GEN (OBJ is used as identifier) that allows for translating an input modification into a modification of the state of the respective object. In case of contradictions the default and all default-caused decisions are revised (see below).\\n\\nMaking an assumption can be influenced by global constraints. An example, which is well studied in psycholinguistics, is the utterance of a noun before the verb has been chosen. If, e.g., the noun ``Besucher'' (English: ``visitor'') is known to be the agent of an action, it may be uttered as subject in the first position of the sentence by default. This treatment presupposes the choice of a `dummy' verb, which at least subcategorizes a subject and has active voice. The use of a dummy verb and an underspecified verbal structure the NP is integrated into allows for a simple global test that rules out the same case value assignment to different NP complements as it is required for most of the German verbs. This rule is represented in the grammar as a part of the description of subcategorization frames for verbs. For reasons of homogeneity we use the information stored in the syntactic knowledge sources of VM-GEN for expressing syntactic constraints during default handling as well. The advantage of this approach is, that processing is continued in a consistent way, which eases the introduction of the input-licensed value. One default for choosing a missing case-value is specified as follows:\\n\\nThe default preconditions of the rule characterize a situation where an object (OBJ) contains no information about the case but identifies the input category as `N' for noun. Furthermore, the semantic function of the object is specified as `agent' but no verb defined yet (lemma(head(OBJ))=NIL) in the head object. That is why, the N-object cannot inherit a case value and also does not know whether it is allowed to occupy the front position in the utterance.\\n\\nEvaluating the default body, the system creates a V-object OBJ'. On the basis of the input information in (ENTITY OBJ' ...) it chooses a minimal syntactic structure from the inheritance net of the grammar, that just desribes a verb category without concrete filler (a dummy verb) plus a subject complement and active voice for the verbal phrase. Now, the N-structure is combined with the V-structure of the introduced V-object as during normal processing. Therefore, the case value can be inherited. Additionally, the first position can be assigned to the subject which can be uttered now.\\n\\nThe basic VM-GEN module provides repair strategies in order to allow for the specification of additions, modifications and deletions of input increments, i.e., to model a flexible input interface. Three features of the system are basically used for repair: First, input increments are uniquely associated with objects of VM-GEN, so that input modifications can be translated into modifications of the objects' states. Second, each modification of an object's state makes it compare new and old information. In case of a difference, the modified parts are sent to all concerned objects. Third, the dependency relations that determine the communication links between objects allow for a hierarchical organization of the objects, which is the basis for synchronizing repair.\\n\\nA repair must be triggered in the example described above if, e.g., a verb with voice passive is actually specified. In this case, the mapping of the semantic role `agent' to the syntactic function `subject' is revised. The agent now has to be realized as part of a ``von''-phrase, e.g. ``dieser Termin wird von dem Besucher gewnscht.'' (word-for-word: ``this date is whished by the visitor (dative object)''). Furthermore, the object checks whether the previously uttered part of the sentence includes some of the revised material (i.e., whether the object itself has participated in uttering). If this is the case, it sends an error message up to the uppermost object of the hierarchy that actually is engaged in uttering. This object is able to synchronize global repair. Up to now, we just realized a simple repair strategy that consists of repeating the concerned parts of the utterance, e.g. ``der Besucher ...h ...dieser Termin wird von dem Besucher gewnscht''.\\n\\nDISCUSSION\\n\\nThis paper proposes a processing-conforming default handler for generation realizing repair by non-monotonic changes. We provide the system with default descriptions. The set of default preconditions expresses possible reasons for dead-end situations. A default is triggered, if the preconditions match the current situation and the certainty value of the default exceeds the predefined threshold. The default body is expressed in terms of the missing input specification in order to make the system work homogeneously. We have verified the advantages of processing-conforming default handling by implementing a default handler for VM-GEN.\\n\\nAs future work, we will extend the default preconditions towards handling complex contextual information. We will apply default handling to microplanning and lexical choice within VERBMOBIL. With respect to a sophisticated output, we aim to combine VM-GEN with a flexible repair component.\\n\\nThe system VM-GEN is used in the VERBMOBIL scenario for multilingual generation (English, German, and Japanese). We mean by multilinguality that the same processing is applied for different languages. In the underlying knowledge sources language-specific constraints are defined. Default handling can be easily adapted to the requirements of multilingual generation by using language-specific default-descriptions.\\n\\nFor all knowledge sources the question arises how knowledge can be shared. We intend to use core knowledge sources for representing common phenomena. The core set of default descriptions for English and German, e.g., contains the description of a reaction to a missing number value for a noun. We aim to develop an efficient storing mechanism using a hierarchy of locally intersecting core descriptions.\\n\\nFootnotes\\n\\nAlternatively, the system could use the dialogue context to infer a negation value +/-. Humans often fill such pauses with fillers like ``er''  or ``what shall I say''. Sometimes, correction is unnecessary if (the speaker believes that) the hearer can infer the intended utterance from erroneous speech. An explicit parameter expressing the desired degree of fluency influences the time-limits. The basis for assigning certainty values to defaults should be a corpus study that can be used to find statistical evidence for various features with alternative values (like number, voice, ..., see, e.g., (Bock and Warren, 1985)). The difference between incremental and non-incremental generation becomes smaller, if we assume that defaults in  a non-incremental system can be triggered after the system has only considered parts of its input information. In this case, the input considered after default handling becomes comparable to later increments. If some phrases influenced by defaults have already been verbalized, the effect of verbalization can be cancelled by using repair words like ``oops'' or ``sorry'' when starting the modified utterance. `ENTITY' introduces information about a lexical  item. For reasons of incrementality, there may be several ENTITY-packages specified for the same item which are composed to receive the global information. For certainty values, we use values between 0 and 1, where 1 means high reliability.\\n\\nIn the actual implementation we preselect candidates with missing values for reasons of efficiency. For ongoing work on repair in VM-GEN see (Finkler, 1994). This kind of expansion is called ``provisional upward expansion'' by (De Smedt, 1990). `RELATION' introduces the specification of a relation between two lexical items which are identified by the names of their objects.\", metadata={'source': '../data/raw/cmplg-xml/9410033.xml'}),\n",
       " Document(page_content=\"Abstract Machine for Typed Feature Structures\\n\\nThis paper describes an abstract machine for linguistic formalisms that are based on typed feature structures, such as HPSG. The core design of the abstract machine is given in detail, including the compilation process from a high-level language to the abstract machine language and the implementation of the abstract instructions. The machine's engine supports the unification of typed, possibly cyclic, feature structures. A separate module deals with control structures and instructions to accommodate parsing for phrase structure grammars. We treat the linguistic formalism as a high-level declarative programming language, applying methods that were proved useful in computer science to the study of natural languages: a grammar specified using the formalism is endowed with an operational semantics.\\n\\nIntroduction\\n\\nResearch in the semantics of programming language has undergone much progress in recent years. At the same time, linguistic theories have become more formal and grammars for natural languages are nowadays specified with rigor, resembling computer programs. The interaction of computer science and linguistics enables the use of techniques and results of the former to be applied to the latter.\\n\\nThe abstract machine ensures that a grammars specified using our system are endowed with well defined meaning. It enables, for example, to formally verify the correctness of a compiler for HPSG, given an independent definition. The design of such an abstract architecture must be careful enough to compromise two, usually conflicting, requirements: the closer the machine is to common architectures, the harder it is to develop compilers for it; on the other hand, if such a machine is too complex, then while a compiler for it is easier to produce, it becomes more complicated to execute its language on normal architectures.\\n\\nThe Framework\\n\\nFundamental Notions\\n\\n,\\n\\nthe least type, and\\n\\n, the greatest one. Types are ordered by subsumption (\\n\\n) according to their information content, not set inclusion of their denotation. Hence,\\n\\nis the most general type, subsuming every other, and\\n\\nis the contradictory type, subsumed by every other.\\n\\nThe inheritance hierarchy is required to be bounded complete: every set of consistent types\\n\\nmust have a unique least upper bound\\n\\n. Every partial order can be naturally extended to a bounded complete one. The appropriateness function\\n\\nApprop(t,f) is required to be monotone and to comply with the feature introduction condition. However, we allow appropriateness specifications to contain loops.\\n\\nThe basic operation performed on TFSs is unification (\\n\\n.\\n\\nType Specification\\n\\nintro\\n\\n.\\n\\nwhere\\n\\nare types,\\n\\nare features and\\n\\n. This statement, which is said to characterize t, means that\\n\\nare (immediate) subtypes of t (i.e., for every\\n\\nappropriate for it. Moreover, these features are introduced by t, i.e., they are not appropriate for any type t'such that\\n\\n. Finally, the statement specifies that\\n\\nApprop(t,fi) = ri for every i. Each type (except\\n\\nand\\n\\n) must be characterized by exactly one statement. The arity of a type t, Ar(t), is the number of features appropriate for it.\\n\\nThe full subsumption relation is the reflexive transitive closure of the immediate relation determined by the characterization statements. If this relation is not a bounded complete partial order, the specification is rendered invalid. The same is true in case it is not an appropriateness specification.\\n\\n.\\n\\nThe type\\n\\nis systematically omitted from type specifications.\\n\\nRepresentation of Feature Structures\\n\\nThe most convenient graphical representation of TFSs is attribute-value matrices (AVMs). However, to represent a (totally well-typed) feature structure linearly we use an FOT-like notation, based upon At-Kaci's\\n\\nA TFS Unification Engine\\n\\nFirst-Order Terms vs. Feature Structures\\n\\nWhile TFSs resemble FOTs in many aspects, it is important to note the differences between them. First, TFSs are typed, as opposed to (ordinary) FOTs. TFSs are interpreted over more specific domains than FOTs. In addition, TFSs label the arcs by feature names, whereas FOTs use a positional encoding for argument structure. More importantly, while FOTs are essentially trees, with possibly shared leaves, TFSs are directed graphs, within which variables can occur anywhere. Moreover, our system doesn't rule out cyclic structures, so that infinite terms can be represented, too. FOTs are consistent only if they have the same functor and the same arity. TFSs, on the contrary, can be unified even if their types differ (as long as they have a non-degenerate least upper bound). Moreover, their arity can differ, and the arity of the unification result can be greater than that of any of the unificands. Consequently, many diversions from the original WAM were necessary in our design. In the following sections we try to emphasize the points where such diversions were made.\\n\\nProcessing Scheme\\n\\nThe machine's engine is designed for unifying two TFSs: a program and a query. The program is compiled once to produce machine instructions. Each query is compiled before its execution; the resulting code is executed prior to the execution of the compiled program. Processing a query builds a graph representation of the query in the machine's memory. The processing of a program produces code that, during run-time, unifies the program with a query already resident in memory. The result of the unification is a new TFS, represented as a graph in the machine's memory. In what follows we interleave the description of the machine, the TFS language it is designed for and the compilation of programs in this language.\\n\\nMemory Representation of Feature Structures\\n\\nIt is important to note that STR cells differ from their WAM analogs in that they can be dereferenced when a type is becoming more specific. In such cases, a chain of REF cells leads to the dereferenced STR cell. Thus, if a TFS is modified, only its STR cell has to be changed in order for all pointers to it to `feel' the modification automatically. The use of self-referential REF cells is different, too: there are no real (Prolog-like) variables in our system, and such cells stand for features whose values are temporarily unknown.\\n\\nFlattening Feature Structures\\n\\nBefore processing a TFS, its linear representation is transformed to a set of ``equations'', each having a flat (nesting free) format. To facilitate this a set of registers\\n\\nProcessing of a Query\\n\\nWhen processing an equation of the form\\n\\nrepresenting part of a query, two different instructions are generated. The first is put_node t/n, Xi0, where n = Ar(t). Then, for every argument Xij, an instruction of the form put_arc Xi0, j, Xij is generated. put_node creates a representation of a node of type t on top of the heap and stores its address in Xi0; it also increments H to leave space for the arcs. put_arc fills this space with REF cells.\\n\\nCompilation of the Type Hierarchy\\n\\n. Moreover, this table lists also the arity of t, its features and their `origin': whether they are appropriate for t1, t2, both or none of them. Out of this table a series of abstract machine language functions are generated. The functions are arranged as a two-dimensional array called unify_type, indexed by two types t1,t2. Each such function receives one parameter, the address of a TFS on the heap. When executed, it builds on the heap a skeleton for the unification result: an STR cell of the type\\n\\n, and a REF cell for each appropriate feature of it.\\n\\nConsider unify_type[t1,t2](addr) where addr is the address of a TFS A (of type t2) in memory. Let\\n\\nThis example code is rather complex; often the code is much simpler: for example, when t2 is subsumed by t1, nothing has to be done. As another example, if t1 is subsumed by t2, then additional features of the program term have to be added to A. But if no such features exist, the only required effect is a change of the type of A. Another case is when t1 and t2 are not compatible: unify_type[t1,t2] returns `fail'. This leads to a call to the function fail, which aborts the unification.\\n\\nProcessing of a Program\\n\\nThe program is stored in a special memory area, the CODE area. Unlike the WAM, in our framework registers that are set by the execution of a query are not helpful when processing a program. The reason is that there is no one-to-one correspondence between the sub-terms of the query and the program, as the arities of the TFSs can differ. The registers are used, but (with the exception of X1) their old values are not retained during execution of the program.\\n\\nThe get_structure instruction is generated for a TFS Ap (of type t) which is associated with a register Xi. It matches Apagainst a TFS Aq that resides in memory using Xi as a pointer to Aq. Since Aq might have undergone some type inference or previous binding (for example, due to previous unifications caused by other instructions), the value of Xi must first be dereferenced. This is done by the function deref which follows a chain of REF cells until it gets to one that does not point to another, different REF-cell. The address of this cell is the value it returns.\\n\\nThe dereferenced value of Xi, addr, can either be a self-referential REF cell or an STR cell. In the first case, the TFS has to be built by the program. A new TFS is being built on top of the heap (using code similar to that of put_structure) with addr set to point to it. For every feature of this structure, a `copy' item is pushed onto the stack. The second case, in which Xipoints to an existing TFS of type t', is the more interesting one. An existing TFS has to be unified with a new one whose type is t. Here the pre-compiled unify_type[t,t'] is invoked.\\n\\nWhen a sequence of instructions that were generated for some TFS is successfully executed on some query, the result of the unification of both structures is built on the heap and every register Xi stores the value of its corresponding node in this graph. The stack S is empty.\\n\\nLazy Evaluation of Feature Structures\\n\\nParsing\\n\\nThe previous section delineated a very simple abstract machine, capable of unifying two simple TFSs. We now add to this machine control structures that will enable parsing. We define rules, grammars and parsing, and then describe how the basic machine is extended to accommodate the application of a single rule. We sketch the extensions necessary for manipulating a whole grammar (program). These extensions were not tested yet.\\n\\nGrammars\\n\\nA multi-rooted structure (MRS) is a directed, labeled, finite graph with an ordered non-empty set of distinguished nodes, roots, from which all the nodes are reachable. A rule is a MRS, where the graph that is reachable from the last root is the rule's head, and the ones  that are reachable from the rest of the roots form its body. A MRS is linearly represented as a sequence of terms, separated by commas, where two occurrences of the same tag, even within two different terms, denotes reentrancy (that is, the scope of the tags is the entire sequence of terms). The head is preceded by `\\n\\nApplication of a rule amounts to unifying its body with a MRS resident in memory and producing its head as a result. When two TFSs A1 and A2 are parts of MRSs\\n\\nand\\n\\n, respectively, the unification of A1 and A2 in the context of\\n\\nand\\n\\nis defined just like ordinary unification, but\\n\\nand\\n\\nmight be affected by the process. As an example, the rule\\n\\nconsists of a MRS of length three. When it is applied to the MRS\\n\\n, the result is a new MRS whose head is a(d2,d1).\\n\\n's head is modified even though it does not participate directly in the unification, as it is part of the context.\\n\\nA grammar is a finite set\\n\\nof rules together with a start feature structure As. The lexicon associates with every word w a TFS Aw, its category, by means of special rules of the form\\n\\nThe input for the parser, therefore, is a MRS rather than a string of words. A MRS\\n\\nis derived by some TFS A if there exists a rule\\n\\nsuch that Ais obtained by unifying\\n\\nwith\\n\\n's body in the context of\\n\\n's head. We abuse the term `derive' to denote also the reflexive transitive closure of this relation. A is a category if it derives a substring of some input. The language generated by the grammar is the set of strings of words\\n\\nsuch that the category of wi is Ai for\\n\\nand As derives\\n\\n.\\n\\nA dotted rule (or edge) is a MRS that is more specific than some rule in the grammar, with an additional dot, indicating a location preceding some element in the MRS.  An edge is complete if its dot precedes the head and is active otherwise. We denote dotted rules by\\n\\nInformally, such a dotted rule asserts that each of\\n\\nderives a string\\n\\nsuch that\\n\\nis a\\n\\nsubstring of the input.\\n\\nstill have to derive\\n\\nin order for A0 to be a category deriving\\n\\n.\\n\\nParsing as Operational Semantics\\n\\nwhere i,j are natural numbers and\\n\\nis a dotted rule. A state is a finite set of items. A computation is triggered by some input string of words\\n\\nof length n ] 0. The initial state,\\n\\n,\\n\\nis\\n\\nwhere Ai is the\\n\\ncategory of wi and\\n\\n. For any state S, the next state S' is constructed by the following transition relation `\\n\\n' (the fundamental rule):\\n\\nFor every\\n\\nand\\n\\nsuch that\\n\\n,\\n\\nadd\\n\\nto S',\\n\\nwhere\\n\\nis obtained by unifying Band B'' in the contexts of\\n\\nand\\n\\nrespectively.\\n\\nA computation is an infinite sequence of states\\n\\n,\\n\\nsuch\\n\\nthat\\n\\nand for every\\n\\n,\\n\\n. A computation is terminating if there exists some\\n\\nfor which\\n\\nSm = Sm+1 (i.e., a fixed-point is reached). A successful computation is a terminating one, the final state of which contains an item of the form\\n\\nwhere\\n\\n; otherwise, the computation fails. The presence of more than one such item in the final state indicates that the input can be analyzed in more than one way.\\n\\nTo represent a state of the computation the machine uses a chart, structured as a two-dimensional array storing, in the (i,j)entry, all the dotted rules\\n\\nsuch that\\n\\nis a member of the state. Items are added to the chart by means of an agenda that controls the order of addition.\\n\\nApplication of a Single Rule\\n\\nTo allow the application of a single rule, the syntax of queries is extended from simple TFSs to MRSs. The same code is generated for the queries, with additional advance instructions preceding each TFS of the query. The advance instruction simply increments the indices of the chart item being manipulated. As a result of executing the query, the (i,i+1) diagonal of the chart is initialized with singleton sets of edges.\\n\\nThe syntax of programs is extended, too, from a TFS to a single MRS. Again, the same code is generated for the TFSs of a program: program-code for each element of the rule's body and query-code for the head. Before the first TFS, a start_rule instruction is generated. A move_dot and next_item instructions are generated between two consecutive structures, and after the last one, the head, an end_rule instruction concludes the generated code.\\n\\nTo understand the effect of these instructions, one must understand the non-uniform internal representation of dotted rules. Each such rule is represented by a record, edge, containing three fields. The seen field is a list of pointers to the roots of an MRS, for the part of the dotted rule preceding the dot. The to_see field is a pointer to the code area, for the rest of the rule. A complete edge is represented as a single TFS, its head, since the rest of the structures (that are unaccessible from the head) are irrelevant. An edge with an initial dot is simply a pointer to code.\\n\\nSince the rules are applied incrementally, a TFS at a time, care must be taken of reentrancies. The rules manipulate registers which must contain the right values when used. To that end the values of the registers are stored after execution of a part of a rule (that is, before moving the dot), and the right values are loaded prior to each such execution (after moving the dot). The field regs of an edge stores the saved registers.\\n\\nstart_rule sets the stage for the application of the rule: it stores the address of the beginning of the query in X1, where get_structure expects to find it. It also records the values of i,j and k of the current edges. move_dot is executed after the successful unification of one TFS; it copies the newly created edge, including the values of the registers, to the chart (and interacts with the agenda). next_item just restores the registers' values and resumes execution. end_rule is executed once a complete edge is constructed; it adds the edge to the chart and selects the next edge to work on.\\n\\nControl Structures\\n\\nWhen designing the control module, three parameters have to be set: the order of searching chart entries that can combine with a complete edge e; the order of searching active edges within this chart entry; and the search strategy: are all the edges that can combine with ecomputed first, and then their consequences (BFS), or rather all the consequences of the first such edge, then the next etc. (DFS). The order the chart is searched for active edges is right to left: from (i,i) to (0,i). There is no way to decide that a certain edge in the chosen chart entry is appropriate save by trying to unify it with the complete edge that was just entered. Hence all edges in a chart entry are considered a disjunctive value, and each of them is tried in turn. Furthermore, upon initialization each entry on the diagonal (i,i) of the chart is set to be a disjunction of all the rules in the grammar. As for the search strategy, we chose to employ BFS; some way to record all the edges that were added as consequences of e is needed, in order to compute their consequences next.\\n\\nDetermining the values of these parameters is program-independent: the maintenance of the chart is fixed. This fact results from the nature of the process the machine implements, namely parsing, and has a desirable consequence: one might change some of these parameters easily without having to modify the compiler or even the set of machine instructions. What has to be changed is the data structures that support the control mechanism. For lack of space we don't detail the control module. Essentially, it employs a list of edges, agenda, and interacts with the machine instructions described above through designated functions.\\n\\nConclusion\\n\\nAs linguistic formalisms become more rigorous, the necessity of well defined semantics for grammar specifications increases. We presented an operational semantics for TFS-based formalisms, making use of an abstract machine specifically tailored for  this kind of applications. In addition, we described a compiler for a general TFS-based language. The compiled code, in terms of abstract machine instructions, can be interpreted and executed on ordinary hardware. The use of abstract machine techniques is expected to result in highly efficient processing.\\n\\nThe TFS unification engine and the type hierarchy compiler were already implemented; the control module will be implemented shortly. We then plan to enhance the machine by adding specific values for lists (and perhaps sets). The implementation will serve as a platform for developing an HPSG grammar for the Hebrew language.\\n\\nAcknowledgments\\n\\nPart of the work described herein was done while the first author was visiting the Seminar fr Sprachwiessenschaft in Tbingen, Germany. We wish to thank the Minerva Stipendien Komitee for funding this visit, and the members of the SFB-340 B4 project in Tbingen, especially Paul King, Thilo Gtz and John Griffith, for stimulating discussions. We also wish to thank Bob Carpenter for his help during this project, and the anonymous referees for enlightening comments. This work is supported by a grant from the Israeli Ministry of Science: ``Programming Languages Induced Computational Linguistics''. The work of the second author was also partially supported by the Fund for the Promotion of Research in the Technion.\\n\\nBibliography\\n\\nH. At-Kaci. Warren's Abstract Machine: A Tutorial Reconstruction. Logic Programming. The MIT Press, Cambridge, Massachusetts, 1991.\\n\\nH. At-Kaci. An introduction to LIFE - programming with logic, inheritance, functions and equations. In D. Miller, editor, Logic Programming - Proceedings of the 1993 International Symposium, pages 52-68. MIT Press, 1993.\\n\\nH. At-Kaci, R. Boyer, P. Lincoln, and R. Nasr. Efficient implementation of lattice operations. ACM TOPLAS, 11(1):115-146, January 1989.\\n\\nH. At-Kaci and R. Di Cosmo. Compiling order-sorted feature term unification. PRL Technical Note 7, Digital Paris Research Laboratory, December 1993.\\n\\nH. At-Kaci and R. Nasr. LOGIN: a logic programming language with built-in inheritance. Journal of Logic Programming, 3:185-215, 1986.\\n\\nH. At-Kaci, A. Podelski, and S. C. Goldstein. Order-sorted feature theory unification. In D. Miller, editor, Logic Programming - Proceedings of the 1993 International Symposium, pages 506-524, Cambridge, Mass., 1993. MIT Press.\\n\\nB. Carpenter. ALE - the attribute logic engine: User's guide. Technical report, Laboratory for Computational Linguistics, Philosophy Department, Carnegie Mellon University, Pittsburgh, PA 15213, Dec. 1992.\\n\\nB. Carpenter. The Logic of Typed Feature Structures. Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 1992.\\n\\nB. Carpenter, C. Pollard, and A. Franz. The specification and implementation of constraint-based unification grammars. In Proceedings of the Second International Workshop on Parsing Technology, Cancun, Mexico, 1991.\\n\\nG. Erbach. ProFIT: Prolog with features, inheritance and templates. CLAUS Report 42, Computerlinguistik, Universitt des Saarlandes, July 1994.\\n\\nT. W. Gtz. A normal form for types feature structures. Master's thesis, Eberhard-Karls Universitt, Tbingen, March 1994.\\n\\nN. Haddock, E. Klein, and G. Morill, editors. Categorial Grammar, Unification and Parsing, volume 1 of   Working Papers in Cognitive Science. University of Edingburgh, Center for Cognitive Science, 1987.\\n\\nG. Penn. A comprehensive HPSG grammar in ALE. Technical report, Laboratory for Computational Linguistics, Carnegie Mellon University, Pittsburgh, PA, 1993.\\n\\nC. Pollard and I. A. Sag. Information Based Syntax and Semantics. Number 13 in CSLI Lecture Notes. CSLI, 1987.\\n\\nC. Pollard and I. A. Sag. Head-Driven Phrase Structure Grammar. University of Chicago Press and CSLI Publications, 1994.\\n\\nS. M. Shieber, Y. Schabes, and F. C. N. Pereira. Principles and implementation of deductive parsing. Technical Report TR-11-94, Center for Research in Computing Technology, Division of Applied Sciences, Harvard University, 1994.\\n\\nS. Wintner and N. Francez. Abstract machine for typed feature structures. Technical Report LCL 94-8, Laboratory for Computational Linguistics, Technion, Israel Institute of Technology, Haifa 32000, Israel, July 1994.\\n\\nR. Zajac.\\n\\nInheritance and constraint\\n\\n\\n\\nbased grammar formalisms.\\n\\nComputational Linguistics, 18(2):159\\n\\n\\n\\n182, 1992.\\n\\nFootnotes\\n\\nthen\\n\\n.\", metadata={'source': '../data/raw/cmplg-xml/9504009.xml'}),\n",
       " Document(page_content=\"A Deductive Account of Quantification in LFG Introduction LFG and Linear Logic Syntactic Framework Lexically-Specified Semantics Logical Representation of Semantic Compositionality Meaning and glue Linear logic Relationship with Categorial Syntax and Semantics\\n\\nOn the other hand, categorial semantics in the undirected Lambek calculus and other related commutative calculi provides an analysis of the possibilities of meaning combination independently of the syntactic realizations of those meanings, but does not provide a mechanism for relating semantic combination possibilities to the corresponding syntactic combination possibilities.\\n\\nThus, while the ``propositional skeleton'' of an analysis in our system can be seen as a close relative of the corresponding categorial semantics derivation in the undirected Lambek calculus, the first-order part of our analysis (notably the f, g, and h in the example above) explicitly carries the connection between f-structures and their contributions to meaning. In this way, we can take advantage of the principled description of potential meaning combinations of categorial semantics without losing track of the constraints imposed by syntax on the possible combinations of those meanings.\\n\\nQuantification\\n\\nQuantified noun phrase meanings\\n\\nThis rule states that the determiner Det and noun N each contribute to the f-structure for the NP. Lexical specifications ensure that the noun contributes the  attribute and its value, and the determiner contributes the  attribute and its value. The f-structure for the noun phrase every voter is: The semantic contributions of common nouns and determiners were described in the previous section.\\n\\nFrom these two premises, the semantic contribution for every voter follows:\\n\\nSimple example of quantification\\n\\nBefore we look at quantifier scope ambiguity and interactions between scope and bound anaphora, we demonstrate the basic operation of our proposed representation of the semantic contribution of a determiner. We use the  following sentence with a single quantifier and no scope ambiguities: The premises for the derivation are the semantic contributions for Bill and convinced together with the contribution derived above for the quantified noun phrase every voter:\\n\\nGiving the name bill\\n\\n\\n\\nconvinced to the formula\\n\\nwe have the derivation:\\n\\ncould at first sight be considered another possible, but erroneous, scope. However, the type subscripting of the  relation used in the determiner lexical entry requires the scope to represent a dependency of a proposition on an individual, while this formula represents the dependency of an individual on an individual (itself). Therefore, it does not provide a valid scope for the quantifier.\\n\\nQuantifier scope ambiguities\\n\\nWe select a manager to take narrower scope by using universal instantiation and transitivity of implication to combine the first form with a-manager to yield\\n\\nWe have thus the following derivation\\n\\nAlternatively, we could have chosen every candidate to take narrow scope, by combining the second equivalent form of appointed with every-candidate to produce:\\n\\nThis gives the derivation\\n\\nConstraints on quantifier scoping\\n\\nEvery candidate appointed an admirer of his.\\n\\nHowever, no such ambiguity is found if the pronoun his is taken to corefer with the subject every candidate. In this case, only one reading is available, in which an admirer of his takes narrow scope. Intuitively, this noun phrase may not take wider scope than the quantifier every candidate, on which its restriction depends.\\n\\nFirst, we rewrite admirer into the equivalent form\\n\\nWe can use this formula to rewrite the the second conjunct in the consequent of his, yielding\\n\\nIn turn, the second conjunct in the consequent of admirer-of-his matches the first conjunct in the antecedent of a given appropriate variable substitutions, allowing us to derive\\n\\nAt this point the other formulas available are:\\n\\nWe have thus the meanings of the two quantified noun phrases. The antecedent implication of every-candidate has an atomic conclusion and hence cannot be satisfied by an-admirer-of-his, which has a conjunctive conclusion. Therefore, the only possible move is to combine appointed and an-admirer-of-his. We do this by first putting appointed in the equivalent form\\n\\nAfter universal instantiation of Z with X, this can be used to rewrite the first conjunct in the consequent of an-admirer-of-his to derive\\n\\nUniversal instantiation of H and S together with modus ponens with the two conjuncts in the consequent as premises yield\\n\\nFinally, this formula can be combined with every-candidate to give the meaning of the whole sentence:\\n\\nIn fact, this is the only derivable conclusion, showing that our analysis blocks those putative scopings in which variables occur outside the scope of their binders.\\n\\nConclusion\\n\\nOur approach exploits the f-structure of LFG for syntactic information needed to guide semantic composition, and also exploits the resource-sensitive properties of linear logic to express the semantic composition requirements of natural language. The use of linear logic as the glue language in a deductive semantic framework allows a natural treatment of quantification which automatically gives the right results for quantifier scope ambiguities and interactions with bound anaphora.\\n\\nAcknowledgments\\n\\nWe thank Johan van Benthem, Bob Carpenter, Jan van Eijck, Angie Hinrichs, David Israel, Ron Kaplan, John Maxwell, Michael Moortgat, John Nerbonne, Stanley Peters, Henriette de Swart and an anonymous reviewer for discussions and comments. They are not responsible for any remaining errors, and we doubt that they will endorse all our analyses and conclusions, but we are sure that the end result is much improved for their help.\\n\\nSyntax of the Meaning and Glue Languages\\n\\nThe meaning language is based on Montague's intensional higher-order logic. In fact, in the present paper we just use an extensional fragment with the following syntax:\\n\\nBibliography\\n\\nAlsina, Alex. 1993. Predicate Composition: A Theory of Syntactic Function Alternations. Ph.D. thesis, Stanford University.\\n\\nBarwise, Jon and Robin Cooper. 1981. Generalized quantifiers and natural language. Linguistics and Philosophy, 4:159-219.\\n\\nBresnan, Joan and Jonni M. Kanerva. 1989. Locative inversion in Chichewa: A case study of factorization in grammar. Linguistic Inquiry, 20(1):1-50. Also in E. Wehrli and T. Stowell, eds., Syntax and Semantics 26: Syntax and the Lexicon. New York: Academic Press.\\n\\nButt, Miriam. 1993. The Structure of Complex Predicates. Ph.D. thesis, Stanford University.\\n\\nDalrymple, Mary. 1993. The Syntax of Anaphoric Binding. Number 36 in CSLI Lecture Notes. Center for the Study of Language and Information.\\n\\nDalrymple, Mary, John Lamping, and Vijay Saraswat. 1993. LFG semantics via constraints. In Proceedings of the Sixth Meeting of the European ACL, University of Utrecht, April. European Chapter of the Association for Computational Linguistics.\\n\\nDalrymple, Mary, Angie Hinrichs, John Lamping, and Vijay Saraswat. 1993. The resource logic of complex predicate interpretation. In Proceedings of the 1993 Republic of China Computational Linguistics Conference (ROCLING), Hsitou National Park, Taiwan, September. Computational Linguistics Society of R.O.C.\\n\\nDalrymple, Mary, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. 1994. Intensional verbs without type-raising or lexical ambiguity. In Conference on Information-Oriented Approaches to Logic, Language and Computation, Moraga, California. Saint Mary's College. To appear.\\n\\nFenstad, Jens Erik, Per-Kristian Halvorsen, Tore Langholm, and Johan van Benthem. 1987. Situations, Language and Logic. D. Reidel, Dordrecht.\\n\\nGamut, L. T. F. 1991. Logic, Language, and Meaning, volume 2: Intensional Logic and Logical Grammar. The University of Chicago Press, Chicago.\\n\\nGirard, J.\\n\\n\\n\\nY.\\n\\n1987.\\n\\nLinear logic.\\n\\nTheoretical Computer Science, 45:1\\n\\n\\n\\n102.\\n\\nHalvorsen, Per\\n\\n\\n\\nKristian.\\n\\n1983.\\n\\nSemantics for Lexical\\n\\n\\n\\nFunctional Grammar.\\n\\nLinguistic Inquiry, 14(4):567\\n\\n\\n\\n615.\\n\\nHalvorsen, Per-Kristian. 1988. Situation Semantics and semantic interpretation in constraint-based grammars. In Proceedings of the International Conference on Fifth Generation Computer Systems, FGCS-88, pages 471-478, Tokyo, Japan, November. Also published as CSLI Technical Report CSLI-TR-101, Stanford University, 1987.\\n\\nHalvorsen, Per-Kristian and Ronald M. Kaplan. 1988. Projections and semantic description in Lexical-Functional Grammar. In Proceedings of the International Conference on Fifth Generation Computer Systems, pages 1116-1122, Tokyo, Japan. Institute for New Generation Systems.\\n\\nHendriks, Herman. 1993. Studied Flexibility: Categories and Types in Syntax and Semantics. ILLC dissertation series 1993--5, University of Amsterdam, Amsterdam, Holland.\\n\\nHepple, Mark. 1990. The Grammar and Processing of Order and Dependency: a Categorial Approach. Ph.D. thesis, University of Edinburgh.\\n\\nHoward, W.A. 1980. The formulae-as-types notion of construction. In J.P. Seldin and J.R. Hindley, editors, To H.B. Curry: Essays on Combinatory Logic, Lambda Calculus and Formalism. Academic Press, London, England, pages 479-490.\\n\\nKaplan, Ronald M. 1987. Three seductions of computational psycholinguistics. In Peter Whitelock, Harold Somers, Paul Bennett, Rod Johnson, and Mary McGee Wood, editors, Linguistic Theory and Computer Applications. Academic Press, London, pages 149-188.\\n\\nKaplan, Ronald M. and Joan Bresnan. 1982. Lexical-Functional Grammar: A formal system for grammatical representation. In Joan Bresnan, editor, The Mental Representation of Grammatical Relations. The MIT Press, Cambridge, MA, pages 173-281.\\n\\nKlein, Ewan and Ivan A. Sag. 1985. Type-driven translation. Linguistics and Philosophy, 8:163-201.\\n\\nLambek, Joachim. 1958. The mathematics of sentence structure. American Mathematical Monthly, 65:154-170.\\n\\nMiller, Dale A. 1990. A logic programming language with lambda abstraction, function variables and simple unification. In Peter Schroeder-Heister, editor, Extensions of Logic Programming, Lecture Notes in Artificial Intelligence. Springer-Verlag.\\n\\nMontague, Richard. 1974. The proper treatment of quantification in ordinary English. In Richmond H. Thomason, editor, Formal Philosophy. Yale University Press, New Haven, Connecticut.\\n\\nMoortgat, Michael. 1988. Categorial Investigations: Logical and Linguistic Aspects of the Lambek Calculus. Ph.D. thesis, University of Amsterdam, Amsterdam, The Netherlands, October.\\n\\nMoortgat, Michael. 1992a. Generalized quantifiers and discontinuous type constructors. In W. Sijtsma and H. van Horck, editors, Discontinuous Constituency. Mouton de Gruyter, Berlin, Germany. To appear.\\n\\nMoortgat, Michael. 1992b. Labelled deductive systems for categorial theorem proving. In P. Dekker and M. Stokhof, editors, Proceedings of the Eighth Amsterdam Colloquium, pages 403-423, Amsterdam. Institute for Logic, Language and Computation.\\n\\nMorrill, Glyn.\\n\\n1990.\\n\\nIntensionality and boundedness.\\n\\nLinguistics and Philosophy, 13(6):699\\n\\n\\n\\n726.\\n\\nMorrill, Glyn V. 1993. Type Logical Grammar: Categorial Logic of Signs. Studies in Linguistics and Philosophy. Kluwer Academic Publishers, Dordrecht, Holland. To appear.\\n\\nNeale, Stephen. 1990. Descriptions. The MIT Press, Cambridge, MA.\\n\\nOehrle, Richard T. 1993. String-based categorial type systems. Workshop ``Structure of Linguistic Inference: Categorial and Unification-Based Approaches,'' European Summer School in Logic, Language and Information, Lisbon, Portugal.\\n\\nPereira, Fernando C. N.\\n\\n1990.\\n\\nCategorial semantics and scoping.\\n\\nComputational Linguistics, 16(1):1\\n\\n\\n\\n10.\\n\\nPereira, Fernando C. N. 1991. Semantic interpretation as higher-order deduction. In Jan van Eijck, editor, Logics in AI: European Workshop JELIA'90, pages 78-96, Amsterdam, Holland. Springer-Verlag.\\n\\nPollard, Carl and Ivan A. Sag. 1987. Information-Based Syntax and Semantics, Volume I. Number 13 in CSLI Lecture Notes. Center for the Study of Language and Information, Stanford University.\\n\\nPollard, Carl and Ivan A. Sag. 1993. Head-Driven Phrase Structure Grammar. The University of Chicago Press, Chicago.\\n\\nRoorda, Dirk. 1991. Resource Logics: Proof-theoretical Investigations. Ph.D. thesis, University of Amsterdam.\\n\\nSaraswat, Vijay A. 1989. Concurrent Constraint Programming Languages. Ph.D. thesis, Carnegie-Mellon University. Reprinted by MIT Press, Doctoral Dissertation Award and Logic Programming Series, 1993.\\n\\nSaraswat, Vijay A. 1993. A brief introduction to linear concurrent constraint programming. Technical report, Xerox Palo Alto Research Center, April.\\n\\nSaraswat, Vijay A. and Patrick Lincoln. 1992. Higher-order, linear concurrent constraint programming. Technical report, Xerox Palo Alto Research Center, August.\\n\\nScedrov, Andre. 1993. A brief guide to linear logic. In G. Rozenberg and A. Salomaa, editors, Current Trends in Theoretical Computer Science, pages 377-394. World Scientific Publishing Co.\\n\\nvan Benthem, Johan. 1986. Categorial grammar and lambda calculus. In D. Skordev, editor, Mathematical Logic and its Application. Plenum Press, New York, New York, pages 39-60.\\n\\nvan Benthem, Johan. 1988. The Lambek calculus. In Richard T. Oehrle, Emmon Bach, and Deirdre Wheeler, editors,   Categorial Grammars and Natural Language Structures. D. Reidel, Dordrecht, pages 35-68.\\n\\nvan Benthem, Johan. 1991. Language in Action: Categories, Lambdas and Dynamic Logic. North-Holland, Amsterdam.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9404009.xml'}),\n",
       " Document(page_content=\"THE SPEECH-LANGUAGE INTERFACE IN THE SPOKEN LANGUAGE TRANSLATOR\\n\\nThe Spoken Language Translator (SLT) is a prototype for practically useful systems capable of translating continuous spoken language within restricted domains. The prototype system translates air travel (ATIS) queries from spoken English to spoken Swedish and to French. It is constructed, with as few modifications as possible, from existing pieces of speech and language processing software. The speech recognizer and language understander are connected by a fairly conventional pipelined N-best interface. This paper focuses on the ways in which the language processor makes intelligent use of the sentence hypotheses delivered by the recognizer. These ways include (1) producing modified hypotheses to reflect the possible presence of repairs in the uttered word sequence; (2) fast parsing with a version of the grammar automatically specialized to the more frequent constructions in the training corpus; and (3) allowing syntactic and semantic factors to interact with acoustic ones in the choice of a meaning structure for translation, so that the acoustically preferred hypothesis is not always selected even if it is within linguistic coverage.\\n\\nOVERVIEW OF THE SLT SYSTEM\\n\\nThe Spoken Language Translator (SLT) is a prototype system that translates air travel (ATIS) queries from spoken English to spoken Swedish and to French. It is constructed, with as few modifications as possible, from existing pieces of speech and language processing software. This section gives a brief overview of the speech recognition and language analysis parts of the SLT system and the philosophy underlying them; for a longer treatment, including details of transfer, generation, and speech synthesis, the reader is referred to Agns et al, 1994. After the overview, we describe three ways in which the language analyser makes intelligent use of the N-best list of sentence hypotheses it receives from the recognizer.\\n\\nAt the highest level of generality, the design of SLT has two guiding themes. The first is that of intelligent reuse of standard components: most of the system is constructed from previously existing pieces of software, which have been adapted for use in the speech translation task with as few changes as possible. The second theme is that of robust interfacing. In this paper, we focus on an important means by which robustness is achieved: the delaying of decisions about words, utterances and utterance meanings until sufficient information is available to make those decisions reliably.\\n\\nThe speech recognizer used is a fast version of SRI's DECIPHER [TM] speaker-independent continuous speech recognition system (Murveit et al, 1991). It uses context-dependent phonetic-based hidden Markov models (HMMs) with discrete observation distributions for four features: cepstrum, delta-cepstrum, energy and delta-energy. The models are gender-independent and the system is trained on 19,000 sentences and has a 1381-word vocabulary. A bigram language model is used. The output is an N-best hypothesis list, produced using a progressive recognition search (Murveit et al, 1993) in which the space of possible utterances is pruned by successively more powerful but more costly techniques. The motivation for this kind of search is to avoid making hard decisions without sufficient evidence, while at the same time maintaining reasonable efficiency.\\n\\nFully-fledged linguistic analysis can be viewed from the perspective of the speech recognition task as the final stage of progressive search: the most powerful, most costly techniques used in the system, exploiting complex syntactic and semantic knowledge, are applied, reducing an already fairly limited set of possible utterances to a single choice. Another, equally valid, perspective on language analysis is from the standpoint of utterance understanding: the purpose of source language processing in SLT is to map from the acoustic signal to a representation of the utterance meaning, and identifying the correct word sequence is a by-product of this process rather than being a goal in its own right.\\n\\nLanguage analysis in SLT is performed by the SRI Core Language Engine (CLE), a general natural-language processing system developed at SRI Cambridge (Alshawi, 1992). The English grammar used for this is a large general-purpose feature grammar, which has been augmented with a small number of domain-specific rules. It associates surface strings with meaning representations in Quasi Logical Form (QLF; Alshawi and Crouch, 1992). Transfer and generation are performed by a second copy of the CLE loaded with a French or Swedish grammar and transfer rules for the appropriate language pair.\\n\\nThe system components are connected together in a pipelined sequence as follows. First, DECIPHER processes the input signal and constructs a list of N-best hypotheses, each tagged with an associated acoustic score; N=5 gives a good tradeoff between speed and accuracy. The construction of this list using the progressive search technique constitutes a thorough pruning of the original search space of all possible word sequences.\\n\\nThe hypothesis list is passed to the English-language version of the CLE, which implements the final phase of progressive search by applying the three processing stages outlined below and described more fully in the remainder of this paper. The CLE achieves robustness in the speech-language interface by postponing the selection of a correct utterance (and utterance meaning) until all available knowledge has been applied. This strategy is made acceptably efficient by the use of a specialized fast parsing technique. The processing stages are these:\\n\\nAs described in Section 2 below, the CLE examines the hypotheses for evidence of speech repairs, and if it finds any, it adds possible corrected versions to the list without removing the originals, thus postponing a decision about whether the correction is valid or not.\\n\\nIt then uses the grammar, specialized and compiled for both speed and accuracy as described in Section 3, to analyze each speech hypothesis (original and repaired) and extract a set of possible QLF representations. This typically results in a set of between 5 and 50 QLFs per hypothesis.\\n\\nThe CLE's preference component is then used to give each QLF a numerical score reflecting its a priori linguistic (acoustic, syntactic, semantic and, within limits, pragmatic) plausibility. The final score for a QLF is calculated as a weighted sum of the scores assigned to it by a range of preference functions, and the highest-scoring QLF is passed on for transfer and target language generation. We describe the functioning of this component in Section 4 below.\\n\\nWe now move on to examining these stages in more detail, starting with the repair mechanism.\\n\\nDETECTION AND CORRECTION OF REPAIRS\\n\\nOne important way in which spoken language differs from its written counterpart is in the prevalence of self-repairs to speaker errors. Examples such as the following occur in the ATIS domain: 1. list LIST FLIGHTS BETWEEN OAKLAND AND DENVER. 2. does this DOES THIS FLIGHT SERVE BREAKFAST. 3. COULD I HAVE MORE DETAILS ON FLIGHT d l sixteen D L SEVEN TWO SIX. 4. SHOW ME ROUND TRIP FARES FOR flight two SORRY FLIGHT FOUR FOUR OH ZERO. 5. I WANT A FLIGHT from boston FROM DENVER TO BOSTON. 6. OK WHAT TYPES OF AIRCRAFT do DOES DELTA FLY. In each case, the reparandum (material to be repaired) is shown in lower case and the repair itself in ITALICS, with any explicit repair marker, such as ``sorry'', shown in BOLD. Note that, once the reparandum and any repair marker have been identified, the location of the right hand end of the repair does not affect the interpretation of the sentence (e.g. the repair in (3) could be viewed as ``D L seven two six'').\\n\\nIn (1) and (2) the reparandum and repair are identical. In (3)-(6) they differ. (3) shows the substitution of a word after the repeated material, (4) shows the use of an explicit repair marker, (5) is an example of the additional material in the repair being inserted, rather than appended, and (6) shows a correction of a suffix, with no strictly identical words occurring.\\n\\nHowever, not all repeated word sequences and (possible) explicit repair markers indicate repairs; items (1') to (4') below are non-repairs superficially similar to (1) to (4) above, with (5') providing additional evidence that not all repetitions are repairs. The typographic conventions show how the word sequences might be misconstrued as repairs. 1'. SHOW ME ROUND TRIP FARES FOR U S FLIGHT four FOUR oh oh. 2'. IS u s U S AIR. 3'. ARE ANY OF THE flights NONSTOP FLIGHTS. 4'. I WANT a flight with NO STOPS. 5'. FROM PHILADELPHIA FROM DENVER AND FROM PITTSBURGH.\\n\\nIt is known that repairs are often indicated acoustically (Bear et al, 1992; Nakatani and Hirschberg, 1993) and DECIPHER could be modified to detect possible repair indicators and pass the information on to the CLE. However, this raises some difficult issues of identification, representation and transportability, and it is worth investigating how effectively repairs can be dealt with on the basis of word strings alone.\\n\\nIn line with the philosophy behind progressive search, that of postponing decisions until sufficient information is available, the CLE's repair mechanism has the following novel feature: when a possible repair is located, no immediate decision is made on whether it is genuine. The (putatively) corrected word sequence is added to the N-best list, and given a reduced acoustic score, without the original hypothesis being removed. Thus QLFs can be built from either sequence, and the final choice of a word sequence is a by-product of the choice of a QLF, which, just as for choices between original hypotheses, takes advantage of full linguistic processing of all parts of the sentence.\\n\\nThis methodology allows a range of repairs to be hypothesized by a fairly straightforward algorithm while minimizing the number of false positives found. Given the word sequence actually uttered, it is in general possible to determine the reality of a repair on the basis of (a) specific, fairly superficial knowledge of what kinds of word sequence tend (in the ATIS domain) to indicate repairs, (b) general and ATIS-specific syntactic and semantic considerations, and (c) knowledge of the discourse and reasoning about the domain. In the translation task, a false positive -- ``correcting'' a non-existent repair -- is a more serious error than failing to deal with a repair that has occurred, because the former kind of error is likely to confuse the user and to be viewed as much less acceptable. The repair detection algorithm therefore attempts to hypothesize just those possible corrections that seem plausible on the basis of type (a) knowledge and that, if they are false, are likely to be detectable using type (b) knowledge, i.e. by syntactic, semantic and preference processing. Type (c) knowledge is not available within the SLT system.\\n\\nThe detection mechanism identifies possible repairs by first searching for repeated roots in the sentence, i.e. pairs of words (other than numbers, which are often repeated intentionally) that can be analysed morphologically by the CLE as having the same root. Examples are ``...flight...flight...'', ``...do...does...'' and ``...is...are...''. It combines these pairs to identify sequences that begin and end with the same roots, e.g. I WANT TO GO FROM BOSTON NO FROM DENVER TO BOSTON ON TUESDAY. Sequences that have intervening material and consist only of one of a set of very common words (``a'', ``and'', ``from'', ``in'', ``of'', ``or'' and ``to'') are discarded at this point, as inspection of the data suggests they are likely to lead to false positives. In all other cases, however, the two sequences (underlined above) are first matched from left to right. Two points are awarded for a shared root, and one is deducted when a word is skipped in either sequence. The match proceeds (by dynamic programming) so as to maximize the score. In the example, two points are awarded for the matches on each of ``from'' and ``Boston'' and one is deducted for skipping each of ``Denver'' and ``to''.\\n\\nIf there is no intervening material, the match is now complete, and the hypothesized repair is returned. If there is intervening material (as with ``no'' above) it may form part of either the repair or the reparandum. Similar, but more general, matches are therefore carried out in both the forward and the backward directions.\\n\\nThe forward match begins at the start of the intervening material and just after the end of the second repetition sequence, i.e., at ``no'' and ``on'' in the example, and continues forwards until all the intervening material is consumed. The backward match starts at the end of the intervening material and just before the start of the first sequence, i.e. at ``no'' and ``go'', and words backwards. One point is deducted for skipping a word in either sequence, unless the match is forward and the word is known to be an explicit repair indicator, in which case a point is awarded. (Explicit repairs are counted only in the forward match to ensure they are identified as material to be deleted). Two words match each other, with no adjustment in the score, only if they share a major category.\\n\\nOnce all matches have been completed for all possible pairs of root sequences, the best one(s) are selected. Higher-scoring matches are preferred, with those involving the deletion of fewer words being favoured when scores are equal. If there are non-overlapping repairs (e.g. ``I want to go from from Boston to San San Francisco'') then the best options for both are accepted.\\n\\nIn the example above, the best path is for the forward match. It consists simply of recognizing ``no'' as a repair indicator and not progressing the second pointer at all. This gives a reparandum of ``from Boston no'' and a repair of ``from Denver to Boston'' with a total score of three.\\n\\nRestricting attention to sentences for which some QLFs were found, of the 79 sentences involving repairs for which a QLF for a repaired version was chosen, the repaired string was correct, or as plausible as any other choice, in 77 cases. In the other two cases, a wrong repair, and no repair, were selected respectively. When no repair was actually present, the preferred QLF was for the unrepaired version in all but 2 of 37 cases. Thus the repair mechanism caused 77 sentences to receive an analysis for the correct string where this would not otherwise have happened, and caused 2 sentences to receive a bogus interpretation when they would not otherwise have received one. In other words, it increased coverage on the training set by (77-2)/4615, or 1.6%.\\n\\nOf course, performance on reference versions (corresponding to perfect speech recognition) of training sentences is likely not to be a good indicator of performance on errorful recognizer outputs for unseen sentences; and in fact, applying the current repair mechanism to such outputs does tend to result in the acceptance of noticeably more bogus repairs, nearly all arising from incorrect sentence hypotheses. As already indicated, this is quite undesirable.\\n\\nHowever, many if not most errors of this type are due to the fact that the repair mechanism is being applied to a qualitatively different kind of data from that used to guide its design. We are encouraged by the fact that, for the reference sentences, a relatively simple repair suggestion algorithm can lead to such accurate decisions on the validity of the repair by the much more sophisticated subsequent language processing (only 4 wrong choices of string out of 116 cases where a choice was made). Further work will involve redesigning the algorithm, and probably training it automatically, to handle the kinds of output characteristic of the recognizer. As Section 4 below will argue more fully, training language processing decisions on typical recognizer behaviours rather than only on reference sentences can enhance decision-making considerably.\\n\\nGRAMMAR SPECIALIZATION FOR FAST PARSING\\n\\nLanguage models used in the context of speech recognition are normally some variety of finite-state grammar. Bigram grammars are probably still the most popular choice, and one is used by the version of DECIPHER incorporated in SLT. Trigram or higher N-gram models and stochastic context-free grammars are also fairly common. The advantages of finite-state models are well-known: they are fast, robust, and easy to train. The disadvantages are also clear: viewed as grammatical formalisms, they are insufficiently expressive to capture many important types of linguistic regularities, and so although they are useful in the non-final stages of the progressive search task, they are not adequate for the final stage, nor indeed for constructing a sufficiently rich semantic representation to support translation.\\n\\nHowever, use of more powerful and expressive grammar formalisms tends to be impractical due to the excessively slow processing times associated with most known parsing algorithms. This would be especially problematical in the SLT system when the language analysis carried out by the CLE counts as a single, final stage of progressive search, so that many possibilities are considered before any are ruled out.\\n\\nIn the language analysis part of the SLT system, we have therefore implemented what we think is an interesting compromise between the opposing positions of fast finite-state language models and general linguistically-motivated grammars. The bulk of this work (most of which has carried out in collaboration with Christer Samuelsson of SICS, Stockholm) has been reported elsewhere (Rayner, 1988; Rayner and Samuelsson, 1990; Samuelsson and Rayner, 1991; Samuelsson, 1994). We content ourselves here with a brief summary relating it to the themes of the present paper.\\n\\nWe start with a general, linguistically motivated grammar, which has been given enough specialized vocabulary to have good domain coverage. In the SLT project, we used the CLE grammar for English (Alshawi, 1992, chapters 4 and 5; Agns et al, 1994, chapter 7), but the techniques do not make any special use of its peculiarities, and would be applicable to any general unification-based phrase-structure grammar. The key point is that the general grammar is unsuitable for the language modelling task because it is over-general; in particular, there is no need in the context of a normal spoken language domain to have a fully recursive grammar.\\n\\nWe specialize the grammar to the domain by first using it to parse a substantial corpus of examples (in the concrete experiments carried out, we used a set of about 5000 ATIS sentences). We then extract a much simpler grammar from the original one by cutting up the analysis trees from the parsed corpus into sub-trees, where each sub-tree corresponds to a linguistic ``chunk'' or unit; we used only four chunk types (utterance, noun phrase, non-recursive noun-phrase and preposition phrase), compared to about twenty-five different phrase types in the original grammar. The rules contained in each sub-tree are then ``collapsed'' into a single rule for the appropriate chunk-type, using the so-called Explanation-Based Learning algorithm (van Harmelen and Bundy, 1988; Hirsh, 1987). With a suitable choice of chunk-types, we can produce a specialized grammar whose rules correspond to chunk patterns occurring in the training corpus.\\n\\nBy construction, the specialized grammar has strictly less coverage on the domain than the original one. Our experiments suggest, however, that given a substantial training corpus the loss of coverage is on the order of a few percent at most. This loss of coverage is more than counterbalanced by the greatly simplified structure of the specialized grammar, which can be parsed nearly two orders of magnitude more quickly than the general one, using an LR parsing algorithm (Samuelsson, 1994). The gain in speed is due to the fact that the grammar, after specialization, is nearly finite-state; we have in effect automatically squeezed a general grammar into a finite-state format, after cutting off the few pieces that refuse to fit.\\n\\nApart from the enormous gain in speed, it is also worth noting that the specialized grammar is less ambiguous than the general one; for a given sentence, it normally produces substantially fewer different analyses. This implies that the task of identifying a correct analysis becomes correspondingly simpler. The ``preference component'' described in the next section has less work to do, and makes incorrect choices less often. In practice, we have discovered that this extra accuracy more or less cancels out the loss of grammatical coverage; the few sentences outside specialized grammar coverage tend to be so complex and ambiguous that there is a high chance of an incorrect analysis being preferred.\\n\\nDISAMBIGUATION\\n\\nOnce zero or more QLFs have been produced for each of the original and repaired sentence hypotheses in the N-best list, the preference component of the CLE has the task of selecting the most appropriate one for translation. It does this by assigning a score to each QLF and selecting the highest-scoring one, as we will now describe. A full account is given in Alshawi and Carter (1994).\\n\\nPreference Functions\\n\\nThe score assigned to a QLF is a scaled linear sum of the scores returned by a set of about twenty individual preference functions. Preference functions are of three types.\\n\\nFirstly, there is a speech function which simply returns the acoustic score for the sentence hypothesis that gave rise to the QLF (or a default low score if the hypothesis was suggested by the repair algorithm).\\n\\nSecondly, structural functions examine some aspect of the overall shape of the QLF. Typically, the number of occurrences of some relatively unlikely type of grammatical construction is counted, so that readings which contain instances of it can be penalized relative to those that do not.\\n\\nThirdly, combining functions collect instances of linguistic objects such as: N-grams in the underlying word string; the syntax rules used to create the QLF; and triples of the form\\n\\n(H1,R,H2), where H1 and H2 are the head predicates of QLF substructures representing words or phrases in the sentence and R indicates the relationship (e.g. a preposition or an argument position) between them. Semantic classes are used to group place names, numbers and other sets with similar distributions. For example, the set of triples for the correct analysis of ``Show me the flights to Boston'' includes these: (show_CauseToSee,3,flight) (flight,to,*place) the second of which indicates the attachment of ``to Boston'' to ``flights'' rather than to ``show''. The combining function calculates, by addition or averaging, a score for the QLF based on the scores for the individual objects. The objects in turn take their scores from the pattern of their occurrence in correct and incorrect QLFs observed in training on recognizer outputs on a corpus for the domain in question. Roughly, an object score is intended to be an estimate of the log probability that a QLF from which the object arises is the correct one.\\n\\nScaling Factors\\n\\nThe scaling factors used to derive a single summed score for a QLF from the scores returned for that QLF by the various preference functions are also trained automatically in order to maximize of the chances of the highest-scoring QLF being correct. Scaling factor training has two phases.\\n\\nThe first phase makes use of a measure of the similarity between each QLF for a sentence and the correct QLF (selected in advance by interaction with a developer) for that sentence. This measure is sensitive to differences both in the underlying word sequences and in the groupings of the words into phrases by the QLFs. Linear (least squares) optimization is carried out to find the scaling factor values that make the preference scores for QLFs resemble the similarity measures as closely as possible. This is an analytic process that can be carried out fairly quickly. However, its objective function, that of modelling similarity to the correct QLF, is only approximately related to the behaviour we want, that of ensuring that the correct QLF is placed first in the preference ordering, regardless of the scores of incorrect QLFs relative to each other.\\n\\nIn the second phase, therefore, scaling factors are adjusted iteratively to increase the number of training sentences for which the correct QLF gains the highest score; that is, attention is focused on selecting correctly among the few most plausible QLFs, and not on predicting the scores of clearly implausible ones, whose relative merits are unimportant. Since this task is non-linear, it is fairly computationally intensive, and may only find a local optimum, so that the first, linear phase is essential to find a good starting point for it.\\n\\nAfter scaling in this way, the preference functions are able to select the correct QLF (as judged by an expert) in 90 to 95% of cases when trained on four fifths of a corpus of the reference versions of 4092 within-domain, within-coverage ATIS sentences of up to 15 words in length and tested on the other one fifth, with each one fifth being held out in turn for testing. This result is for the QLFs produced with a version of the grammar that had not undergone the specialization process described in Section 3 above. The figure would be still higher if only the smaller number of QLFs arising from the specialized grammar were compared. Thus, as remarked earlier, the tendency of grammar specialization to reduce coverage slightly is largely offset by the fact that, for sentences that are still in coverage, fewer erroneous QLFs are produced which may be preferred over the correct one.\\n\\nA Comparison\\n\\nTo appreciate the importance of some of the points in the above description, it is instructive to compare the process described above with the somewhat simpler training procedure used in an earlier version of the system. For clarity, we will call the earlier version SLT-0, and current version, implementing the above procedure, SLT-1. SLT-0 lost some accuracy because in it, the various scores and scaling factors were optimized for tasks related to, but not identical to, that encountered at run-time.\\n\\nThe first difference is that in SLT-0, the linguistic objects used by some of the combining metrics were scored not by comparing good and bad QLFs but on the basis only of their frequency of occurrence in good QLFs. As we will see in the next section, this is suboptimal, because an object is not a good predictor of correctness simply because it occurs frequently in good QLFs; it may occur just as often in bad ones.\\n\\nSLT-0's second drawback was that training with respect to the corpus was decoupled from training with respect to the speech recognizer. That is, object scores and all the non-speech scaling factors were calculated by looking only at QLFs for the reference versions of corpus sentences, and not at recognizer outputs. The scaling factor for the speech function was then found by trial and error on a separate training corpus. Thus SLT-0 had no opportunity to adapt to and compensate for typical recognizer errors.\\n\\nQLF selection accuracy turned out in fact to be relatively insensitive to the value of the acoustic factor, which can be doubled or halved without noticeable effect. However, the lack of training on incorrect sentence hypotheses was a more serious drawback. There are syntactic and semantic patterns which seldom occur in analyses of correct sentence hypotheses and therefore were not assigned very large scores, but which often crop up as a consequence of certain kinds of recognizer error. A known example of this behaviour is number disagreement between subject and predicate in a sentence hypothesis with main verb ``be'', for example ``What is the first flights to Boston?''. This is grammatically possible but most unlikely to be correct, and usually indicates that the head noun of the predicate phrase has been recognized with the wrong number: in the example, the word spoken would actually have been ``flight''. There are also examples of semantic triples, and perhaps also syntax rules, which likewise tend to characterize analyses of wrong hypotheses but which, for that very reason, are not observed when training only on correct word strings. It is not sufficient to finesse this problem by penalizing objects only observed infrequently in training on reference sentences, because there is no a priori way of knowing whether such an object, when encountered at run time, indicates a recognizer error or just an unusual, but genuine, form of words.\\n\\nTuning to the N\\n\\n\\n\\nBest Task\\n\\nThe deficiencies just described for SLT-0 had the effect that selecting a sentence hypothesis using the trained combination of speech, structural and combining preference functions only yielded a 2% increase in sentence accuracy (as measured on a 1000-sentence unseen test set) over using the speech score alone. This figure is in a sense misleadingly pessimistic, since we are interested in translation rather than recognition per se, and the combined functions always select a hypothesis for which a QLF, and therefore potentially a translation, is found, whereas the recognizer alone sometimes prefers an unanalysable string, which even if correct will not be translated. Nevertheless, it seemed likely that introducing linguistic factors, if done optimally, should improve sentence accuracy by more than a couple of per cent.\\n\\nThe speech function, returning the recognizer score.\\n\\nTwo structural functions: one which returned 1 if any QLFs were found for the sentence using the specialized grammar, and otherwise 0; and one which returned 1 if the best QLF for the string (as judged by the existing preference module) contained a subject-predicate number mismatch, and otherwise 0.\\n\\nTwo combining functions: one for grammar rules used in the best QLF for the string, and one for the semantic triples for that QLF.\\n\\nWe concluded from these results that it is well worth training linguistic functions in this way. One further possible improvement is that for sentence recognition (although probably not for translation, because of the risk of errors), it would also be desirable to derive QLF analyses of parts of a sentence when no full analysis could be found; this would allow linguistic functions always to make some contribution, even if only an imperfect one, and would improve accuracy on utterances for which no hypothesis was perfectly correct and those which included constructions outside the coverage of the grammar.\\n\\nSUMMARY AND CONCLUSIONS\\n\\nWe have described the ways in which language analysis in SLT makes intelligent use of the N-best hypothesis list delivered by the speech recognizer, implementing the final stage of progressive search by avoiding nearly all hard decisions about word identities or sentence meanings until all available linguistic knowledge has been applied. That is, the CLE creates its whole search space before pruning away any of it. Thus alternative QLF analyses for the same recognizer hypothesis, for different recognizer hypotheses, and for repaired as well as unrepaired versions of hypotheses are all constructed and compared in a uniform way. The use of an automatically tuned grammar and associated fast parser makes this generate-and-test process acceptably fast (typically a few seconds per speech hypothesis on a SPARCstation 10) by eliminating many impossible search paths and some possible but unlikely ones.\\n\\nIt would be possible to speed up the system further by parallelizing it. Each recognizer hypothesis could be analysed separately, and the highest-scoring QLF (if any) resulting from it returned for a final choice to be made.\\n\\nThe unattainable ideal in any search problem is for the search space constructed to consist only of the correct solution, or of solutions that are equally likely to be correct. We approximate this ideal in the speech understanding task by training and selecting grammar rules (the objects that generate possible solutions) on human-transcribed reference material, so that, as far as possible, correct solutions will fall within the search space and incorrect ones will fall outside it. In practice, of course, by no means all incorrect solutions will be excluded in this way; so we train preference functions on recognizer and language analysis output, to maximize our chances of distinguishing correct from incorrect solutions, whatever stage of processing they arise from.\\n\\nIn Section 4.4 above we gave performance details for speech and language analysis. Sentence recognition accuracy using optimized speech (DECIPHER) and language (CLE and N-gram) information on unseen ATIS data is 73.7%. Full details of the performance of an earlier version of the full system (roughly SLT-0) are given by Rayner et al, 1993. Briefly, however, for sentences within the ATIS domain and up to twelve words in length, if a correct speech hypothesis is selected then a Swedish translation is produced on about three occasions in four, and 90% of those translations are acceptable. The remaining 10% can nearly all be clearly identified by the hearer as errors because they are ungrammatical or unnatural; divergences in meaning, which might lead to more serious forms of dialogue failure, are extremely rare.\", metadata={'source': '../data/raw/cmplg-xml/9411028.xml'}),\n",
       " Document(page_content=\"A Labelled Analytic Theorem Proving Environment for Categorial Grammar\\n\\nWe present a system for the investigation of computational properties of categorial grammar parsing based on a labelled analytic tableaux theorem prover. This proof method allows us to take a modular approach, in which the basic grammar can be kept constant, while a range of categorial calculi can be captured by assigning different properties to the labelling algebra. The theorem proving strategy is particularly well suited to the treatment of categorial grammar, because it allows us to distribute the computational cost between the algorithm which deals with the grammatical types and the algebraic checker which constrains the derivation.\\n\\nBackground\\n\\nA Family of Categorial Calculi and Their Linguistic Applications\\n\\nCategorial Grammars can be formalised in terms of a hierarchy of well understood and mathematically transparent logics, which yield as theorems  a range of combinatorial operations. However the precise nature of the combinatorial power required for an adequate characterisation of natural language is still very much a matter of debate. For this reason, it is desirable to have a means of systematically testing the linguistic consequences of adopting various calculi. In this section we give an overview of the linguistic applications of some of the calculi in the hierarchy, with a view towards motivating the usefulness of a generic categorial theorem prover as a tool for linguistic study.\\n\\nA framework for Categorial Deduction\\n\\nThe core syntax\\n\\nWe assume that there is a finite set of atomic grammatical categories which will be represented by special symbols: NP for noun phrases, S for sentences, etc. So, the set of well-formed categories can be defined as below.\\n\\nThe generalised parsing strategy\\n\\nIf we had restricted the system to dealing with signed formulae, we would have a proof procedure for an implicational fragment of standard propositional logic enriched with backwards implication and conjunction. However, we have seen that the Lambek calculus does not exhibit any of the structural properties of standard logic, and that different calculi may be obtained by varying structural transformations. Therefore, we need a mechanism for keeping track of the structure of our proofs. This mechanism is provided by labelling each formula in the derivation with information tokens.\\n\\ny \\\\ensuremath{\\\\circ \\\\;} (x \\\\ensuremath{\\\\swarrow} y)  \\\\ensuremath{\\\\sqsubseteq \\\\;}  x \\\\end{eqnarray} -->\\n\\n\\\\textnormal{\\\\texttt{1}}{}  \\\\ensuremath{\\\\sqsubseteq \\\\;}  x \\\\ensuremath{\\\\swarrow} x \\\\end{eqnarray} -->\\n\\n(x \\\\ensuremath{\\\\swarrow} y) \\\\ensuremath{\\\\circ \\\\;} z  \\\\ensuremath{\\\\sqsubseteq \\\\;}  (x \\\\ensuremath{\\\\circ \\\\;} z) \\\\ensuremath{\\\\swarrow} y \\\\end{eqnarray} -->\\n\\n(x \\\\ensuremath{\\\\swarrow} y) \\\\ensuremath{\\\\swarrow} z  \\\\ensuremath{\\\\sqsubseteq \\\\;}  x \\\\ensuremath{\\\\swarrow} (y \\\\ensuremath{\\\\circ \\\\;} z) \\\\end{eqnarray} -->\\n\\nComparison with Existing Approaches\\n\\nConclusions and Further Work\\n\\nWe have described a framework for the study of categorial logics with different degrees of expressivity on a uniform basis, providing a tool for testing the adequacy of different CGs to a variety of linguistic phenomena. From a practical point of view, we have investigated the effectiveness and generality issues of a parsing strategy for CG opening an avenue for future developments. Moreover, we have pointed out some strategies for improving on efficiency and for dealing with more expressive languages, including structural modalities.\\n\\nBibliography\\n\\nA. Ades and M. Steedman. On the order of words. Linguistics and Philosphy, 4:517-558, 1982.\\n\\nK. Ajdukiewicz. Die syntaktische konnexitt. Studia Philosophica, 1(1-27), 1935. (Translation in S. McCall (ed) Polish Logic 1920-1939 Oxford).\\n\\nAssociation for Computational Linguistics. 7[th] Conference of the European Chapter of the ACL, Dublin, Ireland, March 1995. Morgan Kaufmann.\\n\\nGuy Barry and Glyn Morrill. Studies in Categorial Grammar, volume 5 of Edinburgh Working Papers in Cognitive Science. Centre for Cognitive Science, 1990.\\n\\nJ. Barwise, D. Gabbay, and C. Hartonas. On the logic of information flow. Bulletin of the IGPL, 3(1):7-50, 1995. http://www.mpi-sb.mpg.de/guide/staff/ohlbach/igpl/Bulletin.html.\\n\\nJohan van Benthem. The semantics of variety. In Wojciech Buszkowski, Witold Marciszewski, and Johan van Benthem, editors, Categorial Grammar, volume 25, chapter 6, pages 141-151. John Benjamins Publishing Company, Amsterdam, 1988.\\n\\nMarcello D'Agostino and Dov Gabbay. A generalization of analytic deduction via labelled deductive systems I: Basic substructural logics. Journal of Automated Reasoning, To appear. Revised in March 1994.\\n\\nMarcello D'Agostino and Marco Mondadori. The taming of the cut. Journal of Logic and Computation, 4:285-319, 1994.\\n\\nDavid Dowty. Type raising, functional composition, and non-constituent conjunction. In Deirdre Wheeler Richard T. Oehrle, Emmon Bach, editor,   Categorial Grammars and Natural Language Structures, pages 153-197. Reidel Publishing Co, Dordrecht, 1988.\\n\\nMelvin Fitting. First-order Logic and Automatic Theorem Proving. Texts and Monographs in Computer Science. Springer-Verlag, New York, 1990.\\n\\nDov M. Gabbay. LDS - Labelled Deductive Systems, volume 1 -- foundations. Technical Report MPI-I-94-223, Max-Planck-Institut fr Informatik, 1994.\\n\\nJoachim Lambek. On the calculus of syntactic types. In Proceedings of the Symposia in Applied Mathematics, volume XII, pages 166-178, Providence, Rhode Island, 1961. American Mathematics Society.\\n\\nSaturnino F. Luz Filho and Patrick Sturt. A new approach to categorial theorem proving. In preparation.\\n\\nMichael Moortgat.\\n\\nCategorial Investigations.\\n\\nForis Publications, Dordrecht, 1988.\\n\\nMichael Moortgat. Labelled deductive systems for categorial theorem proving. Technical Report OTS-WP-CL-92-003, OTS, Utrecht, NL, 1992.\\n\\nMichael Moortgat. Lecture notes on categorial grammar. Reader for the course on Categorial Grammar given at the 5[th]  ESSLLI - University of Lisbon, August 1993.\\n\\nMartin Pickering and Guy Barry. Dependency categorial grammar and coordination. Linguistics, 31(5):855-902, 1993.\\n\\nRaymond M Smullyan. First-Order Logic, volume 43 of Ergebnisse der Mathematik und ihrer Grenzgebiete. Springer-Verlag, Berlin, 1968.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9508009.xml'}),\n",
       " Document(page_content=\"No Title\\n\\nIntelligent Voice Prosthesis is a communication tool which reconstructs the meaning of an ill-structured sequence of icons or symbols, and expresses this meaning into sentences of a Natural Language (French). It has been developed for the use of people who cannot express themselves orally in natural language, and further, who are not able to comply to grammatical rules such as those of natural language. We describe how available corpora of iconic communication by children with Cerebral Palsy has led us to implement a simple and relevant semantic description of the symbol lexicon. We then show how a unification-based, bottom-up semantic analysis allows the system to uncover the meaning of the user's utterances by computing proper dependencies between the symbols. The result of the analysis is then passed to a lexicalization module which chooses the right words of natural language to use, and builds a linguistic semantic network. This semantic network is then generated into French sentences via hierarchization into trees, using a lexicalized Tree Adjoining Grammar. Finally we describe the modular, customizable interface which has been developed for this system.\\n\\nIntroduction\\n\\nusers' needs\\n\\nSome people are unable to speak not only because of phonatory or articulatory reasons, but because a neurological handicap deprives them, temporarily or permanently, of their language ability. They suffer from various types of language difficulties, which can consist of missing words, loss of the semantic link between a word and its meaning, inability to structure their speech, etc.\\n\\nFor all these people, no voice synthesis with an interface based on letter or word selection would make up for the speech impairment. They are unable not only to speak, but also, for various reasons, to compose a written sentence.\\n\\niconic communication\\n\\nDuring supervised communication sessions, a person used to communicating with the speech impaired (parent, orthophonist, ergotherapist ...) goes into a process of intelligently interpreting the sequence of icons designated, and then formulating it back in natural language sentences. The aim of our system is to make this process automatic and thus widely available.\\n\\nThe observation of practical communication situations with the patients in Kerpape convinced us that a semantic interpretation approach was necessary for this purpose. A simple icon-to-word translation proved to be unsufficient to model the process: the sequences of icons used are not organized into regular structures, but are generally arranged in an order depending on the message's topicality.\\n\\nAn example of an utterance met in these iconic communication corpora will illustrate this point:\\n\\n[past\\n\\n\\n\\ntense\\n\\n\\n\\nindicator] boat to_eat\\n\\nThis was intended to mean ``I had a meal in a boat'' (stressing the boat context).\\n\\nNo parser based on Context Free Grammars (CFG), whatever the number of rules, can account for the different meanings of:\\n\\nboat to_eat(``I eat in a boat'')\\n\\nand:\\n\\nbeefsteak to_eat(``I eat a\\n\\nbeefsteak'').\\n\\nThe only way to cope with this type of different dependencies is to model some of the natural expertise which allows the experienced communication partner to assign a correct meaning to such utterances, on the basis of a semantic interpretation.\\n\\nThat is why our system is based on the following processes:\\n\\nanalysis of the semantic content of the icons used, and attribution of a semantic role to each of them;\\n\\nlexical choice: determining the best words to use to convey the semantic content;\\n\\nlinguistic generation: generation of a natural language utterance from a topicalized representation adapted to linguistic semantics.\\n\\nExtracting the lexicon from corpora\\n\\nThe methodological choices described were supported by the analysis of corpora of iconic communication from children with Cerebral Palsy in the Kerpape Rehabilitation Centre.\\n\\nThese corpora were produced by the disabled people ...\\n\\nin a situation when they had to communicate;\\n\\nin a situation of exercise (during ergotherapy training sessions, which means with no communicative urge nor time limitations);\\n\\nin an alternated dialogue situation, when some interlocutor could make answers, guess meanings, complete utterances ...\\n\\nThe first situation will most strongly determine the design of the PVI system, since it represents the most important requirements we are trying to meet. The first corpus has thus been thoroughly analysed, the two others giving complementary information on some elements of the lexicon.\\n\\nThe corpus-based building of the lexicon guarantees the most relevant description of the available lexical field, as it allows the system builder (a) not to forget anything which is in the corpus, and (b) not to put anything superfluous in the lexicon.\\n\\nThe regularities observed in the syntagmatic dimension (classes of icons which systematically occur together in the sequences) help us build the casual structure of predicative concepts. For example the icon representing `to eat' will in most cases, in the corpus, go together with an icon representing a human being or an animal, and with an icon belonging to the class which we have identified to contain `beefsteak', `pizza', ... This will be expressed in the icon lexicon by giving the icon `to eat' a default casual structure implying a first casual function which we will call agent, and a second casual function which we will call object.\\n\\nEach icon has its semantic content determined (a) by the taxeme it belongs to, and (b) by elementary meaning features, the specific features, which allow icons of the same taxeme to be distinguished.\\n\\nAfter the analysis, the icon lexicon is given its structure:\\n\\nWhat is the meaning of an icon?\\n\\nThe symbols treated by PVI are represented in a structural semantic system, where the meaning content of every icon lies in fact in the features which distinguish it from the other icons. The elementary features used to describe this meaning are valuated attributes (most of the time binary), which constitute the semantic primitives of the system.\\n\\nSemantic analysis\\n\\nThe semantic analysis process tries to reconstruct the meaning of the sequence of icons pointed at by the user. It builds a meaning representation of the user's utterance, in which every icon in the sequence is attributed a semantic role.\\n\\nA free node in a partly instantiated network is a slot whose content is an uninstantiated variable. If the specifications attached to the node (the case features) are compatible with a given icon, the variable is instantiated: it takes the icon as its value. If the icon is itself predicative, i.e. it is the head of another partially instantiated network, this second network becomes attached to the first. The process goes on until all possible free slots have been unified.\\n\\nThe search for a solution can be described in the following way:\\n\\n1 The system scans the input sequence of icons from left to right. When finding a predicative icon, it looks at its casual structure and picks a free slot;\\n\\n2 Every icon in the remainder of the sequence is then looked up, aiming to find one which will fill the slot, identifying it as a case filler of the current predicate;\\n\\n3 When an acceptable solution has been found, another free slot is picked;\\n\\n4 When an acceptable solution has been found for every slot, the system goes back to scan the input sequence for other predicative icons.\\n\\nThe notion of compatibility between semantic features, used to determine whether an icon is an acceptable filler for a given casual slot, can be defined in different ways depending on the selectivity expected from the PVI system. It is a binary relation defined on two sets of semantic features. When applied to (a) the set of ``case features'' and (b) the semantic features of the candidate, its value characterizes the good candidates.\\n\\nThis binary relation may be:\\n\\nmere inclusion, if the semantic constraint expressed by the case features is mandatory: C(a,b) is 1 if all features in a are present in b and have the same value, 0 otherwise, (this means that a good candidate must have all the semantic features expected from the functor);\\n\\na scaled product between the two sets if more or less acceptable solutions may be found, for example: C(a,b) is the number of features of a which are present and have the same value in b, divided by the total number of features in a, (this means that approximate solutions are allowed).\\n\\nThe result of an analysis is a semantic network expressed in a linear form. The linear order of the network results from the processing order of the different predicates, i.e. from the order in which they appeared in the input sequence. It represents the topical orientation of the message.\\n\\nLexical choice\\n\\nThis lexical choice step is motivated by the observation that there is no simple bijection between icons and words, and that their meaning content is not necessarily isomorphic. The semantic network resulting from the analysis of a message composed with icons is not, strictly speaking, made up of sememes but of what we might call semioms: clusters of semantic features which do not necessarily match up linguistic entities.\\n\\nDuring the lexical choice phase, clusters of semantic features of a linguistic nature will be chosen to express those semioms in natural language in the best possible way.\\n\\nThe implemented mechanisms for this lexical choice component are:\\n\\nThese mechanisms have already been explored for automatic translation studies, the problem of lexical choice being also an important issue in this field.\\n\\nGeneration\\n\\nThe last phase is the generation of a natural language sentence conveying the meaning of the linguistic semantic network. This operation is based on the principle that every sememe may be expressed through a small number of lexemes (in many cases one -- sometimes two or three, depending on the syntactic function, e.g. `work' [noun] vs. `to work' [verb]), each of which is in its turn lexicalised through a certain number of morphemes (for inflexion).\\n\\nThe semantic structure of casual relations linking a sememe to its casual fillers is itself expressed in natural language through a morpho-syntactic structure which native speakers of a language will identify. Among the mechanisms that natural languages have developed in this purpose, French uses chiefly the following three:\\n\\nword order (e.g. [subject] [verb] [object] ...);\\n\\ninflexion (plural of nouns, conjugation of verbs ...);\\n\\nfunctional morphemes (e.g. `'' (at, to), ``de'' (of, from), ``sur'' (on) ...).\\n\\nOur lexicon stores, for every entry, elementary syntactic trees representing possible phrase constructions. Each of these elementary syntactic trees specifies the following information:\\n\\nthe lexeme corresponding to the sememe;\\n\\nthe morphosyntactic structure expressing its casual structure.\\n\\nThis is a lexicalized grammar.\\n\\nIn our system, the generation of the sentence corresponding to the semantic network is done during a scan over the network, considering predicates in turn in the topical order, following the semantic links. It corresponds to a one shot run over a spanning tree of the network.\\n\\nFor every node (sememe) met in the network, a corresponding elementary tree is selected. Elementary trees are assembled using the following operations:\\n\\nadjunction, for the ``optional'' functors (like, for example, if we had a locative complement in the example above), or for a new predicative sememe for which an already generated sememe acts as functor (like, for example, a qualifying adjective).\\n\\nWhen the system comes to a new predicative icon which cannot be generated through an adjunction to the sentence tree currently being built, a new sentence tree is generated. A network can thus be generated into more than one tree.\\n\\nThe output sentences are the list of inflected morphological forms of the terminal nodes of those trees. These morphological forms are found in a morphological lexicon.\\n\\nThe sentences are eventually vocalized by a text-to-speech voice synthesis device.\\n\\nInterface\\n\\nSome potential users of the PVI system have neuromotor troubles which make the use of classical, widespread interfaces difficult for them. One of the goals of the system is to provide them with an adapted interface.\\n\\nA Human-Computer Interface (HCI) consists of a set of material devices and logical routines allowing the person and the computer to exchange information. The PVI system is to be installed and run on a MacintoshTM type computer, which offers in its standard operating system a graphical window manager interface and supports many input/output devices. Some disabled people may not use very well the most common input devices: keyboard, mouse, trackball or joystick. Some more specific devices, such as a tongue contactor or ultrasonic ``headphones'' (a head-commanded pointing device) may be used in some cases. The simplest device might consist of a mere push button.\\n\\nThese users are compelled to use simplified hardware devices. The information supplied to the machine is all the poorer. The support of these devices thus demands from the interface software specific methods to assist the user in his/her interaction with the machine.\\n\\nArchitecture\\n\\nIn the case we are interested in, we expect the interface to allow the user to point at a sequence of symbols, and to be able to synthetize an oral French sentence. Given the variability of possible neuromotor impairments challenging some users of PVI, and the diversity of devices able to be used, it is vital to design an open and flexible system. The interface for PVI is in nearly every detail customizable.\\n\\nThe graphical interface is a segmented window displaying icons. It deals with four data types: text, pictures, sounds and moving pictures. The internal representation of an icon contains data of one or more of these types, which means that an icon can be as much a sound icon as a visual icon. For example, the `cat' icon displays the drawing of a cat, the string ``chat'', and a meowing sound.\\n\\nApart from their perceptive content, the icons may be of two types: symbols or actions. The symbols are transmitted to the semantic analyser, whereas the actions directly command the parameters of the interface (change window, louder sound, etc. ).\\n\\nThe windows are used to group homogeneous icons. All the static aspects (position, size, color of icons and windows, sound volume, etc.) and all the dynamic aspects (pointing method used, editing commands, etc.) of the interface are customizable.\\n\\nDynamics\\n\\nWhat are the methods that may be used to point at the icons composing the message?\\n\\nIn a first case, the user has sufficient motor abilities to use a pointing device commanding a graphic cursor (like a mouse). (S)he may then click on the selected icon, or else leave the cursor unmoved for a minimum time to activate an implicit selection.\\n\\nIn a second case, the user is able to use a keyboard and (s)he may access a large number of keys. Every key will then correspond to an icon on the screen, possibly on a special keypad. This is direct access designation.\\n\\nIn the last case, the user's mobility is reduced and (s)he can only use a binary information device (like a push button). The system has to make up for the missing information by a motion automaton: since the user cannot move the cursor, the cursor moves from one square to another in the window, and the user just has to validate the selected icon when the cursor is in front of it.\\n\\nThere are more or less sophisticated motion automata:\\n\\nthe cursor is displayed on every square in the window in turn;\\n\\nthe cursor is a window-wide line and is displayed on each line in turn in the window. The user stops it when it is positioned on the line where the selected icon is. Then a cursor moves from one square to another along the line.\\n\\nthe cursor is a surface which can encircle a region of the screen. The window is divided into groups of squares. The cursor is displayed in turn on each of these groups of squares. When the user has selected one group, the cursor may continue to move, within it, on subgroups of squares, or on the squares themselves.\\n\\nFor example, if the window is composed of 32 squares arranged on 4 lines and 8 columns, designating an icon might take from 1 to 32 moves with the first method, from 2 (1+1) to 12 (4+8) moves with the second one, and -- assuming that the selected group of squares is divided in two at each step --, will always take 5 moves with the third method.\\n\\nConclusion\\n\\nThe PVI system is a fully-implemented system, although as yet limited to a small semantic domain. It carries out the entire processing chain for converting messages from one sign system to another. For the needs of our application, we were led to develop a semantic interpretation mechanism to understand the icon sequences. We also have developed a generation module which implements the operations on trees in Tree Adjoining Grammars; these operations constitute a good model to express semantic unification in natural language.\\n\\nPVI has been designed to have a reasonable robustness inside its domain. For field test, the system has entered a preliminary validation phase in the Kerpape Rehabilitation Centre. It has received encouraging comments regarding its modularity and flexibility, which are important features for disabled users.\\n\\nFuture work will be dedicated to a subtler analysis of the relation between the semantic contents of two different sign systems. Context analysis should also lead to a better processing of ambiguities and reference.\\n\\nAcknowledgements\\n\\nWe would like to thank J.-P. Departe (Kerpape Centre) and M. Zock (LIMSI) for their valuable advice on the requirements and advancement of PVI, A. Werts (Thomson) for his support and advice about leading the project, and M. Cavazza (Thomson) for valuable discussions during the project, and detailed comments on this paper. We also owe thanks for useful comments to M. Zurfluh, X. Pouteau and J. Fowler.\\n\\nBibliography\\n\\nA. Abeill. Une Grammaire Lexicalise d'Arbres Adjoints pour le Franais. PhD thesis, Universit Paris 7, 1991.\\n\\nC. K. Bliss.\\n\\nSemantography.\\n\\n1965.\\n\\nM. Cavazza. La description du contenu lexical. In F. Rastier, M. Cavazza, and A. Abeill, eds.,   Smantique pour l'analyse, chapter IV. Masson, Paris, 1994.\\n\\nA. K. Joshi, L.S. Levy, and M. Takahashi. Tree adjunct grammars. Journal of Computer and System Sciences, 1975.\\n\\nA. Kim. Graded unification: a framework for interactive processing (cmp-lg/9406013). In ACL'94 Student Session, 1994.\\n\\nP. Pedelucq. Apprhension des troubles de communication de l'Infirme Moteur Crbral. Technical report, Kerpape Rehabilitation Center, 1993.\\n\\nF. Rastier. Smantique Interprtative. Formes Smiotiques. PUF, Paris, 1987.\\n\\nF. Rastier. Smantique et recherches cognitives, chapter La perception smantique, pages 214-216. Formes Smiotiques. PUF, Paris, 1991.\\n\\nM. Zock. Is content generation a one-shot process or a cyclical activity of gradual refinement? The case of lexical choice. In H. Horacek and M. Zock, eds., New Concepts In Natural Language Generation: Planning, Realization, Systems. Pinter Publishers, London, 1992.\\n\\nFootnotes\\n\\nThe PVI (Prothse Vocale Intelligente) project has been funded by AGEFIPH and Thomson-CSF. It has been the object of a CAP-HANDI contract involving the Centre de Rducation et Radaptation Fonctionnelle of Kerpape, the LIMSI-CNRS, and Thomson-CSF's Central Research Laboratory (Corbeville). As a matter of fact, no computer program could eventually replace the understanding skills of a human being used to communicating with the speech impaired (parent, specialist, etc.). The system is therefore designed, in the first place, to give the user access to an autonomous communication device allowing him/her to talk to any person, even unprepared, in an unspecified context. translaters are familiar with this type of content redistribution, every natural language having its unique way of expressing meaning, like the German phrase ber den Flu schwimmen' being expressed rather in French by `traverser la rivire  la nage'.\", metadata={'source': '../data/raw/cmplg-xml/9506018.xml'}),\n",
       " Document(page_content=\"A Categorial Framework for Composition in Multiple Linguistic Domains\\n\\nWe describe a computational framework for a grammar architecture in which different linguistic domains such as morphology, syntax, and semantics are treated not as separate components but compositional domains. Word and phrase formation are modeled as uniform processes contributing to the derivation of the semantic form. The morpheme, as well as the lexeme, has lexical representation in the form of semantic content, tactical constraints, and phonological realization. The model is based on Combinatory Categorial Grammars.\\n\\nIntroduction\\n\\nThe division of morphology and syntax in agglutinative languages is difficult compared to relatively more isolating languages. For instance, in Turkish, there is a significant amount of interaction between morphology and syntax. Typical examples are: causative suffixes change the valence of the verb, and the reciprocal suffix subcategorize the verb for a noun phrase marked with the comitative case. Moreover, the head that a bound morpheme modifies may be not its stem but a compound head crossing over the word boundaries, e.g.,\\n\\n3iyi  oku\\n\\n\\n\\nmus  ocuk\\n\\nwell  read\\n\\n\\n\\nREL  child\\n\\n'well\\n\\n\\n\\neducated child'\\n\\nMorpheme\\n\\n\\n\\nbased Compositions\\n\\nWhen the morpheme is given the same status as the lexeme in terms of its lexical, syntactic, and semantic contribution, the distinction between the process models of morphotactics and syntax disappears. In this case, new scoping problems arise in word and phrase formation.\\n\\n3uzun  kol\\n\\n\\n\\nlu  gmlek\\n\\nlong  sleeve\\n\\n\\n\\nADJ  shirt\\n\\nMulti-domain Combination Operator Information Structure and Tactical Constraints\\n\\nSyntactic and semantic information are of grammatical (g) sign and semantic (s) sign, respectively. These properties include agreement features such as person, number, and possessive, and selectional restrictions:\\n\\nBasic and derived categories of CG are of p (property) or f(function) sign, respectively.\\n\\nRES-OP-ARG is the categorial notation for the element. Every RES and  ARG feature has an f or p sign.\\n\\nLexical and phrasal elements have functional representation (f or p sign) and the  PHON feature. PHON represents the phonological string. Lexical elements may have (a) phonemes, (b) meta-phonemes such as  H for high vowel, and  D for a dental stop whose voicing is not yet determined, and (c) optional segments, e.g., -(y)lA, to model vowel/consonant drops,  in the  PHON feature. During composition, the surface forms of composed elements are mapped and saved in  PHON. PHON also allows efficient lexicon search. For instance, the causative suffix -DHr has eight different realizations but only one lexical entry.\\n\\nConclusion\\n\\nBibliography\\n\\nA. E. Ades and M. Steedman. 1982. On the order of words. Linguistics and Philosophy, 4:517-558.\\n\\nJack Hoeksema and Richard D. Janda. 1988. Implications of process-morphology for categorial grammar. In R. T. Oehrle, E. Bach, and D. Wheeler, editors, Categorial Grammars and Natural Language Structures, D. Reidel, Dordrecht, 1988.\\n\\nBeryl Hoffman. 1992. A CCG approach to free word order languages. In Proceedings of the 30th Annual Meeting of the ACL, Student Session, 1992.\\n\\nMichael Moortgart. 1988. Mixed Composition and Discontinuous Dependencies. In R. T. Oehrle, E. Bach, and D. Wheeler, editors, Categorial Grammars and Natural Language Structures, D. Reidel, Dordrecht, 1988.\\n\\nRichard T. Oehrle. 1988. Multi-dimensional compositional functions as a basis for grammatical analysis. In R. T. Oehrle, E. Bach, and D. Wheeler, editors, Categorial Grammars and Natural Language Structures, D. Reidel, Dordrecht, 1988.\\n\\nRichard T. Oehrle.\\n\\n1994.\\n\\nTerm\\n\\n\\n\\nlabeled Categorial Type Systems.\\n\\nLinguistics and Philosophy, 17:633\\n\\n\\n\\n678.\\n\\nFernando C. N. Pereira.\\n\\n1990.\\n\\nCategorial Semantics and Scoping.\\n\\nComputational Linguistics, 16(1):1\\n\\n\\n\\n10.\\n\\nC. Pollard and I. A. Sag. 1994. Head-driven Phrase Structure Grammar. University of Chicago Press.\\n\\nMark Steedman. 1985. Dependencies and coordination in the grammar of Dutch and English. Language, 61:523-568.\\n\\nMark Steedman.\\n\\n1991.\\n\\nStructure and intonation.\\n\\nLanguage, 67:260\\n\\n\\n\\n296.\\n\\nMasaru Tomita.\\n\\n1987.\\n\\nAn Efficient Augmented\\n\\n\\n\\nContext\\n\\n\\n\\nFree Parsing Algorithm.\\n\\nComputational Linguistics, 13(1\\n\\n\\n\\n2):31\\n\\n\\n\\n46.\\n\\nDeirdre Wheeler. 1988. Consequences of some categorially-motivated phonological assumptions. In R. T. Oehrle, E. Bach, and D. Wheeler, editors, Categorial Grammars and Natural Language Structures, D. Reidel, Dordrecht, 1988.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9506025.xml'}),\n",
       " Document(page_content=\"Countability and Number in Japanese-to-English Machine Translation\\n\\nThis paper presents a heuristic method that uses information in the Japanese text along with knowledge of English countability and number stored in transfer dictionaries to determine the countability and number of English noun phrases. Incorporating this method into the machine translation system ALT-J/E, helped to raise the percentage of noun phrases generated with correct use of articles and number from 65% to 73%.\\n\\nIntroduction\\n\\nThis paper describes a method that extracts information relevant to countability and number from the Japanese text and combines it with knowledge about countability and number in English. First countability in English is discussed at the noun phrase and then the noun level. As a noun phrase's countability in English is affected by its referential property (generic, referential or ascriptive) we present a method of determining the referential use of Japanese noun phrases. Next the process of actually determining noun phrase countability and number is described. This is followed by some examples of sentences translated by the proposed method and a discussion of the results.\\n\\nCountability\\n\\nNoun Phrase Countability\\n\\nWhere ``the phrase `falls within the scope [or domain] of a denumerator' means `is denumerated' by it; i.e the NP reference is quantified by the denumerator as a number of discrete entities.''\\n\\nNot all nouns in English can become the head of a countable noun phrase. In particular, noun phrases whose heads fall within the scope of a denumerator (`denumerated' noun phrases) must be headed by a noun that has both singular and plural forms. Nouns that do not have both forms, like equipment or scissors, require a classifier to be used. The classifier becomes the head of a countable noun phrase with the original noun attached as the complement of a prepositional phrase headed by of: a pair of scissors, a piece of equipment.\\n\\nWhether a noun can be used to head a countable noun phrase or not depends both on how it is interpreted, and on its inherent countability preference. Noun countability preferences are discussed in the next section.\\n\\nNoun Countability Preferences\\n\\nA noun's countability preference determines how it will behave in different environments. We classify nouns into seven countability preferences, five major and two minor, as described below.\\n\\nThe two most basic types are `fully countable' and `uncountable'. Fully countable nouns, such as knife have both singular and plural forms, and cannot be used with determiners such as   much. Uncountable nouns, such as furniture, have no plural form, and can be used with much.\\n\\nBetween these two extremes there are a vast number of nouns, such as cake, that can be used in both countable and uncountable noun phrases. They have both singular and plural forms, and can also be used with much. Whether such nouns will be used countably or uncountably depends on whether their referent is being thought of as made up of discrete units or not. As it is not always possible to explicitly determine this when translating from Japanese to English, we divide these nouns into two groups: `strongly countable', those that are more often used to refer to discrete entities, such as   cake, and `weakly countable', those that are more often used to refer to unbounded referents, such as beer.\\n\\nThe last major type of countability preference is `pluralia tanta': nouns that only have a plural form, such as scissors. They can neither be denumerated nor modified by much. We further subdivide pluralia tanta into two types, those that can use the classifier pair to be denumerated, such as a pair of scissors and those that can't, such as clothes. `pair' pluralia tanta have a singular form when used as modifiers (a scissor movement). Pluralia tanta such as clothes, use the plural form even as modifiers (a clothes horse), and need a countable word of similar meaning to be substituted when they are denumerated: a garment, a suit, ....\\n\\nThe two minor types are subsets of fully countable and uncountable nouns respectively. Unless explicitly indicated, they will be treated the same as their supersets. `Collective' nouns share all the properties of fully countable nouns. In addition they can have singular or plural verb agreement with the singular form of the noun: The government has/have decided. `Semi-countable' nouns share the properties of uncountable nouns, except that they can be modified directly by a/an; for example a knowledge [of Japanese].\\n\\nDetermination of NP Referentiality\\n\\nThe first stage in generating the countability and number of a translated English noun phrase is to determine its referentiality. We distinguish three kinds of referentiality: `generic', `referential' and `ascriptive'.\\n\\nGeneric noun phrases\\n\\nThe use all three kinds of generic noun phrases is not acceptable in some contexts, for example * a mammoth evolved. Sometimes a noun phrase can be ambiguous, for example I like the elephant, where the speaker could like a particular elephant, or all elephants.\\n\\nDetermination of NP Countability and Number\\n\\nNoun phrases modified by Japanese/English pairs that are translated as denumerators we call denumerated. For example a noun modified by   onoono-no ``each'' is denumerated - singular, while one modified by ryouhou-no ``both'' is denumerated - plural. Uncountable and pluralia tantum nouns in denumerated environments are translated as the prepositional complement of a classifier. A default classifier is stored stored in the dictionary for uncountable nouns and pluralia tanta. Ascriptive noun phrases whose subject is countable will also be denumerated.\\n\\nSometimes the choice of the English translation of a modifier will depend on the countability of the noun phrase. For example,   kazukazu-no and takusan-no can all be translated as ``many''. kazukazu-no implies that it's modificant is made up of discrete entities, so the noun phrase it modifies should be translated as denumerated - plural. takusan-no does not carry this nuance so ALT-J/E will translate a noun phrase modified by it as mass - uncountable, and takusan-no as many if the head is countable and much otherwise.\\n\\nRules that translate the nouns with different noun countability preferences into other combinations of countable and uncountable are also possible. For example, sometimes even fully countable nouns can be used in uncountable noun phrases. If an elephant is referred to not as an individual elephant but as a source of meat, then it will be expressed in an uncountable noun phrase: I ate a slice of elephant. To generate this the following rule is used: ``nouns quantified with the classifier kire ``slice'' will be generated as the prepositional complement of slice, they will be singular with no article unless they are pluralia tanta, when they will be plural with no article''.\\n\\nNote that countable indefinite singular noun phrases without a determiner will have a/an generated. Countable indefinite plural noun phrases and uncountable noun phrases may have some generated; a full discussion of this is outside the scope of this article.\\n\\nExperimental Results\\n\\nIn the newspaper articles tested, there were an average of 7.0 noun phrases in each sentence. For a sentence to be judged as correct all the noun phrases must be correct. The introduction of the proposed method improved the percentage of correct sentences from 5% to 12%.\\n\\nSome examples of translations before and after the introduction of the new processing are given below. The translations before the proposed processing was implemented are marked  OLD, the translations produced by ALT-J/E using the proposed processing are marked NEW.\\n\\nzetumetu ``die out'', is entered in the lexicon as a verb whose subject must be generic. manmosu ``mammoth'' is fully countable so the generic noun phrase is translated as a bare plural.\\n\\nThe old version recognizes that a denumerated noun phrase headed by an uncountable noun tofu requires a classifier but does not generate the correct structure neither does it generate a classifier for the pluralia tanta scissors. The version using the proposed method does.\\n\\nAs the subject of the copula that is countable it's complement is judged to be denumerated by the proposed method. As the complement is headed by an uncountable noun it must be embedded in the prepositional complement of a classifier.\\n\\nThere are three main problems still remaining. The first is that currently the rules for determining the noun phrase referentiality are insufficiently fine. We estimate that if referentiality could be determined 100% correctly then the percentage of noun phrases with correctly generated articles and number could be improved to 96% in the test set we studied. The remaining 4% require knowledge from outside the sentence being translated. The biggest problem is noun phrases requiring world knowledge that cannot be expressed as a dictionary default. These noun phrases cannot be generated correctly by the purely heuristic methods proposed here. The last problem is noun phrases whose countability and number can be deduced from information in other sentences. We would like to extend our method to use this information in the future.\\n\\nConclusion\\n\\nThe quality of the English in a Japanese to English Machine Translation system can be improved by the method proposed in this paper. This method uses the information available in the original Japanese sentence along with information about English countability at both the noun phrase and noun level that can be stored in Japanese to English transfer dictionaries. Incorporating this method into the machine translation system ALT-J/E helped to improve the percentage of noun phrases with correctly generated articles and number from 65% to 73%.\\n\\nBibliography\\n\\nALLAN, KEITH.\\n\\n1980.\\n\\nNouns and countability.\\n\\nLanguage 56.541\\n\\n\\n\\n67.\\n\\nBOND, FRANCIS,   KENTARO OGURA. 1993. Determination of whether an English noun phrase is countable or not using 6 levels of lexical countability. In Proceedings of the 46th Annual Convention IPSJ Japan, 6:107-108. (in Japanese).\\n\\nHUDDLESTON, RODNEY. 1984. Introduction to the Grammar of English. Cambridge textbooks in linguistics. Cambridge: Cambridge University Press.\\n\\nIKEHARA, SATORU,  SATOSHI SHIRAI,  AKIO YOKOO, HIROMI NAKAIWA. 1991. Toward an MT system without pre-editing - effects of new methods in ALT-J/E-. In Proceedings of MT Summit III, 101-106. (cmp-lg/9510008).\\n\\nKUNO, SUSUMU. 1973. The Structure of the Japanese Language. Cambridge, Massachusetts, and London, England: MIT Press.\\n\\nMURATA, MASAKI,   MAKOTO NAGAO. 1993. Determination of referential property and number of nouns in Japanese sentences for machine translation into English. In Proceedings of the Fifth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-93), 218-25.\\n\\nFootnotes\\n\\nJapanese does not have obligatory plural morphemes. Plurality can be marked but only rarely is, for example by adding a suffix such as tachi ``and others'' (this can normally only be used with people or animals). The determiners much, little, a little, less and overmuch. can all be used for this test We called these environments `mass' because they both can be used to show a mass or unbounded interpretation.\", metadata={'source': '../data/raw/cmplg-xml/9511001.xml'}),\n",
       " Document(page_content='Sublanguage Terms: Dictionaries, Usage, and Automatic Classification\\n\\nThe use of terms from natural and social scientific titles and abstracts is studied from the perspective of sublanguages and their specialized dictionaries. Different notions of sublanguage distinctiveness are explored. Objective methods for separating hard and soft sciences are suggested based on measures of sublanguage use, dictionary characteristics, and sublanguage distinctiveness. Abstracts were automatically classified with a high degree of accuracy by using a formula that considers the degree of uniqueness of terms in each sublanguage. This may prove useful for text filtering or information retrieval systems.\\n\\nIntroduction\\n\\nWhen specialists in a particular field or discipline communicate informally, the discipline\\'s SL facilitates their communication by allowing them to be precise in their terminology, and frequently, to be more concise in their expression. When specialists communicate in more formal settings (e.g., a journal article), their language acquires some of the characteristics of general language. The grammar of their language will be that of general language and will conform to the norms of standard language. Their language will still, however, contain the vocabulary of the SL. These specialized terms may be the best (or only) way to describe their topic. The language in this formal setting will thus be a blend of general language and SL characteristics.\\n\\nOne place that this type of language, with mostly standard structure, but a highly specialized vocabulary, appears is in journal titles, abstracts, and articles. Here, it provides a challenge for a variety of language analysis, filtering, and retrieval systems. A highly specialized vocabulary contains terms that are not in standard dictionaries, resulting in coverage gaps for systems that depend on a dictionary for syntactic and/or semantic information. Haas haa:cove found that 20% of the word tokens in a set of computer science abstracts were not found in a standard college dictionary. Sixty-two percent of the SL word tokens (technical terms) identified by an expert were not found in the dictionary. The lack of coverage of the SL terms is not surprising, given the wealth of terms used in various disciplines, the speed with which new terminology is adopted, and the propensity of authors to coin new terms. SL words that do appear in the standard dictionary are frequently marked as belonging to that domain, for instance, with the name of the domain appearing in italics after the headword.\\n\\nSpecialized dictionaries may be sought to expand the coverage of the standard dictionary. A dictionary written to describe words and concepts in a particular discipline may be expected to contain more of the SL terms, and more of the SL senses or meanings of terms, than a general dictionary. Specialized dictionaries also indicate what the important terms and concepts in their disciplines are. The coverage of these specialized dictionaries, however, may vary widely. Haas haa:cove studied the coverage provided by a set of computer science dictionaries of vocabulary in a set of abstracts, and found that coverage of SL word tokens ranged from 20% to 78%.\\n\\nGoals of Research\\n\\nThe overall goal of this research is to examine language usage in titles and abstracts drawn from an array of disciplines to determine interesting usage differences between them. We looked at usage from several perspectives. First, we looked at word frequencies, and especially at the frequencies of SL terms. Next, we looked at the coverage of the SL terms in the titles and abstracts by special dictionaries for each of the domains, and by a general dictionary. Finally, we examined the use of vocabulary as a means for identifying the domain of an abstract.\\n\\nUsage refers to both the words that are used and the word senses that are used. The list of SL terms used in a discipline may include words that are not used (or are used very infrequently) in general language, such as the Physics term ``dipole\". SL terms may also include words that are familiar in general language, but are used in a special, very specific sense in the language of the domain. An example of this is the Biology term ``linear\", which refers either to a long, narrow leaf, or a row of pollen grains. Word and word sense usage in a particular discipline may also be ``borrowed\" from related domains. In this case, it can be difficult to say that a term is or is not in the SL of the discipline. The most common example of borrowing seen in this research is the use of mathematical terms in Physics, Electrical Engineering, and Biology abstracts. This kind of borrowing is to be expected, since Mathematic  is an important tool in these fields. Whether borrowed terms should be considered part of the SL may be a matter of definition.\\n\\nThere is another class of terms used in the discourse of a discipline that is harder to classify as SL or not SL. These are terms that are crucial to the discussion of the important topics in the discipline, but are not specific to that discipline. For example, many different disciplines use the term ``model\" to describe a set of concepts and the relationships between them. It would be difficult to discuss research without using this term. We cannot say that this is an SL term in Physics or in Sociology or some other discipline because its usage is the same in all the disciplines. On the other hand, in studying the language that experts in the field use to discuss their work, this is clearly an integral term. Bonzi (1984) chose not to include such terms in her study.\\n\\nUsage in a specific discipline can also be studied by looking at special dictionaries for the discipline. Although editorial policy differs from dictionary to dictionary, the terms chosen for inclusion in a dictionary are generally those used often enough that a reader of literature in the field may be expected to encounter them, and which are central to the field, so that it is important that a reader knows what they mean. Terms may be identified for inclusion by a variety of means, including looking at textbooks, journals and other standard works in the field. The dictionary can be viewed as an authority on SL usage in the discipline. It is important to point out, however, that the words included in such a dictionary will be only a portion of the words in the discipline\\'s SL. One problem is that of new terms; a dictionary, especially in print format, will always be a few years behind current usage. Nor can a dictionary include words that are coined by a single researcher, but not picked up by researchers in the field as a whole. The reader must expect that such terms are defined in the particular paper in which they are used. Finally, no dictionary is complete:  the restrictions of space and time will always lead to the omission from the dictionary of terms that another person might consider important.\\n\\nA specialized dictionary may also include some of the terms described in the preceding section that are crucial to the discourse of the domain, but not specific to just that SL. One way these can be identified is by examining specialized dictionaries from several disciplines. Not only will these terms appear in more than one dictionary, they will also be defined in approximately the same way.\\n\\nGeneral, or standard, college or unabridged dictionaries contain SL terms from many disciplines. Frequently the words or word senses are explicitly marked with the name of the field in which they are used. Definitions given in a general dictionary may not be as detailed or complete as those in a specialized dictionary. For example, the term ``linear\" referred to earlier had two senses given in the Biology dictionary. The standard dictionary, however, lists only the first one as a Biology SL sense. The other sense is not listed at all. Damerau dam:gene,dam:eval discusses some additional problems with using standard dictionaries as an aid in identifying SL terms.\\n\\nIn summary, which terms are considered to be part of the SL may vary according to several factors. Dictionaries will necessarily be incomplete, and will also include words that may be borrowed from another SL, or that are common to several disciplines\\' SLs. Domain experts will similarly have some disagreement on whether such terms are part of the SL. To define an SL as consisting of only those core terms that are unique to it, and agreed upon by all sources ignores the realities of scholarly discourse. Yet it is clear that there are boundaries between SLs, however fuzzy they may be. By examining terms in light of both dictionary and text based standards, we hope to discover some ways in which SLs can be differentiated.\\n\\nMaterials\\n\\nSpecialized dictionaries for each domain were chosen from the UNC-CH library. In cases where more than one dictionary was available, recent (within the last decade) dictionaries were chosen over older dictionaries, and those claiming a larger number of entries over those with a smaller number. The recency of a dictionary may have some effect on its coverage of SL terms. Given the time lag inherent in publishing a printed dictionary, a dictionary published in 1990, for example, may not include important terms from abstracts published in the same year. This disparity will be greater in disciplines where the vocabulary changes rapidly, such as Computer Science, and less noticeable in disciplines whose vocabularies evolve less rapidly. Since no date restrictions were explicitly placed on the abstracts, we do not expect this to be a problem for this research. Rather, this will reflect the realities of using specialized dictionaries as a knowledge source. The rate of change of SL vocabulary, and its effect on the use of dictionaries in various information tasks, deserves further investigation, particularly with the advent of online dictionaries. Appendix A contains a complete list of the general and specialized dictionaries.\\n\\nA systematic sample of 500 terms was taken from  each database. A list of  all terms in a given database was sorted by frequency of occurrence and then a systematic sample (every nth term so that the sample size was 500) taken from that database. Each author then separately identified those words from the sample that could possibly be a SL terms, omitting stop words (function words) and other obviously general words such as ``present\". Those words that were considered to be possible SL terms by either author were looked up in the appropriate specialized dictionary. Words were coded according to the following four categories: - No entry in the specialized dictionary. 1 - Headword of entry exactly matches the target word. 2 - Headword is phrase that starts with the target word. 3 - Headword of entry is inflectional variant of the target word (e.g., a different verb form, or a singular/plural variant). Those few terms that might be ``true\" SL terms that were not included in the dictionary were considered not to be SL terms for the purposes of the data analysis described below.\\n\\nTerm Frequencies in Sublanguages\\n\\nIn most cases, more terms occurred at the start of  a phrase in the dictionary than as an exact match for the single term in the dictionary. If one combines the number of terms that are exact matches with the exact matches at the start of a phrase, the percent of terms varies from 7 and 11 percent (Sociology and History) to 34 percent (Electrical Engineering and Physics).\\n\\nThe last two categories offer the possibility of ``false positives\", where the term in the dictionary is not the same as that used in the abstracts. Many words in the ``start of phrase\" category occurred at the beginning of more than one entry. In Physics, for example, the word ``atomic\" was the initial word of six entries. One or more of these entry phrases may have been used in the text, or the word may have been used in a different phrase. In general, however, entries starting with the same word are somewhat related. It should also be noted that merely finding a word in the dictionary does not mean that it was used in its SL sense in the abstracts.\\n\\nIn the case of the ``term variant\" category, a false positive would mean that an inflectional variant of a term had a different meaning from the term itself, for example, that a past tense meant something different from the infinitive. While this is theoretically possible, it was not noticed in this data.\\n\\nIt is interesting to note that Mathematics had the highest average term frequency for the exact match category. This indicates that the vocabulary used in the Mathematics abstracts matches that listed in the dictionary better than that of the other disciplines. It is possible that this reflects a slower rate of change in the Mathematics domain, i.e., in the adoption of new terms, thus allowing the dictionary to remain current for a longer period of time. This possibility requires more investigation.\\n\\nThese term frequencies may be useful in separating terms likely to occur in a specialized dictionary from terms not likely to occur in a specialized dictionary, that is, the separation of SL terms from general terms. While the averages suggest that we may be able to discriminate based on term frequencies, preliminary investigations into the correlations between term discrimination measures and term sublanguage status showed that little useful discrimination is actually obtained. This suggests that term frequencies alone are inadequate for accurate automatic identification of specialized terminology. We did not pursue this line of investigation.\\n\\nUsage\\n\\nUsage was examined from several perspectives, including term frequency and specialized and standard dictionary definitions of terms, to identify differences between disciplines and families of disciplines.\\n\\nFrequencies\\n\\nTerms were ranked for each database by their Poisson percentile. This percentile provides a measure of the degree to which a term has a higher than expected frequency of occurrence in the database in question. The average frequency of term occurrence is computed by dividing the total number of occurrences of the term in question from across all the different databases by the number of databases. This average is then compared statistically with the number of term occurrences in the database in question.\\n\\nThis distribution assumes that x is an integer and that the production process is memoryless, that is, the presence of a term in one location in a text doesn\\'t affect its presence either way in later produced sections of the text. While this may be a weak assumption to make when working with single documents, where stylistic considerations have a great impact on the choice of terms used by an author, the Poisson distribution assumption is reasonable in a database consisting of hundreds of different titles and abstracts written independently by different authors.\\n\\nWhile the technique described here assumes that terms are distributed in a manner roughly similar to the Poisson distribution, other distributions may prove useful in other circumstances. For example, using the normal distribution instead of the Poisson distribution may be desirable in cases where the average frequency of occurrence approaches the hundreds or higher and minimizing computation speed is important for the application.\\n\\nIt is interesting to note that although there is no overlap between disciplines in the top ranked terms, there is some in the bottom ranked terms. This is due in part to the ranking procedure, which gives a high rank to terms that are relatively rare (whether as SL terms or not), while those terms at the bottom of the lists are those terms likely to occur more evenly throughout the databases. These latter terms, while being specialized vocabulary for some domains, do occur in other domains and therefore receive a low position using this ranking method.\\n\\nComparison with Specialized Dictionary Definitions\\n\\nMerely finding a word from a particular set of abstracts in the specialized dictionary for that discipline does not guarantee that the word is being used in the abstracts in its SL sense, as defined in the dictionary. Many words that have a very specific meaning in a particular domain are also used with a different, more general meaning, in general language. The usage of words classified as 1\\'s, that is, with exact matches in the dictionaries, were of greatest interest to us. We wished to determine what portion of these words were being used in their SL sense and what portion in a general sense. We were specifically interested in two types of comparisons. First, we wondered if SL words with the highest Poisson rankings were  used differently from those with the lowest rankings within any discipline. Second, we wondered if there were any differences in usage between disciplines, or between classes of disciplines.\\n\\nSSL - Same sense, SL. The word token in the abstract used the same SL word sense as that defined in the special dictionary. For example, in the Electrical Engineering dictionary, the word ``array\" is defined as ``1) photovoltaic converter - a combination of panels coordinated in structure and function. 2) solar cell - a combination of solar cell panels or paddles coordinated in structure and function.\" The majority (96%) of its occurrences in the Electrical Engineering abstracts were used in one of these two senses.\\n\\nSG - Same sense, general. The word token in the abstract used the same sense as that defined in the special dictionary, which was a general definition. That is, the special dictionary defined the word to mean the same as its meaning in general language. For example, the economics dictionary defined the word ``merger\" as ``An amalgamation of two or more firms into a new firm.\" In general language, it is used in the same way.\\n\\nDSL - Different sense, SL. The word token in the abstract is used in a different sense from that defined in the special dictionary, but it is still used as an SL term. For example, the Biology dictionary defined the word ``linear\" as ``1) a leaf having parallel sides, and at least 4 to 5 times as long as broad. 2) a tetrad of pollen grains in a single row.\" In the Biology abstracts, it was generally used to refer to a mathematical array.\\n\\nDG - Different sense, general. The word token in the abstract is used in a different sense from that defined in the special dictionary, and its usage was in a general language sense. For example, the Physics dictionary defined the word ``period\" as ``the time occupied in one complete movement of a vibration or oscillation\". In the Physics abstracts, it was used in the general sense of ``a span of time.\"\\n\\nThere are some very interesting differences in the distribution of the terms. In the scientific disciplines, a total of 73.8% of the occurrences were SL usages (SSL + DSL), while in the humanities and social science disciplines only  9.9% were. In the humanities and social sciences, the majority of the usages were classified as SG. Not only were terms mostly used in a general sense, it is these general senses that were defined in the specialized dictionaries. For example, the History dictionary contained an entry for ``France\", which was defined as a country in Western Europe, etc. Obviously, the word ``France\" has the same meaning in general discourse.\\n\\nThe picture changes somewhat in looking at the bottom 10 1\\'s. In the scientific disciplines, 60.9% of the term occurrences were used in a general sense. Interestingly, 70.8% of the humanities and social science term occurrences were used in a general sense, a smaller proportion than the top-ranked terms. However, about half of these were used in a general sense different from that defined in the special dictionary. Note that none of the terms in the humanities and social sciences were used in an SL sense different from that defined in the special dictionary.\\n\\nIn summary, most of the occurrences of the top-ranked words used the same sense as that defined in the discipline\\'s special dictionary, whether that was an SL or a general sense. Most of the occurrences of the bottom-ranked words in the scientific disciplines were used in a different sense from that given in the dictionary. Only one third of the humanities and social science terms\\' occurrences were used in a different sense. This finding indicates some interesting characteristics about the vocabulary used in scholarly abstracts, and how the disciplines differ in this regard. In scientific disciplines such as Mathematics or Physics,  the words that occur more frequently than expected (the top-ranked terms) are generally used in a specific, SL sense. The infrequent words that could also be used in an SL sense are not used that way. There is a shift in usage between the top ranked words and the bottom ranked words. This particular shift does not occur in the other disciplines. In these disciplines, the vocabulary is less distinct from that of general language, as seen by the number of occurrences classified as SG. The shift in usage between the top and bottom ranked words seen here is that the top ranked words are primarily used in the same general sense as that defined in the specialized dictionary, while the bottom ranked words are used almost as frequently in a different general sense. In addition, there are more SSL occurrences.\\n\\nMeasures of Sublanguage Characteristics\\n\\nGiven the usage patterns found in the scientific and humanities and social science disciplines, the next question is whether these patterns can be used to measure the characteristics of a particular SL. We propose a usage based measure,\\n\\nThe ratio of the usage measure Mu for the top terms to that for the bottom terms provides a measure of the difference between these two areas:\\n\\nComparison with Standard Dictionary Definitions\\n\\nA further difference in the distinctiveness of the scientific and humanities and social science vocabularies can be seen by examining entries in the standard college dictionary. The terms classified as 1\\'s from each discipline were looked up in the dictionary, first to see whether there was an entry for the word, and next to see if one or more of the word senses was marked as being a term from  that discipline. There was little difference in the number of terms with entries. 78.7% of the scientific disciplines\\' terms had entries,  ranging from 94% (Biology) to 40.4% (Physics). In the humanities and social sciences, 81.2% of the terms had entries, ranging from 100% of the Sociology terms to 52% of the History terms. It is interesting to note that many of the History terms were proper nouns (e.g., names of countries or people), which are not listed in the main entries of most standard dictionaries. The History dictionary, on the other hand, contained a large number of proper noun entries. An average of 12.7% of the technical entries were marked with the specific discipline name. None of the Sociology and History terms were marked, and 4.2% of the economics terms were marked, yielding an average number of marked terms for the humanities and social sciences of 1.4%. Few of the humanities and social science terms have meanings which are distinct from those of general usage. When one considers the topics covered by these disciplines, this finding makes sense.\\n\\nThey study objects and concepts that make up ordinary life, which are also the topics of general conversation and writing. In contrast, the highly scientific disciplines such as Physics or Mathematics are concerned with more esoteric concepts that are not the subject of general discourse.\\n\\nDifferent fields have SLs that may be understood to be distinctive in several different senses. For example, a particular SL may be seen as distinct from another SL when the two SLs  use different terminology. The term ``muon\" occurring in the Physics SL would not be expected to occur in a  Psychology SL. This form of distinctiveness is probably the easiest form to measure with automated techniques capable of looking for string occurrences in either or both SLs. A different form of distinction occurs when an SL uses a term in a different sense than in another SL. For example, the term ``affect\" occurs in both the psychological and physics literature, but has a specialized meaning for psychologists in addition to the meaning common to both of the SLs. An extreme example of the use of different senses for different SLs would be the case where two SLs have exactly the same vocabulary but different senses in every case. If philosophers who suggest that we each have our own meaning for terms are correct, the SL that each of us uses (idiolect) is distinctive in this ``sense\" sense.\\n\\nWe may measure the general distinctiveness of an SL by noting the overlap between terms defined in an SL dictionary with those terms defined in a general dictionary. It may be computed as\\n\\nThese distinctiveness results may be interpreted in two ways, depending on the assumptions one makes concerning the inherent technicality of SLs. If we believe that SLs are  unequal in terms of their technicality, that is, some languages are inherently more specialized than others, the DS,G may measure the degree to which more specialized terminology is used in the more technical languages.\\n\\nIf, on the other hand, all SLs are assumed to be specialized to the same degree, the differing DS,G values may be understood as the degree to which the differences between the SL and the general language moves from being that of a terminological difference to being a difference in sense or meaning. Physics might be interpreted as having a greater degree of terminological distinctiveness than Sociology;  the latter more frequently uses the same terminology as the general language but must use the terms in different senses if the SLs are to be equally specialized.\\n\\nThe definitions in general dictionaries for terms in the humanities and social sciences were suggested above to seldom have different senses in the SL and the general language. If we accept these dictionary definitions as capturing the true meanings of the terms, then disciplines such as Physics do have more distinctive SLs than do disciplines such as Sociology and the hypothesis that SLs have the same degree of specialization is wrong. If, on the other hand, we assume that the Sociological definition of ``crowd\" is the same as the general definition of ``crowd\" but that, for a sociologist, the term brings to mind a specialized constellation of images, then we might wish to claim that the senses of the general and SL terms are in fact different, despite the similarity of the definitions, and we do not have empirical support for the claim that sublanguages have different degrees of distinctiveness.\\n\\nIf one accepts the hypothesis that sublanguages are of equal technical specificity, it becomes necessary to address the question of the degree to which an SL can be different from a general language. Our data for Physics suggests that it has less than half of its specialized terminology occurring in the general language, while all of the specialized terminology from Sociology occurred in the general language. This may be seen as providing a possible range of values for technical terminology differences in SLs. If we accept the assumption that SLs are of equal technical specificity, there must be about this much variation in the sense differences between the SLs.\\n\\nIn addition to comparing SLs with the general language, two SLs may be compared with one another. More generally, the asymmetric distinctiveness of an SL x from an SL y may be measured as\\n\\nwhere SLx represents the set of terms in the dictionary for sublanguage x.\\n\\nThese two distinctiveness measures may be used to examine a number of SL phenomena. As defined, they may measure the distinctiveness of those terms included in a specialized dictionary. To the extent that a sublanguage may be defined as those specialized terms occurring in a discipline specific dictionary, these dictionary based measures may be used as measures of sublanguage distinctiveness. A second approach would be to examine the definitions of terms in different SLs to study the extent to which the definitions overlap. This definition based approach is probably superior to an entry based measure at capturing the true distinctiveness of an SL. A third possibility would be to combine dictionary definitions with meanings extracted from the context in which the terms are used in the abstracts. It would be difficult to represent these meanings in a useful way, but this approach would probably be superior to the other two. Other types of differences in meaning may similarly be used as the basis for measurement.\\n\\nAutomatic Classification into Discipline\\n\\nExperimental learning systems often gain knowledge from one set of data and then use this knowledge to process another set of data. For example, classification systems may learn from half the data and then classify the other half. They may also learn from an entire data set and then classify the set. This latter approach results in the classification system learning from an item and then using this knowledge to classify  the item. A superior experimental approach to learning for classification is to learn from every item in the data set except for the item to be classified. When this is done for every item in the data set, the maximum information about database characteristics is obtained that can be obtained without tainting the learning with knowledge from the specific item to be classified. The latter experimental technique is more realistic than learning from half the data and then classifying the other half of the data. It simulates the production environment in which incoming documents are classified on the basis of all previously classified ones. The results obtained with our method of learning from all but the document to be classified provides classification performance that is both superior to that obtained with half-and-half testing and is closer to that which would be obtained with a production classification system.\\n\\nThe Poisson percentile is computed somewhat differently for use in this classification procedure to provide a more conservative and realistic test of the power of this classification scheme. The term frequency for both the database in which the term occurs and for the set of databases as a whole is decreased by the term frequency for the document in question. This effectively allows for classification of a document to be based on term frequencies from all the other documents and not from the document being classified. In other words, the document being classified is not used in learning the Poisson percentile used to classify that individual document. This mimics the situation found in a document filtering system where a set of percentiles would be developed from one set of documents, already filtered, and then an arriving document would be classified based on earlier learning.\\n\\nThe weight for a given document was computed as the sum of the Poisson percentiles for the individual terms. Earlier tests using the product of term weights resulted in little consistent change in classification performance from that obtained using additive methods. No normalization for abstract or title size was used. Documents were classified by assigning weights (for each database) and the document being classified as a member of the database which resulted in the highest document weight for that particular document.\\n\\nTerms from a list of 203 stopwords were removed from the database. For a few titles, terms occurred only in that document and, when these frequencies are subtracted from the database frequencies, the terms have a frequency of 0 with an average frequency of 0. These terms have an indeterminate and thus unusable Poisson percentile. This, combined with deletion of the stopwords, results in some titles effectively having no terms that could be used to classify the document. For our experiments these titles were randomly assigned to databases for classification purposes. The classification results reported here are thus somewhat lower than would be obtained if more ad hoc methods had been used to classify these particular titles.\\n\\nThese classification results appear to be very good. Since the classification procedure ``learned\" from other documents than the one being classified, there must be something  about the titles and abstracts that allowed them to be classified correctly at such a high rate. We believe that the high degree of accuracy obtained was due to a combination of the relatively large number of terms present in a title or an abstract and thus used in the classification procedures and the discriminating quality of the terms. While traditional information retrieval applications often discriminate based on a few keywords in the query, our abstracts and title/abstract combinations were classified based on the much larger number of terms occurring in the title, abstract, or the title and abstract combined. Classification of titles alone probably performed so well because of the high discrimination ability of terms included in the titles.\\n\\nQualitative Analysis of Classification Failures\\n\\nThe other discipline that ``attracted\" erroneous classifications was Mathematics. This is not surprising at all, since Mathematics is used as a language or form of expression in several fields, including Biology, Electrical Engineering, and Physics. The misclassifications of Physics abstracts as Biology, Electrical Engineering, and Mathematics can also be understood as being a result of their common mathematical language.\\n\\nConclusions and Discussion\\n\\nVocabulary in abstracts from different disciplines varies considerably from discipline to discipline. This is not surprising; after all, the vocabulary must represent the topics and concepts of the discipline. The more important results of our research are how these differences can be characterized and measured.\\n\\nThe second result, related to the first, is a measure of the distinctiveness of an SL. This measure can be based on either the terms of the SL or on the terms\\' definitions. The difference between these two measures may further characterize the SL. The consequences of assuming that SLs are equal in degree of technicality were examined, as well as the more traditional assumption that they differ in technicality.\\n\\nThe third result of this research is the development of a highly accurate method of classifying abstracts by discipline, based on word frequencies. This technique will be useful in a variety of filtering and retrieval tasks, for example, as a first task in identifying a set of abstracts that are potentially relevant to a query or information need. The misclassifications that may be characterized as ``near misses\" would not necessarily be irrelevant to the query, and the few remaining misclassifications could probably be easily filtered out in subsequent processing stages.\\n\\nDictionaries used in this study\\n\\nThe size of each of the dictionaries below was estimated by multiplying the average number of entries on sample pages by the number of pages.\\n\\nGeneral:  The American Heritage Dictionary, Second College Edition. Boston: Houghton Mifflin Company. 1985. 57,000 entries.\\n\\nBiology:  Chambers Biology Dictionary. Peter M. B. Walker, Ed. Cambridge: Chambers. 1989. 10,000 entries.\\n\\nEconomics:  Dictionary of Economics. Donald Rutherford. New York: Routledge. 1992. 4,000 entries.\\n\\nElectrical Engineering: IEEE Standard Dictionary of Electrical and Electronics Terms, Third Edition. Frank Jay, Ed. New York:  IEEE. 1984. 22,000 entries.\\n\\nHistory: Macmillan Concise Dictionary of World History. Bruce Wetterau, Ed. New York:  Macmillan Books. 1986. 16,000 entries.\\n\\nMath:  Mathematics Dictionary, Fifth Edition. G. James  R. James. New York: Van Nostrand Reinhold. 1992. 2,000 entries.\\n\\nPhysics:  Dictionary of Physics, Third Edition. H. J. Gray  Alan Isaacs, Eds. Harlow, Essex:  Longman Group. 1991. 8,000 entries.\\n\\nPsychology:  The International Dictionary of Psychology. Stuart Sutherland. New York: Continuum. 1989. 9,000 entries.\\n\\nSociology:  Sociology. David Jary  Julia Jary. New York:  Harper Collins. 1991. 2,000 entries.\\n\\nFootnotes\\n\\nWe wish to thank  Nicole Magas for her assistance with the data collection described here.', metadata={'source': '../data/raw/cmplg-xml/9411001.xml'}),\n",
       " Document(page_content=\"Incremental Interpretation of Categorial Grammar\\n\\nThe paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation. The parser does not require fragments of sentences to form constituents, and thereby avoids problems of spurious ambiguity. The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG, Dependency Grammar and the Lambek Calculus. It also includes a discussion of some of the issues which arise when parsing lexicalised grammars, and the possibilities for using statistical techniques for tuning to particular languages.\\n\\nIntroduction\\n\\nThere is a large body of psycholinguistic evidence which suggests that meaning can be extracted before the end of a sentence, and before the end of phrasal constituents (e.g. Marslen-Wilson 1973, Tanenhaus et al. 1990). There is also recent evidence suggesting that, during speech processing, partial interpretations can be built extremely rapidly, even before words are completed (Spivey-Knowlton et al. 1994). There are also potential computational applications for incremental interpretation, including early parse filtering using statistics based on logical form plausibility, and interpretation of fragments of dialogues (a survey is provided by Milward and Cooper, 1994, henceforth referred to as MC).\\n\\nNeither approach is without problems. If a grammar is augmented with operations which are powerful enough to make most initial fragments constituents, then there may be unwanted interactions with the rest of the grammar (examples of this in the case of CCG and the Lambek Calculus are given in Section 2). The addition of extra operations also means that, for any given reading of a sentence there will generally be many different possible derivations (so-called `spurious' ambiguity), making simple parsing strategies such as shift-reduce highly inefficient.\\n\\nThe main difference between our approach and that of Milward (1992, 1994) is that it is based on a more expressive grammar formalism, Applicative Categorial Grammar, as opposed to Lexicalised Dependency Grammar. Applicative Categorial Grammars allow categories to have arguments which are themselves functions (e.g. very can be treated as a function of a function, and given the type   (n/n)/(n/n) when used as an adjectival modifier). The ability to deal with functions of functions has advantages in enabling more elegant linguistic descriptions, and in providing one kind of robust parsing: the parser never fails until the last word, since there could always be a final word which is a function over all the constituents formed so far. However, there is a corresponding problem of far greater non-determinism, with even unambiguous words allowing many possible transitions. It therefore becomes crucial to either perform some kind of ambiguity packing, or language tuning. This will be discussed in the final section of the paper.\\n\\nApplicative Categorial Grammar\\n\\nApplicative Categorial Grammar is the most basic form of Categorial Grammar, with just a single combination rule corresponding to function application. It was first applied to linguistic description by Adjukiewicz and Bar-Hillel in the 1950s. Although it is still used for linguistic description (e.g. Bouma and van Noord, 1994), it has been somewhat overshadowed in recent years by HPSG (Pollard and Sag 1994), and by Lambek Categorial Grammars (Lambek 1958). It is therefore worth giving some brief indications of how it fits in with these developments.\\n\\nHowever, there are problems with having just composition, the most basic of the non-applicative operations. In CGs which contain functions of functions (such as very, or slowly), the addition of composition adds both new analyses of sentences, and new strings to the language. This is due to the fact that composition can be used to form a function, which can then be used as an argument to a function of a function. For example, if the two types, n/n and n/n are composed to give the type n/n, then this can be modified by an adjectival modifier of type (n/n)/(n/n). Thus, the noun very old dilapidated car can get the unacceptable bracketing, [[very [old dilapidated]] car]. Associative CGs with Composition, or the Lambek Calculus also allow strings such as   boy with the to be given the type n/n predicting very boy with the car to be an acceptable noun. Although individual examples might be possible to rule out using appropriate features, it is difficult to see how to do this in general whilst retaining a calculus suitable for incremental interpretation.\\n\\nGiven that our arguments have produced a categorial grammar which looks very similar to HPSG, why not use HPSG rather than Applicative CG? The main reason is that Applicative CG is a much simpler formalism, which can be given a very simple syntax semantics interface, with function application in syntax mapping to function application in semantics,]. This in turn makes it relatively easy to provide proofs of soundness and completeness for an incremental parsing algorithm. Ultimately, some of the techniques developed here should be able to be extended to more complex formalisms such as HPSG.\\n\\nAB Categorial grammar with Associativity (AACG)\\n\\nIn this section we define a grammar similar to Bar-Hillel's first grammar. However, unlike Bar-Hillel, we allow one argument to be absorbed at a time. The resulting grammar is equivalent to AB Categorial Grammar plus associativity.\\n\\nAn Incremental Parser\\n\\nMost parsers which work left to right along an input string can be described in terms of state transitions i.e. by rules which say how the current parsing state (e.g. a stack of categories, or a chart) can be transformed by the next word into a new state. Here this will be made particularly explicit, with the parser described in terms of just two rules which take a state, a new word and create a new state. There are two unusual features. Firstly, there is nothing equivalent to a stack mechanism: at all times the state is characterised by a single syntactic type, and a single semantic value, not by some stack of semantic values or syntax trees which are waiting to be connected together. Secondly, all transitions between states occur on the input of a new word: there are no `empty' transitions (such as the reduce step of a shift-reduce parser).\\n\\nParsing Lexicalised Grammars\\n\\nWhen we consider full sentence processing, as opposed to incremental processing, the use of lexicalised grammars has a major advantage over the use of more standard rule based grammars. In processing a sentence using a lexicalised formalism we do not have to look at the grammar as a whole, but only at the grammatical information indexed by each of the words. Thus increases in the size of a grammar don't necessarily effect efficiency of processing, provided the increase in size is due to the addition of new words, rather than increased lexical ambiguity. Once the full set of possible lexical entries for a sentence is collected, they can, if required, then be converted back into a set of phrase structure rules (which should correspond to a small subset of the rule based formalism equivalent to the whole lexicalised grammar), before being parsing with a standard algorithm such as Earley's (Earley 1970).\\n\\nIn incremental parsing we cannot predict which words will appear in the sentence, so cannot use the same technique. However, if we are to base a parser on the rules given above, it would seem that we gain further. Instead of grammatical information being localised to the sentence as a whole, it is localised to a particular word in its particular context: there is no need to consider a pp as a start of a sentence if it occurs at the end, even if there is a verb with an entry which allows for a subject pp.\\n\\nHowever there is a major problem. As we noted in the last paragraph, it is the nature of parsing incrementally that we don't know what words are to come next. But here the parser doesn't even use the information that the words are to come from a lexicon for a particular language. For example, given an input of 3 nps, the parser will happily create a state expecting 3 nps to the left. This might be a likely state for say a head final language, but an unlikely state for a language such as English. Note that incremental interpretation will be of no use here, since the semantic representation should be no more or less plausible in the different languages. In practical terms, a naive interactive parallel Prolog implementation on a current workstation fails to be interactive in a real sense after about 8 words.\\n\\nWhat seems to be needed is some kind of language tuning. This could be in the nature of fixed restrictions to the rules e.g. for English we might rule out uses of prediction when a noun phrase is encountered, and two already exist on the left list. A more appealing alternative is to base the tuning on statistical methods. This could be achieved by running the parser over corpora to provide probabilities of particular transitions given particular words. These transitions would capture the likelihood of a word having a particular part of speech, and the probability of a particular transition being performed with that part of speech.\\n\\nThere has already been some early work done on providing statistically based parsing using transitions between recursively structured syntactic categories (Tugwell 1995). Unlike a simple Markov process, there are a potentially infinite number of states, so there is inevitably a problem of sparse data. It is therefore necessary to make various generalisations over the states, for example by ignoring the R2 lists.\\n\\nThe full processing model can then be either serial, exploring the most highly ranked transitions first (but allowing backtracking if the semantic plausibility of the current interpretation drops too low), or ranked parallel, exploring just the n paths ranked highest according to the transition probabilities and semantic plausibility.\\n\\nConclusion\\n\\nThe paper has presented a method for providing interpretations word by word for basic Categorial Grammar. The final section contrasted parsing with lexicalised and rule based grammars, and argued that statistical language tuning is particularly suitable for incremental, lexicalised parsing strategies.\\n\\nReferences\\n\\nAdes, A.  Steedman, M.: 1972, `On the Order of Words', Linguistics Philosophy 4, 517-558. Bar-Hillel, Y.: 1953, `A Quasi-Arithmetical Notation for Syntactic Description', Language 29, 47-58. Bar-Hillel, Y.: 1964, Language  Information: Selected Essays on Their Theory  Application, Addison-Wesley. Bouma, G.: 1987, `A Unification-Based Analysis of Unbounded Dependencies', in Proceedings of the 6th Amsterdam Colloquium, ITLI, University of Amsterdam. Bouma, G.  van Noord, G.: 1994, `Constraint-Based Categorial Grammar', in Proceedings of the 32nd ACL, Las Cruces, U.S.A. Earley, J.: 1970, `An Efficient Context-free Parsing Algorithm', ACM Communications 13(2), 94-102. Gaifman, H.: 1965, `Dependency Systems  Phrase Structure Systems', Information  Control 8: 304-337. Gazdar, G., Klein, E., Pullum, G.K.,  Sag, I.A. : 1985, Generalized Phrase Structure Grammar, Blackwell, Oxford. Hays, D.G. : 1964, `Dependency Theory: A Formalism  Some Observations', Language 40, 511-525. Joshi, A.K. : 1987, `An Introduction to Tree Adjoining Grammars', in Manaster-Ramer (ed. ), Mathematics of Language, John Benjamins, Amsterdam. Lambek, J.: 1958, `The Mathematics of Sentence Structure', American Mathematical Monthly 65, 154-169. Marcus, M., Hindle, D.,  Fleck, M.: 1983, `D-Theory: Talking about Talking about Trees', in Proceedings of the 21st ACL, Cambridge, Mass. Marslen-Wilson, W.: 1973, `Linguistic Structure  Speech Shadowing at Very Short Latencies', Nature 244, 522-523.\\n\\nMilward, D.: 1992, `Dynamics, Dependency Grammar  Incremental Interpretation', in Proceedings of COLING 92, Nantes, vol 4, 1095-1099. Milward, D.  Cooper, R.: 1994, `Incremental Interpretation: Applications, Theory  Relationship to Dynamic Semantics', in Proceedings of COLING 94, Kyoto, Japan, 748-754. Milward, D.: 1994, `Dynamic Dependency Grammar', to appear in Linguistics  Philosophy 17, 561-605. Mitchell, D.C., Cuetos, F.,  Corley, M.M.B. : 1992, `Statistical versus linguistic determinants of parsing bias: cross-linguistic evidence'. Paper presented at the 5th Annual CUNY Conference on Human Sentence Processing, New York. Moore, R.C. : 1989, `Unification-Based Semantic Interpretation', in Proceedings of the 27th ACL, Vancouver. Moortgat, M.: 1988, Categorial Investigations: Logical  Linguistic Aspects of the Lambek Calculus, Foris, Dordrecht. Morrill, G., Leslie, N., Hepple, M. Barry,G. : 1990, `Categorial Deductions  Structural Operations', in Barry, G. Morrill, G. (eds. ), Studies in Categorial Grammar, Edinburgh Working Papers in Cognitive Science, 5. Polanyi, L.  Scha, R.: 1984, `A Syntactic Approach to Discourse Semantics', in Proceedings of COLING 84, Stanford, 413-419. Pollard, C.  Sag, I.A. : 1994, Head-Driven Phrase Structure Grammar, University of Chicago Press  CSLI Publications, Chicago. Pulman, S.G.: 1986, `Grammars, Parsers,  Memory Limitations', Language Cognitive Processes 1(3), 197-225.\\n\\nSpivey-Knowlton, M., Sedivy, J., Eberhard, K.,  Tanenhaus, M.: 1994, `Psycholinguistic Study of the Interaction Between Language Vision', in Proceedings of the 12th National Conference on AI, AAAI-94. Stabler, E.P. : 1991, `Avoid the Pedestrian's Paradox', in Berwick, R.C. et al. (eds. ), Principle-Based Parsing: Computation  Psycholinguistics, Kluwer, Netherlands, 199-237. Steedman, M.J.: 1991, `Type-Raising  Directionality in Combinatory Grammar', in Proceedings of the 29th ACL, Berkeley, U.S.A. Tanenhaus, M.K., Garnsey, S.,  Boland, J.: 1990, `Combinatory Lexical Information  Language Comprehension', in Altmann, G.T.M. Cognitive Models of Speech Processing, MIT Press, Cambridge Ma. Tugwell, D.: 1995, `A State-Transition Grammar for Data-Oriented Parsing', in Proceedings of the 7th Conference of the European ACL, EACL-95, Dublin, this volume. Vijay-Shanker, K.: 1992, `Using Descriptions of Trees in a Tree Adjoining Grammar', Computational Linguistics 18(4), 481-517.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9503015.xml'}),\n",
       " Document(page_content=\"SKOPE: A connectionist/symbolic architecture of spoken Korean processing\\n\\nSpoken language processing requires speech and natural language integration. Moreover, spoken Korean calls for unique processing methodology due to its linguistic characteristics. This paper presents SKOPE, a connectionist/symbolic spoken Korean processing engine, which emphasizes that: 1) connectionist and symbolic techniques must be selectively applied according to their relative strength and weakness, and  2) the linguistic characteristics of Korean must be fully considered for phoneme recognition, speech and language integration, and morphological/syntactic processing. The design and implementation of SKOPE demonstrates how connectionist/symbolic hybrid architectures can be constructed for spoken agglutinative language processing. Also SKOPE presents many novel ideas for speech and language processing. The phoneme recognition, morphological analysis, and syntactic analysis experiments show that SKOPE is a viable approach for the spoken Korean processing.\\n\\nIntroduction\\n\\nThis paper presents one of the such endeavors, SKOPE (Spoken Korean Processing Engine), that has the following unique features: 1) The connectionist and symbolic techniques are selectively used according to their strength and weakness. The learning capability, fault-tolerant property, and ability of simultaneous integration of multiple signal-level sources make the connectionist techniques suitable to the phoneme recognition from the speech signals, but the structure manipulation and powerful matching (binding) properties of the symbolic techniques are the better choices for the complex morphological processing of Korean. However, the parallel multiple constraint relaxation capability of the connectionist techniques are applied together with the symbolic structure binding techniques for the syntactic processing. 2) The linguistic characteristics of Korean are fully considered in phoneme recognition, speech and language integration, and morphological/syntactic processing. 3) The SKOPE provides multi-level application program interfaces (APIs) which can utilize the phoneme-level or the morphological level or the syntactic level services for the applications such as spoken language interface, voice information retrieval and spoken language translation.\\n\\nWe hope the experience of SKOPE development provide viable answers to some of the open questions to the speech and language processing, such as 1) how learning and encoding can be synergetically combined in speech and language processing, 2) which aspects of system architecture have to be considered in spoken language processing, especially in connectionist/symbolic hybrid systems, and finally 3) what are the most efficient way of speech and language integration, especially for agglutinative languages.\\n\\nCharacteristics of spoken Korean\\n\\nThis section briefly explains the linguistic characterists of spoken Korean before describing the SKOPE system. In this paper, Yale romanization is used for representing the Korean phonemes. 1) A Korean word, called Eojeol, consists of more than one morphemes with clear-cut morpheme boundaries. 2) Korean is a postpositional language with many kinds of noun-endings, verb-endings, and prefinal verb-endings. These functional morphemes determine the noun's case roles, verb's tenses, modals, and modification relations between Eojeols. 3) Korean is a basically SOV language but has relatively free word order compared to the rigid word-order languages, such as English, except for the constraints that the verb must appear in a sentence-final position. However, in Korean, some word-order constraints do exist such that the auxiliary verbs representing modalities must follow the main verb, and the modifiers must be placed before the word (called head) they modify. 4) The unit of pause in speech (which is called Eonjeol) may be different from that of a written text (an Eojeol). The spoken morphological analysis must deal with an Eonjeol since no Eojeol boundary can be provided in the speech. 5) Phonological changes can occur in a morpheme, between morphemes in an Eojeol, and even between Eojeols in an Eonjeol. These changes include consonant and vowel assimilation, dissimilation, insertion, deletion, and contraction.\\n\\n6) Korean has many rising diphthongs that are very similar to mono-vowels at signal level. Korean has well-developed syllable structures, and unlike Japanese that has only CV type syllable, Korean has all different types such as CV, VC, V, CVC. Moreover, in CVC type syllable, first and second consonants are almost same in pronunciation. These signal characteristics make it difficult to directly use phonemes or syllables as sub-word recognition units.\\n\\nThe SKOPE architecture Diphone-based connectionist phoneme recognition Table-driven morphological and phonological analysis Table-driven connectionist/symbolic syntax analysis\\n\\nThe phoneme lattice-based morphological analysis produces the morphologically analyzed (segmented and stem reconstructed) morpheme sequences. Since there are usually more than one analysis results due to the errors of speech recognition process, the outputs are usually organized as morpheme lattice. For the seamless integration of the morphological analysis with the syntax analysis, we employ the same table-driven control for the syntax analysis as well as the morphological analysis.\\n\\nright cancellation: b/{a1,a2, ..., an} ai results in b/{a1, a2, ..., ai-1, ai+1, ..., an}\\n\\nThe node generation and constituent search positions are controlled by the triangular table. When the node a(i,j) acts as an argument, it generates node only in the position (k,j) where 1[k[j, and the generated node searches for the constituents (functors) only in the position (k,i-1). Or when the node is generated in the position (i,k) where\\n\\nj[k[number-of-morphemes, it searches for the position (j+1,k) for its constituents. When the node acts as a functor, the same position restrictions also apply for the node generation and the argument searching. The position control combined with the interactive relaxation guarantees an efficient, lexically oriented, and robust syntax analysis of spoken languages.\\n\\nImplementation and experiments\\n\\nThe SKOPE was fully implemented in UNIX/C platform, and have been extensively tested in practical domains such as natural language interface to operating systems. The phoneme recognition module targets 1000 morpheme continuous speech, currently speaker dependent due to the short of standard speech database for Korean. The unified morpheme-level phonetic dictionary has about 1000 morpheme entries and compiled into the trie structure. The morpheme-connectivity-matrix and phoneme-connectivity-matrix are encoded with the special Korean POS (part-of-speech) symbols and compressed.\\n\\nConclusions and future works\\n\\nThis paper explains the design and implementation of spoken Korean processing engine, which is a connectionist/symbolic hybrid model of spoken language processing by utilizing the linguistic characteristics of Korean. The SKOPE model demonstrates the synergetic integration of connectionist and symbolic techniques by considering the relative strength and weakness of two different techniques, and also demonstrates the phoneme level speech and language integration for general morphological processing for agglutinative languages. Besides the above two major contributions, the SKOPE architecture has the following unique features in spoken language processing: 1) the diphones are newly developed as a sub-word recognition unit for connectionist Korean speech recognition, 2) the morphological and syntactic analysis are tightly coupled by using the uniform table-driven control, 3) the phonological and orthographic rules are uniformly co-modeled declaratively, and 4) the table-driven interactive relaxation parsing and extension of the categorial grammar can provide robust handing of word-order variations in Korean.\\n\\nHowever, current implementation of the system still suffers from excessive continuous speech recognition errors. Since the large vocabulary continuous speech recognition is still an open problem, we cannot hope for the 100% correct speech recognition results in the near future. Currently, we are pursuing multi-strategic approaches to the advanced spoken language processing model, including optimizing TDNN-based phoneme recognition module, integrating HMM-based morpheme recognition module into the connectionist phoneme recognition, and incorporating probabilistic searches into the morphological analysis process as well as the syntactic analysis process. We are also developing applications on top of our SKOPE, including speech-to-speech translation system and intelligent interface agent for UNIX operating system. We hope our approach could be extended to other agglutinative languages such as Japanese, Finish, and Turkish, and also to the languages that have complex morphological phenomena such as German and Dutch.\\n\\nAcknowledgments\\n\\nThis research was supported partly by a grant from KOSEF (Korea Science and Engineering Foundation) and PIRL (Postech Information Research Laboratory). The SKOPE's various modules were programmed by our students: the phoneme recognition module by Kyunghee Kim  Kyubong Baac, the morphological analysis module by EunChul Lee  Wonil Lee, and finally the syntax analysis module by Wonil Lee.\\n\\nFootnotes\\n\\nC: consonant, V: vowel We believe that the semantic and pragmatic processing should be integrated into the domain knowledge for practical application under the current NLP technology, so we excluded the semantic and pragmatic processing from our general model. Unlike English, the Korean alphabet is truly phonetic in the sense that each phoneme is pronounced as it is written. That is why we sometimes use phonetic and phonemic interchangeably. This legality comes from the Korean phonology rule glotalization (one form of consonant dissimilation) stating that s sound becomes ss sound after l sound. Recall we generated average 2.3 phonemes per single correct phoneme.\", metadata={'source': '../data/raw/cmplg-xml/9504008.xml'}),\n",
       " Document(page_content=\"Bayesian Grammar Induction for Language Modeling\\n\\nWe describe a corpus-based induction algorithm for probabilistic context-free grammars. The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside-Outside algorithm. We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks. In two of the tasks, the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques. The third task involves naturally-occurring data, and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm.\\n\\nIntroduction\\n\\nGrammar Induction as Search\\n\\nMost work in language modeling, including n-gram models and the Inside-Outside algorithm, falls under the maximum-likelihood paradigm, where one takes the objective function to be the likelihood of the training data given the grammar. However, the optimal grammar under this objective function is one which generates only strings in the training data and no other strings. Such grammars are poor language models, as they overfit the training data and do not model the language at large. In n-gram models and the Inside-Outside algorithm, this issue is evaded by bounding the size and form of the grammars considered, so that the ``optimal'' grammar cannot be expressed. However, in our work we do not wish to limit the size of the grammars considered.\\n\\nThe goal of grammar induction is taken to be finding the grammar with the largest a posteriori probability given the training data, that is, finding the grammar G' where\\n\\nand where we denote the training data as O, for observations. As it is unclear how to estimate p(G|O) directly, we apply Bayes' Rule and get\\n\\nHence, we can frame the search for G' as a search with the objective function\\n\\np(O|G) p(G), the likelihood of the training data multiplied by the prior probability of the grammar.\\n\\np(G) = 2[\\n\\n\\n\\nl(G)]\\n\\nwhere l(G) is the length of the description of the grammar in bits. The universal a priori probability has many elegant properties, the most salient of which is that it dominates all other enumerable probability distributions multiplicatively.\\n\\nSearch Algorithm\\n\\nAs described above, we take grammar induction to be the search for the grammar G' that optimizes the objective function\\n\\np(O|G)p(G). While this framework does not restrict us to a particular grammar formalism, in our work we consider only probabilistic context-free grammars.\\n\\nWe assume a simple greedy search strategy. We maintain a single hypothesis grammar which is initialized to a small, trivial grammar. We then try to find a modification to the hypothesis grammar, such as the addition of a grammar rule, that results in a grammar with a higher score on the objective function. When we find a superior grammar, we make this the new hypothesis grammar. We repeat this process until we can no longer find a modification that improves the current hypothesis grammar.\\n\\nEvaluating the Objective Function\\n\\nConsider the task of calculating the objective function\\n\\np(O|G)p(G) for some grammar G.  Calculating\\n\\np(G) = 2[-l(G)] is inexpensive; however, calculating p(O|G) requires a parsing of the entire training data. We cannot afford to parse the training data for each grammar considered; indeed, to ever be practical for data sets of millions of words, it seems likely that we can only afford to parse the data once.\\n\\nTo achieve this goal, we employ several approximations. First, notice that we do not ever need to calculate the actual value of the objective function; we need only to be able to distinguish when a move applied to the current hypothesis grammar produces a grammar that has a higher score on the objective function, that is, we need only to be able to calculate the difference in the objective function resulting from a move. This can be done efficiently if we can quickly approximate how the probability of the training data changes when a move is applied.\\n\\nTo make this possible, we approximate the probability of the training data p(O|G) by the probability of the single most probable parse, or Viterbi parse, of the training data. Furthermore, instead of recalculating the Viterbi parse of the training data from scratch when a move is applied, we use heuristics to predict how a move will change the Viterbi parse. For example, consider the case where the training data consists of the two sentences\\n\\nNow, let us consider the move of adding the rule\\n\\nNotice that our predicted Viterbi parse can stray a great deal from the actual Viterbi parse, as errors can accumulate as move after move is applied. To minimize these effects, we process the training data incrementally. Using our initial hypothesis grammar, we parse the first sentence of the training data and search for the optimal grammar over just that one sentence using the described search framework. We use the resulting grammar to parse the second sentence, and then search for the optimal grammar over the first two sentences using the last grammar as the starting point. We repeat this process, parsing the next sentence using the best grammar found on the previous sentences and then searching for the best grammar taking into account this new sentence, until the entire training corpus is covered.\\n\\nDelaying the parsing of a sentence until all of the previous sentences are processed should yield more accurate Viterbi parses during the search process than if we simply parse the whole corpus with the initial hypothesis grammar. In addition, we still achieve the goal of parsing each sentence but once.\\n\\nParameter Training\\n\\nIn this section, we describe how the parameters of our grammar, the probabilities associated with each grammar rule, are set. Ideally, in evaluating the objective function for a particular grammar we should use its optimal parameter settings given the training data, as this is the full score that the given grammar can achieve. However, searching for optimal parameter values is extremely expensive computationally. Instead, we grossly approximate the optimal values by deterministically setting parameters based on the Viterbi parse of the training data parsed so far. We rely on the post-pass, described later, to refine parameter values.\\n\\nConstraining Moves\\n\\nPost\\n\\n\\n\\nPass\\n\\nA conspicuous shortcoming in our search framework is that the grammars in our search space are fairly unexpressive. Firstly, recall that our grammars model a sentence as a sequence of independently generated symbols; however, in language there is a large dependence between adjacent constituents. Furthermore, the only free parameters in our search are the parameters p(A); all other symbols (except S) are fixed to expand uniformly. These choices were necessary to make the search tractable.\\n\\nPrevious Work\\n\\nSimilar research includes work by Cook et al. (1976) and Stolcke and Omohundro (1994). This work also employs a heuristic search within a Bayesian framework. However, a different prior probability on grammars is used, and the algorithms are only efficient enough to be applied to small data sets.\\n\\nResults\\n\\nTo evaluate our algorithm, we compare the performance of our algorithm to that of n-gram models and the Inside-Outside algorithm.\\n\\nwhere T denotes the set of terminal symbols in the domain. All parameters are initialized randomly. From this starting point, the Inside-Outside algorithm is run until convergence.\\n\\nBecause of the computational demands of our algorithm, it is currently impractical to apply it to large vocabulary or large training set problems. However, we present the results of our algorithm in three medium-sized domains. In each case, we use 4500 sentences for training, with 500 of these sentences held out for smoothing. We test on 500 sentences, and measure performance by the entropy of the test data.\\n\\nIn the first two domains, we created the training and test data artificially so as to have an ideal grammar in hand to benchmark results. In particular, we used a probabilistic grammar to generate the data. In the first domain, we created this grammar by hand; the grammar was a small English-like probabilistic context-free grammar consisting of roughly 10 nonterminal symbols, 20 terminal symbols, and 30 rules. In the second domain, we derived the grammar from manually parsed text. From a million words of parsed Wall Street Journal data from the Penn treebank, we extracted the 20 most frequently occurring symbols, and the 10 most frequently occurring rules expanding each of these symbols. For each symbol that occurs on the right-hand side of a rule but which was not one of the most frequent 20 symbols, we create a rule that expands that symbol to a unique terminal symbol. After removing unreachable rules, this yields a grammar of roughly 30 nonterminals, 120 terminals, and 160 rules. Parameters are set to reflect the frequency of the corresponding rule in the parsed corpus.\\n\\nFor the third domain, we took English text and reduced the size of the vocabulary by mapping each word to its part-of-speech tag. We used tagged Wall Street Journal text from the Penn treebank, which has a tag set size of about fifty.\\n\\nWe achieve a moderate but significant improvement in performance over n-gram models and the Inside-Outside algorithm in the first two domains, while in the part-of-speech domain we are outperformed by n-gram models but we vastly outperform the Inside-Outside algorithm.\\n\\nNotice that our algorithm produces a significantly more compact model than the n-gram model, while running significantly faster than the Inside-Outside algorithm even though we use an Inside-Outside post-pass. Part of this discrepancy is due to the fact that we require a smaller number of new nonterminal symbols to achieve equivalent performance, but we have also found that our post-pass converges more quickly even given the same number of nonterminal symbols.\\n\\nDiscussion\\n\\nOur algorithm consistently outperformed the Inside-Outside algorithm in these experiments. While we partially attribute this difference to using a Bayesian instead of maximum-likelihood objective function, we believe that part of this difference results from a more effective search strategy. In particular, though both algorithms employ a greedy hill-climbing strategy, our algorithm gains an advantage by being able to add new rules to the grammar.\\n\\nOutperforming n-gram models in the first two domains demonstrates that our algorithm is able to take advantage of the grammatical structure present in data. However, the superiority of n-gram models in the part-of-speech domain indicates that to be competitive in modeling naturally-occurring data, it is necessary to model collocational information accurately. We need to modify our algorithm to more aggressively model n-gram information.\\n\\nConclusion\\n\\nThis research represents a step forward in the quest for developing grammar-based language models for natural language. We induce models that, while being substantially more compact, outperform n-gram language models in medium-sized domains. The algorithm runs essentially in time and space linear in the size of the training data, so larger domains are within our reach.\\n\\nHowever, we feel the largest contribution of this work does not lie in the actual algorithm specified, but rather in its indication of the potential of the induction framework described by Solomonoff in 1964. We have implemented only a subset of the moves that we have developed, and inspection of our results gives reason to believe that these additional moves may significantly improve the performance of our algorithm.\\n\\nSolomonoff's induction framework is not restricted to probabilistic context-free grammars. After completing the implementation of our move set, we plan to explore the modeling of context-sensitive phenomena. This work demonstrates that Solomonoff's elegant framework deserves much further consideration.\\n\\nAcknowledgements\\n\\nWe are indebted to Stuart Shieber for his suggestions and guidance, as well as his invaluable comments on earlier drafts of this paper. This material is based on work supported by the National Science Foundation under Grant Number IRI-9350192 to Stuart M. Shieber.\\n\\nBibliography\\n\\nD. Angluin and C.H. Smith. 1983. Inductive inference: theory and methods. ACM Computing Surveys, 15:237-269.\\n\\nL.R. Bahl, J.K. Baker, P.S. Cohen, F. Jelinek, B.L. Lewis, and R.L. Mercer. 1978. Recognition of a continuously read natural corpus. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 422-424, Tulsa, Oklahoma, April.\\n\\nLalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-5(2):179-190, March.\\n\\nJ.K. Baker. 1975. The DRAGON system - an overview. IEEE Transactions on Acoustics, Speech and Signal Processing, 23:24-29, February.\\n\\nJ.K. Baker. 1979. Trainable grammars for speech recognition. In Proceedings of the Spring Conference of the Acoustical Society of America, pages 547-550, Boston, MA, June.\\n\\nL.E. Baum and J.A. Eagon. 1967. An inequality with application to statistical estimation for probabilistic functions of Markov processes and to a model for ecology. Bulletin of the American Mathematicians Society, 73:360-363.\\n\\nPeter F. Brown, Vincent J. DellaPietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479, December.\\n\\nA.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(B):1-38.\\n\\nFrederick Jelinek and Robert L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice, Amsterdam, The Netherlands: North-Holland, May.\\n\\nM.D. Kernighan, K.W. Church, and W.A. Gale. 1990. A spelling correction program based on a noisy channel model. In Proceedings of the Thirteenth International Conference on Computational Linguistics, pages 205-210.\\n\\nK. Lari and S.J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35-56.\\n\\nK. Lari and S.J. Young. 1991. Applications of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 5:237-257.\\n\\nMing Li and Paul Vitnyi. 1993. An Introduction to Kolmogorov Complexity and its Applications. Springer-Verlag.\\n\\nMichael K. McCandless and James R. Glass. 1993. Empirical acquisition of word and phrase classes in the ATIS domain. In Third European Conference on Speech Communication and Technology, Berlin, Germany, September.\\n\\nFernando Pereira and Yves Schabes. 1992. Inside-outside reestimation from partially bracket corpora. In Proceedings of the 30th Annual Meeting of the ACL, pages 128-135, Newark, Delaware.\\n\\nP. Resnik. 1992. Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. In Proceedings of the 14th International Conference on Computational Linguistics.\\n\\nJ. Rissanen. 1978. Modeling by the shortest data description. Automatica, 14:465-471.\\n\\nY. Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Proceedings of the 14th International Conference on Computational Linguistics.\\n\\nC.E. Shannon. 1951. Prediction and entropy of printed English. Bell Systems Technical Journal, 30:50-64, January.\\n\\nR.J. Solomonoff. 1960. A preliminary report on a general theory of inductive inference. Technical Report ZTB-138, Zator Company, Cambridge, MA, November.\\n\\nR.J. Solomonoff. 1964. A formal theory of inductive inference. Information and Control, 7:1-22, 224-254, March, June.\\n\\nRohini Srihari and Charlotte Baltus. 1992. Combining statistical and syntactic methods in recognizing handwritten sentences. In AAAI Symposium: Probabilistic Approaches to Natural Language, pages 121-127.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9504034.xml'}),\n",
       " Document(page_content=\"Redundancy in Collaborative Dialogue\\n\\nIn dialogues in which both agents are autonomous, each agent deliberates whether to accept or reject the contributions of the current speaker. A speaker cannot simply assume that a proposal or an assertion will be accepted. However, an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection. Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer's next dialogue contribution. In this paper, I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance. The model (1) requires a theory of mutual belief that supports mutual beliefs of various strengths; (2) explains the function of a class of informationally redundant utterances that cannot be explained by other accounts; and (3) contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption.\\n\\nIntroduction\\n\\nThe various formulations of this `no redundancy' rule permeate many computational analyses of natural language and notions of cooperativity. However consider the following excerpt from the middle of an advisory dialogue between Harry (h), a talk show host, and Ray (r) his caller.\\n\\nExample 1: ( 6) r. uh 2 tax questions. one: since April 81 we have had an 85 year old mother living with us. her only income has been social security plus approximately $3000 from a certificate of deposit and i wonder what's the situation as far as claiming her as a dependent or does that income from the certificate of deposit rule her out as a dependent? ( 7) h. yes it does. ( 8) r. IT DOES. ( 9) h. YUP THAT KNOCKS HER OUT. .........\\n\\nIn standard information theoretic terms, both (8) and (9) are  REDUNDANT. Harry's assertion in (9) simply paraphrases what was said in (7) and (8) and so it cannot be adding beliefs to the common ground. Furthermore, the truth of (9) cannot be in question, for instead of (9), Harry could not say Yup, but that doesn't knock her out. So why does Ray (r) in (8)  REPEAT Harry's (h) assertion of it does, and why does Harry PARAPHRASE himself and Ray in (9)?\\n\\nFirst consider the notion of evidence. One reason why agents need  EVIDENCE for beliefs is that they only have partial information about: (1) the state of world; (2) the effects of actions; (3) other agent's beliefs, preferences and goals. This is especially true when it comes to modelling the effects of linguistic actions. Linguistic actions are different than physical actions. An agent's prior beliefs, preferences and goals cannot be ascertained by direct inspection. This means that it is difficult for the speaker to verify when an action has achieved its expected result, and so giving and receiving evidence is critical and the process of establishing mutual beliefs is carefully monitored by the conversants.\\n\\nThe characterization of IRU's as informationally redundant follows from an axiomatization of action in dialogue that I will call the  DETERMINISTIC MODEL. This model consists of a number of simplifying assumptions such as: (1) Propositions are are either believed or not believed, (2) Propositions representing beliefs and intentions get added to the context by the unilateral action of one conversant, (3) Agents are logically omniscient. (4) The context of a discourse is an undifferentiated set of propositions with no specific relations between them. I claim that these assumptions must be dropped in order to explain the function of IRU's in dialogue.\\n\\nMutual Beliefs in a Shared Environment\\n\\nThus mutual beliefs depend on a defeasible inference process. All inferences depend on the evidence to support them, and stronger evidence can defeat weaker evidence. So a mutual belief supported as an inference can get defeated by linguistic information. In addition, I adopt an an assumption that a chain of reasoning is only as strong as its weakest link:\\n\\nWeakest Link Assumption: The strength of a belief P depending on a set of underlying assumptions\\n\\nai,...anis MIN(Strength (\\n\\nai,...an))\\n\\nThis seems intuitively plausible and means that the strength of belief depends on the strength of underlying assumptions, and that for all inference rules that depend on multiple premises, the strength of an inferred belief is the weakest of the supporting beliefs.\\n\\nInference of Understanding\\n\\nsay(A, B, u, p)  --A-] understand(B, u, p) [evidence-type] Assumptions = {   copresent(A, B, u)     [evidence-type] attend(B, u)           [evidence-type] hear(B, u)             [evidence-type] bel(B, realize(u, p))  [evidence-type] }\\n\\nThis schema means that when A says u to B intending to convey p, that this leads to the mutual belief that B understands u as p under certain assumptions. The assumptions are that A and B were copresent, that B was attending to the utterance event, that B heard the utterance, and that B believes that the utterance u realizes the intended meaning p.\\n\\nThe [evidence-type] annotation indicates the strength of evidence supporting the assumption. All of the assumptions start out supported by no evidence; their evidence type is therefore hypothesis. It isn't until after the addressee's next action that an assumption can have its strength modified.\\n\\nIt is also possible that A intends that BY saying u, which realizes p, B should make a certain inference q. Then B's understanding of u should include B making this inference. This adds an additional assumption:\\n\\nbel(B, license(p,q))  [evidence\\n\\n\\n\\ntype]\\n\\nThus assuming that q was inferred relies on the assumption that B believes that p licenses q in the context.\\n\\nExample of a Repetition\\n\\nsay(harry, ray, u7, p7) --A-] understand(Ray, u7, p7) [default] Assumptions = {   copresent(harry, ray, u7) [linguistic] attend(ray, u7)           [linguistic] hear(ray, u7)             [linguistic] bel(ray, realize(u7, p7)) [default] }\\n\\nBecause of the  WEAKEST LINK assumption, the belief about understanding is still a default.\\n\\nExample of a Paraphrase\\n\\nConsider the following excerpt:\\n\\nExample 2: (18) h. i see. are there any other children beside your wife? (19) d. no (20) h. YOUR WIFE IS AN ONLY CHILD (21) d. right. and uh wants to give her some security ..........\\n\\nHarry's utterance of (20) is said with a falling intonational contour and hence is unlikely to be a question. This utterance results in an instantiation of the inference rule as follows:\\n\\nsay(harry, ray, u20, p20) --A-] understand(Ray, u20, p20) [linguistic] Assumptions = {   copresent(harry, ray, u7) [linguistic] attend(ray, u7)           [linguistic] hear(ray, u7)             [linguistic] bel(ray, realize(u7, p7)) [linguistic] }\\n\\nIn this case, the belief about understanding is supported by linguistic evidence since all of the supporting assumptions are supported by linguistic evidence. Thus a paraphrase provides excellent evidence that an agent actually understood what another agent meant.\\n\\nIn addition, these IRU's leave a proposition salient, where otherwise the discourse might have moved on to other topics. This is part of the  CENTERING function of IRU's and is left to future work.\\n\\nMaking Inferences Explicit\\n\\nFor example the logical omniscience assumption would mean that if 1(a) and (b) below are in the context, then (c) will be as well since it is entailed from (a) and (b).\\n\\n[a.] You can buy an I R A if and only if you do NOT have an existing pension plan. [b.] You have  an existing pension plan. [c.] You cannot buy an I R A.\\n\\nThe following excerpt demonstrates this structure. Utterance (15) realizes 0a, utterance (16) realizes 0b, and utterance (17) makes the inference explicit that is given in 0c for the particular tax year of 1981.\\n\\nExample 3: (15) h. oh no. I R A's were available as long as you are not a participant in an existing pension (16) j. oh i see. well i did work i do work for a company that has a pension (17) h. ahh. THEN YOU'RE NOT ELIGIBLE FOR EIGHTY ONE (18) j. i see, but i am for 82\\n\\nAfter (16), since the propositional content of (17) is inferrable, the assumption that Harry has made this inference is supported by the inference evidence type:\\n\\nbel(H, license(p16, p17))  [inference]\\n\\nbel(H, license(p16, p17))  [linguistic]\\n\\nSupporting Inferences\\n\\nExample 4: ( 1) j. hello harry, my name is jane ( 2) h. welcome jane ( 3) j. i just retired december first, and in addition to my pension and social security, I have a supplemental annuity ( 4) h. yes ( 5) j. which i contributed to while i was employed ( 6) h. right ( 7) j. from the state of NJ mutual fund. and I'm entitled to a lump sum settlement which would be between 16,800 and 17,800, or a lesser life annuity. and the choices of the annuity um would be $125.45 per month. That would be the maximum with no beneficiaries ( 8) h. You can stop right there: take your money. ( 9) j. take the money. (10) h. absolutely. YOU'RE ONLY GETTING 1500 A YEAR. at 17,000, no trouble at all to get 10 percent on 17,000 bucks.\\n\\nHarry interrupts her at (8) since he believes he has enough information to suggest a course of action, and tells her take your money. To provide  SUPPORT for this course of action he produces an inference that follows from what she has told him in (7), namely You're only getting 1500 (dollars) a year. SUPPORT is a general relation that holds between beliefs and intentions in this model.\\n\\nPresumably Jane would have no trouble calculating that $125.45 a month for 12 months amounts to a little over $1500 a year, and thus can easily accept this statement that is intended to provide the necessary  SUPPORT relation, ie. the juxtaposition of this fact against the advice to take the money conveys that the fact that she is only getting 1500 dollars a year is a reason for her to adopt the goal of taking the money, although this is not explicitly stated.\\n\\nEvidence of Acceptance\\n\\nAchieving understanding and compensating for resource bounds are issues for a model of dialogue whether or not agents are autonomous. But agents' autonomy means there are a number of other reasons why A's utterance to B conveying a proposition p might not achieve its intended effect: (1) p may not cohere with B's beliefs, (2) B may not think that p is relevant, (3) B may believe that p does not contribute to the common goal, (4) B may prefer doing or believing some q where p is mutually exclusive with q, (5) If p is about an action, B may want to partially modify p with additional constraints about how, or when p.\\n\\nEvidence of acceptance may be given explicitly, but acceptance can be inferred in some dialogue situations via the operation of a simple principle of cooperative dialogue:\\n\\nCOLLABORATIVE PRINCIPLE: Conversants must provide evidence of a detected discrepancy in belief as soon as possible.\\n\\nThis principle claims that evidence of conflict should be made apparent in order to keep default inferences about acceptance or understanding from going through. IRU's such as  PROMPTS,  REPETITIONS, PARAPHRASES, and making an  INFERENCE explicit cannot function as evidence for conflicts in beliefs or intentions via their propositional content since they are informationally redundant. If they are realized with question intonation, the inference of acceptance is blocked.\\n\\nIn the dialogue below between Harry (h) and Ruth (r), Ruth in (39), first ensures that she understood Harry correctly, and then provides explicit evidence of non-acceptance in (41), based on her autonomous preferences about how her money is invested. .\\n\\nExample 5: (38) h. and I'd like 15 thousand in a 2 and a half year certificate (39) r. the full 15 in a 2 and a half? (40) h. that's correct (41) r. GEE. NOT AT MY AGE\\n\\nIn the following example, Joe in (14) makes a statement that provides propositional content that conflicts with Harry's statement in (13) and thus provides evidence of non-acceptance.\\n\\nExample 6 (13) h. and  -- there's no reason why you shouldn't have an I R A  for last year (14) j. WELL I THOUGHT THEY JUST STARTED THIS YEAR\\n\\nJoe's statement is based on his prior beliefs. In both of these cases this evidence for conflict is given immediately. However when there is no evidence to the contrary, and goals of the discourse require achievement of acceptance, inferences about acceptance are licensed as default. They can be defeated later by stronger evidence.\\n\\nWithout this principle, a conversant might not bring up an objection until much later in the conversation, at which point the relevant belief and some inferences following from that belief will have been added to the common ground as defaults. The result of this is that the retraction of that belief results in many beliefs being revised. The operation of this principle helps conversants avoid replanning resulting from inconsistency in beliefs, and thus provides a way to manage the augmentation of the common ground efficiently.\\n\\nOther hypotheses\\n\\nThe first point to note is that the examples here are only a subset of the types of IRU's that occur in dialogues. I use the term antecedent to refer to the most recent utterance which should have added the proposition to the context. This paper has mainly focused on cases where the IRU: (1) is adjacent to its antecedent, rather than remote; (2) realizes a proposition whose antecedent was said by another conversant, (3) has only one antecedent. It is with respect to this subset of the data that the alternate hypotheses are examined.\\n\\nA distributional analysis of a subset of the corpus (171 IRU's from 24 dialogues consisting of 976 turns), on the relation of an IRU to its antecedent and the context, shows that 35% of the tokens occur remotely from their antecedents, that 32% have more than one antecedent, that 48% consist of the speaker repeating something that he said before and 52% consist of the speaker repeating something that the other conversant said. So the data that this paper focuses on accounts for about 30% of the data.\\n\\nIndirect Question Hypothesis\\n\\nOf 171 IRU's, only 28 are realized with rising question intonation. Of these 28, 6 are actually redundant questions with question syntax, and 14 are followed by affirmations.\\n\\nIf these are generally questions, then one possible answer to what the question is about is that Ray is questioning whether he actually heard properly. But then why doesn't he use an intonational contour that conveys this fact as Ruth does in example 5? On an efficiency argument, it is hard to imagine that it would have cost Ray any more effort to have done so.\\n\\nFinally, if it were a question it would seem that it should have more than one answer. While 50 of these IRU's are followed by an affirmation such as that's correct, right, yup, none of them are ever followed by a denial of their content. It seems an odd question that only has one answer.\\n\\nDead Air Hypothesis\\n\\nAnother hypothesis is that IRU's result from the radio talk show environment in which silence is not tolerated. So agents produce IRU's because they cannot think of anything else to say but feel as though they must say something.\\n\\nThe dead air hypothesis would seem to rely on an assumption that at unpredictable intervals, agents just can't think very well. My claim is that IRU's are related to goals, that they support inferencing and address assumptions underlying mutual beliefs, ie. they are not random. In order to prove this it must be possible to test the hypothesis that it is only important propositions that get repeated, paraphrased or made explicit. This can be based on analyzing when the information that is repeated has been specifically requested, such as in the caller's opening question or by a request for information from Harry. It should also be possible to test whether the IRU realizes a proposition that plays a role in the final plan that Harry and the caller negotiate. However this type of strong evidence against the dead air hypothesis is left to future work.\\n\\nDiscussion\\n\\nThe occurrence of IRU's in dialogue has many ramifications for a model of dialogue. Accounting for IRU's has two direct effects on a dialogue model. First it requires a model of mutual beliefs that specifies how mutual beliefs are inferred and how some mutual beliefs can be as weak as mutual suppositions. One function of IRU's is to address the assumptions on which mutual beliefs are based. Second the assumption that propositions representing beliefs and intentions get added to the context by the unilateral action of one conversant must be dropped. This account replaces that assumption with a model in which the evidence of the hearer must be considered to establish mutual beliefs. The claim here is that both understanding and acceptance are monitored. The model outlined here can be used for different types of dialogue, including dialogues in which agents are constructing mutual beliefs to support future action by them jointly or alone.\\n\\nHow and when agents decide to augment the strength of evidence for a belief has not been addressed in this work as yet. Future work includes analyzing the corpus with respect to whether the IRU plays a role in the final plan that is negotiated between the conversants.\\n\\nAcknowledgements\\n\\nDiscussions with Aravind Joshi, Ellen Prince and Bonnie Webber have been extremely helpful in the development of these ideas. In addition I would like to thank Herb Clark, Sharon Cote, Julia Galliers, Ellen Germain, Beth Ann Hockey, Megan Moser, Hideyuki Nakashima, Owen Rambow, Craige Roberts, Phil Stenton, and Steve Whittaker for the influence of their ideas and for useful discussions.\\n\\nBibliography\\n\\nJean C. Carletta. Risk Taking and Recovery in Task-Oriented Dialogue. PhD thesis, Edinburgh University, 1992.\\n\\nHerbert H. Clark and Catherine R. Marshall. Definite reference and mutual knowledge. In Joshi, Webber, and Sag, editors, Elements of Discourse Understanding, pages 10-63. CUP, Cambridge, 1981.\\n\\nHerbert H. Clark and Edward F. Schaefer. Contributing to discourse. Cognitive Science, 13:259-294, 1989.\\n\\nJulia R. Galliers. Cooperative interaction as strategic belief revision. In M.S. Deen, editor, Cooperating Knowledge Based Systems, page 1. Springer Verlag, 1991.\\n\\nH. P. Grice.\\n\\nWilliam James Lectures.\\n\\n1967.\\n\\nBarbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. Towards a computational theory of discourse interpretation. Unpublished Manuscript, 1986.\\n\\nBarbara J. Grosz and Candace L. Sidner. Attentions, intentions and the structure of discourse. Computational Linguistics, 12:175-204, 1986.\\n\\nBarbara J. Grosz and Candace L. Sidner. Plans for discourse. In Cohen, Morgan and Pollack, eds. Intentions in Communication, MIT Press, 1990.\\n\\nJulia Hirschberg. A Theory of Scalar Implicature. PhD thesis, University of Pennsylvania, Computer and Information Science, 1985.\\n\\nDavid Lewis.\\n\\nConvention.\\n\\nHarvard University Press, 1969.\\n\\nDiane Litman and James Allen. Recognizing and relating discourse intentions and task-oriented plans. In Cohen, Morgan and Pollack, eds. Intentions in Communication, MIT Press, 1990.\\n\\nMartha Pollack, Julia Hirschberg, and Bonnie Webber. User participation in the reasoning process of expert systems. In AAAI82, 1982.\\n\\nEllen F. Prince. On the function of existential presupposition in discourse. In Papers from 14th Regional Meeting. CLS, Chicago, IL, 1978.\\n\\nRobert C. Stalnaker. Assertion. In Peter Cole, editor, Syntax and Semantics, Volume 9: Pragmatics, pages 315-332. Academic Press, 1978.\\n\\nMarilyn A. Walker and Steve Whittaker. Mixed initiative in dialogue: An investigation into discourse segmentation. In Proc. 28th Annual Meeting of the ACL, pages 70-79, 1990.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9503017.xml'}),\n",
       " Document(page_content=\"A Computational Approach to Aspectual Composition Introduction\\n\\nIn recent years, it has become common in the linguistics and philosophy literature to assume that events and processes are ontologically distinct entities, on a par with objects and substances. At the same time, the idea that episodic knowledge should be represented as a collection of interrelated eventualities has gained increasing acceptance in the computational linguistics and artificial intelligence literature.\\n\\nMotivation\\n\\nIt should be evident that this structure can be straightforwardly translated into the English sentence Jack filled five buckets in twenty minutes. To see that this representation is also concise, note that one can also derive numerous other sentences from this structure, given appropriate rules of inference: for example, Jack filled a bucket, Jack filled something, etc. On the other hand, this representation also supports underspecificity, since from this structure one cannot determine which five buckets were filled (e.g., bucket A, ..., bucket E).\\n\\nAs a second example, let us now consider the following representation of a pouring process e1:\\n\\nIt should be evident once again that this structure can be straightforwardly translated into the English sentence Jack poured water into bucket A for thirty seconds. Moreover, this representation is similarly concise: given appropriate rules of inference, one can also derive Jack poured water into bucket A for twenty-five seconds, Jack poured water into bucket A for twenty seconds, and so forth. Finally, this representation likewise supports underspecificity, since in the absence of any information about the rate of transfer one cannot determine how much water was poured into bucket A.\\n\\nAs the careful reader may have noticed, the choice of temporal adverbial in the preceding examples is conditioned by the sort of eventuality in question, which again depends on the verb. This is not the whole story, however: given a particular amount of water in the second example, the appropriate adverbial changes (cf. Jack poured five gallons of water into bucket A *for/in thirty seconds); contrariwise, switching to a bare plural in the first example, we may note a switch in the opposite direction (cf. Jack filled buckets for/*in twenty minutes). Adequately explaining dependencies such as these is the problem of aspectual composition, to which we now turn.\\n\\nA Sortal Approach to Aspectual Composition\\n\\nComparing this representation to the previous one (for Jack poured water into bucket A for thirty seconds), two questions naturally arise:\\n\\n, which he introduces to distinguish both events (\\n\\n)\\n\\nfrom processes (\\n\\n)\\n\\nand objects (\\n\\n)\\n\\nfrom\\n\\nsubstances (\\n\\n). According to Jackendoff, ``a speaker uses a\\n\\nconstituent to refer to an entity whose boundaries are not in view or not of concern; one can think of the boundaries as outside the current field of view.'' Although this idea has some appeal when one focuses on the discourse-backgrounding function that atelic sentences can have, it does not seem particularly apt here, where our view of what has taken place remains constant.\\n\\nWhile our two example sentences clearly convey different information, it is not at all obvious how to make sense of this difference in terms of perspectives. For this reason, I will pursue an alternative approach below which obviates the need to do so. As we shall see, this will require us to develop an alternative conception of the event/process (and object/substance) distinction than Jackendoff appears to have in mind.\\n\\nAs Krifka points out, the conventional wisdom about aspectual composition does not appear to be compatible with a sortal approach:\\n\\nFor consider a concrete event of running and a concrete event of running a mile; then surely both events have a terminal point (both events might even be identical). The difference is that an event of running might be part of another event of running which has a later terminal point, whereas this is not possible for an event of running a mile.\\n\\nTo relate these continua to their particulars, I will borrow Jackendoff's composed-of relation (though not necessarily its original semantics). Following Jackendoff, I will assume that this relation forms part of the meaning of measure phrases, which include adjectival ones such as five gallons of and adverbial ones such as for thirty seconds. This yields the following representations for our example  sentences:\\n\\nTo paraphrase, in the first case e1 is an event of duration thirty seconds which is composed of a process e in which Jack pours the substance x, which is water, into bucket A. In the second case, e1 is instead an event (again of duration thirty seconds) in which Jack pours the object x1 into bucket A, where x1 is a five-gallon quantity (composed-) of the substance x(which is again water).\\n\\nAt this point we may answer the two questions with which we began this section. With respect to the first question (is it necessary to introduce a new eventuality\\n\\nto represent the second of our two sentences? ), the above representations show that we can now simply treat these two sentences as two different descriptions of the same event e1 -- much as in Krifka's treatment -- without causing a sortal clash.\\n\\nA Calculus of Eventualities\\n\\nTo illustrate how the present approach to aspectual composition maintains the conciseness advantage of eventuality-based knowledge representations cited previously, let us now consider two examples in some detail.\\n\\nFirst, suppose our knowledge base contains the first representation of the event e1 given near the end of the preceding section, repeated below:\\n\\nFrom this representation of e1, we should be able to derive the existence of an event e2 in which Jack pours water into bucket A for twenty-five seconds, as well as an event e3 in which he does so for twenty seconds, and so forth. This can be achieved using the following rule:\\n\\nThis rule reflects a pair of assumptions regarding processes (i.e., process continua) such as e.  First, it is assumed that the particular events making up the continuum e are closed under the subpart relation; thus, if the event e1 composed-of ehas a subevent e2, then e2 must also be composed-of e. Second, if e1 has duration N1 Us, then for all non-negative numbers N2 less than N1, e1 is assumed to have a subevent e2of duration N2 Us (clearly a simplifying assumption!). Taken together, these two assumptions yield the above rule as a theorem.\\n\\nSecond, let us now suppose that we have a method for calculating the rate of transfer for the pouring process e above, and that this rate multiplied by thirty seconds turns out to be five gallons. Using this information, we should then be able to derive the second representation of the event e1 given near the end of the preceding section (repeated below) from the first one.\\n\\nThis can be achieved using the following rule:\\n\\nSince this rule may look overly complicated at first glance, let us pause to consider why it makes sense. Because the process Eof which the event E1 is composed is understood to be a continuum of pouring events, all with agent A and goal G, we can derive that E1 in particular is a pouring event with agent A and goal G. As for the patient of E1, we may note that since the patient of Eis understood to the continuum X of quantities poured, one of these must be the patient of E1; supposing that its index is X1, we can then derive that X1 is both the patient of E1 and composed of X. (The calculation of X1's amount, N2 U2s, is not of particular concern here, and thus is assumed to be straightforward.)\\n\\nSpace and Time Linked\\n\\nThe present account extends naturally to cover motion verbs, which are assumed to form incremental thematic relations between paths and eventualities (rather than between material entities and eventualities). For the most part, all that is required is to introduce the appropriate counterparts of the substances and processes into the domain of directed spatial entities.\\n\\nFollowing Krifka, I will assume that events of directed motion have spatial traces as well as temporal ones, i.e., that each directed motion event has a unique (delimited) path and time interval associated with it. With directed motion processes, however, this cannot be the case, given the present conception of predication over continua: while a continuum of motion events may have the same agent and the same manner of motion, the path traversed does not remain constant. Consequently, if a directed motion process is to have a unique path associated with it, the path must likewise be a continuum, i.e. what I shall call a non-delimited path.\\n\\nOnce the notion of incremental thematic relation has been extended to the case of directed motion eventualities in this way, it remains only to examine the sortal restrictions which make sense for various path predicates. For example, with to-phrases (e.g. to the bridge), which specify endpoints, we may naturally assume that their translations are only well-sorted with delimited paths, since endpoints do not remain constant across a continuum of such paths. As a result, expressions such as Jack run to the bridge will give rise to predicates restricted to events, in contrast to Jack run (which can apply to processes as well). This explains why * Jack ran to the bridge for thirty seconds is not well-formed, as illustrated below (recall that the composed-of relation serves to map processes to events):\\n\\nUnlike to-phrases, towards-phrases do make sense for non-delimited paths, since these specify direction rather than endpoints (and direction can remains constant across a continuum). As such, Jack ran towards the bridge for thirty seconds receives the following well-formed translation:\\n\\nAs an aside, it is worth observing that non-delimited paths need not be unbounded. This is especially important with predicates such as towards, since the reference object here serves to impose an upper limit on how far the continuum can extend. For example, consider the process e above of Jack running towards the bridge (assumed to be of more or less constant speed and direction). Although the continuum e may contain events larger than e1, it cannot contain any events larger than the event in which Jack reaches the bridge, as the path of any such event would no longer satisfy the predicated yielded by towards the bridge. Because the present notion of delimitedness is independent of boundedness (in the mathematical sense), the presence of upper bounds in cases such as this one is entirely unproblematic.\\n\\nAnother interesting case is that of along. In general, distance cannot be predicated of a non-delimited path, as distance varies according to the endpoints. This is not the case, however, with proximal distance, i.e. the distance between the path and the reference object, which can remain constant across a path continuum. This explains why sentences like Jack ran along the river, two hundred yards from the shore, for thirty seconds should be well-formed.\\n\\nFinally we turn to distance phrases. As mentioned above, distance cannot be sensibly predicated of non-delimited paths, so I will assume that distance predication is restricted to delimited paths. Consequently, bare distance phrases will behave just like to-PPs, which explains both why Jack ran two miles to the bridge is fine and why * Jack ran two miles for ten minutes is out. Now, what about distance phrases headed by for? Remarkably, these adverbials have been almost completely ignored in the literature. As with their temporal counterparts, I will assume that distance for-adverbials form measure phrases, i.e. serve to introduce the composed-of mapping. By making this natural assumption, we may then explain the curious fact that * Jack ran to the bridge for two miles is horrible, in sharp contrast to both Jack ran two miles to the bridge and Jack ran along the river for two miles. This is illustrated below:\\n\\nOne consequence of the present approach is that Jack run along the river for two miles and Jack run two miles along the river are assigned rather different representations. Although space precludes further discussion here, this should not be considered particularly troublesome since these sentences can easily be made to be mutually entailing.\\n\\nConclusion\\n\\nBibliography\\n\\nEmmon Bach.\\n\\n1986.\\n\\nThe algebra of events.\\n\\nLinguistics and Philosophy, 9:5\\n\\n\\n\\n16.\\n\\nGreg Carlson. 1977. Reference to kinds in English. Ph.D. thesis, University of Massachussets, Amherst.\\n\\nRobert Dale. 1992. Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes. MIT Press.\\n\\nDavid R. Dowty. 1972. Studies in the Logic of Verb Aspect and Time Reference. Ph.D. thesis, University of Texas.\\n\\nDavid R. Dowty. 1979. Word Meaning and Montague Grammar. Reidel.\\n\\nDavid Dowty.\\n\\n1991.\\n\\nThematic proto\\n\\n\\n\\nroles and argument selection.\\n\\nLanguage, 67(3):547\\n\\n\\n\\n615.\\n\\nKurt Eberle. 1992. On representing the temporal structure of a natural language text. In Proceedings of the Fifteenth International Conference on Computational Linguistics, COLING 92.\\n\\nErhard Hinrichs. 1985. A Compositional Semantics for Aktionsarten and NP Reference in English. Ph.D. thesis, The Ohio State University.\\n\\nChung Hee Hwang and Lenhart K. Schubert. 1992. Tense trees as the ``fine structure'' of discourse. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics.\\n\\nRay Jackendoff.\\n\\n1991.\\n\\nParts and boundaries.\\n\\nCognition, 41:9\\n\\n\\n\\n45.\\n\\nRay Jackendoff. 1994. The proper treatment of measuring out, telicity, and perhaps even quantification in English. Natural Language and Linguistic Theory. Forthcoming.\\n\\nStuart John Harding Kent. 1993. Modelling Events from Natural Language. Ph.D. thesis, Imperial College, University of London.\\n\\nManfred Krifka. 1992. Thematic relations as links between nominal reference and temporal constitution. In Ivan A. Sag and Anna Szabolcsi, editors, Lexical Matters. CSLI.\\n\\nAlex Lascarides, Nicholas Asher, and Jon Oberlander. 1992. Inferring discourse relations in context. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics.\\n\\nFriederike Moltmann.\\n\\n1991.\\n\\nMeasure adverbials.\\n\\nLinguistics and Philosophy, 14:629\\n\\n\\n\\n660.\\n\\nJon Oberlander and Robert Dale. 1991. Generating expressions referring to eventualities. In Proceedings of the 13th Annual Conference of the Cognitive Science Society, University of Chicago.\\n\\nTerence Parsons. 1990. Events in the Semantics of English: A Study in Subatomic Semantics, volume 21 of Current Studies in Linguistics. MIT Press.\\n\\nJames Pustejovsky. 1991. The syntax of event structure. Cognition, 41:47-81.\\n\\nLenhart K. Schubert and Francis Jeffry Pelletier. 1987. Problems in the representation of the logical form of generics, plurals, and mass nouns. In New Directions in Semantics, pages 385-451. Academic Press.\\n\\nZeno Vendler.\\n\\n1967.\\n\\nLinguistics in Philosophy.\\n\\nCornell University Press.\\n\\nH. J. Verkuyl. 1972. On the Compositional Nature of the Aspects. Reidel.\\n\\nHenk J. Verkuyl. 1993. A theory of aspectuality: The interaction between temporal and atemporal structure, volume 64 of Cambridge Studies in Linguistics. Cambridge University Press.\\n\\nMichael White. 1993a. Delimitedness and trajectory-of-motion events. In Proceedings of the Sixth Conference of the European Chapter of the Association for Computational Linguistics (EACL '93).\\n\\nMichael White. 1993b. The imperfective paradox and trajectory-of-motion events. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics.\\n\\nMichael White. 1994a. A calculus of eventualities. In Proceedings of the AAAI-94 Workshop on Spatial and Temporal Reasoning.\\n\\nMichael White. 1994b. A Computational Approach to Aspectual Composition. Ph.D. thesis, University of Pennsylvania.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9506026.xml'}),\n",
       " Document(page_content=\"Efficient Tabular LR Parsing\\n\\nWe give a new treatment of tabular LR parsing, which is an alternative to Tomita's generalized LR algorithm. The advantage is twofold. Firstly, our treatment is conceptually more attractive because it uses simpler concepts, such as grammar transformations and standard tabulation techniques also know as chart parsing. Secondly, the static and dynamic complexity of parsing, both in space and time, is significantly reduced.\\n\\nIntroduction\\n\\nOur grammar transformation produces a so called cover for the input grammar, which together with the filtering condition fully captures the specification of the method, abstracting away from algorithmic details such as data structures and control flow. Since this cover can be easily precomputed, implementing our LR parser simply amounts to running the standard tabular algorithm. This is very attractive from an application-oriented perspective, since many actual systems for natural language processing are based on these kinds of parsing algorithm.\\n\\nDefinitions\\n\\nLR automata\\n\\n2LR Automata\\n\\nIn a stack symbol of the form (X, q), the X serves to record the grammar symbol that has been recognized last, cf. the symbols that formerly were found immediately before the dots.\\n\\nThe algorithm\\n\\nWe are now ready to give a formal specification of the tabular algorithm.\\n\\nAnalysis of the algorithm\\n\\nEmpirical results\\n\\nOur implementation is merely a prototype, which means that absolute duration of the parsing process is little indicative of the actual efficiency of more sophisticated implementations. Therefore, our measurements have been restricted to implementation-independent quantities, viz. the number of elements stored in the parse table and the number of elementary steps performed by the algorithm. In a practical implementation, such quantities will strongly influence the space and time complexity, although they do not represent the only determining factors. Furthermore, all optimizations of the time and space efficiency have been left out of consideration.\\n\\nDiscussion\\n\\nIt is conceptually simpler, because we make use of simple concepts such as a grammar transformation and the well-understood CYK algorithm, instead of a complicated mechanism working on graph-structured stacks.\\n\\nOur algorithm requires fewer LR states. This leads to faster parser generation, to smaller parsers, and to reduced time and space complexity of parsing itself.\\n\\nParsing algorithms that are not based on the LR technique have however been left out of consideration, and so were techniques for unification grammars and techniques incorporating finite-state processes.\\n\\nAcknowledgements\\n\\nThe first author is supported by the Dutch Organization for Scientific Research (NWO), under grant 305-00-802. Part of the present research was done while the second author was visiting the Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD.\\n\\nWe received kind help from John Carroll, Job Honig, Kees Koster, Theo Vosse and Hans de Vreught in finding the grammars mentioned in this paper. Generous help with locating relevant literature was provided by Anton Nijholt, Rockford Ross, and Arnd Rumann.\\n\\nBibliography\\n\\nBillot, S. and B. Lang. 1989. The structure of shared forests in ambiguous parsing. In 27th Annual Meeting of the ACL, pages 143-151.\\n\\nBooth, T.L. 1967. Sequential Machines and Automata Theory. Wiley, New York.\\n\\nCarroll, J.A. 1993. Practical unification-based parsing of natural language. Technical Report No. 314, University of Cambridge, Computer Laboratory, England. PhD thesis.\\n\\nEarley, J. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94-102.\\n\\nHarrison, M.A. 1978. Introduction to Formal Language Theory. Addison-Wesley.\\n\\nLang, B. 1974. Deterministic techniques for efficient non-deterministic parsers. In Automata, Languages and Programming, 2nd Colloquium, LNCS 14, pages 255-269, Saarbrcken. Springer-Verlag.\\n\\nLeermakers, R. 1989. How to cover a grammar. In 27th Annual Meeting of the ACL, pages 135-142.\\n\\nLeermakers, R. 1992a. A recursive ascent Earley parser. Information Processing Letters, 41(2):87-91.\\n\\nLeermakers, R. 1992b. Recursive ascent parsing: from Earley to Marcus. Theoretical Computer Science, 104:299-312.\\n\\nNederhof, M.J. 1993. Generalized left-corner parsing. In Sixth Conference of the European Chapter of the ACL, pages 305-314.\\n\\nNederhof, M.J. 1994a. Linguistic Parsing and Program Transformations. Ph.D. thesis, University of Nijmegen.\\n\\nNederhof, M.J. 1994b. An optimal tabular parsing algorithm. In 32nd Annual Meeting of the ACL, pages 117-124.\\n\\nNederhof, M.J. and K. Koster. 1992. A customized grammar workbench. In J. Aarts, P. de Haan, and N. Oostdijk, editors, English Language Corpora: Design, Analysis and Exploitation, Papers from the thirteenth International Conference on English Language Research on Computerized Corpora, pages 163-179, Nijmegen. Rodopi.\\n\\nNederhof, M.J. and J.J. Sarbo. 1993. Increasing the applicability of LR parsing. In Third International Workshop on Parsing Technologies, pages 187-201.\\n\\nNederhof, M.J. and G. Satta. 1994. An extended theory of head-driven parsing. In 32nd Annual Meeting of the ACL, pages 210-217.\\n\\nPager, D. 1970. A solution to an open problem by Knuth. Information and Control, 17:462-473.\\n\\nRekers, J. 1992. Parser Generation for Interactive Environments. Ph.D. thesis, University of Amsterdam.\\n\\nSchabes, Y. 1991. Polynomial time and space shift-reduce parsing of arbitrary context-free grammars. In 29th Annual Meeting of the ACL, pages 106-113.\\n\\nSchauerte, R. 1973. Transformationen von LR(k)-grammatiken. Diplomarbeit, Universitt Gttingen, Abteilung Informatik.\\n\\nSchoorl, J.J. and S. Belder. 1990. Computational linguistics at Delft: A status report. Report WTM/TT 90-09, Delft University of Technology, Applied Linguistics Unit.\\n\\nShann, P. 1991. Experiments with GLR and chart parsing. In TO91a, chapter 2, pages 17-34.\\n\\nSheil, B.A. 1976. Observations on context-free parsing. Statistical Methods in Linguistics, pages 71-109.\\n\\nSippu, S. and E. Soisalon-Soininen. 1990. Parsing Theory, Vol. II: LR(k) and LL(k) Parsing. Springer-Verlag.\\n\\nTomita, M. 1986. Efficient Parsing for Natural Language. Kluwer Academic Publishers.\\n\\nTomita, M., editor.\\n\\n1991.\\n\\nGeneralized LR Parsing.\\n\\nKluwer Academic Publishers.\\n\\nvan Wijngaarden, A. et al. 1975. Revised report on the algorithmic language ALGOL 68. Acta Informatica, 5:1-236.\\n\\nVillemonte de la Clergerie, E. 1993. Automates  Piles et Programmation Dynamique -- DyALog: Une application  la Programmation en Logique. Ph.D. thesis, Universit Paris VII.\\n\\nVosse, T.G. 1994. The Word Connection. Ph.D. thesis, University of Leiden.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9605018.xml'}),\n",
       " Document(page_content=\"Computational dialectology in Irish Gaelic\\n\\nDialect groupings can be discovered objectively and automatically by cluster analysis of phonetic transcriptions such as those found in a linguistic atlas. The first step in the analysis, the computation of linguistic distance between each pair of sites, can be computed as Levenshtein distance between phonetic strings. This correlates closely with the much more laborious technique of determining and counting isoglosses, and is more accurate than the more familiar metric of computing Hamming distance based on whether vocabulary entries match. In the actual clustering step, traditional agglomerative clustering works better than the top-down technique of partitioning around medoids. When agglomerative clustering of phonetic string comparison distances is applied to Gaelic, reasonable dialect boundaries are obtained, corresponding to national and (within Ireland) provincial boundaries.\\n\\nIntroduction\\n\\nDefining dialects is one of the first tasks that linguists need to pursue when approaching a language. Knowing the dialect areas helps one allocate resources in language research and has implications for language learners, publishers, broadcasters, educators, and language planners. Unfortunately, dialect definition can be a time-consuming and ill-defined process. The traditional approach has been to plot isoglosses, delineating regions where the same word is used for the same concept, or perhaps the same pronunciation for the same phoneme. But isoglosses are frustrating. The first problem, as Gaston Paris noted (apud Durand, 1889:49), is that isoglosses rarely coincide. At best, isoglosses for different features approach each other, forming vague bundles; at worst, isoglosses may cut across each other, describing completely contradictory binary divisions of the dialect area. That is, language may vary geographically in many dimensions, but the requirements we usually impose require that a specific site be placed in a unique dialect. Traditional dialectological methodology gives little guidance as to how to perform such reduction to one dimension.\\n\\nA second problem is that many isoglosses do not neatly bisect the language area. Often variants do not neatly line up on two sides of a line, but are intermixed haphazardly. More importantly, for some sites information may be lacking, or the question is simply not applicable. When comparing how various sites pronounce the first consonant of a particular word, it is meaningless to ask that question if the site does not use that word. So the isogloss is incomplete and cannot be meaningfully compared with isoglosses based on different sets of sites.\\n\\nThe third problem is that most languages have dialect continua, such that the speech of one community differs little from the speech of its neighbours. Even though the cumulative effects of such differences may be great when one considers the ends of the continua (such as southern Italian versus northern French), still it seems arbitrary to draw major dialect boundaries between two villages with very similar speech patterns. Such conundrums led Paris and others to conclude that the dialect boundary, and therefore the very notion of dialect, is an ill-defined concept.\\n\\nMore recently, the field of dialectometry, as introduced by Sguy (1971, 1973), has addressed these issues by developing several techniques for summarizing and presenting variation along multiple dimensions. They replace isoglosses with a distance matrix, which compares each site directly with all other sites, ultimately yielding a single figure that measures the linguistic distances between each pair of sites. There is however no firm agreement on just how to compute the distance matrices. Sguy's earliest work (1971) was based on lexical correspondences: sites differed in the extent to which they used different words for the same concept. Sguy (1973), Philps (1987), and Durand (1989) use some combination of lexical, phonological, and morphological data. Babitch (1988) described the dialectal distances in Acadian villages by the degree to which their fishing terminology varied. Babitch and Lebrun (1989) did a similar analysis based on the varying pronunciation of /r/. Elsie (1986) grouped the Gaelic dialects on the basis of whether the vocabulary matched. Ebobisse (1989) grouped the Sawabantu languages of Cameroon by whether phonological correspondences in matching vocabulary items were complete, partial, or lacking. There seems to be a certain bias in favour of working with lexical correspondences, which is understandable, since deciding whether two sites use the same word for the same concept is perhaps one of the easiest linguistic judgements to make.\\n\\nNot all dialectometrists agree on the wisdom of delineating dialect areas. Sguy (1973:18) insisted that the concept of dialect boundaries was meaningless, and his emphasis on the gradience of language similarity has been widely maintained. But those who do look for firm dialect affiliations (such as Babitch and Ebobisse) use bottom-up agglomerative techniques. The two linguistically closest sites are grouped into one dialect, and thenceforth treated as a unit. The process continues recursively until all sites are grouped into one superdialect embracing the entire language area under consideration. This yields a binary tree. But Kaufman and Rousseeuw (1990:44) suggest that when the emphasis in a clustering problem is on the top-level clusters--here, finding the two main dialects--then such bottom-up methods, which can potentially introduce error at each of several steps, are less reliable than top-down partitioning methods. Perhaps past researchers have used inferior bottom-up techniques simply because the necessary algorithms are computationally more tractable. Comparing all possible pairs of sites is a O(N[2]) problem, whereas considering all possible two-way partitions of the dialect area is O(2[N]).\\n\\nThe current state of dialectometry thus presents two main questions which constitute the methodological focus of this paper. The first deals with distance matrices. Is there a way to build accurate distance matrices that minimize editorial decisions without discarding relevant data? My research suggests that this may be done by using string distances computed directly on phonetic transcriptions, and that this is better than restricting the study to lexical comparisons. The second deals with clustering. Do bottom-up or top-down techniques work best? My conclusion is that the traditional bottom-up technique works better than a typical top-down method. These conclusions are based partly on an analysis of the mathematical properties of the clusters themselves, partly on how well they correlate with analyses based on more traditional isogloss techniques, and partly on how well they compare with previously-published descriptions of dialects in a specific language, Irish Gaelic.\\n\\nAt one time the Gaelic language group was spoken throughout Ireland, from where it spread to the Isle of Man and to much of Scotland. Currently fully native use of Gaelic is limited to a few discontiguous areas in the westernmost reaches of Ireland and Scotland. In the case of Ireland, everyone agrees that Gaelic is nowadays found in three main dialects: that of Ulster, that of Connacht, and that of Munster ( Siadhail, 1989). But several questions are raised that are less easily answered. Do the three provinces separate out so neatly for intrinsic linguistic reasons, or simply because their speakers have become so widely separated from each other geographically as speakers in intervening areas have adopted English? Does the language of Connacht naturally group with that of Ulster or with that of Munster? And looking beyond Ireland, many have commented that the language of Ulster in general is similar to that of Scotland. Are Irish, Manx, and Scottish Gaelic considered three separate languages for intrinsic linguistic reasons, or because they are spoken in different countries? To a large extent, dialectologists have found these questions difficult to answer because they accepted Paris's conundrum. For Siadhail, the ultimate scientific justification in adopting the three-dialect account is the fact that the Gaeltacht (Irish-speaking territory) is so fragmented nowadays that it no longer forms a continuum.\\n\\nCuv (1951:4-49) felt that there can be no dialect boundaries because transitions are gradual. Elsie (1986:240) considers a dialect to be an area where all communities are linguistically more similar to each other than any community is to any site outside the dialect. Such notions provide a very firm, absolute notion of dialecthood: a set of communities either constitutes a dialect area, or it does not. But as the dialectometrists have shown, other notions of clustering are equally scientific and may more accurately correspond to intuitive notions of what it means to be a dialect.\\n\\nData\\n\\nThe data for my study were taken from Wagner 1958. Wagner administered a questionnaire to native speakers of Irish Gaelic in 86 sites. Most of the informants were over seventy years old and had not spoken Irish since their youth. The atlas is therefore an approximate reconstruction of the linguistic landscape of the turn of the century, when the Gaeltacht was more continuous. Wagner also presents material from the Isle of Man and seven sites in Scotland. The mapped entries are presented in a very narrow phonetic transcription based on the International Phonetic Alphabet.\\n\\nVolume 1 of Wagner 1958 consists of 300 maps, plotting about 370 concepts. I used the first 51 concepts, or about 4500 different string tokens, as part of an ongoing project to enter all of the atlas into machine readable format. These 51 concepts were represented by 312 different Gaelic words or phrases, whose stems derived from 171 different etymons.\\n\\nMethodology and results\\n\\nDistance matrices\\n\\nTo form a baseline for comparison, I analysed the distribution of each of the 51 plotted concepts, finding a total of 3,337 features by which two or more sites differed. For example, for the concept `sell', I identified two sets, one using the word dol (most sites in Ireland), and one using the word creic (Rathlin Island, the Isle of Man, and Scotland). The dialects partitioned in a different way according to how much stress they placed on the verb relative to the pronoun in `I sold' (even stress in Dunlewy and four of the Scottish sites, else extra stress on the verb). Not all partitions covered the entire map. In this example, only sites that used the word dol were compared on the basis of whether a schwa developed in the sequence [i:l]. In some cases the divisions were more than two-way: for example, Wagner distinguishes whether the final consonant in creic is unpalatalized, palatalized, or slightly palatalized. Distance between sites was determined by counting 0 whenever two sites were in the same set and 1 whenever two sites were in contrasting sets, then taking the mean. This baseline approach corresponds formally to determining distance by the number of isoglosses that separate sites, which is in principle the traditional technique.\\n\\nThis baseline was compared to several other approaches. The etymon identity metric averaged the number of times the sites agreed in using words whose stem had the same ultimate derivation. For example, the dialects differed as to whether they used some form of bull- or damh- for the word `bullock'. Etymon identity is one of the more common approaches in dialectometry; Elsie for example used it in his study of the Gaelic dialects (1986). Closely related is the idea of word identity, where the words are not counted the same unless all of their morphemes agree. In this analysis, sites that used some form of the word bulln, with the suffix -n, were distinguished from those using the suffix -g.\\n\\nAnother set of approaches for computing distance was based on the phonetics. This computed the Levenshtein distance between phonetic strings. The Levenshtein distance is the cost of the least expensive set of insertions, deletions, or substitutions that would be needed to transform one string into the other (Sankoff and Kruskal, 1983). The simplest technique used was phone string comparison. In this approach, all operations cost 1 unit. Thus in comparing the forms [AL:i] and [aLi] for eallaigh `cattle', the (minimal) distance was 2, for the substitutions [a]/[A] and [L:]/[L]. (For this measure, diacritics such as the length mark `:' were counted as part of the letter, and different diacritics were adjudged to make for different letters.) A pair of unrelated words like [AL:i] and [khruh] (for crodh, another word for `cattle') would get a much larger score, 5.\\n\\nIn the above technique, very small phonetic differences, such as that between a moderately palatalized and a very palatalized [t], count the same as major differences, such as that between a [t] and an [e]. It would seem to be more accurate to assign a greater distance to substitutions involving greater phonetic distinctions. Unfortunately I know of no comprehensive study on the differences between phones, at least not for all 277 contrasts made by Wagner. Instead I distinguished them on the basis of twelve phonetic features that systematically account for all of the distinctions in Wagner's inventory: nasality, stricture, laterality, articulator, glottis, place, palatalization, rounding, length, height, strength, and syllabicity. The features were given discrete ordinal values scaled between 0 and 1, the exact values being arbitrary. For example, place took on the values glottal=0, uvular=0.1, postvelar=0.2, velar=0.3, prevelar=0.4, palatal=0.5, alveolar=0.7, dental=0.8, and labial=1. The distance between any two phones was judged to be the difference between the feature values, averaged across all twelve features. These distances were used instead of uniform 1-unit costs in computing Levenshtein distance. The resulting metric was called feature string comparison.\\n\\nIt could be argued that it is meaningless to compare the phonetics of different words, as in the case of eallaigh vs. crodh mentioned above. Therefore the feature string comparison was also computed only for pairs of citations that used the same word, so that [AL:i] vs. [aLi] would be compared, but [AL:i] vs. [khruh] would be ignored. The different approaches are called all-word vs. same-word feature string comparisons.\\n\\nKc =\\n\\nAverage(sign((Xij\\n\\n\\n\\nXik)(Yij\\n\\n\\n\\nYik))\\n\\nOf course, I do not expect that this technique using flat 1-unit costs will prove superior to all methods that are more sensitive to phonetic details. Feature comparison may work better if features were weighted differentially, or if the numeric values they assume were assigned less arbitrarily, or if the Manhattan-style distance computation were replaced by some formula that did not assume that the features are independent of each other. An ideal comparison would be based on data telling how likely it is for the one phone to turn into the other in the course of normal language change. In the method described here, [s] is adjudged closer to [g] than to [h]. But [s] often changes into [h] in the world's languages, and so the pair should have a small distance; whereas the change of [s] to [g] has never occurred to my knowledge, and so should have a very large distance. The unfortunate fact that such ideal data are lacking is compensated for by the fact that the inexpensive phone-string comparison employed in this study performs quite well.\\n\\nClustering techniques\\n\\nThe traditional agglomerative technique for clustering has been described above. There is some variation in how the distance between two clusters is measured. For this study I used the average distance between all pairs of elements that are in different clusters. I compared agglomeration to a top-down method that Kaufman and Rousseeuw (1990) call partitioning around medoids. This model reduces the O(2[N]) intractability of top-down approaches discussed above by dramatically reducing the number of binary partitions that are considered. Here one seeks to divide the sites into two groups by finding the two representative sites (the medoids) around which all the other sites cluster in such a way as to give the least average distance between the sites and their representatives. This is therefore a O(N[3]) algorithm, comparable in efficiency to agglomeration. The process was repeated recursively on each dialect.\\n\\nGaelic dialects\\n\\nSince agglomeration is the better clustering technique, the best dialect analysis should be obtained by agglomerating the isogloss matrix. The best automated approximation should be agglomerating the distance matrix computed by phonetic string comparison, and indeed the top-level topologies produced by both techniques are virtually identical. Both group into one loosely-connected entity all the sites in Scotland, and into another all the sites in Ireland. The isogloss approach groups Manx as a cousin of the Scottish dialects, and the phonetic approach makes it a cousin of the Irish dialects, but in both cases the s of Manx is very small (less than 0.06), making it essentially intermediate between the two groups. Thus the popular notion that Scottish, Irish and Manx Gaelic are distinct entities is well supported by the analyses. Both analyses group Rathlin Island very weakly with the rest of Irish, but the s for Rathlin is so low (less than 0.09) that its grouping too is essentially arbitrary. This aligns with the fact that authorities disagree as to whether it is a dialect of Irish (as does Wagner) or of Scottish (O'Rahilly 1932:191). Except for Rathlin Island, both methods group the Irish sites into one group containing all the sites in Ulster, and another, Southern, group, which itself breaks into a group containing all the sites in Connacht and one containing all the sites in Munster. Both methods agree on how the 87 sites are distributed among these dialects.\\n\\nThis three-way division accords with the universal perception that Ulster, Connacht and Munster form the three major dialect groups. The special status of Ulster contradicts the position of O'Rahilly (1932:18) that Connacht groups with Ulster to form a Northern dialect over against Munster. But it agrees with Elsie's finding (1986:255) that that province is lexicostatistically more remote from Connacht and Munster than those two are from each other. Furthermore, Hindley reports (1990:63) that speakers of other dialects often switch off radio broadcasts in Ulster Irish, `which is very distinctive'.\\n\\nThus the classification of the major Gaelic dialects gives the same general results by both distance metrics, if one discounts Manx and Rathlin Island Gaelic, which are flagged as indifferent in both schemes. It is encouraging that the resultant dialect areas are continuous, align with traditional provincial boundaries, and agree with commonly accepted taxonomies. However, dialect groupings at narrower levels, such as the exact subgrouping of the major provincial dialects, are at this point unstable. This is no doubt to be explained by the fairly small number of mapped concepts on which the distance metrics are based (51). As language differences get smaller, one expects that more data will be required in order to elucidate them.\\n\\nConclusions\\n\\nThis experiment shows that an automatic procedure can reliably group a language into its dialect areas, starting from nothing more than phonetic transcriptions as commonly found in linguistic surveys. Accurate distance matrices, corresponding highly to those obtained by the tedious uncovering of thousands of isoglosses, can be obtained by averaging the Levenshtein distance between phonetic strings, weighting equally all insertion, deletion, and substitution operations on the constituent phones. This turns out to be quite a bit more precise than the common technique of measuring distances by judging etymon identity, and requires even less editorial work. That phonetic comparison is more precise is not particularly surprising, since etymon identity ignores a wealth of phonetic, phonological, and morphological data, whereas comparing phones has the side effect of also counting higher-level variation: if words differ in morphemes, their phonetic difference is going to be high. As for clustering the sites into dialect areas, the familiar bottom-up agglomeration method proves superior to top-down partitioning around medoids.\\n\\nOf course simply knowing the dialect areas is not the last word in dialectology. There remain such essential problems as identifying the differing linguistic structures that characterize the dialects, and discovering their history. But all of these tasks will be greatly facilitated by a quick and accurate grouping of the dialects.\\n\\nBibliography\\n\\nBabitch, Rose Mary (1988). ``The areal structure of three syntagmatic variables in the terminology of Acadian fishermen.'' In Papers from the Tenth Annual Meeting of the Atlantic Provinces Linguistic Association, pages 121-137, University College of North Wales, August 1987. Multilingual Matters, Clevedon, Avon.\\n\\nBabitch, Rose Mary, and Lebrun, Eric (1989). ``Dialectometry as computerized agglomerative hierarchical classification analysis.'' Journal of English Linguistics, 22:83-90.\\n\\nDietz, E. Jacquelin (1983). ``Permutation tests for association between two distance matrices.'' Systematic Zoology, 32:21-26.\\n\\nDurand, J.-P. (1889). ``Notes de philologie rouergate, 18''. Revue des langues romanes, 33:47-84.\\n\\nEbobisse, Carl (1989). ``Dialectomtrie lexicale des parlers sawabantu.'' Journal of West African Languages, 19,2:57-66.\\n\\nElsie, Robert (1986). Dialect relationships in Goidelic: A study in Celtic dialectology. Helmut Buske, Hamburg.\\n\\nHindley, Reg (1990). The death of the Irish language. Routledge, London.\\n\\nKaufman, Leonard, and Rousseeuw, Peter J. (1990). Finding groups in data: An introduction to cluster analysis. John Wiley  Sons, New York.\\n\\nCuv, Brian (1951). Irish dialects and Irish speaking districts. Dublin Institute for Advanced Studies.\\n\\nO'Rahilly, Thomas Francis (1972). Irish dialects past and present. Dublin Institute for Advanced Studies.\\n\\nSiadhail, Mchel (1989). Modern Irish: Grammatical structure and dialectal variation. Cambridge University Press, Cambridge, Mass.\\n\\nPhilps, Dennis (1987). ``La relation entre distance linguistique et distance spatiale dans l'Atlas linguistique de la Gascogne.'' In Actes du Premier Congrs International de l'Association Internationale d'tudes Occitanes, 551-569. Westfield College, University of London.\\n\\nSankoff, David, and Kruskal, Joseph B., eds. (1983). Time warps, string edits, and macromolecules: The theory and practice of sequence comparison. Addison-Wesley, Reading, Mass.\\n\\nSguy, Jean (1971). ``La relation entre la distance spatiale et la distance lexicale.'' Revue de linguistique romane, 35:335-357.\\n\\nSguy, Jean (1973). ``La dialectomtrie dans l'Atlas linguistique de la Gascogne.'' Revue de linguistique romane, 37:1-24.\\n\\nWagner, Heinrich (1958). Linguistic atlas and survey of Irish dialects. Dublin Institute for Advanced Studies, 1958-1969. 4 vol.\\n\\nFootnotes\\n\\nI thank Martin Kay, Paul Kiparsky, Tom Wasow, and a reviewer for helpful comments. The overall algorithm is O(N[3]) since for each new group one must compute the distances between it and each of the other sites or groups. The atlas also maps for Kilkenny some information gathered from another source. That is, for each site i, one considers all other pairs of sites, j and k, and asks whether the linguistic difference between i and j is greater or less than that between i and k.  One counts 1 if the answer is the same for both distance matrices, -1 if it is different. Kc is the average of these numbers. The one site in Co. Cavan is intermediate between the Ulster and the Southern group. Wagner also gives two sites in Leinster. The more southern site, in Kilkenny, groups with the Southern group, and the more northern site, in Co. Louth, groups with Ulster, and indeed the county used to be considered part of that province. Sguy (1973) cites empirical research suggesting that general dialectometry requires about a hundred concepts.\", metadata={'source': '../data/raw/cmplg-xml/9503002.xml'}),\n",
       " Document(page_content=\"TAKTAG: Two-phase learning method for hybrid statistical/rule-based part-of-speech disambiguation\\n\\nBoth statistical and rule-based approaches to part-of-speech (POS) disambiguation have their own advantages and limitations. Especially for Korean, the narrow windows provided by hidden markov model (HMM) cannot cover the necessary lexical and long-distance dependencies for POS disambiguation. On the other hand, the rule-based approaches are not accurate and flexible to new tag-sets and languages. In this regard, the statistical/rule-based hybrid method that can take advantages of both approaches is called for the robust and flexible POS disambiguation. We present one of such method, that is, a two-phase learning architecture for the hybrid statistical/rule-based POS disambiguation, especially for Korean. In this method, the statistical learning of morphological tagging is error-corrected by the rule-based learning of Brill [1992] style tagger. We also design the hierarchical and flexible Korean tag-set to cope with the multiple tagging applications, each of which requires different tag-set. Our experiments show that the two-phase learning method can overcome the undesirable features of solely HMM-based or solely rule-based tagging, especially for morphologically complex Korean.\\n\\nIntroduction The system is designed to be very accurate in tagging especially the ambiguous Korean morphemes that have more than one part-of-speeches. The accuracy is very important in Korean tagging since Korean has much poorer tagging performance compared with English due to its linguistic characteristics. Although some of the POS ambiguities cannot be resolved at the morphology level, we tried to correct as much as tagging errors by introducing the rule-based error correction scheme.\\n\\nThe system is flexible so that it can tune to the new tag-sets and new languages. In other words, the system doesn't rely on the enormous amounts of pre-existing tagged corpus for its training. This is very important since the Korean tag sets are not stabilized yet, nor are the standard Korean tagged corpus provided yet.\\n\\nIn TAKTAG, the tag-sets are hierarchically organized so that they can be adjustable according to the given applications such as information retrieval, speech synthesis, text data extraction, and so on.\\n\\nHierarchical tag\\n\\n\\n\\nsets for Korean morphology\\n\\nKorean is classified as agglutinative languages in which the words (which is called Eojeol in Korean) consist of several morphemes that have clear-cut morpheme boundaries. Below are the characteristic of Korean that must be considered for POS tag-set and tagging system design. 1. Korean is a postpositional language with many kinds of noun-endings, verb-endings, and prefinal verb-endings. It is the functional morphemes, not Eojeol's order that determine the most of the grammatical relations such as the noun's case roles, verb's tenses, modals, and modification relations between Eojeols. So contextual information for POS disambiguation must be selectively applied to the functional (bound) morphemes or content (free) morphemes. 2. Sometimes a Korean Eojoel corresponds to an English phrase, not to a single word, so the tagging must be done on morpheme basis, not Eojeol basis. The morphological analyzer must precede the tagging system because the morpheme segmentation is much more important and difficult than POS assignment in Korean. 3. Korean is basically SOV languages but has relatively free word order compared to English, except for the constraints that the verb must appear in a sentence-final position. However, in Korean, some word-order constraints do exist such that the modifiers must be placed before the word (called head) they modify. So some order constraints must be applied as contextual information, but some must not. 4.\\n\\nComplex spelling changes can occur between morphemes when two morphemes combine to form an Eojeol. These spelling changes make it difficult to segment the morphemes before assigning the POS tags. Also a lot of allomorphs are generated from the spelling changes. For the above reasons, a morphological analysis play important roles in Korean POS tagging system. It is the morphological analysis process which initially segments the morphemes out of the Eojoels, reconstructs the spelling changes, and assigns the initial POS tags to each morpheme by consulting the dictionary. Later, the tagging system disambiguates the POS assignments by selecting the single morpheme sequence for each sentence and the single POS tag for each morpheme by consulting the lexical and contextual information acquired from the corpus.\\n\\nTwo-phase learning of POS disambiguation\\n\\nLearning HMM\\n\\n\\n\\nbased disambiguation\\n\\nLearing rule\\n\\n\\n\\nbased tagging correction\\n\\nExperiments\\n\\nConclusions\\n\\nWe presented a new POS tagging architecture which integrates the statistical approach with the rule learning approach in a synergistic way. Our hybrid tagging architecture is proved to be useful, especially for the morphologically complex agglutinative languages such as Korean. The system TAKTAG can provide the following two unique properties for desirable Korean tagging: 1) The system can provide accurate results even with the morpheme tagging which usually results in very poor performance, and 2) The system can be flexibly tuned to the new tag-sets without massive retraining. The performance of the two-phase learning for tagging is determined how well the error-corrector can compensate the deficiencies of the statistical tagging, and in that sense, our TAKTAG is much successful since it increased the overall tagging results more than 10%. The next step will be to analyze the learned rules carefully to extract the more desirable rule schema for Korean. The robust unknown word handling scheme with more efficient morphological analyzers also should be studied with the well-engineered HMM taggers that fully consider the linguistic characteristics of Korean.\\n\\nAcknowledgments\\n\\nThis research was partially supported by POSCO (Pohang iron and steel company). We thank to NamHee Hong and Wonil Lee for re-classification of Korean part-of-speech and re-implementation of Korean morphological analyzer. The corpus was selected from the one provided by ETRI (Electronic and Telecommunication Research Institute), Korea.\\n\\nFootnotes\\n\\nThe 300000 Eojeol (about 700000 morpheme) untagged corpus was provided from the ETRI (Electronics and Telecommunication Research Institute) in Korea. We selected 70000 morpheme sentences among them with careful consideration to the corpus balance.\", metadata={'source': '../data/raw/cmplg-xml/9504023.xml'}),\n",
       " Document(page_content=\"An Efficient Generation Algorithm for Lexicalist MT\\n\\nThe lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions. However, the Shake-and-Bake generation algorithm of [] is NP-complete. We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism.\\n\\nIntroduction\\n\\nShake-and-Bake translation assumes a source grammar, a target grammar and a bilingual dictionary which relates translationally equivalent sets of lexical signs, carrying across the semantic dependencies established by the source language analysis stage into the target language generation stage.\\n\\nThe translation process consists of three phases: 1. A parsing phase, which outputs a multiset, or bag, of source language signs instantiated with sufficiently rich linguistic information established by the parse to ensure adequate translations. 2. A lexical-semantic transfer phase which employs the bilingual dictionary to map the bag of instantiated source signs onto a bag of target language signs. 3. A  generation phase which imposes an order on the bag of target signs which is guaranteed grammatical according to the monolingual target grammar. This ordering must respect the linguistic constraints which have been transferred into the target signs.\\n\\nThe Shake-and-Bake generation algorithm of [] combines target language signs using the technique known as generate-and-test. In effect, an arbitrary permutation of signs is input to a shift-reduce parser which tests them for grammatical well-formedness. If they are well-formed, the system halts indicating success. If not, another permutation is tried and the process repeated. The complexity of this algorithm is O(n!) because all permutations (n! for an input of size n) may have to be explored to find the correct answer, and indeed must be explored in order to verify that there is no answer.\\n\\nOur novel generation algorithm has polynomial complexity (O(n[4])). The reduction in theoretical complexity is achieved by placing constraints on the power of the target grammar when operating on instantiated signs, and by using a more restrictive data structure than a bag, which we call a target language normalised commutative bracketing (TNCB). A TNCB records dominance information from derivations and is amenable to incremental updates. This allows us to employ a greedy algorithm to refine the structure progressively until either a target constituent is found and generation has succeeded or no more changes can be made and generation has failed.\\n\\nIn the following sections, we will sketch the basic algorithm, consider how to provide it with an initial guess, and provide an informal proof of its efficiency.\\n\\nA Greedy Incremental Generation Algorithm\\n\\nWe begin by describing the fundamentals of a greedy incremental generation algorithm. The crucial data structure that it employs is the TNCB. We give some definitions, state some key assumptions about suitable TNCBs for generation, and then describe the algorithm itself.\\n\\nTNCBs\\n\\nWe assume a sign-based grammar with binary rules, each of which may be used to combine two signs by unifying them with the daughter categories and returning the mother. Combination is the commutative equivalent of rule application; the linear ordering of the daughters that leads to successful rule application determines the orthography of the mother.\\n\\nIf the linguistic data that two signs contain allows them to combine, it is because they are providing a semantics which might later become more specified. For example, consider the bag of signs that have been derived through the Shake-and-Bake process which represent the phrase: (1) The big brown dog Now, since the determiner and adjectives all modify the same noun, most grammars will allow us to construct the phrases: (2) The dog (3) The big dog (4) The brown dog as well as the `correct' one. Generation will fail if all signs in the bag are not eventually incorporated in the final result, but in the nave algorithm, the intervening computation may be intractable.\\n\\nIn the algorithm presented here, we start from observation that the phrases (2) to (4) are not incorrect semantically; they are simply under-specifications of (1). We take advantage of this by recording the constituents that have combined within the TNCB, which is designed to allow further constituents to be incorporated with minimal recomputation.\\n\\nA TNCB is composed of a sign, and a history of how it was derived from its children. The structure is essentially a binary derivation tree whose children are unordered. Concretely, it is either NIL, or a triple:\\n\\nThe second and third items of the TNCB triple are the child TNCBs. The value of a TNCB is the sign that is formed from the combination of its children, or INCONSISTENT, representing the fact that they cannot grammatically combine, or UNDETERMINED,  i.e. it has not yet been established whether the signs combine.\\n\\nLet us introduce some terminology.\\n\\nwell-formed iff its value is a sign,\\n\\nill-formed iff its value is INCONSISTENT,\\n\\nundetermined (and its value is UNDETERMINED) iff it has not been demonstrated whether it is well-formed or ill-formed.\\n\\nmaximal iff it is well-formed and its parent (if it has one) is ill-formed. In other words, a maximal TNCB is a largest well-formed component of a TNCB. Since TNCBs are tree-like structures, if a TNCB is undetermined or ill-formed then so are all of its ancestors (the TNCBs that contain it).\\n\\nWe define five operations on a TNCB. The first three are used to define the fourth transformation (move) which improves ill-formed TNCBs. The fifth is used to establish the well-formedness of undetermined nodes. In the diagrams, we use a cross to represent ill-formed nodes and a black circle to represent undetermined ones.\\n\\nSuitable Grammars\\n\\nThe Shake-and-Bake system of [] employs a bag generation algorithm because it is assumed that the input to the generator is no more than a collection of instantiated signs. Full-scale bag generation is not necessary because sufficient information can be transferred from the source language to severely constrain the subsequent search during generation.\\n\\nThe two properties required of TNCBs (and hence the target grammars with instantiated lexical signs) are:\\n\\n1. Precedence Monotonicity. The order of the orthographies of two combining signs in the orthography of the result must be determinate -- it must not depend on any subsequent combination that the result may undergo. This constraint says that if one constituent fails to combine with another, no permutation of the elements making up either would render the combination possible. This allows bottom-up evaluation to occur in linear time. In practice, this restriction requires that sufficiently rich information be transferred from the previous translation stages to ensure that sign combination is deterministic.\\n\\n2. Dominance Monotonicity. If a maximal TNCB is adjoined at the highest possible place inside another TNCB, the result will be well-formed after it is re-evaluated. Adjunction is only attempted if conjunction fails (in fact conjunction is merely a special case of adjunction in which no nodes are disrupted); an adjunction which disrupts i nodes is attempted before one which disrupts i+1nodes. Dominance monotonicity merely requires all nodes that are disrupted under this top-down control regime to be well-formed when re-evaluated. We will see that this will ensure the termination of the generation algorithm within n-1 steps, where n is the number of lexical signs input to the process.\\n\\nWe are currently investigating the mathematical characterisation of grammars and instantiated signs that obey these constraints. So far, we have not found these restrictions particularly problematic.\\n\\nThe Generation Algorithm\\n\\nIf we were continuing in the spirit of the original Shake-and-Bake generation process, we would now form some arbitrary mutation of the TNCB and retest, repeating this test-rewrite cycle until we either found a well-formed TNCB or failed. However, this would also be intractable due to the undirectedness of the search through the vast number of possibilities. Given the added derivation information contained within TNCBs and the properties mentioned above, we can direct this search by incrementally improving on previously evaluated results.\\n\\nWe enter the rewrite phase, then, with an ill-formed TNCB. Each move operation must improve it. Let us see why this is so.\\n\\nThe ancestors of the new well-formed node will be at least as well-formed as they were prior to the movement. We can verify this by case:\\n\\n1. When two maximal TNCBs are conjoined, nodes dominating the new node, which were previously ill-formed, become undetermined. When re-evaluated, they may remain ill-formed or some may now become well-formed.\\n\\n2. When we adjoin a maximal TNCB within another TNCB, nodes dominating the new well-formed node are disrupted. By dominance monotonicity, all nodes which were disrupted by the adjunction must become well-formed after re-evaluation. And nodes dominating the maximal disrupted node, which were previously ill-formed, may become well-formed after re-evaluation.\\n\\nWe thus see that rewriting and re-evaluating must improve the TNCB.\\n\\nAfter finding that big may not be conjoined with the brown dog, we try to adjoin it within the latter. Since it will combine with brown dog, no adjunction to a lower TNCB is attempted.\\n\\nWe thus see that during generation, we formed a basic constituent, the dog, and incrementally refined it by adjoining the modifiers in place. At the heart of this approach is that, once well-formed, constituents can only grow; they can never be dismantled.\\n\\nEven if generation ultimately fails, maximal well-formed fragments will have been built; the latter may be presented to the user, allowing graceful degradation of output quality.\\n\\nInitialising the Generator\\n\\nConsidering the algorithm described above, we note that the number of rewrites necessary to repair the initial guess is no more than the number of ill-formed TNCBs. This can never exceed the number of interior nodes of the TNCB formed from n lexical signs (i.e. n-2). Consequently, the better formed the initial TNCB used by the generator, the fewer the number of rewrites required to complete generation. In the last section, we deliberately illustrated an initial guess which was as bad as possible. In this section, we consider a heuristic for producing a motivated guess for the initial TNCB.\\n\\nIf we mirror the Japanese bracketing structure in English to form the initial TNCB, we obtain: ((book the) (red is)). This will produce the correct answer in the test phase of generation without the need to rewrite at all.\\n\\nEven if there is not an exact isomorphism between the source and target commutative bracketings, the first guess is still reasonable as long as the majority of child commutative bracketings in the target language are isomorphic with their equivalents in the source language. Consider the French sentence:\\n\\nThe Complexity of the Generator\\n\\nStructural transfer operations only affect the efficiency and not the functionality of generation. Transfer specifications may be incrementally refined and empirically tested for efficiency. Since complete specification of transfer operations is not required for correct generation of grammatical target text, the version of Shake-and-Bake translation presented here maintains its advantage over traditional transfer models, in this respect.\\n\\nThe monotonicity constraints, on the other hand, might constitute a dilution of the Shake-and-Bake ideal of independent grammars. For instance, precedence monotonicity requires that the status of a clause (strictly, its lexical head) as main or subordinate has to be transferred into German. It is not that the transfer of information per se compromises the ideal -- such information must often appear in transfer entries to avoid grammatical but incorrect translation (e.g. a great man translated as un homme grand). The problem is justifying the main/subordinate distinction in every language that we might wish to translate into German. This distinction can be justified monolingually for the other languages that we treat (English, French, and Japanese). Whether the constraints will ultimately require monolingual grammars to be enriched with entirely unmotivated features will only become clear as translation coverage is extended and new language pairs are added.\\n\\nConclusion\\n\\nBibliography\\n\\nV. Allegranza, P. Bennett, J. Durand, F. van Eynde, L. Humphreys, P. Schmidt, and E. Steiner. 1991. Linguistics for Machine Translation: The Eurotra Linguistic Specifications. In C. Copeland, J. Durand, S. Krauwer, and B. Maegaard, editors,   The Eurotra Formal Specifications. Studies in Machine Translation and Natural Language Processing 2, pages 15-124. Office for Official Publications of the European Communities.\\n\\nD. Arnold, S. Krauwer, L. des Tombe, and L. Sadler. 1988. `Relaxed' Compositionality in Machine Translation. In Second International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages, Carnegie Mellon Univ, Pittsburgh.\\n\\nJohn L. Beaven. 1992a. Lexicalist Unification-based Machine Translation. Ph.D. thesis, University of Edinburgh, Edinburgh.\\n\\nJohn L. Beaven. 1992b. Shake-and-Bake Machine Translation. In Proceedings of COLING 92, pages 602-609, Nantes, France.\\n\\nChris Brew. 1992. Letting the Cat out of the Bag: Generation for Shake-and-Bake MT. In Proceedings of COLING 92, pages 29-34, Nantes, France.\\n\\nPeter F. Brown, John Cocke, A Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A Statistical Approach to Machine Translation. Computational Linguistics, 16(2):79-85, June.\\n\\nHsin-Hsi Chen and Yue-Shi Lee. 1994. A Corrective Training Algorithm for Adaptive Learning in Bag Generation. In International Conference on New Methods in Language Processing (NeMLaP), pages 248-254, Manchester, UK. UMIST.\\n\\nBonnie Jean Dorr. 1993. Machine Translation: A View from the Lexicon. Artificial Intelligence Series. The MIT Press, Cambridge, Mass.\\n\\nSergei Nirenburg, Jaime Carbonell, Masaru Tomita, and Kenneth Goodman. 1992. Machine Translation: A Knowledge-Based Approach. Morgan Kaaufmann, San Mateo, CA.\\n\\nFred Popowich. 1994. Improving the Efficiency of a Generation Algorithm for Shake and Bake Machine Translation using Head-Driven Phrase Structure Grammar. Technical Report CMPT-TR 94-07, School of Computing Science, Simon Fraser University, Burnaby, British Columbia, CANADA V5A 1S6.\\n\\nV. Poznanski, John L. Beaven, and P. Whitelock. 1993. The Design of SLEMaT Mk II. Technical Report IT-1993-19, Sharp Laboratories of Europe, LTD, Edmund Halley Road, Oxford Science Park, Oxford OX4 4GA, July.\\n\\nP. Whitelock. 1992. Shake and Bake Translation. In Proceedings of COLING 92, pages 610-616, Nantes, France.\\n\\nP. Whitelock. 1994. Shake-and-Bake Translation. In C. J. Rupp, M. A. Rosner, and R. L. Johnson, editors,   Constraints, Language and Computation, pages 339-359. Academic Press, London.\\n\\nFootnotes\\n\\nWe wish to thank our colleagues Kerima Benkerimi, David Elworthy, Peter Gibbins, Ian Johnson, Andrew Kay and Antonio Sanfilippo at SLE, and our anonymous reviewers for useful feedback and discussions on the research reported here and on earlier drafts of this paper.\", metadata={'source': '../data/raw/cmplg-xml/9504027.xml'}),\n",
       " Document(page_content=\"Corpus Statistics Meet the Noun Compound: Some Empirical Results\\n\\nA variety of statistical methods for noun compound analysis are implemented and compared. The results support two main conclusions. First, the use of conceptual association not only enables a broad coverage, but also improves the accuracy. Second, an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents, even though the latter is more prevalent in the literature.\\n\\nBackground\\n\\nCompound Nouns\\n\\nIf parsing is taken to be the first step in taming the natural language understanding task, then broad coverage NLP remains a jungle inhabited by wild beasts. For instance, parsing noun compounds appears to require detailed world knowledge that is unavailable outside a limited domain (Sparck Jones, 1983). Yet, far from being an obscure, endangered species, the noun compound is flourishing in modern language. It has already made five appearances in this paragraph and at least one diachronic study shows a veritable population explosion (Leonard, 1984). While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984), techniques suitable for broad coverage parsing remain unavailable. This paper explores the application of corpus statistics (Charniak, 1993) to noun compound parsing (other computational problems are addressed in Arens et al, 1987; Vanderwende, 1993 and Sproat, 1994).\\n\\nEach of two analysis models are applied: adjacency and dependency.\\n\\nEach of a range of training schemes are employed.\\n\\nResults are computed with and without tuning factors suggested in the literature.\\n\\nEach of two parameterisations are used: associations between words and associations between concepts.\\n\\nResults are collected with and without machine tagging of the corpus.\\n\\nTraining Schemes\\n\\nWhile Hindle and Rooth (1993) use a partial parser to acquire training data, such machinery appears unnecessary for noun compounds. Brent (1993) has proposed the use of simple word patterns for the acquisition of verb subcategorisation information. An analogous approach to compounds is used in Lauer (1994) and constitutes one scheme evaluated below. While such patterns produce false training examples, the resulting noise often only introduces minor distortions.\\n\\nA more liberal alternative is the use of a co-occurrence window. Yarowsky (1992) uses a fixed 100 word window to collect information used for sense disambiguation. Similarly, Smadja (1993) uses a six content word window to extract significant collocations. A range of windowed training schemes are employed below. Importantly, the use of a window provides a natural means of trading off the amount of data against its quality. When data sparseness undermines the system accuracy, a wider window may admit a sufficient volume of extra accurate data to outweigh the additional noise.\\n\\nNoun Compound Analysis\\n\\nThere are at least four existing corpus-based algorithms proposed for syntactically analysing noun compounds. Only two of these have been subjected to evaluation, and in each case, no comparison to any of the other three was performed. In fact all authors appear unaware of the other three proposals. I will therefore briefly describe these algorithms.\\n\\nThree of the algorithms use what I will call the ADJACENCY MODEL, an analysis procedure that goes back to Marcus (1980, p253). Therein, the procedure is stated in terms of calls to an oracle which can determine if a noun compound is acceptable. It is reproduced here for reference:\\n\\nIf either [n1 n2] or [n2 n3] is not semantically acceptable then build the alternative structure;\\n\\notherwise, if [n2 n3] is semantically preferable to [n1 n2] then build [n2 n3];\\n\\notherwise, build [n1 n2].\\n\\nOnly more recently has it been suggested that corpus statistics might provide the oracle, and this idea is the basis of the three algorithms which use the adjacency model. The simplest of these is reported in Pustejovsky et al  (1993). Given a three word compound, a search is conducted elsewhere in the corpus for each of the two possible subcomponents. Whichever is found is then chosen as the more closely bracketed pair. For example, when backup compiler disk is encountered, the analysis will be:\\n\\nThe proposal of Liberman and Sproat (1992) is more sophisticated and allows for the frequency of the words in the compound. Their proposal involves comparing the mutual information between the two pairs of adjacent words and bracketing together whichever pair exhibits the highest. Again, there is no evaluation of the method other than a demonstration that four examples work correctly.\\n\\nThe third proposal based on the adjacency model appears in Resnik (1993) and is rather more complex again. The  SELECTIONAL ASSOCIATION between a predicate and a word is defined based on the contribution of the word to the conditional entropy of the predicate. The association between each pair of words in the compound is then computed by taking the maximum selectional association from all possible ways of regarding the pair as predicate and argument. Whilst this association metric is complicated, the decision procedure still follows the outline devised by Marcus (1980) above. Resnik (1993) used unambiguous noun compounds from the parsed Wall Street Journal (WSJ) corpus to estimate the association values and analysed a test set of around 160 compounds. After some tuning, the accuracy was about 73%, as compared with a baseline of 64% achieved by always bracketing the first two nouns together.\\n\\nDetermine how acceptable the structures [n1 n2] and [n1 n3] are;\\n\\nif the latter is more acceptable, build [n2 n3] first;\\n\\notherwise, build [n1 n2] first.\\n\\nThe dependency model attempts to choose a parse which makes the resulting relationships as acceptable as possible. For example, when backup compiler disk is encountered, the analysis will be:\\n\\nAnother significant difference between the models is the predictions they make about the proportion of left and right-branching compounds. Lauer and Dras (1994) show that under a dependency model, left-branching compounds should occur twice as often as right-branching compounds (that is two-thirds of the time). In the test set used here and in that of Resnik (1993), the proportion of left-branching compounds is 67% and 64% respectively. In contrast, the adjacency model appears to predict a proportion of 50%.\\n\\nThe dependency model has also been proposed by Kobayasi et al (1994) for analysing Japanese noun compounds, apparently independently. Using a corpus to acquire associations, they bracket sequences of Kanji with lengths four to six (roughly equivalent to two or three words). A simple calculation shows that using their own preprocessing hueristics to guess a bracketing provides a higher accuracy on their test set than their statistical model does. This renders their experiment inconclusive.\\n\\nMethod\\n\\nExtracting a Test Set\\n\\nAccuracy figures in all the results reported below were computed using only those 244 compounds which received a parse.\\n\\nConceptual Association\\n\\nOne problem with applying lexical association to noun compounds is the enormous number of parameters required, one for every possible pair of nouns. Not only does this require a vast amount of memory space, it creates a severe data sparseness problem since we require at least some data about each parameter. Resnik and Hearst (1993) coined the term  CONCEPTUAL ASSOCIATION to refer to association values computed between groups of words. By assuming that all words within a group behave similarly, the parameter space can be built in terms of the groups rather than in terms of the words.\\n\\nTraining\\n\\nUsing each of these different training schemes to arrive at appropriate counts it is then possible to estimate the parameters. Since these are expressed in terms of categories rather than words, it is necessary to combine the counts of words to arrive at estimates. In all cases the estimates used are:\\n\\nAnalysing the Test Set\\n\\nFor the adjacency model, when the given compound is\\n\\nw1 w2 w3, we can estimate this ratio as:\\n\\nFor the dependency model, the ratio is:\\n\\nResults Dependency meets Adjacency windowed training schemes with window widths of 2, 3, 4, 5, 10, 50 and 100 words.\\n\\nIn the case of the pattern training scheme, the difference between 68.9% for adjacency and 77.5% for dependency is statistically significant at the 5% level (\\n\\np = 0.0316), demonstrating the superiority of the dependency model, at least for the compounds within Grolier's encyclopedia.\\n\\nIn no case do any of the windowed training schemes outperform the pattern scheme. It seems that additional instances admitted by the windowed schemes are too noisy to make an improvement.\\n\\nInitial results from applying these methods to the EMA corpus have been obtained by Wilco ter Stal (1995), and support the conclusion that the dependency model is superior to the adjacency model.\\n\\nTuning\\n\\na factor favouring left-branching which arises from the formal dependency construction; and\\n\\nfactors allowing for naive estimates of the variation in the probability of categories.\\n\\nLexical Association\\n\\nUsing a Tagger\\n\\nConclusion\\n\\nThe experiments above demonstrate a number of important points. The most general of these is that even quite crude corpus statistics can provide information about the syntax of compound nouns. At the very least, this information can be applied in broad coverage parsing to assist in the control of search. I have also shown that with a corpus of moderate size it is possible to get reasonable results without using a tagger or parser by employing a customised training pattern. While using windowed co-occurrence did not help here, it is possible that under more data sparse conditions better performance could be achieved by this method.\\n\\nThe significance of the use of conceptual association deserves some mention. I have argued that without it a broad coverage system would be impossible. This is in contrast to previous work on conceptual association where it resulted in little improvement on a task which could already be performed. In this study, not only has the technique proved its worth by supporting generality, but through generalisation of training information it outperforms the equivalent lexical association approach given the same information.\\n\\nIn all, the most accurate technique achieved an accuracy of 81% as compared to the 67% achieved by guessing left-branching. Given the high frequency of occurrence of noun compounds in many texts, this suggests that the use of these techniques in probabilistic parsers will result in higher performance in broad coverage natural language processing.\\n\\nAcknowledgements\\n\\nThis work has received valuable input from people too numerous to mention. The most significant contributions have been made by Richard Buckland, Robert Dale and Mark Dras. I am also indebted to Vance Gledhill, Mike Johnson, Philip Resnik, Richard Sproat, Wilco ter Stal, Lucy Vanderwende and Wayne Wobcke. Financial support is gratefully acknowledged from the Microsoft Institute and the Australian Government.\\n\\nBibliography\\n\\n1 Arens, Y., Granacki, J. and Parker, A. 1987. Phrasal Analysis of Long Noun Sequences. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, Stanford, CA. pp59-64.\\n\\n2 Brent, Michael. 1993. From Grammar to Lexicon: Unsupervised Learning of Lexical Syntax. In Computational Linguistics, Vol 19(2), Special Issue on Using Large Corpora II, pp243-62.\\n\\n3 Brill, Eric. 1993. A Corpus-based Approach to Language Learning. PhD Thesis, University of Pennsylvania, Philadelphia, PA..\\n\\n4\\n\\nCharniak, Eugene.\\n\\n1993.\\n\\nStatistical Language Learning.\\n\\nMIT Press, Cambridge, MA.\\n\\n5 Finin, Tim. 1980. The Semantic Interpretation of Compound Nominals. PhD Thesis, Co-ordinated Science Laboratory, University of Illinois, Urbana, IL.\\n\\n6 Hindle, D. and Rooth, M. 1993. Structural Ambiguity and Lexical Relations. In Computational Linguistics Vol. 19(1), Special Issue on Using Large Corpora I, pp103-20.\\n\\n7 Isabelle, Pierre. 1984. Another Look At Nominal Compounds. In Proceedings of COLING-84, Stanford, CA. pp509-16.\\n\\n8 Karp, D., Schabes, Y., Zaidel, M. and Egedi, D. 1992. A Freely Available Wide Coverage Morphological Analyzer for English. In Proceedings of COLING-92, Nantes, France, pp950-4.\\n\\n9 Kobayasi, Y., Tokunaga, T. and Tanaka, H. 1994. Analysis of Japanese Compound Nouns using Collocational Information. In Proceedings of COLING-94, Kyoto, Japan, pp865-9.\\n\\n10 Lauer, Mark. 1994. Conceptual Association for Compound Noun Analysis. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, Student Session, Las Cruces, NM. pp337-9.\\n\\n11 Lauer, M. and Dras, M. 1994. A Probabilistic Model of Compound Nouns. In Proceedings of the 7th Australian Joint Conference on Artificial Intelligence, Armidale, NSW, Australia. World Scientific Press, pp474-81.\\n\\n12 Leonard, Rosemary. 1984. The Interpretation of English Noun Sequences on the Computer. North-Holland, Amsterdam.\\n\\n13 Levi, Judith. 1978. The Syntax and Semantics of Complex Nominals. Academic Press, New York.\\n\\n14 Liberman, M. and Sproat, R. 1992. The Stress and Structure of Modified Noun Phrases in English. In Sag, I. and Szabolcsi, A., editors, Lexical Matters CSLI Lecture Notes No. 24. University of Chicago Press, pp131-81.\\n\\n15 Marcus, Mitchell. 1980. A Theory of Syntactic Recognition for Natural Language. MIT Press, Cambridge, MA.\\n\\n16\\n\\nMcDonald, David B.\\n\\n1982.\\n\\nUnderstanding Noun Compounds.\\n\\nPhD Thesis, Carnegie\\n\\n\\n\\nMellon University, Pittsburgh, PA.\\n\\n17 Pustejovsky, J., Bergler, S. and Anick, P. 1993. Lexical Semantic Techniques for Corpus Analysis. In Computational Linguistics Vol 19(2), Special Issue on Using Large Corpora II, pp331-58.\\n\\n18 Resnik, Philip. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. PhD dissertation, University of Pennsylvania, Philadelphia, PA.\\n\\n19 Resnik, P. and Hearst, M. 1993. Structural Ambiguity and Conceptual Relations. In Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, June 22, Ohio State University, pp58-64.\\n\\n20 Ryder, Mary Ellen. 1994. Ordered Chaos: The Interpretation of English Noun-Noun Compounds. University of California Press Publications in Linguistics, Vol 123.\\n\\n21 Smadja, Frank. 1993. Retrieving Collocations from Text: Xtract. In Computational Linguistics, Vol 19(1), Special Issue on Using Large Corpora I, pp143-177.\\n\\n22 Sparck Jones, Karen. 1983. Compound Noun Interpretation Problems. In Fallside, F. and Woods, W.A., editors, Computer Speech Processing. Prentice-Hall, NJ. pp363-81.\\n\\n23 Sproat, Richard. 1994. English noun-phrase accent prediction for text-to-speech. In Computer Speech and Language, Vol 8, pp79-94.\\n\\n24 Vanderwende, Lucy. 1993. SENS: The System for Evaluating Noun Sequences. In Jensen, K., Heidorn, G. and Richardson, S., editors, Natural Language Processing: The PLNLP Approach. Kluwer Academic, pp161-73.\\n\\n25 ter Stal, Wilco. 1995. Syntactic Disambiguation of Nominal Compounds Using Lexical and Conceptual Association. Memorandum UT-KBS-95-002, University of Twente, Enschede, Netherlands.\\n\\n26 Yarowsky, David. 1992. Word-Sense Disambiguation Using Statistical Models of Roget's Categories Trained on Large Corpora. In Proceedings of COLING-92, Nantes, France, pp454-60.\\n\\nFootnotes\\n\\nLauer and Dras (1994) give a formal construction motivating the algorithm given in Lauer (1994). We would like to thank Grolier's for permission to use this material for research purposes. The 1911 version of Roget's used is available on-line and is in the public domain. It contains 1043 categories. If either probability estimate is zero, the other analysis is chosen. If both are zero the analysis is made as if the ratio were exactly unity.\", metadata={'source': '../data/raw/cmplg-xml/9504033.xml'}),\n",
       " Document(page_content=\"Empirical Discovery in Linguistics\\n\\nA discovery system for detecting correspondences in data is described, based on the familiar induction methods of J. S. Mill. Given a set of observations, the system induces the ``causally'' related facts in these observations. Its application to empirical linguistic discovery is described. The paper is organized as follows. I begin the discussion by revealing two developments, the transformationalists' critique of ``discovery procedures'' and naive inductivism, which have led to the neglect of discovery issues, arguing that more attention needs to be paid to discovery in linguistics. Then, Mill's methods are introduced, and the system, incorporating them, is described, using as one illustration the discovery of (a part of) the famous Germanic Consonant Shift, known as Grimm's Law.\\n\\nIntroduction\\n\\nScientific discovery was one of the favourite topics of Renaissance scholars (F.  Bacon, Descartes, Leibnitz). These early efforts suffered a long period of oblivion (basically, due to the critiques of Whewell and Hume), but this century has witnessed a steady, though perhaps not a completely uninterrupted,  process of revival of interest. Very significant contributions to a general understanding of the discovery process have come from various scientific disciplines, incl. mathematics (Poincar, Hadamard, Polya), psychology (Wertheimer, Duncker), philosophy (Nickles 1980b, Nickles 1980c), and AI (Newell and Simon 1972, Langley, Simon, Bradshaw and Zytkow 1987); cf. also the collection of more recent advances (Shrager and Langley 1990). In effect, in many disciplines to date discovery is considered quite a respectable object of investigation.\\n\\nContemporary linguistics, unfortunately, did not follow the general tendency in the other sciences. In the next section, I briefly discuss two major reasons for this state of affairs, arguing that more attention needs to be paid to discovery in linguistics. Then, J. S. Mill's methods for induction are introduced, and a system incorporating these methods is described, using as  illustrations a simple deciphering problem and the discovery of (a part of) the famous Germanic Consonant Shift, known as Grimm's Law.\\n\\nThe Problem of Discovery  in Linguistics The Transformationalists' Critique of Discovery Procedures\\n\\nThe study of discovery in linguistics is not fashionable today. Discovery has had its good days, reaching its climax in the works of American descriptivists, and esp. Zellig Harris (Harris 1951). The heritage from descriptivists, however, is by no means uncontroversial. It is indeed true that their ``discovery procedures'' (of a segmentation-and-classification type), purporting to uncover the grammar of a language from a corpus of that language, significantly contributed to the understanding of the process, and served as a basis for later grammar learning systems and toolkits for linguistic fieldwork. However, the descriptivists' reduction of linguistic theory to a manual of procedures, and doing linguistics to following these procedures proved to be a too extreme and simplistic view to act as an incentive for the further study of discovery issues by later generations of linguists.\\n\\nThe attempts of descriptivists, even if somewhat one-sided, suffered a severe blow with the advent of transformationalists. In his influential book Syntactic Structures Chomsky made a devastating criticism of descriptivists' discovery procedures, totally shifting the focus on grammar justification. He stated of linguistic theory that its ``fundamental concern...is the problem of justification of grammars'' (Chomsky 1957: 49; italics mine); and ``we shall never consider how one might have arrived at the grammar'', whether this be ``by intuition, guess-work, all sorts of partial methodological hints, reliance on past experience, etc.'' (op. cit. p.  56).\\n\\nChomsky expressed his doubt as to the attainability in principle of the discovery task by descriptivist techniques:\\n\\nI think that it is very questionable that this goal is attanable in any interesting way, and I suspect that any attempt to meet it will lead into a maze of more and more elaborate and complex analytic procedures that will fail to provide answers for many important questions about the nature of linguistic structure. (pp. 52-53). ...it is questionable whether /procedures of analysis/ can be formulated rigorously, exhaustively and simply enough to qualify as practical and mechanical discovery procedure. (p. 56; italics mine).\\n\\nHe also objected to some concrete attempts in this direction, arguing that despite their proclaimed goal, they are not in fact discovery, but rather ``evaluation procedures'', helpful for choosing from among alternative grammars, already discovered in some way or another (p. 52, fn.3).\\n\\nChomsky's disciples followed suit. Another influential transformationalist, Dougherty, in a review article on linguistic methodology simply repeated Chomsky's words:\\n\\nI have nothing to say about the creative process by which a linguist develops a new grammar, I am only concerned with the method of selecting the superior grammar from a given field of proposed grammars. (Dougherty 1973: 435).\\n\\nand Teeter 1964 comments on the question in an article with the indicative title Descriptive linguistic in America:  triviality vs. irrelevance, to mention but a few of the published reactions.\\n\\nThus, more than 20 years after Popper, and in phrasing closely reminiscent of the former, Chomsky achieved an effect in linguistics very much the same as the one achieved by Popper in philosophy (cf. fn.1); but linguistics, unlike philosophy, never fully recovered from the blow. Not only discovery rules, as conceived by descriptivists, but the mere word ``discovery'' have eversince acquired strongly negative connotations for the influential transformational grammarians, so that many other linguists have had to be very diplomatic on the subject. This holds even for some outstanding linguists, ouside the transformational school.\\n\\nThus, some linguists with continuing methodological interests have taken much care to divert an eventual suspicion that their concerns have anything to do with discovery, claiming their work to fall entirely into the line of justification; cf. e.g. Leech 1970, Labov 1971:413-414.\\n\\nCuriously, attempts have been made to re-interpret the work of the very proponents of discovery, the descriptivists, in the line of justification, e.g. Lyons 1970, Miller 1973, Sampson 1979, just in order to rehabilitate them in the hypersensitive eyes of Chomskyans. Taking for granted that a concern with discovery is sinful, it was claimed that:\\n\\n...it is undeniable that the leading theorists /as Bloomfield, Harris, Hockett, Wells/ (with the exception of Pike) were not concerned with the development of discovery procedures. ...in the work of these linguistis a distinction is carefully drawn between the actual process of discovering the structure of a language and the business of describing a structure which has already been discovered. (Miller 1973: 123).\\n\\nIt was only the minor representatives of the school, then, who could be charged with being friends of discovery:\\n\\nWhereas the four linguists cited in the preceding paragraph were the leading theoreticians of the structuralist school, there were many linguists, less theoretically minded, who did interpret these techniques of segmentation as discovery techniques for use in the field. (Miller 1973: 125).\\n\\nNaive Inductivism\\n\\nAnother major factor contributing to the neglect of the study of discovery issues in linguistics is a view which may be called ``naive inductivism''. What I have in mind is not some worked out system of beliefs, or a coherent theory, but rather some disparate and vague sentiments, sometimes deeply rooted in linguistic conscience, as to the primary and exclusive role of ``data gathering'' and ``observation/generalization'' in the process of linguistic discovery. The common implication of all these sentiments is the denial of the existence of any systematic rules for discovery.\\n\\nBelow I mention two common embodiments of naive inductivism, briefly revealing their weaknesses.\\n\\nThe Immediate\\n\\n\\n\\nInduction\\n\\n\\n\\nof\\n\\n\\n\\nHypothesis Belief\\n\\nThis amounts to looking upon the process of discovery of a hypothesis (or problem solution) as springing immediately--and without appeal to any systematic modes of reasoning--from ``observation'' and ``generalization'' of the data gathered.\\n\\nThis form of naive inductivism, however, faces serious difficulties (in fact well-known from the writings of philosophers like Popper and Nagel, among others).\\n\\nFirst, mere observation or data gathering--without some prior hypothesis/problem--is a poor starting point for making a discovery since we do not know just which facts to observe or gather. What we need is the relevant facts, which, obviously, presupposes that we already dispose of a hypothesis/problem.\\n\\nSecondly, generalizing from facts is (or at least, may be) an activity which is strictly rule-governed, quite the opposite of what the linguistic proponents of this view suppose.\\n\\nAnd, thirdly, from a body of data not just one, but, as a rule, innumerable inductive generalizations can be made, so that we need to employ plausibility considerations to constrain the choice, and this, again, is liable to rules.\\n\\nThe Large\\n\\n\\n\\nQuantity\\n\\n\\n\\nof\\n\\n\\n\\nData Belief\\n\\nThis sentiment assumes that what facts perhaps cannot do, many facts can, and can be traced in linguistic remarks, with markedly positive connotation, to the effect that someone ``is true to the facts'' or ``bases his/her analysis on large corpuses of data'', etc. Conversely, other linguists are ridiculed for for ``having analysed three sentences and a half''. Consider also the following quote from an authorative source on the history of linguistics:\\n\\nThere are periods in the history of linguistics which look very much like revolutions and sudden shifts of paradigm, but in my opinion the most striking aspect of our science is the gradual accumulation over the century of an immense knowledge about language...To become aware of this may perhaps be one of the most significant revolutions in linguistics. (Hovdhaugen 1982: 11; italics mine).\\n\\nThis form of inductivism is also very vulnerable.\\n\\nFirst, and this point seems quite obvious, the great bulks of data in themselves are not only not conducive to making a discovery, but are also of a significant obstacle to it, the selection of the relevant facts becoming a more difficult task with the increase of these facts.\\n\\nAnd, in the second place, any (linguistic) discovery, contrary to the tenet discussed, is as a rule empirically underdetermined. Norbert Wiener, for instance, has wittily described the situation. To the question ``On how many instances would you be willing to base a generalization?'' he is reported to have answered ``Two instances would be nice, but one is enough!'' (cited in (Wartowski 1980: 6).\\n\\nConcluding this section, we should note that despite the marked tendency in current linguistics to disregard, or even be hostile to, the problems of discovery some linguists remained outside the mainstream (cf. esp. Botha 1980 who devotes a whole chapter to a (philosophically-oriented) treatment of linguistic discovery). However, there is clearly a need for more investment of effort. This paper is a contribution to this trend, but focuses on computational implementation (cf. also Pericliev 1990) where some heuristics are illustrated with a real research problem; a book on linguistic discovery is under preparation).\\n\\nMill's Methods for  Induction\\n\\nSummarizing the well-known ideas of the ``Experimental Science'' of the philosopher F.  Bacon, J. S. Mill (Mill 1879) formulated several methods (``canons'') for discovery of ``causally'' related facts in a set of instances (observations).\\n\\nMill's methods assume that each observation consists of a set of putative causes (or ``accompanying facts/circumstances'') for an effect, their aim, as eliminative induction methods, being to eliminate, from the set of putative causes, all but the ``actual'' one(s).\\n\\nBelow we state Mill's heuristics in his own formulation. Then, the heursitics are provided with somewhat simplistic linguistic examples, and their implications for the computational modeling of (linguistic) discovery are briefly discussed.\\n\\nThe Methods\\n\\nIn the following, ``\\n\\n'' means ``accompanies'', ``\\n\\n''\\n\\nmea\\n\\nns\\n\\n``causes'', ``\\n\\n'' means ``either causes or is an effect of''; ca pital letters denote circumstances, and small-case letters the ``phenomena'' investigated.\\n\\n(1) The Method of Agreement (MA). If two or more instances of the phenomenon under investigation have only one circumstance in common, the circumstance in which alone all the instances agree, is the cause (or effect) of the given phenomenon. Schematically:\\n\\n(2) The Method of Difference (MD). If an instance in which the phenomenon under investigation occurs, and an instance in which it does not occur, have every circumstance in common save one, that one occurring only in the former; the circumstance in which alone the two instances differ, is the effect, or the cause, or an indispensable part of the cause, of the phenomenon. Schematically:\\n\\n(3) The Method of Residues (MR). Subduct from any phenomenon such part as is known by previous inductions to be the effect of certain antecedents, and the residue of the phenomenon is the effect of the remaining antecedents. Symbolically:\\n\\n(4) The Method of Concomitant Variations (MCV). Whatever phenomenon varies in any manner whenever another phenomenon varies in some particular manner, is either a cause or an effect of that phenomenon, or is connected with it through some fact of causation. Symbolically (an apostrophe denotes a variation):\\n\\nExamples\\n\\nAssume now that we are given the morpheme decomposition of (English) words, as well as their decomposition into constituent meanings, and we inquire about the morpheme-meaning correspondences.\\n\\nFrom the following observations, by MA, we may infer that ``let'' and ``diminutive'' are causally connected.\\n\\nThus, since only the morpheme ``let'' occurs when the resultant meaning (=effect) ``diminutive'' occurs, while the other accompanying facts (``book'', ``leaf'' or ``s'') vary, we conclude that ``let'' causes (or is an effect of) ``diminutive''.\\n\\nThe following two are self-explanatory examples of the MD and MR, respectively.\\n\\nAs an illustration of the fourth method Mill proposed, the MCV, consider how one can infer a causal link between accent and grammatical meaning on the basis of the obseration that the shift of the accent of a word (as e.g. in ``condct'' and ``cnduct'') leads to a correspondiong shift of the grammati cal meaning, verbal in the first case, and nominal in the second:\\n\\nSome Implications\\n\\nThe above methods have several features which make their computational modeling in a linguistic system of considerable interest:\\n\\nGenerality\\n\\nThe methods' generality, as reflected in their domain- and subject-independence, makes them applicable to a wide range of linguistic discoveries in diverse linguistic fields. Indeed, it is well known that a substantial part of linguistic ``laws'' are in fact empirical regularities, falling under the general schema 'Forms/meanings of type A correspond to/cause forms/meanings of type B' at the different linguistic levels.\\n\\nFor instance, one of the founders of modern linguistics characterized synchronic linguistics as finding form-meaning correspondences: ``In human speech, different sounds have different meanings. To study the coordination of certain sounds with certain meanings is to study language.'' (Bloomfield 1933). A basic task in diachronic (historical) linguistics is the study of causation of language change in both sounds and meanings. The study of language universals, as initiated by J.  Greenberg, most often amounts to finding so-called ``implicational universals'', etc.\\n\\nPsychological Plausibility\\n\\nMill's methods are psychologically plausible insofar as they are simple and perfectly natural reasoning modes. So they have been widely used in the process of linguistic inquiry. Their use may be unconscious (often disguised as specific ``(litmus) tests'', ``discovery procedures'', etc., in fact directly based on Mill's canons), or it may be conscious (e.g. J.  Greenberg has recently attributed the discovery of the famous Verner's law to his use of the MD). Psychological plausibility is a further feature which a discovery system may profitably possess.\\n\\nHistorical Importance\\n\\nFinally, the embedding of historically important methods into computational systems may serve as test of the particular ideas underlying these methods, and more generally, as a test of the challenging idea of the possibility for a purely ``mechanistic discovery''.\\n\\nSystem Overview and  Examples\\n\\nMILL is a system incorporating the above heuristics. It does not make the minute distinctions Mill assumes in the conclusions of the methods; for user-specified cause-sets and effect-sets in observations it merely identifies a cause for an effect (or vice versa). MILL's data representation is a simple Object-Attribute-Value knowledge structure. Its discovery process is, in essence, an attempt to apply one or more of the above methods. The system iterates through the knowledge base, trying to apply a method. After each successful application, a further simple heuristic is used, the Elimination Method (EM), known to everyone of us, which checks in the data base whether a putative cause always produces the same effect, and if not, rejects the conjecture. A successful conjecture is attempted to be proven by a further method, and the result is recorded. The system repeats this process until all possibilities are exhausted.\\n\\nAs output, MILL produces two interrelated structures, constituting its discoveries:\\n\\n(1) a set of the causally related facts; (2) an elementary form of ``explanation'' for the system's reasoning behaviour, comprising the methods employed to draw a conclusion, as well as the data used to this end.\\n\\nIn the present implementation, structure (2) is only useful if the system is being used as a tool by a linguist as an information helping him/her evaluate the solutions' degree of plausibility. Thus, some methods (e.g. MD) give more plausible results than others (e.g. MA); a discovery using both (i.e. the Joint Method of Agreement and Difference)  is more plausible than such using either taken in isolation, etc.\\n\\nThe operation of the system may be better understood by examining particular examples.\\n\\nA Simple Deciphering Problem\\n\\nA text is given, consisting of six phrases in an unknown language A,  together with their translational equivalents into another unknown language, B. The task is to find the translational equivalents of the following two words from language B:  sth and hzbwb.\\n\\nThe data representation is Object-Attribute-Value. The first observation e.g. will be represented as observation(no(1), cause-set([miz, pi]), effect-set([ysth, zbwb])), meaning that the object ``observation'' has three attributes, number, cause-set and effect-set with their corresponding values. The system knows that the order of symbols in the cause-set and effect-set is insignificant.\\n\\nNeeding to find the correspondences in language A of two specific words from language B, the problem will be formulated  as follows:  ?- causality(Cause1,sth)\\n\\ncausality(Cause2,hzbwb). (Varaibles begin with a capital letter, and constants with a small-case letter.) MILL will try to apply one of the methods, looking first for the cause of\\n\\nsth. Since this word occurs only in obs. No. 3, no other method except MR is applicable, and it will choose MR. Applying MR will invoke a further subgoal, viz. that of finding--by some of the heuristics--the cause of zbwb, the other word in the effect-set of obs. No. 3.  zbwb occurs in obs. Nos. 1 and 3 and is provable, by MA, to correspond to miz. This being the case, what remains to be the cause of sth, according to MR, is pinte. The Elimination Method does not disconfirm the conjecture pinte\\n\\nsth. In a perfectly analogous way MILL will solve the second subgoal, finding the cause of hzbwb to be miza.\\n\\nGrimm's Law\\n\\nConsider the following data, compiled from Mincoff (1967: 77), which obeys the well-known Consonant Shift, familiar under the name ``Grimm's Law'' (1822); The Indo-European sounds (exemplified by the Latin words in the left column) are taken as the causes of the Germanic sounds (exemplified by the Old English words in the right column).\\n\\nLet the  problem be to discover the consonantal alternations exhibited in the data.\\n\\nTo solve the problem MILL will need an explicit encoding of the domain knowledge as to what of the symbols in the data base are c(onsonants) and what are v(owels). The representation, containing this information, taking as an illustration e.g. obs. No. 4 will be observation(no(4),cause-set([c:p,v:e,c:s]), effect-set([c:f,v:o,c:t])).\\n\\nThe problem formulation, unputted to the system, will be:\\n\\n?\\n\\n\\n\\ncausation(c:Cause,c:Effect).\\n\\nNow we may look briefly at the discovery process. Trying to apply MA, the system notices in observations Nos. 1 and 2 the single co-occurring symbol in their cause-sets,t, and the single co-occurring symbol in their effect-sets,\\n\\n. Then it proceeds with EM. This test succeeding, since whenever t occurs\\n\\nalso occurs (as in obs. No. 3) and there is no contradicting data in the base, it is hypothesized that t causes\\n\\n. This hypothesis is then attempted to be proven by a further method (failing in this particular case), and the result is recorded.\\n\\nBy the same method, from observations Nos. 3 and 4, the system will hypothesize that p causes f, the EM will succeed since in observations Nos. 5 and 6 where p occurs the conjectured correspondence f also occurs, and p causes f will be assumed. Again using the MA, and simialr reasoning route the system will infer from observations Nos. 7 and 8 that d causes t.\\n\\nMILL has thus re\\n\\n\\n\\ndiscovered the Indo\\n\\n\\n\\nEuropean\\n\\n\\n\\nGermanic  consonantal\\n\\nalternations t ]\\n\\n, p ] f, d ] t, which form a part of the famous Grimm's Law (1882).\\n\\nConclusion\\n\\nAt this stage of development, MILL has certain limitations, arising both from Mill's conception (our interpretation) of causality and the implementation. To mention but one thing, it is incapable of handling ``exceptions''. E.g. had we encountered in our data base of the consonantal alternation problem an observation such as Latin sto O.E. standan, MILL would have never found the change t ]\\n\\n. Nevertheless, it will be clear that a discovery process, relying on Mill's methods, will make a system simple, and at the same time quite general and powerful; thus the system has re-discovered a number of further sound laws, and was successfully tested in its role as a tool in the solutions of diverse ``field'' linguistic problems. Finally, although we have considered only linguistic discovery, the system may be, obviously, applied to discovering empirical regularities in other fields as well.\\n\\nBibliography\\n\\nBloomfield, L.\\n\\n1933.\\n\\nLanguage.\\n\\nNew York: Holt.\\n\\nBotha, R. 1980. The Conduct of Linguistic Inquiry. The Hague, Paris, New York: Mouton Publishers.\\n\\nChomsky, N. 1957. Syntactic Structures. The Hague, Paris, New York: Mouton Publishers.\\n\\nDougherty, R. 1973. A survey of linguistic methods and arguments. Foundations of Language 10(3):423-490.\\n\\nHanson, N.\\n\\n1958.\\n\\nPatterns of Discovery.\\n\\nCambridge: Cambridge University Press.\\n\\nHarr, R. 1988. Modes of explanation. In Hilton, J., ed., Contemporary Science and Natural Explanation: Commonsense Conceptions of Causality. New York: New York University Press. 129-144.\\n\\nHarris, Z.\\n\\n1951.\\n\\nMethods in Structural Linguistics.\\n\\nChicago: Chicago University Press.\\n\\nHovdhaugen, E. 1982. Foundations of Western Linguistics (From the Beginning to the End of the First Millenium A. D.). Universitetsforlaget.\\n\\nLabov, W. 1971. Methodology. In Dingwall, W., ed., A Survey of Linguistic Science. Maryland: University of Maryland. 412-497.\\n\\nLangley, P.; Simon, H.; Bradshaw, G.; and Zytkow, J. 1987. Scientific Discovery: Computational Investigation of the Creative Process. Cambridge, Mass. : MIT Press.\\n\\nLeech, G. 1970. On the theory and practice of semantic testing. Lingua 24:343-364.\\n\\nLyons, J.\\n\\n1970.\\n\\nChomsky.\\n\\nLondon: Fontana/Collins.\\n\\nMill, J. S. 1879. A System of Logic Ratiocinative and Inductive. London: Longmans Green.\\n\\nMiller, J. 1973. On the so called ``discovery procedures''. Foundations of Language 10(1):123-139.\\n\\nMincoff, M.\\n\\n1967.\\n\\nEnglish Historical Grammar.\\n\\nSofia: Nauka i izkustvo.\\n\\nNewell, A., and Simon, H. 1972. Human Problem Solving. Englewood Cliffs, New Jersey: Prentice Hall, Inc.\\n\\nNickles, T. 1980a. Introductory essay: scientific discovery and the future of philosophy of science. In Nickles, T., ed., Scientific Discovery I: Logic and Rationality. Dordrecht: D. Reidel. 1-59.\\n\\nNickles, T., ed. 1980b. Scientific Discovery I: Logic and Rationality. Dordrecht: D. Reidel.\\n\\nNickles, T., ed. 1980c. Scientific Discovery II: Case Studies. Dordrecht: D. Reidel.\\n\\nPericliev, V. 1990. On heuristic procedures in linguistics. Studia Linguistica 44(1):59-69.\\n\\nSampson, G.\\n\\n1979.\\n\\nWhat was transformational grammar.\\n\\nLingua 48(1):355\\n\\n\\n\\n378.\\n\\nShrager, J., and Langley, P., eds. 1990. Computational Models of Scientific Discovery and Theory Formation. San Mateo, CA: Morgan Kaufmann.\\n\\nSimon, H. 1973. Does scientific discovery have a logic. Philosophy of Science 40:471-480.\\n\\nTeeter, K. 1964. Descriptive linguistics in America: triviality vs. irrelevance. Word 20(2):197-206.\\n\\nWartowski, M. 1980. Scientific judgment: creativity and rationality. In Nickles, T., ed., Scientific Discovery II: Case Studies. Dordrecht: D. Reidel. 1-16.\\n\\nZaliznjak, A. 1963. Lingvisticeskije zadaci (Linguistic problems). In Moloshnaja, T., ed., Issledovanija po strukturnoj tipologii (Investigations on Structural Typology). Moskva: Academii Nauk. 137-159.\\n\\nFootnotes\\n\\nI have in mind esp. the banning of discovery from the realm of a science like philosophy until the late fifties (cf. Hanson 1958), triggered by Popper's denunciation of a logic of discovery in his classic The Logic of Scientific Discovery (German original Logik der Forschung from 1935); for a discussion, cf. e.g. Simon 1973, Nickles 1980a. On a common understanding causality is the logical relation ``If A, then B'', where the antecedent A is called a ``cause'' and the consequent B is called an ``effect'', e.g. Harr. Saying that the cause A has an effect B (or that A causes B) then means simply that A is an invariable antecedent of B, or equivalently, that B occurs whenever A occurs. The method is also particularly applicable to quantitative terms. This could be explained by noting that the methods actually yield demonstrative inferences, only making the assumptions that all accompanying circumstances are enumerated and that there is no more than one cause for an effect; the process of elimination itself is indeed trivial. One is reminded in this context of the words of Bacon in Novum Organum that his Method of discovery of sciences leaves little to the acuteness and strength of wit, and indeed levels wit and intellect. Cf. e.g. MD, where the conclusion is ``the effect or the cause, or an indispensable part of the cause''.\\n\\nAn elementary query to the system is of the form causation(Cause,Effect), where ``Cause'' and ``Effect'' stand for arbitrary terms, variables or constants; a particular problem formulation may be stated in terms of any elementary queries, connected by logical operators. This is actually a somewhat different interpretation of the Joint Method than Mill gives. This problem is adapted from (Zaliznjak 1963: 141). In fact, language A is Albanian, and language B, Old Jewish. The Albanian text is given in the usual orthography. For the old Jewish text, the Latin transliteration of consonantal writing is given. The same conjecture could also have been made by the same method from observations Nos. 1 and 3, had the system used another mode of scanning tha data base than the top-down one. Grimm's Law, together with some other sound laws, like that of Verner, was one of the greatest achievements of 19th c.  linguistics.\", metadata={'source': '../data/raw/cmplg-xml/9506023.xml'}),\n",
       " Document(page_content=\"Using Information Content to Evaluate Semantic Similarity in a Taxonomy\\n\\nThis paper presents a new measure of semantic similarity in an  IS-A taxonomy, based on the notion of information content. Experimental evaluation suggests that the measure performs encouragingly well (a correlation of r = 0.79 with a benchmark set of human similarity judgments, with an upper bound of r = 0.90 for human subjects performing the same task), and significantly better than the traditional edge counting approach (r = 0.66).\\n\\nIntroduction\\n\\nSimilarity and Information Content\\n\\nEvaluation\\n\\nImplementation\\n\\nTask\\n\\nAlthough there is no standard way to evaluate computational measures of semantic similarity, one reasonable way to judge would seem to be agreement with human similarity ratings. This can be assessed by using a computational similarity measure to rate the similarity of a set of word pairs, and looking at how well its ratings correlate with human ratings of the same pairs.\\n\\nIn order to get a baseline for comparison, I replicated Miller and Charles's experiment, giving ten subjects the same 30 noun pairs. The subjects were all computer science graduate students or postdocs at the University of Pennsylvania, and the instructions were exactly the same as used by Miller and Charles, the main difference being that in this replication the subjects completed the questionnaire by electronic mail (though they were instructed to complete the whole thing in a single uninterrupted sitting). Five subjects received the list of word pairs in a random order, and the other five received the list in the reverse order. The correlation between the Miller and Charles mean ratings and the mean ratings in my replication was .96, quite close to the .97 correlation that Miller and Charles obtained between their results and the ratings determined by the earlier study.\\n\\nFor each subject in my replication, I computed how well his or her ratings correlated with the Miller and Charles ratings. The average correlation over the 10 subjects was\\n\\nr = 0.8848, with a standard deviation of 0.08. This value represents an upper bound on what one should expect from a computational attempt to perform the same task.\\n\\nFor purposes of evaluation, three computational similarity measures were used. The first is the similarity measurement using information content proposed in the previous section. The second is a variant on the edge counting method, converting it from distance to similarity by subtracting the path length from the maximum possible path length:\\n\\nResults\\n\\nDiscussion\\n\\nThe experimental results in the previous section suggest that measuring semantic similarity using information content provides quite reasonable results, significantly better than the traditional method of simply counting the number of intervening  IS-A links.\\n\\nCases like this are probably relatively rare. However, the example illustrates a more general concern: in measuring similarity between words, it is really the relationship among word senses that matters, and a similarity measure should be able to take this into account.\\n\\nRelated Work\\n\\nFinally, in the context of current research in computational linguistics, the approach to semantic similarity taken here can be viewed as a hybrid, combining corpus-based statistical methods with knowledge-based taxonomic information. The use of corpus statistics alone in evaluating word similarity -- without prior taxonomic knowledge -- is currently an active area of research in the natural language community. This is largely a reaction to sparse data problems in training statistical language models: it is difficult to come up with an accurate statistical characterization of the behavior of words that have been encountered few times or not at all. Word similarity appears to be one promising way to solve the problem: the behavior of a word is approximated by smoothing its observed behavior together with the behavior of words to which it is similar. For example, a speech recognizer that has never seen the phrase ate a peach can still conclude that John ate a peach is a reasonable sequence of words in English if it has seen other sentences like Mary ate a pear and knows that peach and pear have similar behavior.\\n\\nConclusions\\n\\nThis paper has presented a new measure of semantic similarity in an  IS-A taxonomy, based on the notion of information content. Experimental evaluation was performed using a large, independently constructed corpus, an independently constructed taxonomy, and previously existing human subject data. The results suggest that the measure performs encouragingly well (a correlation of r = 0.79 with a benchmark set of human similarity judgments, against an upper bound of r = 0.90 for human subjects performing the same task), and significantly better than the traditional edge counting approach (r = 0.66).\\n\\nBibliography\\n\\nA. Collins and E. Loftus. A spreading activation theory of semantic processing. Psychological Review, 82:407-428, 1975.\\n\\nW. N. Francis and H. Kucera. Frequency Analysis of English Usage: Lexicon and Grammar. Houghton Mifflin, 1982.\\n\\nClaudia Leacock and Martin Chodorow. Filling in a sparse training space for word sense identification. ms., March 1994.\\n\\nJoon Ho Lee, Myoung Ho Kim, and Yoon Joon Lee. Information retrieval based on conceptual distance in IS-A hierarchies. Journal of Documentation, 49(2):188-207, June 1993.\\n\\nMichael Lesk. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 1986 SIGDOC Conference, pages 24-26, 1986.\\n\\nGeorge A. Miller and Walter G. Charles. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1-28, 1991.\\n\\nGeorge Miller. WordNet: An on-line lexical database. International Journal of Lexicography, 3(4), 1990. (Special Issue).\\n\\nM. Ross Quillian. Semantic memory. In M. Minsky, editor, Semantic Information Processing. MIT Press, Cambridge, MA, 1968.\\n\\nRoy Rada and Ellen Bicknell. Ranking documents with a thesaurus. JASIS, 40(5):304-310, September 1989.\\n\\nRoy Rada, Hafedh Mili, Ellen Bicknell, and Maria Blettner. Development and application of a metric on semantic nets. IEEE Transaction on Systems, Man, and Cybernetics, 19(1):17-30, February 1989.\\n\\nPhilip Resnik. Selection and Information: A Class-Based Approach to Lexical Relationships. PhD thesis, University of Pennsylvania, December 1993.\\n\\nPhilip Resnik. Semantic classes and syntactic ambiguity. In Proceedings of the 1993 ARPA Human Language Technology Workshop. Morgan Kaufmann, March 1993.\\n\\nPhilip Resnik. Disambiguating noun groupings with respect to WordNet senses. In Third Workshop on Very Large Corpora. Association for Computational Linguistics, 1995.\\n\\nSheldon Ross. A First Course in Probability. Macmillan, 1976.\\n\\nHerbert Rubenstein and John Goodenough. Contextual correlates of synonymy. CACM, 8(10):627-633, October 1965.\\n\\nJohn Sinclair (ed.). Collins COBUILD English Language Dictionary. Collins: London, 1987.\\n\\nMichael Sussna. Word sense disambiguation for free-text indexing using a massive semantic network. In Proceedings of the Second International Conference on Information and Knowledge Management (CIKM-93), Arlington, Virginia, 1993.\\n\\nA. Tversky.\\n\\nFeatures of similarity.\\n\\nPsychological Review, 84:327\\n\\n\\n\\n352, 1977.\\n\\nSholom M. Weiss and Casimir A. Kulikowski. Computer systems that learn: classification and prediction methods from statistics, neural nets, machine learning, and expert systems. Morgan Kaufmann, San Mateo, CA, 1991.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9511007.xml'}),\n",
       " Document(page_content=\"The intersection of Finite State Automata and Definite Clause Grammars\\n\\nBernard Lang defines parsing as the calculation of the intersection of a FSA (the input) and a CFG. Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems, in which parsing takes a word lattice as input (rather than a word string). Furthermore, certain techniques for robust parsing can be modelled as finite state transducers. In this paper we investigate how we can generalize this approach for unification grammars. In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG. It is shown that existing parsing algorithms can be easily extended for FSA inputs. However, we also show that the termination properties change drastically: we show that it is undecidable whether the intersection of a FSA and a DCG is empty (even if the DCG is off-line parsable). Furthermore we discuss approaches to cope with the problem.\\n\\nIntroduction\\n\\nIn this paper we are concerned with the syntactic analysis phase of a natural language understanding system. Ordinarily, the input of such a system is a sequence of words. However, following Bernard Lang we argue that it might be fruitful to take the input more generally as a finite state automaton (FSA) to model cases in which we are uncertain about the actual input. Parsing uncertain input might be necessary in case of ill-formed textual input, or in case of speech input.\\n\\nFor example, if a natural language understanding system is interfaced with a speech recognition component, chances are that this compenent is uncertain about the actual string of words that has been uttered, and thus produces a word lattice of the most promising hypotheses, rather than a single sequence of words. FSA of course generalizes such word lattices.\\n\\nAs suggested by an ACL reviewer, one could also try to model haplology phenomena (such as the 's in English sentences like `The chef at Joe's hat', where `Joe's' is the name of a restaurant) using a finite state transducer. In a  straightforward approach this would also lead to a finite-state automaton with cycles.\\n\\nThe intersection of a CFG and a FSA\\n\\nAlthough this construction shows that the intersection of a FSA and a CFG is itself a CFG, it is not of practical interest. The reason is that this construction typically yields an enormous amount of rules that are `useless'. In fact the (possibly enormously large) parse forest grammar might define an empty language (if the intersection was empty). Luckily `ordinary' recognizers/parsers for CFG can be easily generalized to construct this intersection yielding (in typical cases) a much smaller grammar. Checking whether the intersection is empty or not is then usually very simple as well: only in the latter case will the parser terminate succesfully.\\n\\nTo illustrate how a parser can be generalized to accept a FSA as input we present a simple top-down parser.\\n\\nA context-free grammar is represented as a definite-clause specification as follows. We do not wish to define the sets of terminal and non-terminal symbols explicitly, these can be understood from the rules that are defined using the relation rule/2, and where symbols of the rhs are prefixed with `-' in the case of terminals and `+' in the case of non-terminals. The relation   top/1 defines the start symbol. The language L'=a[nbn] is defined as:\\n\\nNext consider the definite clause specification of a FSA. We define the transition relation using the relation trans/3. For start states, the relation start/1 should hold, and for final states the relation final/1 should hold. Thus the following FSA, defining the regular language\\n\\nL=(aa)[*b+](i.e. an even number of a's followed by at least one b) is given as:\\n\\nThe intersection of a DCG and a FSA\\n\\nIn this section we want to generalize the ideas described above for CFG to DCG.\\n\\nThe straightforward approach is to generalize existing recognition algorithms. The same techniques that are used for calculating the intersection of a FSA and a CFG can be applied in the case of DCGs. In order to compute the intersection of a DCG and a FSA we assume that FSA are represented as before. DCGs are represented using the same notation we used for context-free grammars, but now of course the category symbols can be first-order terms of arbitrary complexity (note that without loss of generality we don't take into account DCGs having external actions defined in curly braces).\\n\\nMost existing constraint-based parsing algorithms will terminate for grammars that exhibit the property that for each string there is only a finite number of possible derivations. Note that off-line parsability is one possible way of ensuring that this is the case.\\n\\nThis observation is not very helpful in establishing insights concerning interesting subclasses of DCGs for which termination can be guaranteed (in the case of FSA input). The reason is that there are now two sources of recursion: in the DCG and in the FSA (cycles). As we saw earlier: even for CFG it holds that there can be an infinite number of analyses for a given FSA (but in the CFG this of course does not imply undecidability).\\n\\nIntersection of FSA and off-line parsable DCG is undecidable\\n\\nFirst I give a simple algorithm to encode any instance of a PCP as a pair, consisting of a FSA and an off-line parsable DCG, in such a way that the question whether there is a solution to this PCP is equivalent to the question whether the intersection of this FSA and DCG is empty.\\n\\nEncoding of PCP.\\n\\nThe underlying idea of the algorithm is really very simple. For each pair of strings from the lists A and B there will be one lexical entry (deriving the terminal x) where these strings are represented by a difference-list encoding. Furthermore there is a general combination rule that simply concatenates A-strings and concatenates B-strings. Finally the rule for s states that in order to construct a succesful top category the A and B lists must match.\\n\\nProposition\\n\\nThe question whether the intersection of a FSA and an off-line parsable DCG is empty is undecidable.\\n\\nProof.\\n\\nWhat to do?\\n\\nThe following approaches towards the undecidability problem can be taken:\\n\\nlimit the power of the FSA\\n\\nlimit the power of the DCG\\n\\ncompromise completeness\\n\\ncompromise soundness\\n\\nThese approaches are discussed now in turn.\\n\\nLimit the FSA\\n\\nRather than assuming the input for parsing is a FSA in its full generality, we might assume that the input is an ordinary word graph (a FSA without cycles).\\n\\nThus the techniques for robust processing that give rise to such cycles cannot be used. One example is the processing of an unknown sequence of words, e.g. in case there is noise in the input and it is not clear how many words have been uttered during this noise. It is not clear to me right now what we loose (in practical terms) if we give up such cycles.\\n\\nNote that it is easy to verify that the question whether the intersection of a word-graph and an off-line parsable DCG is empty or not is decidable since it reduces to checking whether the DCG derives one of a finite number of strings.\\n\\nLimit the DCG\\n\\nAnother approach is to limit the size of the categories that are being employed. This is the GPSG and F-TAG approach. In that case we are not longer dealing with DCGs but rather with CFGs (which have been shown to be insufficient in general for the description of natural languages).\\n\\nCompromise completeness\\n\\nCompleteness in this context means: the parse forest grammar contains all possible parses. It is possible to compromise here, in such a way that the parser is guaranteed to terminate, but sometimes misses a few parse-trees.\\n\\nFor example, if we assume that each edge in the FSA is associated with a probability it is possible to define a threshold such that each partial result that is derived has a probability higher than the threshold. Thus, it is still possible to have cycles in the FSA, but anytime the cycle is `used' the probability decreases and if too many cycles are encountered the threshold will cut off that derivation.\\n\\nOf course this implies that sometimes the intersection is considered empty by this procedure whereas in fact the intersection is not. For any threshold it is the case that the intersection problem of off-line parsable DCGs and FSA is decidable.\\n\\nCompromise soundness\\n\\nSoundness in this context should be understood as the property that all parse trees in the parse forest grammar are valid parse trees. A possible way to ensure termination is to remove all constraints from the DCG and parse according to this context-free skeleton. The resulting parse-forest grammar  will be too general most of the times.\\n\\nA practical variation can be conceived as follows. From the DCG we take its context-free skeleton. This skeleton is obtained by removing the constraints from each of the grammar rules. Then we compute the intersection of the skeleton with the input FSA. This results in a parse forest grammar. Finally, we add the corresponding constraints from the DCG to the grammar rules of the parse forest grammar.\\n\\nThis has the advantage that the result is still sound and complete, although the size of the parse forest grammar is not optimal (as a consequence it is not guaranteed that the parse forest grammar contains a parse tree). Of course it is possible to experiment with different ways of taking the context-free skeleton (including as much information as possible / useful).\\n\\nAcknowledgments\\n\\nI would like to thank Gosse Bouma, Mark-Jan Nederhof and John Nerbonne for comments on this paper. Furthermore the paper benefitted from remarks made by the anonymous ACL reviewers.\\n\\nBibliography\\n\\nY. Bar-Hillel, M. Perles, and E. Shamir. 1961. On formal properties of simple phrase structure grammars. Zeitschrift fr Phonetik, SprachWissenschaft und Kommunicationsforschung, 14:143-172. Reprinted in Bar-Hillel's Language and Information - Selected Essays on their Theory and Application, Addison Wesley series in Logic, 1964, pp. 116-150.\\n\\nS. Billot and B. Lang. 1989. The structure of shared parse forests in ambiguous parsing. In 27th Annual Meeting of the Association for Computational Linguistics, pages 143-151, Vancouver.\\n\\nDavid Carter. 1994. Chapter 4: Linguistic analysis. In M-S. Agns, H. Alshawi, I. Bretan, D. Carter, K. Ceder, M. Collins, R. Crouch, V. Digalakis, B Ekholm, B. Gambck, J. Kaja, J. Karlgren, B. Lyberg, P. Price, S. Pulman, M. Rayner, C. Samuelsson, and T. Svensson, editors, Spoken Language Translator: First Year Report. SICS Sweden / SRI Cambridge. SICS research report R94:03, ISSN 0283-3638.\\n\\nBarbara Grosz, Karen Sparck Jones, and Bonny Lynn Webber, editors. 1986. Readings in Natural Language Processing. Morgan Kaufmann.\\n\\nJohn E. Hopcroft and Jeffrey D. Ullman. 1979. Introduction to Automata Theory, Languages and Computation. Addison Wesley.\\n\\nBernard Lang. 1974. Deterministic techniques for efficient non-deterministic parsers. In J. Loeckx, editor, Proceedings of the Second Colloquium on Automata, Languages and Programming. Also: Rapport de Recherche 72, IRIA-Laboria, Rocquencourt (France).\\n\\nBernard Lang. 1988. Parsing incomplete sentences. In Proceedings of the 12th International Conference on Computational Linguistics (COLING), Budapest.\\n\\nBernard Lang. 1989. A generative view of ill-formed input processing. In ATR Symposium on Basic Research for Telephone Interpretation (ASTI), Kyoto Japan.\\n\\nMark-Jan Nederhof and Eberhard Bertsch. 1994. Linear-time suffix recognition for deterministic languages. Technical Report CSI-R9409, Computing Science Institute, KUN Nijmegen.\\n\\nFernando C.N. Pereira and David Warren. 1983. Parsing as deduction. In 21st Annual Meeting of the Association for Computational Linguistics, Cambridge Massachusetts.\\n\\nH. Saito and M. Tomita. 1988. Parsing noisy sentences. In Proceedings of the 12th International Conference on Computational Linguistics (COLING), pages 561-566, Budapest.\\n\\nR. Teitelbaum. 1973. Context-free error analysis by evaluation of algebraic power series. In Proceedings of the Fifth Annual ACM Symposium on Theory of Computing, Austin, Texas.\\n\\nDavid S. Warren. 1992. Memoing for logic programs. Communications of the ACM, 35(3):94-111.\\n\\nFootnotes\\n\\nIn fact, the standard compilation of DCG into Prolog clauses does something similar using variables instead of actual state names. This also illustrates that this method is not very useful yet; all the work has still to be done.\", metadata={'source': '../data/raw/cmplg-xml/9504026.xml'}),\n",
       " Document(page_content=\"The Semantics of Motion\\n\\nIn this paper we present a semantic study of motion complexes (ie. of a motion verb followed by a spatial preposition). We focus on the spatial and the temporal intrinsic semantic properties of the motion verbs, on the one hand, and of the spatial prepositions, on the other hand. Then we address the problem of combining these basic semantics in order to formally and automatically derive the spatiotemporal semantics of a motion complex from the spatiotemporal properties of its components.\\n\\nIntroduction\\n\\nMost of natural languages provide two types of lexical items to describe the motion of an entity with respect to some location: motion verbs (to run; to enter) and spatial prepositions (from; towards). Motion verbs can be used directly with a location, when they are transitive (to cross the town) or with a spatial preposition, when they are intransitive (to go through the town). The latter case is more interesting: most of the French motion verbs are intransitive and the interaction between motion verbs and spatial prepositions gives detailed informations about the way human beeings mentally represent spatiotemporal aspects of a motion. When we describe a motion, the fact to choose a verb instead of another, a preposition instead of another, a syntactic structure instead of another, reveals our mental cognitive representation. We claim that natural languages can be considered as a trace of these representations, in which it is possible, with systematic and detailled linguistic studies, to light up the way spatiotemporal properties are represented and on which basic concepts these representations lie. We present such linguistic investigations on French motion verbs and spatial prepositions and the basic concepts we have found. We also address compositional semantics for motion complexes (ie.\\n\\na motion verb followed by a spatial preposition) and show that the complexity and the refinements of the linguistic studies presented just before are justified and required at the compositional level in order to capture the different behaviours in the compositional processes that exist with the French language. We also compare with the English language and draw some conclusions on the benefits of our approach.\\n\\nLexical Semantics for Motion Verbs\\n\\nFollowing Gruber (1965), Jackendoff (1976), Boons (1985), we approach motion verbs in terms of some ``localist semantical'' role labels. The linguistic study of French intransitive motion verbs (see eg. (Asher Sablayrolles, 1994a)) we have realized has allowed the definition of an ontology for ``location'' in three basic concepts:\\n\\nlocations which are concrete places (a room; a house; a street);\\n\\npositions which are parts of a location (the position where I am in this room);\\n\\npostures which are ways to be in a position (to be standing, sitting, lying).\\n\\nWith the help of this ontology we have realized a typology for intransitive motion verbs. We distinguish 4 categories on the basis of which kind of ``location'' they intrinsically refer to.\\n\\nChange of location (CoL) verbs (entrer-to enter; sortir-to go out) denote a change of location. When we enter some place or go out of some place, we have different spatial relation with the location (ie. inside/outside) before and after the motion.\\n\\nSome denote a change of position which always occur (voyager-to travel). For example, we cannot say voyager sur place-to travel in place. We call these verbs change of position (CoPs) verbs.\\n\\nOthers denote only possible change of position (courir-to run). For example, we can say courir sur place-to run in place. We call these verbs inertial change of position (ICoPs) verbs.\\n\\nChange of posture (CoPtu) verbs (s'asseoir-to sit down; se baisser-to bend down). They denote a change of the relations between the parts of an entity.\\n\\nFor the following, we will focus on CoL verbs (the Change of Location verbs), mainly because they are rich in spatiotemporal informations, but also because we have at disposal exhaustive lists of French CoL verbs. We have realized a systematic and fine linguistic study on these verbs, looking carefully at each of them, one by one (and we have 440 CoL verbs in French), in order to extract their intrinsic spatiotemporal properties. These semantic properties can be characterized by a restructuration of the space induced by the so-called reference location (lref) (cf. (Talmy, 1983)). This lref, implicitly suggested by each CoL verb, can be either the initial location (as with partir-to leave; sortir-to go out), or the path (passer, traverser-to pass through) or the final location (arriver-to arrive; entrer-to enter) of the motion. Indeed, verbs like sortir intrinsically suggest a location of which we have gone out. This space, induced by the lref, is characterized by most of the authors in the literature by a two-part spatial system consisting in the inside and the outside of the lref. We propose to refine this structure with two new concepts, required to distinguish minimal pairs like sortir (to go out)/partir (to leave), and entrer (to enter)/atterir (to land). These concepts are:\\n\\n1. a limit of proximity distinguishing an outside of proximity from a far away outside; indeed, if sortir simply requires to go out of the lref, partir in addition forces the mobile to go sufficiently far away from that lref. 2. an external zone of contact required by verbs like atterir for which the final location is neither the lref (in contrast with entrer) or the outside (or proximity outside) of the lref (in contrast with s'approcher-to approach).\\n\\nWe have so defined a structuration of the space based on 4 zones :\\n\\nthe inside;\\n\\nthe external zone of contact;\\n\\nthe outside of proximity;\\n\\nthe far away outside.\\n\\nThis structuration is close to the way Jackendoff and Landau (1992) encode the space induced by the reference object introduced by a static spatial preposition. As we have come to these distinctions by examining different linguistic material, we conclude that language structures space in the same way whatever sort of lexical items (motion verbs (dynamic)/(static) spatial prepositions) we examine. This has allowed us to classify CoL verbs into 10 classes on the basis of which zones the mobile is inside, at the beginning and at the end of its motion. Note that all the geometrical possibilities are not lexicalized in French.\\n\\nLexical Semantics for Spatial Prepositions\\n\\nWe have followed the same approach with French spatial prepositions, but using a structuration of the space induced by the location introduced in the PP by the preposition, and not induced by the lref as for verbs. Following Laur (1993), we consider simple prepositions (like in) as well as prepositional phrases (like in front of). We have classified 199 such French prepositions into 16 groups using in addition of our zones two other criteria:\\n\\npositional (like in)\\n\\ndirectional (like into)\\n\\nInitial (like from)\\n\\nMedial (like through)\\n\\nFinal (like to)\\n\\nCompositional Semantics for Motion Complexes\\n\\nThe linguistic studies, used for the typologies of CoL verbs and spatial prepositions, have been realized on verbs considered without any adjuncts, in their atemporal form and independently of any context, on the one hand, and on prepositions considered independently of any context, on the other. This methodology, discussed in Borillo  Sablayrolles (1993), has allowed us to extract the intrinsic semantics of these lexical items.\\n\\nSince natural languages put together verbs and prepositions in a sentence, we have developped a formal calculus (see (Asher  Sablayrolles, 1994b)), based on these two typologies, which computes, in a compositional way, the spatiotemporal properties of a motion complex from the semantic properties of the verb and of the preposition. For reason of space we cannot detail our formalism here, but we intend to present it in the talk.\\n\\nThe semantics of a motion complex is not the simple addition of the semantics of its constituents. On the contrary, it is the result of a complex interaction between these properties. It is often the case that from this interaction appear new properties that belong neither to the verb or the preposition. These new properties are only the result of the interaction of the verb with the preposition.\\n\\nLet us consider for example the following VP: sortir dans le jardin-to go out into the garden. The verb sortir-to go out implicitly suggests an initial location; the preposition dans-(which means in, but which is translated here by into) is a positional preposition and, as so, only denotes the static spatial relation inside. The location le jardin-the garden is the final location of the motion. This final information was contained neither in the verb or in the preposition. This information is the result of the interaction of the verb sortir-to go out with the preposition dans-in/into.\\n\\nNote that the combination for such items does not behave the same in English, where the final information is explicitly brought by the preposition into, which is a directional preposition, and where this particular combination does not create new information.\\n\\nThis shows the neccesity to take into account such language specific behaviour in natural languages understanding systems and in natural languages machine translation. We formalize with 11 axioms in a non-monotonic first order logic the behaviour of all possible kinds of verb-preposition association for the French language. We use non-monotonic logic in order to represent defeasible or generic rules and also in order to encode defaults about lexical entries.\\n\\nThese axioms are based on the lexical semantics of CoL verbs and of spatial prepositions. They also take into account the syntactic structure of the sentence (we have supposed an X-bar syntax with a VP internal subject, though this is not essential) and the links which exist at the level of discours between this sentence and the previous and following sentences of the text. These links, called discourse relations, are basic concepts on which texts are structured (cf. (Asher, 1993)).\\n\\nConclusion\\n\\nThe study and the first results we have here presented cover from lexical semantics to discourse structures, with strong interactions between these two ends. Indeed, lexical informations can be used to disambiguate the structure of the discours, as well as discourse relations can be used to disambiguate lexical entries, as shown in (Asher  Sablayrolles, 1994b). Our work is based on systematic and very detailed linguistic studies which lead to rather complex computations for calculating the spatiotemporal semantics of a motion complex. But we have seen that this level of detail and complexity is necessary if one want to understand, to formalize and to compute a right spatiotemporal semantics for motion complexes. We continue our investigations on two directions:\\n\\n1. we compare our results with similar works in course of realization on the Basquian language (by Michel Aurnague) and on the Japanese language (by Junichi Saito); 2. we use the results presented here for refining the notions of the Aktionsart, where the structuration of the space in 4 zones can be used to distinguish sub-classes inside the traditional well known classes of aspectual studies.\\n\\nReferences\\n\\nNicholas Asher and Pierre Sablayrolles. 1994a. A Compositional Spatio-temporal Semantics for French Motion Verbs and Spatial PPs. Proccedings of SALT4, Semantics and Linguistic Theory, Rochester, NY, May 6-8, 1994.\\n\\nNicholas Asher and Pierre Sablayrolles. 1994b. A Typology and Discourse Semantics for Motion Verbs and Spatial PPs in French. Journal of Semantics, in press, 1994.\\n\\nNicholas Asher. 1993. Reference to Abstract Objects in Discourse. Kluwer Academic Publishers, 1993.\\n\\nJean Paul Boons. 1985. Prliminaires  la classification des verbes locatifs : les complments de lieu, leurs critres, leurs valeurs aspectuelles. Linguisticae Investigationes, 9(2):195-267, 1985.\\n\\nMario Borillo and Pierre Sablayrolles. 1993. The Semantics of Motion Verbs in French. Proceedings of the 13th International Conference on Natural Language Processing of Avignon, May 24-28, 1993, Avignon, France.\\n\\nJ.S. Gruber. 1965.\\n\\nStudies in Lexical Relations.\\n\\nDoctoral Dissertation, MIT, 1965.\\n\\nRay Jackendoff. 1976. Towards an Explanatory Semantic Representation. Linguistic Inquiry, 7:89-150.\\n\\nRay Jackendoff and Barbara Landau. 1992. ``What'' and ``Where'' in Spatial Language and Spatial Cognition. BBS report, Cambridge University Press, 1992.\\n\\nDany Laur. 1993. La relation entre le verbe et la prposition dans la smantique du dplacement. Language, La couleur des prpositions:47-67, June 1993.\\n\\nLeonard Talmy. 1983. How Language Structures Space. Spatial Orientation: theory, research and application, Pick and Acredolo (eds), Plenum pub. corporation, NY, 1983.\", metadata={'source': '../data/raw/cmplg-xml/9503007.xml'}),\n",
       " Document(page_content=\"INCREMENTAL INTERPRETATION: APPLICATIONS, THEORY, AND RELATIONSHIP TO DYNAMIC SEMANTICS\\n\\nWhy should computers interpret language incrementally? In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling, suggesting that humans perform semantic interpretation before constituent boundaries, possibly word by word. However, possible computational applications have received less attention. In this paper we consider various potential applications, in particular graphical interaction and dialogue. We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations. Finally, we tease apart the relationship between dynamic semantics and incremental interpretation.\\n\\nAPPLICATIONS\\n\\nFollowing the work of, for example, Marslen-Wilson (1973), Just and Carpenter (1980) and Altmann and Steedman (1988), it has become widely accepted that semantic interpretation in human sentence processing can occur before sentence boundaries and even before clausal boundaries. It is less widely accepted that there is a need for incremental interpretation in computational applications.\\n\\nIn the 1970s and early 1980s several computational implementations motivated the use of incremental interpretation as a way of dealing with structural and lexical ambiguity (a survey is given in Haddock 1989). A sentence such as the following has 4862 different syntactic parses due solely to attachment ambiguity (Stabler 1991). put the bouquet of flowers that you gave me for Mothers' Day in the vase that you gave me for my birthday on the chest of drawers that you gave me for Armistice Day. Although some of the parses can be ruled out using structural preferences during parsing (such as Late Closure or Minimal Attachment (Frazier 1979)), extraction of the correct set of plausible readings requires use of real world knowledge. Incremental interpretation allows on-line semantic filtering, i.e. parses of initial fragments which have an implausible or anomalous interpretation are rejected, thereby preventing ambiguities from multiplying as the parse proceeds.\\n\\nHowever, on-line semantic filtering for sentence processing does have drawbacks. Firstly, for sentence processing using a serial architecture (rather than one in which syntactic and semantic processing is performed in parallel), the savings in computation obtained from on-line filtering have to be balanced against the additional costs of performing semantic computations for parses of fragments which would eventually be ruled out anyway from purely syntactic considerations. Moreover, there are now relatively sophisticated ways of packing ambiguities during parsing (e.g. by the use of graph-structured stacks and packed parse forests (Tomita 1985)). Secondly, the task of judging plausibility or anomaly according to context and real world knowledge is a difficult problem, except in some very limited domains. In contrast, statistical techniques using lexeme co-occurrence provide a relatively simple mechanism which can imitate semantic filtering in many cases. For example, instead of judging bank as a financial institution as more plausible than bank as a riverbank in the noun phrase the rich bank, we can compare the number of co-occurrences of the lexemes   rich and bank1 (= riverbank) versus rich and   bank2 (= financial institution) in a semantically analysed corpus. Cases where statistical techniques seem less appropriate are where plausibility is affected by local context.\\n\\nFor example, consider the ambiguous sentence, The decorators painted a wall with cracks in the two contexts The room was supposed to look run-down vs. The clients couldn't afford wallpaper. Such cases involve reasoning with an interpretation in its immediate context, as opposed to purely judging the likelihood of a particular linguistic expression in a given application domain (see e.g. Cooper 1993 for discussion).\\n\\nAlthough the usefulness of on-line semantic filtering during the processing of complete sentences is debatable, filtering has a more plausible role to play in interactive, real-time environments, such as interactive spell checkers (see e.g. Wirn (1990) for arguments for incremental parsing in such environments). Here the choice is between whether or not to have semantic filtering at all, rather than whether to do it on-line, or at the end of the sentence.\\n\\nThe concentration in early literature on using incremental interpretation for semantic filtering has perhaps distracted from some other applications which provide less controversial applications. We will consider two in detail here: graphical interfaces, and dialogue.\\n\\nThe Foundations for Intelligent Graphics Project (FIG) considered various ways in which natural language input could be used within computer aided design systems (the particular application studied was computer aided kitchen design, where users would not necessarily be professional designers). Incremental interpretation was considered to be useful in enabling immediate visual feedback. Visual feedback could be used to provide confirmation (for example, by highlighting an object referred to by a successful definite description), or it could be used to give the user an improved chance of achieving successful reference. For example, if sets of possible referents for a definite noun phrase are highlighted during word by word processing then the user knows how much or how little information is required for successful reference.\\n\\nHuman dialogue, in particular, task oriented dialogue is characterised by a large numbers of self-repairs (Levelt 1983, Carletta et al. 1993), such as hesitations, insertions, and replacements. It is also common to find interruptions requesting extra clarification, or disagreements before the end of a sentence. It is even possible for sentences started by one dialogue participant to be finished by another. Applications involving the understanding of dialogues include information extraction from conversational databases, or computer monitoring of conversations. It also may be useful to include some features of human dialogue in man-machine dialogue. For example, interruptions can be used for early signalling of errors and ambiguities.\\n\\nLet us first consider some examples of self-repair. Insertions add extra information, usually modifiers e.g. start in the middle with ..., in the middle of the paper with a blue disc (Levelt 1983:ex.3) Replacements correct pieces of information e.g. from left again to uh ..., from pink again to blue (Levelt 1983:ex.2) In some cases information from the corrected material is incorporated into the final message. For example, consider : 375) 0The three main sources of data come, uh ..., they can be found in the references 0John noticed that the old man and his wife, uh ..., that the man got into the car and the wife was with him when they left the house 0Every boy took, uh ..., he should have taken a water bottle with him semsr In (a), the corrected material the three main sources of data come provides the antecedent for the pronoun they. In (b) the corrected material tells us that the man is both old and has a wife. In (c), the pronoun he is bound by the quantifier every boy.\\n\\nFor a system to understand dialogues involving self-repairs such as those in (396) would seem to require either an ability to interpret incrementally, or the use of a grammar which includes self repair as a syntactic construction akin to non-constituent coordination (the relationship between coordination and self-correction is noted by Levelt (1983)). For a system to generate self repairs might also require incremental interpretation, assuming a process where the system performs on-line monitoring of its output (akin to Levelt's model of the human self-repair mechanism). It has been suggested that generation of self repairs is useful in cases where there are severe time constraints, or where there is rapidly changing background information (Carletta, p.c. ).\\n\\nA more compelling argument for incremental interpretation is provided by considering dialogues involving interruptions. Consider the following dialogue from the TRAINS corpus (Gross et al., 1993): 398) A: so we should move the engine at Avon, engine E, to ... B: engine E1 A: E1 B: okay A: engine E1, to Bath ... This requires interpretation by speaker B before the end of A's sentence to allow objection to the apposition, the engine at Avon, engine E. An example of the potential use of interruptions in human computer interaction is the following: 399) User: Put the punch onto ... Computer: The punch can't be moved. It's bolted to the floor. In this example, interpretation must not only be before the end of the sentence, but before a constituent boundary (the verb phrase in the user's command has not yet been completed).\\n\\nCURRENT TOOLS\\n\\nSyntax to Semantic Representation\\n\\nIn this section we shall briefly review work on providing semantic representations (e.g. lambda expressions) word by word. Traditional layered models of sentence processing first build a full syntax tree for a sentence, and then extract a semantic representation from this. To adapt this to an incremental perspective, we need to be able to provide syntactic structures (of some sort) for fragments of sentences, and be able to extract semantic representations from these.\\n\\nOne possibility, which has been explored mainly within the Categorial Grammar tradition (e.g. Steedman 1988) is to provide a grammar which can treat most if not all initial fragments as constituents. They then have full syntax trees from which the semantics can be calculated.\\n\\nThe problem of there being an arbitrary number of different partial trees for a particular fragment is reflected in most current approaches to incremental interpretation being either incomplete, or not fully word by word. For example, incomplete parsers have been proposed by Stabler (1991) and Moortgat (1988). Stabler's system is a simple top-down parser which does not deal with left recursive grammars. Moortgat's M-System is based on the Lambek Calculus: the problem of an infinite number of possible tree fragments is replaced by a corresponding problem of initial fragments having an infinite number of possible types. A complete incremental parser, which is not fully word by word, was proposed by Pulman (1986). This is based on arc-eager left-corner parsing (see e.g. Resnik 1992).\\n\\nTo enable complete, fully word by word parsing requires a way of encoding an infinite number of partial trees. There are several possibilities. The first is to use a language describing trees where we can express the fact that John is dominated by the s node, but do not have to specify what it is immediately dominated by (e.g. D-Theory, Marcus et al. 1983). Semantic representations could be formed word by word by extracting `default' syntax trees (by strengthening dominance links into immediated dominance links wherever possible).\\n\\nA second possibility is to factor out recursive structures from a grammar. Thompson et al. (1991) show how this can be done for a phrase structure grammar (creating an equivalent Tree Adjoining Grammar (Joshi 1987)). The parser for the resulting grammar allows linear parsing for an (infinitely) parallel system, with the absorption of each word performed in constant time. At each choice point, there are only a finite number of possible new partial TAG trees (the TAG trees represents the possibly infinite number of trees which can be formed using adjunction). It should again be possible to extract `default' semantic values, by taking the semantics from the TAG tree (i.e. by assuming that there are to be no adjunctions). A somewhat similar system has recently been proposed by Shieber and Johnson (1993).\\n\\nLogical Forms to Semantic Filtering Incremental Quantifier Scoping\\n\\nSo far we have only considered semantic representations which do not involve quantifiers (except for the existential quantifier introduced by the mechanism above).\\n\\nIn sentences with two or more quantifiers, there is generally an ambiguity concerning which quantifier has wider scope. For example, in sentence (a) below the preferred reading is for the same kid to have climbed every tree (i.e. the universal quantifier is within the scope of the existential) whereas in sentence (b) the preferred reading is where the universal quantifier has scope over the existential. 500) 0A tireless kid climbed every tree. 0There was a fish on every plate. Scope preferences sometimes seem to be established before the end of a sentence. For example, in sentence (a) below, there seems a preference for an outer scope reading for the first quantifier as soon as we interpret child. In (b) the preference, by the time we get to e.g. grammar, is for an inner scope reading for the first quantifier. 588) 0A teacher gave every child a great deal of homework on grammar. 0Every girl in the class showed a rather strict new teacher the results of her attempt to get the grammar exercises correct. This intuitive evidence can be backed up by considering garden path effects with quantifier scope ambiguities (called jungle paths by Barwise 1987). The original examples, such as the following, show that every 11 seconds a man is mugged here in New York city. We are here today to interview him showed that preferences for a particular scope are established and are overturned.\\n\\nTo show that preferences are sometimes established before the end of a sentence, and before a potential sentence end, we need to show garden path effects in examples such as the following: put the information that statistics show that every 11 seconds a man is mugged here in New York city and that she was to interview him in her diary Most psycholinguistic experimentation has been concerned with which scope preferences are made, rather than the point at which the preferences are established (see e.g. Kurtzman and MacDonald, 1993). Given the intuitive evidence, our hypothesis is that scope preferences can sometimes be established early, before the end of a sentence. This leaves open the possibility that in other cases, where the scoping information is not particularly of interest to the hearer, preferences are determined late, if at all.\\n\\nIncremental Quantifier Scoping: Implementation\\n\\nTo imitate jungle path phenomena, these plausibility judgements need to feed back into the scoping procedure for the next fragment. For example, if every man is taken to be scoped outside a book after processing the fragment Every man gave a book, then this preference should be preserved when determining the scope for the full sentence Every man gave a book to a child. Thus instead of doing all quantifier scoping at the end of the sentence, each new quantifier is scoped relative to the existing quantifiers (and operators such as negation, intensional verbs etc.). A preliminary implementation achieves this by annotating the semantic representations with node names, and recording which quantifiers are `discharged' at which nodes, and in which order.\\n\\nDYNAMIC SEMANTICS\\n\\nDynamic semantics adopts the view that ``the meaning of a sentence does not lie in its truth conditions, but rather in the way in which it changes (the representation of) the information of the interpreter'' (Groenendijk and Stokhof, 1991). At first glance such a view seems ideally suited to incremental interpretation. Indeed, Groenendijk and Stokhof claim that the compositional nature of Dynamic Predicate Logic enables one to ``interpret a text in an on-line manner, i.e., incrementally, processing and interpreting each basic unit as it comes along, in the context created by the interpretation of the text so far''.\\n\\nPutting these two quotes together is, however, misleading, since it suggests a more direct mapping between incremental semantics and dynamic semantics than is actually possible. In an incremental semantics, we would expect the information state of an interpreter to be updated word by word. In contrast, in dynamic semantics, the order in which states are updated is determined by semantic structure, not by left-to-right order (see e.g. Lewin, 1992 for discussion). For example, in Dynamic Predicate Logic (Groenendijk  Stokhof, 1991), states are threaded from the antecedent of a conditional into the consequent, and from a restrictor of a quantifier into the body. Thus, in interpreting, will buy it right away, if a car impresses him cat the input state for evaluation of John will buy it right away is the output state from the antecedent a car impresses him. In this case the threading through semantic structure is in the opposite order to the order in which the two clauses appear in the sentence.\\n\\nSome intuitive justification for the direction of threading in dynamic semantics is provided by considering appropriate orders for evaluation of propositions against a database: the natural order in which to evaluate a conditional is first to add the antecedent, and then see if the consequent can be proven. It is only at the sentence level in simple narrative texts that the presentation order and the natural order of evaluation necessarily coincide.\\n\\nThe ordering of anaphors and their antecedents is often used informally to justify left-to-right threading or threading through semantic structure. However, threading from left-to-right disallows examples of optional cataphora, as in example (635), and examples of compulsory cataphora as in: her, every girl could see a large crack Similarly, threading from the antecedents of conditionals into the consequent fails for examples such as: boy will be able to see out of a window if he wants to It is also possible to get sentences with `donkey' readings, but where the indefinite is in the consequent: student will attend the conference if we can get together enough money for her air fare airfare This sentence seems to get a reading where we are not talking about a particular student (an outer existential), or about a typical student (a generic reading). Moreover, as noted by Zeevat (1990), the use of any kind of ordered threading will tend to fail for Bach-Peters sentences, such as: man who loves her appreciates a woman who lives with him bach For this kind of example, it is still possible to use a standard dynamic semantics, but only if there is some prior level of reference resolution which reorders the antecedents and anaphors appropriately. For example, if (665) is converted into the `donkey' sentence: man who loves a woman who lives with him appreciates her\\n\\nWhen we consider threading of possible worlds, as in Update Semantics (Veltman 1990), the need to distinguish between the order of evaluation and the order of presentation becomes more clear cut. Consider trying to perform threading in left-to-right order during interpretation of the sentence, John left if Mary left. After processing the proposition John left the set of worlds is refined down to those worlds in which John left. Now consider processing if Mary left. Here we want to reintroduce some worlds, those in which neither Mary or John left. However, this is not allowed by Update Semantics which is eliminative: each new piece of information can only further refine the set of worlds.\\n\\nIt is worth noting that the difficulties in trying to combine eliminative semantics with left-to-right threading apply to constraint-based semantics as well as to Update Semantics. Haddock (1987) uses incremental refinement of sets of possible referents. For example, the effect of processing the rabbit in the noun phrase the rabbit in the hat is to provide a set of all rabbits. The processing of in refines this set to rabbits which are in something. Finally, processing of the hat refines the set to rabbits which are in a hat. However, now consider processing the rabbit in none of the boxes. By the time the rabbit in has been processed, the only rabbits remaining in consideration are rabbits which are in something. This incorrectly rules out the possibility of the noun phrase referring to a rabbit which is in nothing at all. The case is actually a parallel to the earlier example of Mary introduced someone to something being inappropriate if the final sentence is Mary introduced noone to anybody.\\n\\nAlthough this discussion has argued that it is not possible to thread the states which are used by a dynamic or eliminative semantics from left to right, word by word, this should not be taken as an argument against the use of such a semantics in incremental interpretation. What is required is a slightly more indirect approach. In the present implementation, semantic structures (akin to logical forms) are built word by word, and each structure is then evaluated independently using a dynamic semantics (with threading performed according to the structure of the logical form).\\n\\nIMPLEMENTATION\\n\\nCONCLUSIONS\\n\\nThe paper described some potential applications of incremental interpretation. It then described the series of steps required in mapping from initial fragments of sentences to propositions which can be judged for plausibility. Finally, it argued that the apparently close relationship between the states used in incremental semantics and dynamic semantics fails to hold below the sentence level, and briefly presented a more indirect way of using dynamic semantics in incremental interpretation.\\n\\nREFERENCES\\n\\nAlshawi, H. (1990). Resolving Quasi Logical Forms. Computational Linguistics, 16, p.133-144. Altmann, G.T.M. and M.J. Steedman (1988). Interaction with Context during Human Speech Comprehension. Cognition, 30, p.191-238. Barwise, J. (1987). Noun Phrases, Generalized Quantifiers and Anaphors. In P. Gardenfors, Ed., Generalized Quantifiers, p.1-29, Dordrecht: Reidel. Carletta, J., R. Caley and S. Isard (1993). A Collection of Self-repairs from the Map Task Corpus. Research Report, HCRC/TR-47, University of Edinburgh. Chater, N., M.J. Pickering and D.R. Milward (1994). What is Incremental Interpretation? ms. To appear in Edinburgh Working Papers in Cognitive Science. Cooper, R. (1993). A Note on the Relationship between Linguistic Theory and Linguistic Engineering. Research Report, HCRC/RP-42, University of Edinburgh. Frazier, L. (1979). On Comprehending Sentences: Syntactic Parsing Strategies. Ph.D. Thesis, University of Connecticut. Published by Indiana University Linguistics Club. Groenendijk, J. and M. Stokhof (1991). Dynamic Predicate Logic. Linguistics and Philosophy, 14, p.39-100. Gross, D., J. Allen and D. Traum (1993). The TRAINS 91 Dialogues. TRAINS Technical Note 92-1, Computer Science Dept., University of Rochester. Haddock, N.J. (1987). Incremental semantic interpretation and incremental syntactic analysis. Ph.D. Thesis, University of Edinburgh. Haddock, N.J. (1989). Computational Models of Incremental Semantic Interpretation.\\n\\nLanguage and Cognitive Processes, 4, (3/4), Special Issue, p.337-368. Hobbs, J.R. and S.M. Shieber (1987). An Algorithm for Generating Quantifier Scoping. Computational Linguistics, 3, p47-63. Joshi, A.K. (1987). An Introduction to Tree Adjoining Grammars. In Manaster-Ramer, Ed., Mathematics of Language, Amsterdam: John Benjamins. Just, M. and P. Carpenter (1980). A Theory of Reading from Eye Fixations to Comprehension. Psychological Review, 87, p.329-354. Kurtzman, H.S. and M.C. MacDonald (1993). Resolution of Quantifier Scope Ambiguities. Cognition, 48(3), p.243-279. Lewin, I. (1990). A Quantifier Scoping Algorithm without a Free Variable Constraint. In Proceedings of COLING 90, Helsinki, vol 3, p.190-194. Lewin, I. (1992). Dynamic Quantification in Logic and Computational Semantics. Research report, Centre for Cognitive Science, University of Edinburgh. Levelt, W.J.M. (1983). Modelling and Self-Repair in Speech. Cognition, 14, p.41-104. Marcus, M., D. Hindle, and M. Fleck (1983). D-Theory: Talking about Talking about Trees. In Proceedings of the 21st ACL, Cambridge, Mass. p.129-136. Marslen-Wilson, W. (1973). Linguistic Structure and Speech Shadowing at Very Short Latencies. Nature, 244, p.522-523. Mellish, C.S. (1985). Computer Interpretation of Natural Language Descriptions. Chichester: Ellis Horwood. Milward, D.R. (1991). Axiomatic Grammar, Non-Constituent Coordination, and Incremental Interpretation. Ph.D. Thesis, University of Cambridge. Milward, D.R. (1992).\\n\\nDynamics, Dependency Grammar and Incremental Interpretation. In Proceedings of COLING 92, Nantes, vol 4, p.1095-1099. Moortgat, M. (1988). Categorial Investigations: Logical and Linguistic Aspects of the Lambek Calculus, Dordrecht: Foris. Pulman, S.G. (1986). Grammars, Parsers, and Memory Limitations. Language and Cognitive Processes, 1(3), p.197-225. Resnik, P. (1992). Left-corner Parsing and Psychological Plausibility. In Proceedings of COLING 92, Nantes, vol 1, p.191-197. Shieber, S.M. and M. Johnson (1993). Variations on Incremental Interpretation. Journal of Psycholinguistic Research, 22(2), p.287-318. Shieber, S.M. and Y. Schabes (1990). Synchronous Tree-Adjoining Grammars. In Proceedings of COLING 90, Helsinki, vol 3, p.253-258. Stabler, E.P. (1991). Avoid the pedestrian's paradox. In R. Berwick, S. Abney, and C. Tenny, Eds., Principle-Based Parsing: Computation and Psycholinguistics. Kluwer. Steedman, M. (1988). Combinators and Grammars. In R. Oehrle et al., Eds., Categorial Grammars and Natural Language Structures, p.417-442. Thompson, H., M. Dixon, and J. Lamping (1991). Compose-Reduce Parsing. In Proceedings of the 29th ACL, p.87-97. Tomita, M. (1985). Efficient Parsing for Natural Language. Kluwer. Veltman F. (1990). Defaults in Update Semantics. In H. Kamp, Ed., Conditionals, Defaults and Belief Revision, DYANA Report 2.5.A, Centre for Cognitive Science, University of Edinburgh. Wirn, M. (1990). Incremental Parsing and Reason Maintenance.\\n\\nIn Proceedings of COLING 90, Helsinki, vol 3, p.287-292. Zeevat, H. (1990). Static Semantics. In J. van Benthem, Ed., Partial and Dynamic Semantics I, DYANA Report 2.1.A, Centre for Cognitive Science, University of Edinburgh.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9503013.xml'}),\n",
       " Document(page_content=\"A Specification Language for Lexical Functional Grammars Introduction\\n\\nUnlike most linguistic theories, LFG (see Kaplan and Bresnan (1982)) treats grammatical relations as first class citizens. Accordingly, it casts its linguistic analyses in terms of a composite ontology: two independent domains -- a domain of constituency information (c-structure), and a domain of grammatical function information (f-structure) -- linked together in a mutually constraining manner. As has been amply demonstrated over the last fifteen years, this view permits perspicuous analyses of a wide variety of linguistic data.\\n\\nThe key idea underlying our approach is to think about LFG model theoretically. That is, our first task will be to give a precise -- and transparent -- mathematical picture of the LFG ontology. As has already been noted, the basic entities underlying the LFG analyses are composite structures consisting of a finite tree, a finite feature structure, and a function that links the two. Such structures can straightforwardly be thought of as models, in the usual sense of first order model theory (see Hodges (1993)). Viewing the LFG ontology in such terms does no violence to intuition: indeed, as we shall see, a more direct mathematical embodiment of the LFG universe can hardly be imagined.\\n\\nOnce the ontological issues have been settled we turn to our ultimate goal: providing a specification language for LFG grammars. Actually, with the ontological issues settled it is a relatively simple task to devise suitable specification languages: we simply consider how LFG linguists talk about such structures when they write grammars. That is, we ask ourselves what kind of constraints the linguist wishes to impose, and then devise a language in which they can be stated.\\n\\nThus we shall proceed as follows. After a brief introduction to LFG, we isolate a class of models which obviously mirrors the composite nature of the LFG ontology, and then turn to the task of devising a language for talking about them. We opt for a particularly simple specification language: a propositional language enriched with operators for talking about c- and f-structures, together with a path equality construct for enforcing synchronisation between the two domains. We illustrate its use by showing how to capture the effect of schemata annotated rules, and the LFG uniqueness, completeness and coherence principles.\\n\\nBefore proceeding, a word of motivation is in order. Firstly, we believe that there are practical reasons for interest in grammatical specification languages: formal specification seems important (perhaps essential) if robust large scale grammars are to be defined and maintained. Moreover, the essentially model theoretic slant on specification we propose here seems particularly well suited to this aim. Models do not in any sense ``code'' the LFG ontology: they take it pretty much at face value. In our view this is crucial. Formal approaches to grammatical theorising should reflect linguistic intuitions as directly as possible, otherwise they run the risk of being an obstacle, not an aid, to grammar development.\\n\\nThe approach also raises theoretical issues. The model theoretic approach to specification languages forces one to think about linguistic ontologies in a systematic way, and to locate them in a well understood mathematical space. This has at least two advantages. Firstly, it offers the prospect of meaningful comparison of linguistic frameworks. Secondly, it can highlight anomalous aspects of a given system. For example, as we shall later see, there seems to be no reasonable way to deal with LFG's =cdefinitions using the simple models of the present paper. There is a plausible  model theoretic strategy strategy for extending our account to cover =c; but the nature of the required extension clearly shows that =c is of a quite different character to the bulk of LFG. We discuss the matter in the paper's concluding section.\\n\\nLexical Functional Grammar\\n\\nA lexical functional grammar consists of three main components: a set of context free rules annotated with schemata, a set of well formedness conditions on feature structures, and a lexicon. The role of these components is to assign two interrelated structures to any linguistic entity licensed by the grammar: a tree (the c-structure) and a feature structure (the f-structure). Briefly, the context free skeleton of the grammar rules describes the c-structure, the well-formedness conditions restrict f-structure admissibility, and the schemata synchronise the information contained in the c- and f-structures.\\n\\nGiven the above lexical entries, it is possible to assign a correctly interrelated c-structure and f-structure to the sentence A girl walks. Moreover, the resulting f-structure respects the LFG well formedness conditions, namely the uniqueness, completeness and coherence principles discussed in section 5. Thus A girl walks is accepted by this grammar.\\n\\nModeling the LFG ontology\\n\\nThe ontology underlying LFG is a composite one, consisting of trees, feature structures and links between the two. Our first task is to mathematically model this ontology, and to do so as transparently as possible. That is, the mathematical entities we introduce should clearly reflect the intuitions important to LFG theorising -- ``No coding! '', should be our slogan. In this section, we introduce such a representation of LFG ontology. In the following section, we shall present a formal language  for talking about this representation; that is, a language for specifying LFG grammars.\\n\\nFinally, we take zoomin, the link between c-structure and f-structure information, to be a partial function from T to W.  This completes our mathematical picture of LFG ontology. It is certainly a precise picture (all three components, and how they are related are well defined), but, just as importantly, it is also a faithful picture;   models capture the LFG ontology perspicuously.\\n\\nA Specification Language\\n\\nAlthough models pin down the essence of the LFG universe, our work has only just begun. For a start, not all models are created equal. Which of them correspond to grammatical utterances of English? Of Dutch? Moreover, there is a practical issue to be addressed: how should we go about saying which models we deem `good'? To put in another way, in what medium should we specify grammars?\\n\\nNow, it is certainly possible to talk about models using natural language (as readers of this paper will already be aware) and for many purposes (such as discussion with other linguists) natural language is undoubtedly the best medium. However, if our goal is to specify large scale grammars in a clear, unambiguous manner, and to do so in such a way that our grammatical analyses are machine verifiable, then the use of formal specification languages has obvious advantages. But which formal specification language? There is no  single best answer: it depends on one's goals. However there are some important rules of thumb: one should carefully consider the expressive capabilities required; and a judicious commitment to simplicity and elegance will probably pay off in the long run. Bearing this advice in mind, let us consider the nature of LFG grammars.\\n\\nSpecifying Grammars\\n\\nNow for some examples. Let's first consider how to write specifications which capture the effect of schemata annotated grammar rules. Suppose we want to capture the meaning of rule (1) of Figure 1, repeated here for convenience:\\n\\nRecall that this annotated rule licenses  structures consisting of a binary tree whose mother node m is labeled S and whose daughter nodes n1 and n2 are labeled NP and VP respectively; and where, furthermore, the S and VP nodes (that is, m and n2) are related to the same f-structure node w; while the NP node (that is, n1) is related to the node w' in the f-structure that is reached by making a subj transition from w.\\n\\nThis is precisely the kind of structural constraint that  is designed to specify. We do so as follows:\\n\\nThis says that if we are at a c-struct node which has at least one daughter (that is, a non-terminal node) then one of the subtree licensing disjuncts (or `rules') must be satisfied there. This picks precisely those models in which all the tree nodes are appropriately licensed. Note that the statement is indeed valid in such models: it is true at all the non-terminal nodes, and is vacuously satisfied at terminal tree nodes and nodes of f-struct.\\n\\nWe now turn to the second main component of LFG, the well formedness conditions on f-structures.\\n\\nConsider first the uniqueness principle. In essence, this principle states that in a given f-structure, a particular attribute may have at most one value. In  this restriction is `built in': it follows from the choices made concerning the mathematical objects composing models. Essentially, the uniqueness principle is enforced by two choices. First, Vf associates atoms only with final nodes of f-structures; and as Vf is a function, the atom so associated is unique. In effect, this hard-wires prohibitions against constant-compound and constant-constant clashes into the semantics of . Second, we have modeled features as partial functions on the f-structure nodes - this ensures that any complex valued attribute is either undefined, or is associated with a unique sub-part of the current f-structure. In short, as required, any attribute will have at most one value.\\n\\nFinally, consider the counterpart of the completeness principle, the coherence principle. This applies to the same attributes as the completeness principle and requires that whenever they occur in an f-structure they must also occur in the f-structure associated with its pred attribute. This is tantamount to demanding the validity of the following formula:\\n\\nConclusion\\n\\nThe point is this. Although the LFG equations discussed so far were defining equations, LFG also allows  so-called constraining equations  (written =c). Kaplan and Bresnan explain the difference as follows. Defining equations allow  a feature-value pair to be inserted into an f-structure providing no conflicting information is present. That is, they add a feature value pair to any consistent f-structure. In contrast, constraining equations are intended to constrain the value of an already existing feature-value pair. The essential difference is that constraining equations require that the feature under consideration already has a value, whereas defining equations apply independently of the feature value instantiation level.\\n\\nIn short, constraining equations are essentially a global check on completed structures which require the presence of certain feature values. They have an eminently procedural character, and there is no obvious way to handle this idea in the present set up. The bulk of LFG involves stating constraints about a single model, and  is well equipped for this task, but constraining equations involve looking at the structure of other possible parse trees. (In this respect they are reminiscent of the feature specification defaults of GPSG.) The approach of the present paper has been driven by the view that (a) models capture the essence of LFG ontology, and, (b) the task of the linguist is to explain, in terms of the relations that exist within a single model, what grammatical structure is. Most of the discussion in Kaplan and Bresnan (1982) is conducted in such terms. However constraining equations broaden the scope of the permitted discourse; basically, they allow implicit appeal to possible derivational structure. In short, in common with most of the grammatical formalisms with which we are familiar, LFG seems to have a dynamic residue that resists a purely declarative analysis. What should be done?\\n\\nWe see three possible responses. Firstly, we note that the model theoretic approach can almost certainly be extended to cover constraining equations. The move involved is analogous to the way first order logic (a so-called `extensional' logic) can be extended to cope with intensional notions such as belief and necessity. The basic idea -- it's the key idea underlying first order Kripke semantics -- is to move from dealing with a single model to dealing with a collection of models linked by an accessibility relation. Just as quantification over possible states of affairs yields analyses of intensional phenomena, so quantification over related models could provide a `denotational semantics' for =c. Preliminary work suggests that the required structures have formal similarities to the structures used in preferential semantics for default and non-monotonic reasoning. This first response seems to be a very promising line of work: the requisite tools are there, and the approach would tackle a full blooded version of LFG head on. The drawback is the complexity it introduces into an (up till now) quite simple story. Is such additional complexity really needed?\\n\\nA second response is to admit that there is a dynamic residue, but to deal with it in overtly computational terms. In particular, it may be possible to augment our approach with an explicit operational semantics, perhaps the evolving algebra approach adopted by Moss and Johnson (1994). Their approach is attractive, because it permits a computational treatment of dynamism that abstracts from low level algorithmic details. In short, the second strategy is a `divide and conquer' strategy: treat structural issues using model theoretic tools, and procedural issues with (revealing) computational tools. It's worth remarking that this second response is not incompatible with the first; it is common to provide programming languages with both a denotational and an operational semantics.\\n\\nThe third strategy is both simpler and more speculative. While it certainly seems to be the case that LFG (and other `declarative' formalisms) have procedural residues, it is far from clear that these residues are necessary. One of the most striking features of LFG (and indeed, GPSG) is the way that purely structural (that is, model theoretic) argumentation dominates. Perhaps the procedural aspects are there more or less by accident? After all, both LFG and GPSG drew on (and developed) a heterogeneous collection of traditional grammar specification tools, such as context free rules, equations, and features. It could be the case such procedural residues as =c are simply an artifact of using the wrong tools for talking about models. If this is the case, it might be highly misguided to attempt to capture =c using a logical specification language. Better, perhaps, would be to draw on what is good in LFG and to explore the logical options that arise naturally when the model theoretic view is taken as primary. Needless to say, the most important task that faces this third response is to get on with the business of writing grammars; that, and nothing else, is the acid test.\\n\\nIt is perhaps worth adding that at present the authors simply do not know what the best response is. If nothing else, the present work has made very clear to us that the interplay of static and dynamic ideas in generative grammar is a delicate and complex matter which only further work can resolve.\\n\\nBibliography\\n\\nPatrick Blackburn and Wilfried Meyer-Viol. 1994. Linguistics, Logic and Finite Trees. Bulletin of the IGPL, 2, pp. 3-29. Available by anonymous ftp from theory.doc.ic.ac.uk, directory theory/forum/igpl/Bulletin.\\n\\nWilfrid Hodges.\\n\\n1993.\\n\\nModel Theory.\\n\\nCambridge University Press.\\n\\nRon Kaplan and Joan Bresnan. 1982. Lexical-Functional Grammar: A formal system for grammatical representation. In The Mental Representation of Grammatical Relations, pp. 173 - 280, MIT Press.\\n\\nR. Kasper and W. Rounds. 1990. The Logic of Unification in Grammar. Linguistics and Philosophy, 13, pp. 33-58.\\n\\nLawrence Moss and David Johnson. 1994. Dynamic Interpretations of Constraint-Based Grammar Formalisms. To appear in Journal of Logic, Language and Information.\\n\\nFootnotes\\n\\nThis paper is based upon the original formulation of LFG, that of Kaplan and Bresnan (1982), and  will not discuss such later innovations as functional uncertainty.\", metadata={'source': '../data/raw/cmplg-xml/9503005.xml'}),\n",
       " Document(page_content=\"An Implemented Formalism for Computing Linguistic Presuppositions and Existential Commitments\\n\\nWe rely on the strength of linguistic and philosophical perspectives in constructing a framework that offers a unified explanation for presuppositions and existential commitment. We use a rich ontology and a set of methodological principles that embed the essence of Meinong's philosophy and Grice's conversational principles into a stratified logic, under an unrestricted interpretation of the quantifiers. The result is a logical formalism that yields a tractable computational method that uniformly calculates all the presuppositions of a given utterance, including the existential ones. 2\\n\\nIntroduction\\n\\nThe study of presuppositions is primarily a study of commitment -- commitment to the existence of presupposed definite referents or to the truth of factive complements. The reduction of presupposition to entailment is inadequate because presuppositions are implied, not specified; they are not part of the truth conditions of natural language sentences, and they can be cancelled in negative environments. Trying to explain the whole phenomenon and to provide solutions for the projection problem, linguists have often omitted any explanation for the existential commitment of definite references or their explanation has been a superficial one. Similarly, philosophers who have studied existence and nonexistence have been more concerned with providing formal tools for manipulation of nonexistent objects than tools to capture our commonsense commitment. This puts us in a difficult position. From a linguistic perspective, the literature provides a good set of theories able to more or less explain the commitment to the presupposed truth of factives and the like but not the existential commitment of definite references. From a philosophical perspective, we have quite a few theories which deal with existence and nonexistence, but they too offer no explanation for existential commitment.\\n\\nWe first review the philosophical approaches in studying existence and nonexistence and the linguistic approaches in studying presuppositions, emphasizing their (in)ability to deal with presuppositions and nonexistence respectively. We give a brief introduction to stratified logic, its implementation, and explain the methodological principles of our approach. In section 4 we show how this approach is able not only to deal with nonexistence but also able to explain the existential commitment of definite reference. The rest of the paper is dedicated to a comparison with Parsons's and Hobbs's work.\\n\\nWhat philosophers and linguists have to say Nonexistence and commitment in philosophy and logic Theories of linguistic presupposition  and their relation to  (non)existence Reasoning in stratified logic\\n\\nbecause Mary came to the party is defeasible: John does not regret that Mary came to the party because she did not come.\\n\\nPresuppositions as defeasible information Methodological principles for our approach\\n\\nMC1. Every uttered sentence is ``directed'' towards an ``object'', because every uttered sentence can be seen as a materialization of a mental act.\\n\\nMC2. All uttered sentences  exist (technically, ``have being''). However, this does not imply the existence of their referents,  which are ``ausserseiend'' (beyond being and non-being).\\n\\nMC3. It is not self-contradictory to deny, nor tautologous to affirm, the existence of a referent.\\n\\nMC4. Every referent and every uttered sentence  has properties.\\n\\nMC5. The principles MC2 and MC4 are not inconsistent.\\n\\nCorollary: Even referents of an uttered sentence that do not exist have properties.\\n\\nMC6. (a) Every set of properties (Sosein) corresponds to the utterance of a sentence. (b) Every object of thought can be uttered.\\n\\nMC7. Some referents of an utterance are incomplete (undetermined with respect to some properties).\\n\\nFormalizing presuppositions\\n\\nDefinition: Presuppositions are  defeasible information that is derived from knowledge of language use and that is included in the most optimistic models of a theory described in terms of stratified logic under an unrestricted interpretation of the quantifiers.\\n\\nWhat the approach can do with existent and nonexistent objects\\n\\nAssume that someone utters the sentence The king of Buganda is (not) bald. If we know nothing about Buganda and its king, the complete theory of this utterance and the available knowledge in stratified logic is this:\\n\\nThis theory has one optimistic model that reflects  one's commitment to the king's existence. The king's existence has the status of defeasible information; it is derived using knowledge of language use and is a presupposition of the utterance.\\n\\nFor this theory, we obtain only one model schema:\\n\\nAsserting existence or nonexistence affects defeasible inferences due to knowledge of language use and restricts some of the models. If someone utters The king of Buganda exists and  we know nothing about Buganda, the translation\\n\\ngives  one  model:\\n\\nIf we know that the king of Buganda does not exist, or in other words we evaluate the above sentence against a knowledge base that contains\\n\\nthere is no model for this theory, so the utterance is interpreted as false. It is noteworthy that the inconsistency appears due to specific knowledge about the king's physical existence and not because of a quantification convention as in classical first-order logic. On the other hand, the negation, The king of Buganda does not exist, is consistent with the knowledge base and provides this model:\\n\\nSo far, we have emphasized the way presuppositions of definite references can be handled in this framework. However, the proposed method is general in the sense that it captures the other presuppositional environments as well. Moreover, the cancellation can occur at any moment in discourse. Consider for example the utterance John does not regret that Mary came to the party. Its formalization in stratified logic follows:\\n\\nThe optimistic model computed by our program is this:\\n\\nThis model reflects our intuitions that Mary came to the party and all definite references exist.\\n\\nIf one utters now Of course he doesn't. Mary did not come to the party, the new model computed by our program will reflect the fact that a presupposition has been cancelled, even though this cancellation occurred later in the discourse. Thus, the new optimistic model will be this:\\n\\nOur approach correctly handles references to unactualized objects such as averted strikes or the paper that we had wanted to submit to AAAI-94. The utterance The strike was averted can be formalized thus:\\n\\nThis gives one optimistic model:\\n\\nA comparison with Parsons's and Hobbs's work On Parsons's evidence for his theory of nonexistence\\n\\nParsons argues that is impossible to distinguish between the shape of the logical form of two sentences like these, in which one subject is fictional and the other is real: a. Sherlock Holmes is more famous than any other detective.\\n\\nb. Pel is more famous than any other soccer player. In our approach, similar syntactic translations give different semantic models when interpreted against different knowledge bases. A complete theory for the first sentence is this:\\n\\nThis theory gives only one model:\\n\\nThe theory for the second sentence is this:\\n\\nThis theory exhibits one optimistic model:\\n\\nAs seen, it is needless to mention the existence of specific objects in the knowledge base. The model-ordering relation rejects anyhow models that are not optimistic. In this way, the commitment to Pel's existence is preserved, and appears as a presupposition of the utterance. Parsons's theory provides different logical forms for the above sentences, but fails to avoid the commitment to nonexistent objects.\\n\\nOn Russell's arguments against Meinong's nonexistent objects\\n\\nEven if we agree now that Round squares are round and Round squares are square, we cannot apply Modus Ponens because the existence of round squares is not asserted; therefore, the contradiction vanishes.\\n\\nThe second Russellian objection was this: consider the existent golden mountain; by the satisfaction principle it is golden, it is a mountain, and it exists. Therefore, some gold mountain exists, which is empirically false. The apparent puzzle found in Meinong's answer, The existent golden mountain is existent, but it does not exist, is consistent with our theory, because the adjective existent brings nothing new to our knowledge -- we are already committed to the mountain's existence. The only difference is that this commitment is defeasible, so the model in which it is not acceptable to believe in the existence of the mountain will survive our core knowledge. The translation is\\n\\nA comparison with Hobbs's work\\n\\nWe have mentioned that Hobbs's transparency pertains to relations and not to objects. In our approach, a sentence such as Ross worships Zeus can be satisfied by a set of semantic models that correspond to each possible combination of the existence and non-existence of Ross and Zeus.\\n\\nAmong them, only one is minimal: the one that explains the commitment to both  Ross's  and Zeus's existence.\\n\\nBut let us  assume we know that there is no  entity in the real world that enjoys the property of being Zeus, but rather one who exists outside the real world as a god (EOW! [u]).\\n\\nThis theory is no longer satisfiable by a model in which Zeus exists as a physical entity. However, the optimistic model explains our commitment to Ross's existence.\\n\\nConclusion\\n\\nJoining Meinong's philosophy of nonexistence with Grice's conversational principles provides a very strong motivation for a uniform treatment of linguistic presuppositions. Lejewski's unrestricted interpretation of the quantifiers, Hirst's ontology, and the notion of reasoning with stratified tableaux and model-ordering in stratified logic provide the formal tools to implement the principles. This amounts to a model-theoretic definition for presuppositions that is able to offer a uniform treatment for linguistic presuppositions and an explanation for the existential commitment. A computationally tractable method can be derived from the formalism. Its implementation in Common Lisp finds the natural language presuppositions, including the existential ones, and correctly reflects their cancellation.\\n\\nAcknowledgements\\n\\nThis research was supported in part by a grant from the Natural Sciences and Engineering Research Council of Canada.\\n\\nBibliography\\n\\nJ.D. Atlas. What are negative existence statements about? Linguistics and Philosophy, 11:373-394, 1988.\\n\\nG. Frege. ber sinn und bedeutung. Z. Philos. Philos. Kritik, 100:373-394, 1892. reprinted as: On Sense and Nominatum, In Feigl H. and Sellars W., editors, Readings in Philosophical Analysis, pages 85-102, Appleton-Century-Croft, New York, 1947.\\n\\nG.J.M. Gazdar. Pragmatics: Implicature, Presupposition, and Logical Form. Academic Press, 1979.\\n\\nH.P. Grice. Logic and conversation. In Cole P. and Morgan J.L., editors, Syntax and Semantics, Speech Acts, volume 3, pages 41-58. Academic Press, 1975.\\n\\nK.J.J. Hintikka. Existential presuppositions and existential commitments. Journal of Philosophy, 56:125-137, 1959.\\n\\nG. Hirst. Existence assumptions in knowledge representation. Artificial Intelligence, 49:199-242, 1991.\\n\\nJ.R. Hobbs. Ontological promiscuity. In Proceedings 23rd Annual Meeting of the Association for Computational Linguistics, pages 61-69, 1985.\\n\\nL. Karttunen.\\n\\nPresuppositions of compound sentences.\\n\\nLinguistic Inquiry, 4(2):169\\n\\n\\n\\n193, 1973.\\n\\nP. Kay.\\n\\nThe inheritance of presuppositions.\\n\\nLinguistics  Philosophy, 15:333\\n\\n\\n\\n379, 1992.\\n\\nC. Lejewski. Logic and existence. British Journal for the Philosophy of Science, 5:104-119, 1954.\\n\\nD. Marcu. A formalism and an algorithm for computing pragmatic inferences and detecting infelicities. Master's thesis, Dept. of Computer Science, University of Toronto, September 1994. Also published as Technical Report CSRI-309, Computer Systems Research Institute, University of Toronto.\\n\\nA. Meinong. ber gegenstandstheorie. In Meinong A., editor, Untersuchungen zur Gegenstandstheorie und Psychologie. Barth, Leipzig, 1904. reprinted in: The theory of objects, Chisholm R.M. editor,   Realism and the Background of Phenomenology, pages 76-117. Free Press, Glencoe, IL, 1960.\\n\\nR.E. Mercer. A Default Logic Approach to the Derivation of Natural Language Presuppositions. PhD thesis, Department of Computer Science, University of British Columbia, 1987.\\n\\nR.E. Mercer.\\n\\nPersonal communication, 1993.\\n\\nT. Parsons. Nonexistent Objects. Yale University Press, New Haven, CT, 1980.\\n\\nW.V.O. Quine. Designation and existence. In Feigl H. and Sellars W., editors, Readings in Philosophical Analysis, pages 44-51. Appleton-Century-Croft, New York, 1949.\\n\\nW.J. Rapaport. Meinongian semantics for propositional semantic networks. In Proceedings 23rd Annual Meeting of the Association for Computational Linguistics, pages 43-48, 1985.\\n\\nR. Reiter. A logic for default reasoning. Artificial Intelligence, 13:81-132, 1980.\\n\\nB. Russell. On denoting. Mind n.s., 14:479-493, 1905. reprinted in: Feigl H. and Sellars W. editors, Readings in Philosophical Analysis, pages 103-115. Appleton-Century-Croft, New York, 1949.\\n\\nR.A. van der Sandt. Presupposition projection as anaphora resolution. Journal of Semantics, 9:333-377, 1992.\\n\\nJ.M. Siskind and D.A. McAllester. Nondeterministic Lisp as a substrate for constraint logic programming. In Proceedings of the Twelfth National Conference on Artificial Intelligence, pages 133-138, 1993.\\n\\nJ.M. Siskind and D.A. McAllester. Screamer: A portable efficient implementation of nondeterministic Common Lisp. Technical Report IRCS-93-03, University of Pennsylvania, Institute for Research in Cognitive Science, July 1 1993.\\n\\nS. Soames. How presuppositions are inherited: A solution to the projection problem. Linguistic Inquiry, 13(3):483-545, Summer 1982.\\n\\nH. Zeevat. Presupposition and accommodation in update semantics. Journal of Semantics, 9:379-412, 1992.\", metadata={'source': '../data/raw/cmplg-xml/9504018.xml'}),\n",
       " Document(page_content=\"Statistical Decision\\n\\n\\n\\nTree Models for\\n\\nParsing1\\n\\nSyntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86% precision, 86% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.\\n\\nIntroduction\\n\\nParsing a natural language sentence can be viewed as making a sequence of disambiguation decisions: determining the part-of-speech of the words, choosing between possible constituent structures, and selecting labels for the constituents. Traditionally, disambiguation problems in parsing have been addressed by enumerating possibilities and explicitly declaring knowledge which might aid the disambiguation process. However, these approaches have proved too brittle for most interesting natural language problems.\\n\\nThis work addresses the problem of automatically discovering the disambiguation criteria for all of the decisions made during the parsing process, given the set of possible features which can act as disambiguators. The candidate disambiguators are the words in the sentence, relationships among the words, and relationships among constituents already constructed in the parsing process.\\n\\nSince most natural language rules are not absolute, the disambiguation criteria discovered in this work are never applied deterministically. Instead, all decisions are pursued non-deterministically according to the probability of each choice. These probabilities are estimated using statistical decision tree models. The probability of a complete parse tree (T) of a sentence (S) is the product of each decision (di) conditioned on all previous decisions:\\n\\nEach decision sequence constructs a unique parse, and the parser selects the parse whose decision sequence yields the highest cumulative probability. By combining a stack decoder search with a breadth-first algorithm with probabilistic pruning, it is possible to identify the highest-probability parse for any sentence using a reasonable amount of memory and time.\\n\\nThe claim of this work is that statistics from a large corpus of parsed sentences combined with information-theoretic classification and training algorithms can produce an accurate natural language parser without the aid of a complicated knowledge base or grammar. This claim is justified by constructing a parser, called SPATTER (Statistical PATTErn Recognizer), based on very limited linguistic information, and comparing its performance to a state-of-the-art grammar-based parser on a common task. It remains to be shown that an accurate broad-coverage parser can improve the performance of a text processing application. This will be the subject of future experiments.\\n\\nOne of the important points of this work is that statistical models of natural language should not be restricted to simple, context-insensitive models. In a problem like parsing, where long-distance lexical information is crucial to disambiguate interpretations accurately, local models like probabilistic context-free grammars are inadequate. This work illustrates that existing decision-tree technology can be used to construct and estimate models which selectively choose elements of the context which contribute to disambiguation decisions, and which have few enough parameters to be trained using existing resources.\\n\\nI begin by describing decision-tree modeling, showing that decision-tree models are equivalent to interpolated n-gram models. Then I briefly describe the training and parsing procedures used in SPATTER. Finally, I present some results of experiments comparing SPATTER with a grammarian's rule-based statistical parser, along with more recent results showing SPATTER applied to the Wall Street Journal domain.\\n\\nDecision\\n\\n\\n\\nTree Modeling\\n\\nMuch of the work in this paper depends on replacing human decision-making skills with automatic decision-making algorithms. The decisions under consideration involve identifying constituents and constituent labels in natural language sentences. Grammarians, the human decision-makers in parsing, solve this problem by enumerating the features of a sentence which affect the disambiguation decisions and indicating which parse to select based on the feature values. The grammarian is accomplishing two critical tasks: identifying the features which are relevant to each decision, and deciding which choice to select based on the values of the relevant features.\\n\\nDecision-tree classification algorithms account for both of these tasks, and they also accomplish a third task which grammarians classically find difficult. By assigning a probability distribution to the possible choices, decision trees provide a ranking system which not only specifies the order of preference for the possible choices, but also gives a measure of the relative likelihood that each choice is the one which should be selected.\\n\\nWhat is a Decision Tree?\\n\\nA decision tree is a decision-making device which assigns a probability to each of the possible choices based on the context of the decision: P(f|h), where f is an element of the future vocabulary (the set of choices) and h is a history (the context of the decision). This probability P(f|h) is determined by asking a sequence of questions\\n\\nabout the context, where the ith question asked is uniquely determined by the answers to the i-1 previous questions.\\n\\nFor instance, consider the part-of-speech tagging problem. The first question a decision tree might ask is: 1. What is the word being tagged? If the answer is the, then the decision tree needs to ask no more questions; it is clear that the decision tree should assign the tag\\n\\nwith probability 1. If, instead, the answer to question 1 is bear, the decision tree might next ask the question: 2. What is the tag of the previous word? If the answer to question 2 is determiner, the decision tree might stop asking questions and assign the tag\\n\\nwith very high probability, and the tag\\n\\nEach question asked by the decision tree is represented by a tree node (an oval in the figure) and the possible answers to this question are associated with branches emanating from the node. Each node defines a probability distribution on the space of possible decisions. A node at which the decision tree stops asking questions is a leaf node. The leaf nodes represent the unique states in the decision-making problem, i.e. all contexts which lead to the same leaf node have the same probability distribution for the decision.\\n\\nDecision Trees vs. n\\n\\n\\n\\ngrams\\n\\nA decision-tree model is not really very different from an interpolated n-gram model. In fact, they are equivalent in representational power. The main differences between the two modeling techniques are how the models are parameterized and how the parameters are estimated.\\n\\nModel Parameterization\\n\\nFirst, let's be very clear on what we mean by an n-gram model. Usually, an n-gram model refers to a Markov process where the probability of a particular token being generating is dependent on the values of the previous n-1 tokens generated by the same process. By this definition, an n-gram model has |W|[n] parameters, where |W| is the number of unique tokens generated by the process.\\n\\nHowever, here let's define an n-gram model more loosely as a model which defines a probability distribution on a random variable given the values of n-1 random variables,\\n\\nThere is no assumption in the definition that any of the random variables F or Hi range over the same vocabulary. The number of parameters in this n-gram model is\\n\\nUsing this definition, an n-gram model can be represented by a decision-tree model with n-1 questions. For instance, the part-of-speech tagging model\\n\\nP(ti|witi-1ti-2) can be interpreted as a 4-gram model, where H1 is the variable denoting the word being tagged, H2 is the variable denoting the tag of the previous word, and H3 is the variable denoting the tag of the word two words back. Hence, this 4-gram tagging model is the same as a decision-tree model which always asks the sequence of 3 questions: 1. What is the word being tagged? 2. What is the tag of the previous word? 3. What is the tag of the word two words back?\\n\\nBut can a decision-tree model be represented by an n-gram model? No, but it can be represented by an interpolated n-gram model. The proof of this assertion is given in the next section.\\n\\nModel Estimation\\n\\nThe standard approach to estimating an n-gram model is a two step process. The first step is to count the number of occurrences of each n-gram from a training corpus. This process determines the empirical distribution,\\n\\nThe second step is smoothing the empirical distribution using a separate, held-out corpus . This step improves the empirical distribution by finding statistically unreliable parameter estimates and adjusting them based on more reliable information.\\n\\nA commonly-used technique for smoothing is deleted interpolation. Deleted interpolation estimates a model\\n\\nby using a linear combination of empirical models\\n\\nwhere m [ nand\\n\\nki-1 [ ki [ n for all\\n\\nFor example, a model\\n\\nmight be interpolated as follows:\\n\\nwhere\\n\\nfor all histories\\n\\nh1h2h3. The\\n\\noptimal values for the\\n\\nA decision-tree model can be represented by an interpolated n-gram model as follows. A leaf node in a decision tree can be represented by the sequence of question answers, or history values, which leads the decision tree to that leaf. Thus, a leaf node defines a probability distribution based on values of those questions:\\n\\nwhere m [ n and\\n\\nki-1 [ ki [ n, and where hki is the answer to one of the questions asked on the path from the root to the leaf. But this is the same as one of the terms in the interpolated n-gram model. So, a decision tree can be defined as an interpolated n-gram model where the\\n\\nfunction is\\n\\ndefined as:\\n\\nDecision\\n\\n\\n\\nTree Algorithms\\n\\nThe point of showing the equivalence between n-gram models and decision-tree models is to make clear that the power of decision-tree models is not in their expressiveness, but instead in how they can be automatically acquired for very large modeling problems. As ngrows, the parameter space for an n-gram model grows exponentially, and it quickly becomes computationally infeasible to estimate the smoothed model using deleted interpolation. Also, as ngrows large, the likelihood that the deleted interpolation process will converge to an optimal or even near-optimal parameter setting becomes vanishingly small.\\n\\nOn the other hand, the decision-tree learning algorithm increases the size of a model only as the training data allows. Thus, it can consider very large history spaces, i.e. n-gram models with very large n. Regardless of the value of n, the number of parameters in the resulting model will remain relatively constant, depending mostly on the number of training examples.\\n\\nThe leaf distributions in decision trees are empirical estimates, i.e. relative-frequency counts from the training data. Unfortunately, they assign probability zero to events which can possibly occur. Therefore, just as it is necessary to smooth empirical n-gram models, it is also necessary to smooth empirical decision-tree models.\\n\\nSPATTER Parsing\\n\\nThe SPATTER parsing algorithm is based on interpreting parsing as a statistical pattern recognition process. A parse tree for a sentence is constructed by starting with the sentence's words as leaves of a tree structure, and labeling and extending nodes these nodes until a single-rooted, labeled tree is constructed. This pattern recognition process is driven by the decision-tree models described in the previous section.\\n\\nSPATTER Representation\\n\\nA parse tree can be viewed as an n-ary branching tree, with each node in a tree labeled by either a non-terminal label or a part-of-speech label. If a parse tree is interpreted as a geometric pattern, a constituent is no more than a set of edges which meet at the same tree node. For instance, the noun phrase, ``a brown cow,'' consists of an edge extending to the right from ``a,'' an edge extending to the left from ``cow,'' and an edge extending straight up from ``brown''.\\n\\nIn SPATTER, a parse tree is encoded in terms of four elementary components, or features: words, tags, labels, and extensions. Each feature has a fixed vocabulary, with each element of a given feature vocabulary having a unique representation. The word feature can take on any value of any word. The tag feature can take on any value in the part-of-speech tag set. The label feature can take on any value in the non-terminal set. The extension can take on any of the following five values: right - the node is the first child of a constituent; left - the node is the last child of a constituent; up - the node is neither the first nor the last child of a constituent; unary - the node is a child of a unary constituent; root - the node is the root of the tree.\\n\\nFor an n word sentence, a parse tree has n leaf nodes, where the word feature value of the ith leaf node is the ith word in the sentence. The word feature value of the internal nodes is intended to contain the lexical head of the node's constituent. A deterministic lookup table based on the label of the internal node and the labels of the children is used to approximate this linguistic notion.\\n\\nTraining SPATTER's models\\n\\nSPATTER consists of three main decision-tree models: a part-of-speech tagging model, a node-extension model, and a node-labeling model.\\n\\nWhat is the X at the current node?\\n\\nWhat is the X at the node to the Y?\\n\\nWhat is the X at the node two nodes to the Y?\\n\\nWhat is the X at the current node's first child from the Y?\\n\\nWhat is the X at the current node's second child from the Y?\\n\\nParsing with SPATTER\\n\\nThe parsing procedure is a search for the highest probability parse tree. The probability of a parse is just the product of the probability of each of the actions made in constructing the parse, according to the decision-tree models.\\n\\nBecause of the size of the search space, (roughly\\n\\nO(|T|[n|N|n]),where |T| is the number of part-of-speech tags, n is the number of words in the sentence, and |N| is the number of non-terminal labels), it is not possible to compute the probability of every parse. However, the specific search algorithm used is not very important, so long as there are no search errors. A search error occurs when the the highest probability parse found by the parser is not the highest probability parse in the space of all parses.\\n\\nSPATTER's search procedure uses a two phase approach to identify the highest probability parse of a sentence. First, the parser uses a stack decoding algorithm to quickly find a complete parse for the sentence. Once the stack decoder has found a complete parse of reasonable probability (]10[-5]), it switches to a breadth-first mode to pursue all of the partial parses which have not been explored by the stack decoder. In this second mode, it can safely discard any partial parse which has a probability lower than the probability of the highest probability completed parse. Using these two search modes, SPATTER guarantees that it will find the highest probability parse. The only limitation of this search technique is that, for sentences which are modeled poorly, the search might exhaust the available memory before completing both phases. However, these search errors conveniently occur on sentences which SPATTER is likely to get wrong anyway, so there isn't much performance lossed due to the search errors. Experimentally, the search algorithm guarantees the highest probability parse is found for over 96% of the sentences parsed.\\n\\nExperiment Results\\n\\nIn the absence of an NL system, SPATTER can be evaluated by comparing its top-ranking parse with the treebank analysis for each test sentence. The parser was applied to two different domains, IBM Computer Manuals and the Wall Street Journal.\\n\\nIBM Computer Manuals\\n\\nThe main reason for applying SPATTER to this domain is that IBM had spent the previous ten years developing a rule-based, unification-style probabilistic context-free grammar for parsing this domain. The purpose of the experiment was to estimate SPATTER's ability to learn the syntax for this domain directly from a treebank, instead of depending on the interpretive expertise of a grammarian.\\n\\nWall Street Journal\\n\\nThe experiment is intended to illustrate SPATTER's ability to accurately parse a highly-ambiguous, large-vocabulary domain. These experiments use the Wall Street Journal domain, as annotated in the Penn Treebank, version 2. The Penn Treebank uses 46 part-of-speech tags and 27 non-terminal labels.\\n\\nThe WSJ portion of the Penn Treebank is divided into 25 sections, numbered 00 - 24. In these experiments, SPATTER was trained on sections 02 - 21, which contains approximately 40,000 sentences. The test results reported here are from section 00, which contains 1920 sentences. Sections 01, 22, 23, and 24 will be used as test data in future experiments.\\n\\nThe Penn Treebank is already tokenized and sentence detected by human annotators, and thus the test results reported here reflect this. SPATTER parses word sequences, not tag sequences. Furthermore, SPATTER does not simply pre-tag the sentences and use only the best tag sequence in parsing. Instead, it uses a probabilistic model to assign tags to the words, and considers all possible tag sequences according to the probability they are assigned by the model. No information about the legal tags for a word are extracted from the test corpus. In fact, no information other than the words is used from the test corpus.\\n\\nFor the sake of efficiency, only the sentences of 40 words or fewer are included in these experiments. For this test set, SPATTER takes on average 12 seconds per sentence on an SGI R4400 with 160 megabytes of RAM.\\n\\nRecall\\n\\nCrossing Brackets no. of constituents which violate constituent boundaries with a constituent in the treebank parse.\\n\\nThe precision and recall measures do not consider constituent labels in their evaluation of a parse, since the treebank label set will not necessarily coincide with the labels used by a given grammar. Since SPATTER uses the same syntactic label set as the Penn Treebank, it makes sense to report labelled precision and labelled recall. These measures are computed by considering a constituent to be correct if and only if it's label matches the label in the treebank.\\n\\nConclusion\\n\\nRegardless of what techniques are used for parsing disambiguation, one thing is clear: if a particular piece of information is necessary for solving a disambiguation problem, it must be made available to the disambiguation mechanism. The words in the sentence are clearly necessary to make parsing decisions, and in some cases long-distance structural information is also needed. Statistical models for parsing need to consider many more features of a sentence than can be managed by n-gram modeling techniques and many more examples than a human can keep track of. The SPATTER parser illustrates how large amounts of contextual information can be incorporated into a statistical model for parsing by applying decision-tree learning algorithms to a large annotated corpus.\\n\\nBibliography\\n\\nL. R. Bahl, P. F. Brown, P. V. deSouza, and R. L. Mercer. 1989. A tree-based statistical language model for natural language speech recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing, Vol. 36, No. 7, pages 1001-1008.\\n\\nL. E. Baum. 1972. An inequality and associated maximization technique in statistical estimation of probabilistic functions of markov processes. Inequalities, Vol. 3, pages 1-8.\\n\\nE. Black and et al. 1991. A procedure for quantitatively comparing the syntactic coverage of english grammars. Proceedings of the February 1991 DARPA Speech and Natural Language Workshop, pages 306-311.\\n\\nE. Black, R. Garside, and G. Leech. 1993. Statistically-driven computer grammars of english: the ibm/lancaster approach. Rodopi, Atlanta, Georgia.\\n\\nL. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. Classification and Regression Trees. Wadsworth and Brooks, Pacific Grove, California.\\n\\nP. F. Brown, V. Della Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. 1992. ``Class-based n-gram models of natural language.'' Computational Linguistics, 18(4), pages 467-479.\\n\\nD. M. Magerman. 1994. Natural Language Parsing as Statistical Pattern Recognition. Doctoral dissertation. Stanford University, Stanford, California.\\n\\nFootnotes\\n\\nThis work was sponsored by the Advanced Research Projects Agency, contract DABT63-94-C-0062. It does not reflect the position or the policy of the U.S. Government, and no official endorsement should be inferred. Thanks to the members of the IBM Speech Recognition Group for their significant contributions to this work. Note that in a decision tree, the leaf distribution is not affected by the order in which questions are asked. Asking about h1 followed by h2 yields the same future distribution as asking about h2followed by h1. This treebank also contains coreference information, predicate-argument relations, and trace information indicating movement; however, none of this additional information was used in these parsing experiments. For an independent research project on coreference, sections 00 and 01 have been annotated with detailed coreference information. A portion of these sections is being used as a development test set. Training SPATTER on them would improve parsing accuracy significantly and skew these experiments in favor of parsing-based approaches to coreference. Thus, these two sections have been excluded from the training set and reserved as test sentences. SPATTER returns a complete parse for all sentences of fewer then 50 words in the test set, but the sentences of 41 - 50 words required much more computation than the shorter sentences, and so they have been excluded.\", metadata={'source': '../data/raw/cmplg-xml/9504030.xml'}),\n",
       " Document(page_content=\"A Morphographemic Model for Error Correction in Nonconcatenative Strings\\n\\nThis paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism. Handling of various Semitic error problems is illustrated, with reference to Arabic and Syriac examples. The model handles errors vocalisation, diacritics, phonetic syncopation and morphographemic idiosyncrasies, in addition to Damerau errors. A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined.\\n\\nIntroduction\\n\\nSemitic is known amongst computational linguists, in particular computational morphologists, for its highly inflexional morphology. Its root-and-pattern phenomenon not only poses difficulties for a morphological system, but also makes error detection a difficult task. This paper aims at presenting a morphographemic model which can cope with both issues.\\n\\nA Morphographemic Model\\n\\nThe Formalism\\n\\nThe Model\\n\\nFinding the error\\n\\nMorphological analysis is first called with the assumption that the word is free of errors. If this fails, analysis is attempted again without the `no error' restriction. The error rules are then considered when ordinary morphological rules fail. If no error rules succeed, or lead to a successful partition of the word, analysis backtracks to try the error rules at successively earlier points in the word.\\n\\nSuggesting a correction\\n\\nOnce an error rule is selected, the corrected surface is substituted for the error surface, and normal analysis continues - at the same position. The substituted surface may be in the form of a variable, which is then ground by the normal analysis sequence of lexical matching over the lexicon tree. In this way only lexical words are considered, as the variable letter can only be instantiated to letters branching out from the current position on the lexicon tree. Normal prolog backtracking to explore alternative rules/lexical branches applies throughout.\\n\\nError Checking in Arabic\\n\\nTwo\\n\\n\\n\\nLevel Rules\\n\\nError Rules\\n\\nBelow are outlined error rules resulting from peculiarly Semitic problems. Error rules can also be constructed in a similar vein to deal with typographical Damerau error (which also take care of the issue of wrong vocalisms).\\n\\nVowel Shift\\n\\nIn the rules above, `X' is the shifted vowel. It is deleted from the surface. The partition contextual tuples consist of  [RULE NAME, SURF, LEX]. The LEX element is a tuple itself of  [PATTERN, ROOT, VOCALISM]. In E0 the shifted vowel was analysed earlier as an omitted stem vowel (om_stmv), whereas in E1 it was analysed earlier as an omitted spread vowel (om_sprv). The surface/lexical restrictions in the contexts could be written out in more detail, but both rules make use of the fact that those contexts are analysed by other partitions, which check that they meet the conditions for an omitted stem vowel or omitted spread vowel.\\n\\nDeleted Consonant\\n\\nDeleted Long Vowel\\n\\nAlthough the error probably results from a different fault, a deleted long vowel can be treated in the same way as a deleted consonant. With current transcription practice, long vowels are commonly written as two characters - they are possibly better represented as a single, distinct character.\\n\\nSubstituted Consonant\\n\\nThe `glottal_change' rule would be a normal morphological spelling change rule, incorporating contextual constraints (e.g. for the morpheme boundary) as necessary.\\n\\nBroken Plurals, Diminutive and Deverbal Nouns\\n\\nIn such a case, the two-level model succeeds in finding two-level analyses of the word in question, but fails when parsing the word morphosyntactically: at this stage, the parser is passed a root, vocalism and pattern whose feature structures do not unify.\\n\\nThe same procedure can be applied on diminutive and deverbal nouns.\\n\\nConclusion\\n\\nThe model presented corrects errors resulting from combining nonconcatenative strings as well as more standard morphological or spelling errors. It covers Semitic errors relating to vocalisation, diacritics, phonetic syncopation and morphographemic idiosyncrasies. Morphosyntactic issues of broken plurals, diminutives and deverbal nouns can be handled by a complementary correction strategy which also depends on morphological analysis.\\n\\nOther than the economic factor, an important advantage of combining morphological analysis and error detection/correction is the way the lexical tree associated with the analysis can be used to determine correction possibilities. The morphological analysis proceeds by selecting rules that hypothesise lexical strings for a given surface string. The rules are accepted/rejected by checking that the lexical string(s) can extend along the lexical tree(s) from the current position(s). Variables introduced by error rules into the surface string are then instantiated by associating surface with lexical, and matching lexical strings to the lexicon tree(s). The system is unable to consider correction characters that would be lexical impossibilities.\\n\\nAcknowledgements\\n\\nThe authors would like to thank their supervisor Dr Stephen Pulman. Thanks to Daniel Ponsford for providing data on the broken plural and Nuha Adly Atteya for discussing Arabic examples.\\n\\nBibliography\\n\\nKiraz, G. and Grimley-Evans, E. (1995). Compilation of n:1 two-level rules into finite state automata. Manuscript.\\n\\nFootnotes\", metadata={'source': '../data/raw/cmplg-xml/9504024.xml'}),\n",
       " Document(page_content=\"Prepositional Phrase Attachment through a Backed-Off Model\\n\\nRecent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events - ignoring events which occur less than 5 times in training data reduces performance to 81.6%.\\n\\nIntroduction\\n\\nPrepositional phrase attachment is a common cause of structural ambiguity in natural language. For example take the following sentence:\\n\\nPierre Vinken, 61 years old, joined the board as a nonexecutive director.\\n\\nThe PP `as a nonexecutive director' can either attach to the NP `the board' or to the VP `joined', giving two alternative structures. (In this case the VP attachment is correct):\\n\\nNP-attach: (joined ((the board) (as a nonexecutive director)))\\n\\nVP-attach: ((joined (the board)) (as a nonexecutive director))\\n\\nThis paper proposes a new statistical method for PP-attachment disambiguation based on the four head words.\\n\\nBackground\\n\\nTraining and Test Data\\n\\n((joined (the board)) (as a nonexecutive director))\\n\\nwould give the quintuple:\\n\\n0 joined board as director\\n\\nThe elements of this quintuple will from here on be referred to as the random variables A, V, N1, P, and N2. In the above verb phrase A=0, V=joined, N1=board, P=as, and\\n\\nN2=director.\\n\\nThe data consisted of training and test files of 20801 and 3097 quintuples respectively. In addition, a development set of 4039 quintuples was also supplied. This set was used during development of the attachment algorithm, ensuring that there was no implicit training of the method on the test set itself.\\n\\nOutline of the Problem\\n\\nA PP-attachment algorithm must take each quadruple (V=v, N1=n1, P=p, N2=n2) in test data and decide whether the attachment variable A = 0 or 1. The accuracy of the algorithm is then the percentage of attachments it gets `correct' on test data, using the A values taken from the treebank as the reference set.\\n\\nThe probability of the attachment variable Abeing 1 or 0 (signifying noun or verb attachment respectively) is a probability, p, which is conditional on the values of the words in the quadruple. In general a probabilistic algorithm will make an estimate,\\n\\n,\\n\\nof this probability:\\n\\nFor brevity this estimate will be referred to from here on as:\\n\\nThe decision can then be made using the test:\\n\\nIf this is true the attachment is made to the noun, if not then it is made to the verb.\\n\\nLower and Upper Bounds on Performance\\n\\nWhen evaluating an algorithm it is useful to have an idea of the lower and upper bounds on its performance. Some key results are summarised in the table below. All results in this section are on the IBM training and test data, with the exception of the two `average human' results.\\n\\nA reasonable lower bound seems to be 72.2% as scored by the `Most likely for each preposition' method. An approximate upper bound is 88.2% - it seems unreasonable to expect an algorithm to perform much better than a human.\\n\\nEstimation based on Training Data Counts Notation\\n\\nWe will use the symbol f to denote the number of times a particular tuple is seen in training data. For example\\n\\nf(1,is,revenue,from,research) is the number of times the quadruple\\n\\n(is,revenue,from,research) is seen with a noun attachment. Counts of lower order tuples can also be made - for example\\n\\nf(1,P=from)is the number of times (P=from) is seen with noun attachment in training data,\\n\\nf(V=is,N2=research) is the number of times\\n\\n(V=is,N2=research) is seen with either attachment and any value of N1 and P.\\n\\nMaximum Likelihood Estimation\\n\\nA maximum likelihood method would use the training data to give the following estimation for the conditional probability:\\n\\nUnfortunately sparse data problems make this estimate useless. A quadruple may appear in test data which has never been seen in training data. ie.\\n\\nf(v,n1,p,n2)=0. The above estimate is undefined in this situation, which happens extremely frequently in a large vocabulary domain such as WSJ. (In this experiment about 95% of those quadruples appearing in test data had not been seen in training data).\\n\\nPrevious Work\\n\\nThe Backed\\n\\n\\n\\nOff Estimate\\n\\nBut again the denominator\\n\\nf(w1,w2....wn-1) will frequently be zero, especially for large n. The backed-off estimate is a method of combating the sparse data problem. It is defined recursively as follows:\\n\\nIf\\n\\nf(w1,w2....wn\\n\\n\\n\\n1)]c1\\n\\nElse if\\n\\nf(w2,w3....wn\\n\\n\\n\\n1)]c2\\n\\nElse if\\n\\nf(w3,w4....wn\\n\\n\\n\\n1)]c3\\n\\nElse backing-off continues in the same way.\\n\\nThe idea here is to use MLE estimates based on lower order n-grams if counts are not high enough to make an accurate estimate at the current level. The cut off frequencies (c1, c2....) are thresholds determining whether to back-off or not at each level - counts lower than ci at stage i are deemed to be too low to give an accurate estimate, so in this case backing-off continues. (\\n\\n,\\n\\n,....) are normalisation constants which ensure that conditional probabilities sum to one.\\n\\nNote that the estimation of\\n\\nis analogous to the estimation of\\n\\n, and the above method can therefore also be applied to the PP-attachment problem. For example a simple method for estimation of\\n\\nwould go from MLE estimates of\\n\\nto\\n\\nto\\n\\nto\\n\\nto\\n\\n. However a crucial difference between the two problems is that in the n-gram task the words w1 to wn are sequential, giving a natural order in which backing off takes place - from\\n\\nto\\n\\nto\\n\\nand so on. There is no such sequence in the PP-attachment problem, and because of this there are four possible triples when backing off from quadruples ((v,n1,p), (v,p,n2), (n1,p,n2) and (v,n1,n2)) and six possible pairs when backing off from triples ((v,p), (n1,p), (p,n2), (v,n1), (v,n2) and (n1,n2)).\\n\\nA key observation in choosing between these tuples is that the preposition is particularly important to the attachment decision. For this reason only tuples which contained the preposition were used in backed off estimates - this reduces the problem to a choice between 3 triples and 3 pairs at each respective stage. Section 6.2 describes experiments which show that tuples containing the preposition are much better indicators of attachment.\\n\\nThe following method of combining the counts was found to work best in practice:\\n\\nand\\n\\nNote that this method effectively gives more weight to tuples with high overall counts. Another obvious method of combination, a simple average, gives equal weight to the three tuples regardless of their total counts and does not perform as well.\\n\\nThe cut-off frequencies must then be chosen. A surprising difference from language modeling is that a cut-off frequency of 0 is found to be optimum at all stages. This effectively means however low a count is, still use it rather than backing off a level.\\n\\nDescription of the Algorithm\\n\\nThe algorithm is then as follows:\\n\\n1.\\n\\nIf\\n\\nf(v,n1,p,n2)]0\\n\\n2.\\n\\nElse if\\n\\nf(v,n1,p)+f(v,p,n2)+f(n1,p,n2)]0\\n\\n3.\\n\\nElse if\\n\\nf(v,p)+f(n1,p)+f(p,n2)]0\\n\\n4.\\n\\nElse if f(p)]0\\n\\n5.\\n\\nElse\\n\\n(default is noun attachment).\\n\\nThe decision is then:\\n\\nIf\\n\\nchoose noun attachment.\\n\\nOtherwise choose verb attachment\\n\\nResults\\n\\nThe figure below shows the results for the method on the 3097 test sentences, also giving the total count and accuracy at each of the backed-off stages.\\n\\nResults with Morphological Analysis\\n\\nIn an effort to reduce sparse data problems the following processing was run over both test and training data:\\n\\nAll 4-digit numbers were replaced with the string `YEAR'.\\n\\nAll other strings of numbers (including those which had commas or decimal points) were replaced with the token `NUM'.\\n\\nThe verb and preposition fields were converted entirely to lower case.\\n\\nIn the n1 and n2 fields all words starting with a capital letter followed by one or more lower case letters were replaced with `NAME'.\\n\\nAll strings `NAME-NAME' were then replaced by `NAME'.\\n\\nThe result using this modified corpus was 84.5%, an improvement of 0.4% on the previous result.\\n\\nComparison with Other Work\\n\\nIf\\n\\nthen choose noun attachment, else choose verb attachment.\\n\\nHere f(w,p) is the number of times preposition p is seen attached to word w in the table, and\\n\\n.\\n\\nIf\\n\\nthen choose noun attachment, else choose verb attachment.\\n\\nThis is effectively a comparison of the maximum likelihood estimates of\\n\\nand\\n\\n, a different measure from the backed-off estimate which gives\\n\\n.\\n\\nThe backed-off method based on just the f(v,p) and f(n1,p) counts would be:\\n\\nIf\\n\\nthen choose noun attachment, else choose verb attachment,\\n\\nwhere\\n\\nAn experiment was implemented to investigate the difference in performance between these two methods. The test set was restricted to those cases where f(1,n1)]0, f(0,v)]0, and Hindle and Rooth's method gave a definite decision. (ie. the above inequality is strictly less-than or greater-than). This gave 1924 test cases. Hindle and Rooth's method scored 82.1% accuracy (1580 correct) on this set, whereas the backed-off measure scored 86.5% (1665 correct).\\n\\nA Closer Look at Backing-Off Low Counts are Important\\n\\nA possible criticism of the backed-off estimate is that it uses low count events without any smoothing, which has been shown to be a mistake in similar problems such as n-gram language models. In particular, quadruples and triples seen in test data will frequently be seen only once or twice in training data.\\n\\nThe results were as follows:\\n\\nThe decrease in accuracy from 84.1% to 81.6% is clear evidence for the importance of low counts.\\n\\nTuples with Prepositions are Better\\n\\nWe have excluded tuples which do not contain a preposition from the model. This section gives results which justify this.\\n\\nThe table below gives accuracies for the sub-tuples at each stage of backing-off. The accuracy figure for a particular tuple is obtained by modifying the algorithm in section 4.1 to use only information from that tuple at the appropriate stage. For example for (v,n1,n2), stage 2 would be modified to read\\n\\nIf\\n\\nf(v,n1,n2)]0,\\n\\nAll other stages in the algorithm would be unchanged. The accuracy figure is then the percentage accuracy on the test cases where the (v,n1,n2)counts were used. The development set with no morphological processing was used for these tests.\\n\\nAt each stage there is a sharp difference in accuracy between tuples with and without a preposition. Moreover, if the 14 tuples in the above table were ranked by accuracy, the top 7 tuples would be the 7 tuples which contain a preposition.\\n\\nConclusions\\n\\nThe backed-off estimate scores appreciably better than other methods which have been tested on the Wall Street Journal corpus. The accuracy of 84.5% is close to the human performance figure of 88% using the 4 head words alone. A particularly surprising result is the significance of low count events in training data. The algorithm has the additional advantages of being conceptually simple, and computationally inexpensive to implement.\\n\\nBibliography\\n\\nE. Brill and P. Resnik. A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation. In Proceedings of the fifteenth international conference on computational linguistics (COLING-1994), 1994.\\n\\nW. Gale and K. Church. Poor Estimates of Context are Worse than None. In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop, Hidden Valley, Pennsylvania.\\n\\nDaniel Karp, Yves Schabes, Martin Zaidel and Dania Egedi. A Freely Available Wide Coverage Morphological Analyzer for English. In Proceedings of the 15th International Conference on Computational Linguistics, 1994.\\n\\nD. Hindle and M. Rooth. Structural Ambiguity and Lexical Relations. Computational Linguistics, 19(1):103-120, 1993.\\n\\nS. Katz. Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recogniser. IEEE Transactions on Acoustics, Speech, and Signal Processing, Vol. ASSP-35, No. 3, 1987.\\n\\nM. Marcus, B. Santorini and M. Marcinkiewicz. Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics, 19(2), 1993.\\n\\nA. Ratnaparkhi, J. Reynar and S. Roukos. A Maximum Entropy Model for Prepositional Phrase Attachment. In Proceedings of the ARPA Workshop on Human Language Technology, Plainsboro, NJ, March 1994.\\n\\nFootnotes\\n\\nPersonal communication from Brill. eg. A simple average for triples would be defined as\\n\\nAt stages 1 and 2 backing off was also continued if\", metadata={'source': '../data/raw/cmplg-xml/9506021.xml'}),\n",
       " Document(page_content=\"Using default inheritance to describe LTAG Footnotes\\n\\nOther related work includes Becker (1993) and Habert (1991). See, for example, Bleiching (1992, 1994), Brown  Hippisley (1994), Corbett  Fraser (1993), Cahill (1990, 1993), Cahill  Evans (1990), Fraser  Corbett (in press), Gazdar (forthcoming), Gibbon (1992), Kilgarriff (1993), Kilgarriff Gazdar (in press), Reinhard  Gibbon (1991). See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different implementations available on various platforms and programming languages. And note in addition that the examples in this paper are simplified still further for expositional purposes. Or the principal anchor node, in constructions which have more than one lexical anchor. The parent is not a leaf of the entire tree, of course, but it is a leaf of the subtree above the current node. To gain a superficial understanding of this fragment, read a line such as [] == INTRANS_VERB as ``inherit everything from the definition of . '', and a line such as [parent] == PP_TREE:[] as ``inherit the  subtree from the definition of PP_TREE.'' Inheritance is always default - locally defined feature specifications take priority over inherited ones.\", metadata={'source': '../data/raw/cmplg-xml/9501001.xml'}),\n",
       " Document(page_content=\"An investigation into the correlation of cue phrases, unfilled pauses and the structuring of spoken discourse\\n\\nExpectations about the correlation of cue phrases, the duration of unfilled pauses and the structuring of spoken discourse are framed in light of Grosz and Sidner's theory of discourse and are tested for a directions-giving dialogue. The results suggest that cue phrase and discourse structuring tasks may align, and show a correlation for pause length and some of the modifications that speakers can make to discourse structure.\\n\\nIntroduction\\n\\nBecause an utterance is best understood in the context in which it is delivered, its interpreters must be able to identify the relevant context and recognize when it is altered, supplanted or revived. The transient nature of speech makes this task difficult. However, the difficulty is alleviated by the abundance of lexical and prosodic cues available to a speaker for communicating the location and type of contextual change. The investigation of the interaction between these cues presupposes a theory of contextual change in discourse. The theory relating attention, intentions and discourse structure[#!GROSZ-SIDNER!#] is particularly useful because it provides a computational account of the current context and the mechanisms of contextual change. This account frames the questions I investigate about the correlation between between lexical and prosodic cues. In particular, the theory motivates the selection of the cue phrase[#!GROSZ-SIDNER!#] -- a word or phrase whose relevance is to structural or rhetorical relations, rather than topic -- and the unfilled pause (silent pause) as significant indicators of discourse structure.\\n\\nThe tripartite nature of discourse\\n\\nTo explain the organization of a discourse into topics and subtopics, Grosz and Sidner postulate three interrelated components of discourse -- a linguistic structure, an intentional structure and an attentional state[#!GROSZ-SIDNER!#]. In the linguistic structure, the linear sequence of utterances becomes hierarchical -- utterances aggregate into discourse segments, and the discourse segments are organized hierarchically according to the relations among the purposes or discourse intentions that each satisfies.\\n\\nThe relations among discourse intentions are captured in the intentional structure. It is this organization that is mirrored by the linguistic structure of utterances. However, while the linguistic structure organizes the verbatim content of discourse segments, the intentional structure contains only the intentions that underlie each segment. The supposition of an intentional structure explains how discourse coherence is preserved in the absence of a complete history of the discourse. Rather, discourse participants summarize the verbatim contents of a discourse segment by the discourse intention it satisfies. The contents of a discourse segment are collapsed into an intention, and intentions themselves may be collapsed into intentions of larger scope.\\n\\nThe discourse intention of greatest scope is the Discourse Purpose (DP), the reason for initiating a discourse. Within this, discourse segments are introduced to fulfill a particular Discourse Segment Purpose (DSP) and thereby contribute to the satisfaction of the overall DP. A segment terminates when its DSP is satisfied. Similarly, a discourse terminates when the DP that initiated it is satisfied.\\n\\nThe attentional state is the third component of the tripartite theory. It models the foci of attention that exist during the construction of intentional structures. The global focus of attention encompasses those entities relevant to the discourse segment currently under construction, while the local focus (also called the center[#!CENTERING!#]) is the currently most salient entity in the discourse segment. The local focus may change from utterance to utterance, while the global focus (i.e., current context) changes only from segment to segment.\\n\\nThe linguistic, intentional and attentional components are interrelated. In particular, the attentional state describes the processing of the discourse segment which has been introduced to satisfy the current discourse intention. The functional interrelation is expressed temporally in spoken discourse -- the linguistic, intentional and attentional components devoted to one DSP co-occur. Therefore, a change in one component reflects or induces changes in the rest. For example, changes ascribed to the attentional state indicate changes in the intentional structure, and moreover, are recognized via qualitative changes in the linguistic structure. It is because of their interdependence and synchrony that I can postulate the hypothesis that co-occurring linguistic and attentional phenomena in spoken discourse -- cue phrases, pauses and discourse structure and processing -- are linked.\\n\\nThe part of the theory most directly relevant to my investigation are those constructs that model the attentional state. These are the focus space and the focus space stack. The focus space is the computational representation of processing in the current context, that is, for the discourse segment currently under construction. Within a focus space dwell representations of the entities evoked during the construction of the segment -- propositions, relations, objects in the world and the DSP of the current discourse segment.\\n\\nA focus space lives on a pushdown stack called the focus space stack. The progression of focus in a discourse is modeled via the basic stack operations -- pushes and pops -- applied to the stack elements. For example, closure of a discourse segment is modeled by popping its associated focus space from the stack; introduction of a segment is modeled by pushing its associated focus space onto the stack; retention of the current discourse segment is modeled by leaving its focus space on the stack in order to add or modify its elements.\\n\\nThe contents of a focus space whose DSP is satisfied are accrued in the longer lasting intentional structure. Thus, at the end of a discourse the focus space stack is empty while the intentional structure is fully constructed.\\n\\nThe focus space model abstracts the processing that all participants must do in order to accurately track and affect the flow of discourse. Thus, it treats the emerging discourse structure and the changing attentional foci as publicly accessible properties of the discourse. However, although the participants themselves may act as if they are manipulating public structures, the informational and attentional properties of a discourse are, in fact, modeled only privately.\\n\\nIn explaining certain lexical and prosodic features of discourse, it is often useful to return to these private models. For a speaker's utterance is conditioned both by the state of the her own model and by her beliefs about those of her interlocutors. The time-dependent nature of speech emphasizes the importance of synchronizing private models. Lexical and prosodic focusing cues hasten synchronization. In particular, they guide the listeners in updating their models (among them, the focus space stack) to reflect the attentional changes already in effect for the speaker.\\n\\nFor my analysis, the most relevant private model belongs to the current speaker, whose discourse intentions guide, for the moment, the flow of topic and attention in a discourse and whose spoken contributions provide the richest evidence of attentional state. If cue phrases and unfilled pause durations can be shown to correlate with attentional state (and by definition, the intentional and linguistic structure), the attentional state they reveal belongs to the current speaker, and the attentional changes they denote are the ones the speaker makes in her own private model.\\n\\nMain Hypotheses\\n\\nThe theory of the tripartite nature of discourse frames my hypotheses about the correlation of cue phrases, pause duration and discourse structure. The main hypotheses are these: that particular unfilled pause durations tend to correlate with particular cue phrases and that this correlation is occasioned by changes to the attentional state of the discourse participants, or, equivalently, by the emerging intentional structure of a discourse.\\n\\nCue phrases\\n\\nChanges to the attentional state occur at segment boundaries. Cue phrases by definition evince these changes -- they are utterance- and segment-initial words or phrases and they inform on structural or rhetorical relations rather than on topic. Thus, for cue phrases, the question is not whether they correlate with attentional state, but how. To answer this question, we ask, for each cue phrase (e.g., Now, To begin with, So), whether it signals particular and distinct changes to the attentional state.\\n\\nPauses\\n\\nThe correlation of unfilled pauses with attentional state is less certain because pauses appear at all levels of discourse structure. They are found within and between the smallest grammatical phrase, the sentence, the utterance, the speaking turn and the discourse segment. Their correlation is mainly with the cognitive difficulty of producing a phrase or utterance[#!GOLDMAN-EISLER-1!#]. To link this correlation with the task of producing discourse structure, we must posit a variety of attentional operations with corresponding variability in cognitive difficulty. Specifically, we construct the chain of assumptions that: More than one attentional operation exists (e.g., initiation, retention, closure). The different attentional operations are distinguished by their effect on the attentional state and by the cognitive difficulty of their production. The amount of silence preceding an attentional operation is correlated with the greater or lesser demands it makes on mental processing.\\n\\nTo link unfilled pause duration to discourse structure we must first establish that operations on the attentional state can be distinguished sufficiently to explain the different demands that each operation makes on discourse processing and which, therefore, might be reflected in the duration of segment-initial unfilled pauses.\\n\\nAuxiliary hypotheses\\n\\nThe linking of pause duration to the processing of discourse segments motivates some auxiliary hypotheses that refine notions about the kinds of mental operations sanctioned by the focus space model and about the internal structure of a discourse segment. These auxiliary hypotheses are developed in this section.\\n\\nAttentional operations\\n\\nIn the theory of discourse structure, changes to the attentional state are modeled as operations on the focus space stack. These operations appear reducible to four distinct sequences of stack operations that correspond to four distinct effects on the attentional state, as follows: One push -- Initiate a  new focus space. No push, no pop -- Retain the current focus space. One or more pops -- Return to a previously initiated focus space. One or more pops followed by a push -- Replace a previous focus space(s) with a new one.\\n\\nThe decomposition of focus space operations into stack operation primitives is not merely an attempt to impose a computational patina on descriptive terms. Rather, it suggests that operations that differ in kind and number place different requirements on mental processing for both speaker and therefore might be accompanied by lexical and acoustical phenomena that also differ.\\n\\nStructure of a discourse segment\\n\\nTo further motivate the particular usefulness of cue phrases and unfilled pauses as locators of discourse segment boundaries and markers of attentional state, it is useful to distinguish among three phases in the life of a discourse segment (and its focus space counterpart) -- its initiation, development and closure . We make the additional assumptions that each phase may be marked explicitly or implicitly and by lexical and acoustical phenomena.\\n\\nFrom inspection of dialogue, it appears that the development phase must be instantiated explicitly with lexical contributions, while the boundary phases need not be. However, while lexical marking of segment boundaries is optional, prosodic marking is not. Thus, at initiation of a discourse segment we find, for example, an expanded pitch range[#!COOPER-SORENSEN!#] and at its closure, phrase-final lowering[#!HIRSCHBERG-PIERREHUMBERT!#] and syllable lengthening [#!KLATT75!#].\\n\\nSometimes, the same structural cue is implicit for one segment yet explicit for another. For example, in a Replace operation, explicitly marked closure of one segment implicitly permits the initiation of the next. Conversely, an explicitly marked Initiation of the current segment testifies implicitly to the closure of the previous one.\\n\\nBoundary phenomena are of special relevance toward retrieving discourse structure from a multiplicity of lexical and acoustic clues. The distinction between explicit and implicit correlates for each phase of segment construction admits four classes of segment boundary phenomena -- phenomena that are: explicit and segment-initial; implicit and segment-initial; explicit and segment-final; and implicit and segment-final. An investigation of how cue phrases and unfilled pauses reflect discourse structure and the state of its processing is thus an investigation of the explicit and segment-initial evidence of focus space initiation.\\n\\nThe selection of segment-initial phenomena in no way implies that segment-final phenomena are any less crucial to the communication and recognition of discourse segment boundaries. Nor does the selection of cue phrases and unfilled pauses minimize the contributions of other lexical and prosodic phenomena. Rather, these selections are motivated by features of the focus space model that both cue phrases and unfilled pauses might specially illuminate, and conversely, by features of the model that might specially illuminate the discourse function of cue phrases and unfilled pauses. These features are described in the following two sections.\\n\\nCue phrases, discourse markers and attentional state\\n\\nCue phrases are those words or phrases which introduce an utterance -- e.g., To begin with, First of all, Now, But -- and coordinate the flow of conversation and focus rather than contribute directly to the topic at hand. They provide broad, topic independent indications of how the speaker intends to relate the current utterance to those preceding it, thus locating the utterance in the discourse structure. The information they convey is attentional, intentional or both.\\n\\nThe study of cue phrases and their correlation with discourse structure and focus of attention is most extensive for the discourse marker[#!SCHIFFRIN!#] subcategory. Schiffrin's work in particular, is the basis for my predictions about the structural effects of cue phrases on the focus space model.\\n\\nDiscourse markers\\n\\nDiscourse markers are generally single word phrases, such as Well, Now, Then or So, whose pragmatic role in a discourse usually follows from their syntactic and semantic role in a grammatical phrase. That is, if a word in semantic guise relates propositions in a grammatical phrase, it marks in its pragmatic guise the same or similar relation between utterances in a discourse. For example[#!SCHIFFRIN!#]: And, as a discourse marker, indicates connectedness, conveying the speaker's view that the utterance it heads is connected to the prior discourse. The connection may be to the immediately previous utterance or to the speaker's prior [interrupted] turn. But also a marks connectedness, but connects utterances in a contrast relation. The contrast may be structural (resumption after a digression or interruption) or rhetorical. Like well, it introduces unexpected or undesired material, but in a less cooperative manner. I mean precedes a repair or modification of the speaker's own contribution or highlights something to which the speaker believes the hearer should attend. So may precede a presentation of a result, and indicates transitions to a higher level, in contrast to ``because'' which indicates progressive embedding. Now emphasizes what the speaker is about to do, and is often used to introduce evaluations. Well is often used in response, when the possibilities offered by the previous speaker are inadequate.\\n\\nIt indicates an awareness of conversational expectations but also heralds a violation of the previous speaker's expectations. You know indicates an appeal to shared knowledge and mutual beliefs.\\n\\nDiscourse markers reinterpreted\\n\\nSome of the observations about the conversational role of discourse markers invoke structural effects (embedding, return to a higher level) although without detailing the structure in question. A more unified and computationally driven account might be posed in terms of operations on the focus space stack, as follows: And (connectedness): Retain, Return. But (contrast): Retain, Replace or Return. I mean (modification or repair): Initiate, Retain. So (presentation of a result): Return, Replace. Because (progressive embedding): Initiate. Now (what the speaker is about to do): Replace. Well (inadequate options):  Replace. You know (appeal to shared knowledge): Retain, or Initiate when it precedes an aside.\\n\\nIn addition, there are the cue phrases that highlight structural or propositional ordinality. The first use of such a phrase (e.g., To begin with, In the first place,) is likely to denote a focus space Initiation while subsequent uses (e.g., Secondly, Finally,) denote a focus space Replacement.\\n\\nThese formulations are not deterministic. They illustrate, however, the hypothesis that certain of the discourse markers are more likely to betoken certain focusing operations. Under what conditions might such correspondences exist? Clearly, features of the context in which a cue phrase is used might constrain its effect on focusing, and so explain how conversants are able to track focus from cues that, by themselves, are ambiguous.\\n\\nThus, to select the probable from the possible, corroboration from other quarters is required. Lexical corroboration may be semantic, from domain specific evidence of topic change or continuation. Or it may be syntactic, from those syntactic distributions that tend not to cross segment boundaries (tense, aspect and the scope of referring expressions[#!GROSZ-SIDNER!#]) Alternatively, prosodic features are likely to better identify the current use of a cue phrase from those that are possible.\\n\\nUnfilled pauses and attentional state\\n\\nThe most useful prosodic correlates of discourse segmentation occur at segment boundaries and indicate either the opening of a new segment, closure of the old or both. For example, a phrase-final continuation rise forestalls segment closure while phrase-final lowering confirms it[#!PIERREHUMBERT-HIRSCHBERG!#]. And expanded pitch range tends to mark the introduction of new topics, while reduced pitch range marks subtopics and parentheticals. Similarly, voice quality changes, e.g., from normal to creaky voice, may accompany attentional and intentional changes.\\n\\nFilled pauses (e.g., Um, uh) and unfilled pauses appear at segment boundaries but are also found within a discourse segment and in the smaller groupings it contains. In contrast to the propositional and attentional accounts of intonational cues[#!PIERREHUMBERT-HIRSCHBERG!#], accounts of pausing invoke the demands of cognition and pragmatics. For example, the duration of unfilled pauses has been observed to correlate with the cognitive difficulty involved in producing an utterance[#!GOLDMAN-EISLER-1!#], while filled pauses may function as a floor holding device[#!MACLAY:OSGOOD!#], or perhaps, correlate with the speaker's emotional response to topic[#!GOLDMAN-EISLER-1!#].\\n\\nAs corroborators of attentional interpretations of cue phrases filled pauses are less useful than unfilled pauses because they overlap with cue phrases in both form (partially lexicalized) and function. A more independent measure is provided by unfilled pauses which are not lexicalized and therefore carry neither lexical nor intonational propositions. Rather, as correlates of the cognitive processing, they may also correlate with the specific differences among stack operations, which, after all, are cognitive operations, albeit idealized.\\n\\nThe selection of unfilled pause duration as a possible marker of attention and segmentation also has the practical advantage of being easy to locate instrumentally and easy to check perceptually. Moreover, its measurement is unambiguous instrumentally and requires less from perception, than, for example, intonational prosodic cues. For, while intonational features are categorical according to their type (combinations of the L, H and * tokens[#!PIERREHUMBERT!#]) and the structure to which they apply (word, intermediate phrase, intonational phrase), pause duration is ordinal and is measured on the same continuous linear scale for all levels of linguistic and intonational structures.\\n\\nQuestions and predictions\\n\\nMy investigation is inspired by the theory relating attentions, intentions and discourse structure[#!GROSZ-SIDNER!#]. To the more specific observations linking cue phrases to attentional state[#!GROSZ-SIDNER!#,#!SCHIFFRIN!#] and the duration of unfilled pauses to increased cognitive difficulty[#!GOLDMAN-EISLER-1!#], I add the assumption of four fundamental focusing operations. Together, they motivate my hypotheses that:\\n\\n(1)Specific cue phrases betoken specific focusing operations. (2)Differences in the cognitive difficulty of the focusing operations are reflected in the duration of the pauses that precede them. From these hypotheses come the specific questions that guide the research: Is there a correlation between the focusing operations and the duration of the pause that precedes it? Are cue phrases correlated with focusing operations -- how often and under what circumstances? What is the relation of pausing and cue phrases -- do they substitute for each other, compliment each other or play different roles such that one is required or allowed where the other is not? Is there a unique minimum cognitive cost for each stack primitive (Push, Pop) of which focusing operations are composed, and that would therefore explain differences in segment-initial pause duration?\\n\\nIn addition, the hypotheses raise questions not immediately answerable: If there are indeed patterns of usage, do they differ predictably for different discourse features, for example, by format (monologue or dialogue) or according to the planning effort (prepared or extemporaneous) required in formulating each utterance? If on the other hand, correlations are partial at best, can other lexical or prosodic features provide the missing correlates?\\n\\nResearch into these questions is not without its biases. Thus, I expected to find in my discourse samples the following correlations: Unfilled pause duration and focusing operation are correlated. Cue phrases are correlated with focusing operations. (The particular predictions are discussed previously in Section 5.2.) Cue phrase type and unfilled pause duration are correlated as well.\\n\\nThe hypothesized correlation of unfilled pause duration with focusing operations is based on assumptions about variations in complexity among the operations, such that longer pauses will accompany more complex operations. Complexity is conjectured to correlate with kind and number. That is, it varies according to whether the operation decomposes into pops, a push or both and it increase with the number of segments opened or closed in one operation.\\n\\nThis produces the particular predictions that: Retentions will be preceded by pauses of the smallest duration because they induce neither a push nor pop and therefore are the least costly of the focusing operations. Pause duration is positively correlated with the number of segments affected in one focusing operation. That is, the more segments opened or closed, the longer the preceding pause. Pops are more costly than pushes. This follows from an assumption that adding information (a push) builds on what is currently established and accessible, while removing information (one or more pops) makes the production of subsequent utterances more difficult.\\n\\nData\\n\\nI analyzed two discourse samples -- three minutes of a directions discourse and seven minutes of a manager-employee project meeting. The segmentation of the second proved difficult and is still in progress, so I report results only for the first.\\n\\nIn the directions discourse, Speaker B provides Speaker A with walking directions to a location on the M.I.T. campus. The discourse takes the form of an expert-client dialogue. Although Speaker A initiates the dialogue, most of the discourse segments and their intentions are introduced by Speaker B, the expert.\\n\\nMethods\\n\\nThe search for correlations among cue phrases, unfilled pauses and discourse structure generated three data collection tasks: Identification of cue phrases; Identification and measurement of unfilled pauses; Segmentation of the discourse via the identification of the focusing operations that effected the segmentation.\\n\\nCue phrase identification\\n\\nThe main challenge of cue phrase identification lay in distinguishing cue from non-cue uses of a phrase. Usually, cue uses are utterance- or segment-initial, while non-cue uses are not. However, this is not a reliable criterion for the connectives, And and But, which may head an utterance or phrase as either a cue phrase or a syntactic conjunctive. In cases where the usage was unclear, I decided against the pragmatic usage if the phrase in question provided syntactic coordination of two semantically related propositions. If even this judgment proved difficult, I applied the intonational criteria that distinguished cue and non-cue uses of Now[#!HIRSCHBERG-LITMAN!#]. Thus, if the cue phrase candidate was deaccented, or accented with L* tones or uttered as a complete intonational phrase, it was classified as a cue phrase.\\n\\nPause location and measurement\\n\\nPauses were identified by ear and corroborated and measured using the waveform and the energy track displays of two signal processing programs. The locations of all unfilled pauses were recorded, as were their durations, rounded to the nearest one tenth of a second.\\n\\nIn general, the procedure was straightforward. The only confusion was presented by the silence between the closure and release phase of plosives. This silence was not counted as a genuine unfilled pause.\\n\\nDiscourse segmentation\\n\\nAn accurate discourse segmentation falls out of an accurate classification of the focusing operations by which the segments have been constructed. The tasks are interrelated and both are difficult. Therefore, in this section I will discuss in detail the task, its difficulties and the classification criteria I developed to enhance the accuracy of my judgments.\\n\\nThe task\\n\\nThe segmentation of a completed discourse is equivalently the task of recapturing the attentional state that accompanied each successive utterance. Attentional cues are especially important because topical relations do not always predict discourse structure. The points at which discourse structure diverges from the organization of information in the domain may be precisely the points at which attentional cues are most appropriate.\\n\\nSegmentation of a completed discourse is most straightforward for expository text. In such discourse, domain and attentional hierarchies often coincide -- the relations among segments and of each segment to the overall Discourse Purpose are clear. In spoken and impromptu discourse, however, the alignment of DSPs is not always so felicitous. Even in the task-oriented directions discourse, the relations among steps in the task did not conclusively determine the relations of the discourse segments in which these steps were described.\\n\\nThe particular segmentation difficulties presented by my sample(s) led to the development of explicit criteria for isolating the corroborating features of attentional operations and discourse structure. The criteria help clarify confusion from two sources -- the distinction between attentional and domain hierarchies and the interpretation of underspecified lexical and prosodic attentional cues.\\n\\nSeparating the attentional from the topical.\\n\\nIn prepared discourse (written or spoken) the intentional structure is tightly coupled to the Discourse Purpose. In contrast, impromptu discourse exhibits a looser coupling, owing to its real-time and situated nature. In such discourse, the maintenance of coherence requires the real-time management of cognitive resources upon which competing demands may be made. As a consequence, influences outside the ostensible DP must be managed in support of continuing the conversation at all. Because DSPs that are ostensibly outside the current DP can become temporarily relevant, provision must be made for their principled incorporation into the attentional state and in the linguistic and intentional structures.\\n\\nThis is accomplished via attentional constructions that are more likely to occur in spoken discourse, for example, flashbacks, digressions and interruptions[#!GROSZ-SIDNER!#]. Their relation to the discourse in which they occur illustrates the difficulty of segmenting in hindsight a discourse whose DSPs may satisfy multiple DPs. This recommends against reliance on domain knowledge, since one discourse may invoke more than one domain.\\n\\nTherefore, to locate segment boundaries, I use criteria that emphasize focusing operations independent of the ostensible DP. For example, although the succession of two topically unrelated segments might suggest a Replace operation, it is treated as an Initiate in the presence of explicit indicators of linkage or in the absence of explicit indicators of separation. Consequently, successive segments may be linked hierarchically in the attentional and linguistic structures despite their topical independence.\\n\\nFor example, in the following section of the directions discourse (1) is a topic introduction, (2) a digression and (3) an elaboration, i.e., a subtopic:\\n\\nAlthough (2) is a comment on discourse processing, it functions neither as a cue phrase nor a synchronization device. The digression it represents it not topically subordinate to (1), nor is (3) topically subordinate to (2). However, they are attentionally subordinate to the utterances they follow, as indicated by the continuation rises at the end of (1) and (2). While the semantic and topical differences between successive utterances argue for segment separation, the acoustical concomitants argue against. Therefore, the attentional moves that introduce (2) and (3) contain no pops. Instead, they are Initiations, producing the following segmentation:\\n\\nInterpreting underspecified cues\\n\\nEven when successive utterances are aligned attentionally and topically, their cue phrase and prosodic markings may not conclusively reveal their exact place in discourse structures. The underspecified nature of cue phrase correspondences to focusing operations is discussed in Section 5.2. Prosodic marking is similarly underspecified, and on two counts. First, a particular intonational feature at the (e.g., phrase-final lowering, phrase-initial pitch range expansion) can felicitously indicate more than one focusing operation; second, the intonation at a phrase boundary often indicates stack primitives (push, pop, null) more reliably than the composite focusing operations from which discourse structure is deduced.\\n\\nFor example, in the directions discourse, the cue phrases So, But and And often indicated pops, as did the prosodic changes that accompanied them, e.g., expanded pitch range and a shift from L* to H* tones. However, these cues did not reveal exactly how many segments were popped nor whether a push followed the sequence of pops. Thus, it was not always easy to distinguish a Return (one or more pops) from a Replace (one or more pops, followed by a push).\\n\\nNeither domain nor syntactic knowledge were conclusive in this regard. For example, domain and syntax dictated the following segmentation:\\n\\nbut in contraindication to what was specified intonationally:\\n\\n(The intonationally driven segmentation, in contradiction to the structure of knowledge in the domain, may account for the listener's subsequent confusion about the very point made in this section of the discourse.)\\n\\nClassification criteria\\n\\nBecause semantic clues to attentional state can be confusing and lexical and prosodic markings inconclusive, it became necessary to standardize the procedure and criteria for classifying the focusing operations. An accurate classification depends on the answers to two questions for the phrase undergoing classification: Has a new focus space been opened? Has an old focus space been closed? Most useful in this regard are the lexical and prosodic phenomena within and around the phenomena currently under evaluation for their attentional effect.\\n\\nWhat constitutes current phenomena, and what might constitute its surrounds? I selected as current the speech fragment that begins with one of five fragment-initial tokens and whose terminating boundary is marked by the occurrence of the next fragment-initial token. These tokens are: The unfilled pause; The filled pause; A cue phrase; An acknowledgment form: Ok, Sure, Uh-huh, etc. ; Or the unmarked case: any other sentence-initial grammatical constituent, e.g., a noun phrase, auxiliary verb, complementizer or adverb.\\n\\nMy demarcation of the relevant surrounding phenomena was less bound to structure than to function. For both prior and subsequent phenomena, I selected the smallest speech fragment that could be distinguished by its discourse function, i.e., by its attentional, coordination or topical role. I assign five classifications: A cue phrase; An acknowledgment or prompt; A segment closure (e.g., Good! ); A repair; Or the unmarked case -- development of the topic.\\n\\nCoding the data\\n\\nThe data relevant to every speech fragment was coded for later statistical analysis. This translated into two tasks -- identifying the prior, current and subsequent speech fragment and for each current fragment, recording: The duration of the preceding unfilled pause; The type of fragment-initial constituent, either: A cue phrase; An explicit acknowledgment form (e.g., Ok, Sure. ); A filled pause; Or any other sentence-initial syntactic form whose function is primarily topical, not pragmatic.\\n\\nThe co\\n\\n\\n\\noccurring focusing operation.\\n\\nThe embedding of the current segment in the linguistic structure (number of levels).\\n\\nThe number of segments opened or closed in the focusing operation.\\n\\nThe discourse function of the immediately prior speech (cue phrase, acknowledgment, closure, filled pause, repair, topical but none of the above). The discourse function of the immediately subsequent speech (same categories as for prior speech). Whether the speaker was initiating or continuing a speaking turn with the current fragment.\\n\\nUsing this metric, one hundred speech fragments were identified according and their features coded. The coded representation of the discourse was then analyzed for distributions and statistical correlations. The results are reported in the next section.\\n\\nResults\\n\\nIn this section I summarize the raw data, report the results of statistical tests and offer an explanation of the findings.\\n\\nData\\n\\nThe segmentation of the discourse was reconstructed according to the focusing operations indicated both lexically and acoustically. The segmentation described a discourse with two top level segments. Within the first, the overall task was defined; within the second, it was executed. The task definition segment was itself composed of two top level segments, while the execution segment is composed of nine.\\n\\nStatistical analyses\\n\\nThe predictions were analyzed via statistical tests on the coded representation of the discourse.\\n\\nPause duration and focusing operation Pause duration and number of segments affected in a focusing operation\\n\\nLonger pauses were positively correlated with the number of segments opened or closed during one focusing operation (r = .357, p[.001). This finding might partially explain the long pauses that appear before a Replace, since a Replace is the focusing operation most likely to affect the most focus spaces, By definition, it requires [almost] everything to be popped from the focusing before the initiation (push) of a new focus space.\\n\\nPause duration and depth of embedding\\n\\nA correlation of pause duration and the depth of embedding in the linguistic structure (or equivalently, the number of focus spaces still on the stack) showed no significant effect on pause duration (F(1,98) = 0.1861, p[.7).\\n\\nPause duration, cue phrase and focusing operation Pause duration and marked/unmarked\\n\\nTo compensate for the small sample sizes of the cue phrase data, all explicit lexical markers of structure (cue phrase, acknowledgment, filled pause) were collapsed into the category, marked. The data in this category were compared to the data for lexically unmarked fragments. Because the longest pauses preceded unmarked Returns and Replacements, I predicted that unmarked operations would in general be preceded by longer pauses than marked.\\n\\nDiscussion\\n\\nThus far, analysis of the data identifies significantly longer pauses for the Replace operation than for any other and shows that pause duration is positively correlated with the number of segments affected by one focusing operation. These findings begin to distinguish the focusing operations quantitatively, by number of focus spaces affected, and qualitatively, by whether they occur within an established context (Initiate, Retain, Return) or at its beginning (Replace).\\n\\nThe categorical classification present particular problems. For, uncertainties arose even with the application of a classification metric. Perhaps these uncertainties should have been incorporated into the coding scheme or perhaps the categorical classifications should have been abandoned in favor of additional and quantifiable acoustical and lexical features.\\n\\nRefining the original hypotheses\\n\\nIf this hypotheses is correct, two accounts can be constructed that would jointly predict the appearance of cue phrases. One account emphasizes the processes involved in choosing and communicating the state of global focus. The other emphasizes the mutually recognized (by speaker and hearers) attentional and intentional state of the discourse. Together they identify the factors that would impel a speaker to precede an utterance with a cue phrase, an unfilled pause or both.\\n\\nThe influence of the speaker's internal processes and conversational  goals\\n\\nIf an unfilled pause preceding a lexically unmarked fragment is significantly longer, we might assume that a particular focusing operation is executed in a characteristic amount of time (given adequate consideration of other contextual features). Within this time, we might observe  silence, a cue phrase  or both.\\n\\nBecause both pause and cue phrase can appear at the same location in a phrase, we ask if their functions are equivalent, or instead, complementary. My hypothesis selects the second option, that they are complementary in the cognitive processing each reflects and in the discourse functions each fulfills. For, if the duration of an unfilled pause is evidence of the difficulty of a cognitive task, a cue phrase is evidence of its partial resolution.\\n\\nAs a communicative device, cue phrases are more cooperative than silence. In silence, a listener can only guess at the current contents of the speaker's models. With the uttering of a cue phrase, the listener is at least notified that the speaker is constructing a response. The minimal cue in this regard is the filled pause. Bone fide cue phrases, however, herald not only an upcoming utterance, but a particular direction of focus and even a propositional relation between prior and upcoming speech.\\n\\nCue phrases serve not only the listener but also the speaker. Because they commit to topic structure, but not to specific referents and discourse entities, they buy additional time for the speaker in which to complete a focusing operation and formulate the remainder of the utterance.\\n\\nThe influence of the state of the discourse\\n\\nThe account of the influence of the currently observable state of the discourse rests on two patterns in the data: (1) the difference in pause durations for marked and unmarked Initiates and Retains is minimal; and (2) the difference between marked and unmarked Returns and Replaces is greater. If these patterns can be shown to be significant, they suggest that remaining in the current context is less costly than returning to a former context, or establishing a new one. The corollary is the claim that an expected focusing operation need not be marked, while an unexpected operation is most felicitous when marked.\\n\\nIn other words, remaining in the current context or entering a subordinate context is expected behavior, while exiting the current context is not. Exiting the current context (focus space) carries a greater risk of disrupting a mutual view of discourse structures. The extent of risk is assessed for the listener by the difficulty of tracking the change and for the speaker, by the difficulty of executing it. The risk originates in the nondeterministic definitions of Return and Replace operations -- both contain in their structure one or more pops. In addition, these operations can be confused because both begin identically, with a series of pops.\\n\\nBecause closing a focus space is a marked behavior, the clues to changing focus are most cooperative if they guide the listener toward re-invoking a prior context (i.e., a Return) or establishing a new one (Replace). Thus, certain clues are more likely to mark a return to a former context (e.g., So, Anyway, As I was saying), while others (Now, the ordinal phrases) mark a Replace.\\n\\nFuture work\\n\\nThe goal of future investigations is to establish the bases for predicting the appearance of particular acoustical and lexical features. The speculations presented in this section provide a theoretical framework. If borne out, they can be re-fashioned as characterizations of the circumstances in which cue phrases and unfilled pauses are most likely to be used.\\n\\nConclusion\\n\\nThe relationships among cue phrases, unfilled pauses and the structuring of discourse are investigated within the paradigm of the tripartite model of discourse. Within this model, the postulation of four focusing operations provides an operational framework to which can be tied the discourse functions of cue phrases and the cognitive activity associated with the production of an utterance. Especially, the difficulty of utterance production might be explained by the complexity of the co-occurring focusing operation. Such a correspondence is, in fact, suggested by the positive correlation of pause duration and the number of focus spaces opened or closed in one operation on the focus space stack.\\n\\nHowever, because the classification of focusing operations is uncertain, more data and better tests are required to characterize the relationships among the lexical and acoustical correlates of topic and focus. In addition, the aptness of the tripartite model itself is not assured. The idealizations it contains may undergo modification in light of new results, or be augmented by other accounts of discourse processing. On the other hand, the analysis of more quantitative data may confirm the implications of the model, and its appropriateness as the foundation for investigating the lexical and prosodic features of discourse.\\n\\nAcknowledgments\\n\\nMany thanks to Susan Brennan who selected and ran the statistical tests on the data and to S. Lines for helpful comments. Various stages of this work were supervised in turn by Chris Schmandt and Ken Haase, both of the M.I.T. Media Laboratory.\\n\\nFootnotes\\n\\nDiscourse intentions are those goals or intentions intended to be recognized by each participant as the purpose to which the current segment of talk is devoted. When at least one focus space remains on the stack, the discourse continues. When none remain, however, the discourse is ended. Gestural correlates of discourse structure and processing are outside the scope of this investigation. For example, Walker and Whittaker observe that deictic pronominal reference may cross segment boundaries, while nondeictic pronominal reference does so only rarely[#!WALKER-WHITTAKER!#]. The conversation occurred in a face-to-face encounter and was recorded on a hand-held cassette recorder. SPIRE, written for the LISP machine by Victor Zue's group at M.I.T. and dspB (digital signal processing workBench) written for the DECstation by Dan Ellis at the M.I.T. Media Laboratory. The discourse function classifications and the within-/between-turn distinctions were recorded to track the features influencing the judgment of focusing operation, but were not included in any calculations. However, the importance of this observation is offset by the small sample size and large standard deviation. at least, in this stage of the investigation The discussion will focus on cue phrases, even though the points are relevant to other lexical markers of discourse structure and processing.\", metadata={'source': '../data/raw/cmplg-xml/9511004.xml'}),\n",
       " Document(page_content='Creating a tagset, lexicon and guesser for a French tagger1\\n\\nWe earlier described two taggers for French, a statistical one and a constraint-based one. The two taggers have the same tokeniser and morphological analyser. In this paper, we describe aspects of this work concerned with the definition of the tagset, the building of the lexicon, derived from an existing two-level morphological analyser, and the definition of a lexical transducer for guessing unknown words.\\n\\nBackground\\n\\nTagset\\n\\nWe describe in this section criteria for selecting the tagset. The following is based on what we noticed to be useful during the developing the taggers.\\n\\nThe size of the tagset\\n\\nOur basic French morphological analyser was not originally designed for a (statistical) tagger and the number of different tag combinations it has is quite high. The size of the tagset is only 88. But because a word is typically associated with a sequence of tags, the number of different combinations is higher, 353 possible sequences for single French words. If we also consider words joined with clitics, the number of different combinations is much higher, namely 6525.\\n\\nA big tagset does not cause trouble for a constraint-based tagger because one can refer to a combination of tags as easily as to a single tag. For a statistical tagger however, a big tagset may be a major problem. We therefore used two principles for forming the tagset: (1) the tagset should not be big and (2) the tagset should not introduce distinctions that cannot be resolved at this level of analysis.\\n\\nVerb tense and mood\\n\\nAs distinctions that cannot be resolved at this level of analysis should be avoided, we do not have information about the tense of the verbs. Some of this information can be recovered later by performing another lexicon lookup after the analysis. Thus, if the verb tense is not ambiguous, we have not lost any information and, even if it is, a part-of-speech tagger could not resolve the ambiguity very reliably anyway. For instance, dort (present; sleeps) and dormira (future; will sleep) have the same tag VERB-SG-P3, because they are both singular, third-person forms and they can both be the main verb of a clause. If needed, we can do another lexicon lookup for words that have the tag VERB-SG-P3 and assign a tense to them after the disambiguation. Therefore, the tagset and the lexicon together may make finer distinctions than the tagger alone.\\n\\nOn the other hand, the same verb form dit can be third person singular present indicative or third person singular past historic (pass simple) of the verb dire (to say). We do not introduce the distinction between those two forms, both tagged as VERB-SG-P3, because determining which of the two tenses is to be selected in a given context goes beyond the scope of the tagger. However, we do keep the distinction between dit as a finite verb (present or past) on one side and as a past participle on the other, because this distinction is properly handled with a limited contextual analysis.\\n\\nMorphological information concerning mood is also collapsed in the same way, so that a large class of ambiguity between present indicative and present subjunctive is not resolved: again this is motivated by the fact that the mood is determined by remote elements such as, among others, connectors that can be located at (theoretically) any distance from the verb. For instance, a conjunction like quoique requires the subjunctive mood: Quoique, en principe, ce cas soit frquent. (Though, in principle, this case is [subjunctive] frequent.)\\n\\nThe polarity of the main verb to which a subordinate clause is attached also plays a role. For instance, compare: Je pense que les petits enfants font de jolis dessins. (I think that small kids make [indicative] nice drawings.) Je ne pense pas pas que les petits enfants fassent de jolis dessins. (I do not think that small kids make [subjunctive]  nice drawings.) Consequently, forms like chante are tagged as VERB-P3SG regardless of their mood. In the case of faire (to do, to make) however, the mood information can easily be recovered as the third person plural are font and fassent for indicative and subjunctive moods respectively.\\n\\nPerson\\n\\nThe person seems to be problematic for a statistical tagger (but not for a constraint-based tagger). For instance, the verb pense, ambiguous between the first- and third-person, in the sentence Je ne le pense pas (I do not think so) is disambiguated wrongly because the statistical tagger fails to see the first-person pronoun je and selects more common third-person reading for the verb.\\n\\nWe made a choice to collapse the first- and second-person verbs together but not the third person. The reason why we cannot also collapse the third person is that we have an ambiguity class that contains adjective and first- or second-person verbs. In a sentence like Le secteur matires (NOUN-PL) plastiques (ADJ-PL/NOUN-PL/VERB-P1P2)... the verb reading for plastiques is impossible. Because noun -- third-person sequence is relatively common, collapsing also the third person would cause trouble in parsing.\\n\\nBecause we use the same tag for first- and second-person verbs, the first- and second-person pronouns are also collapsed together to keep the system consistent. Determining the person after the analysis is also quite straightforward: the personal pronouns are not ambiguous, and the verb form, if it is ambiguous, can be recovered from its subject pronoun.\\n\\nLexical word\\n\\n\\n\\nform\\n\\nGender and number\\n\\nWe have not introduced gender distinctions as far as nouns and adjectives (and incidentally determiners) are concerned. Thus a feminine noun like chaise (chair) and a masculine noun like tabouret (stool) both receive the same tag NOUN-SG.\\n\\nHowever, we have introduced distinctions between singular nouns (NOUN-SG), plural nouns (NOUN-PL) and number-invariant nouns (NOUN-INV) such as taux (rate/rates). Similar distinctions apply to adjectives and determiners. The main reason for this choice is that number, unlike gender, plays a major role in French with respect to subject/verb agreement, and the noun/verb ambiguity is one of the major cases that we want the tagger to resolve.\\n\\nDiscussion on Gender Gender information would provide better disambiguation,\\n\\nGender ambiguous nouns should be resolved, and\\n\\nDisplaying gender provides more information.\\n\\nThere is obviously a strong objection against leaving out gender information as this information may provide a better disambiguation in some contexts. For instance in le diffuseur diffuse, the word diffuse is ambiguous as a verb or as a feminine adjective. This last category is unlikely after a masculine noun like diffuseur.\\n\\nHowever, one may observe that gender agreement between nouns and adjectives often involve long distance dependencies, due for instance to coordination or to the adjunction of noun complements as in une envie de soleil diffuse where the feminine adjective diffuse agrees with the feminine noun envie. In other words, introducing linguistically relevant information such as gender into the tagset is fine, but if this information is not used in the linguistically relevant context, the benefit is unclear. Therefore, if a (statistical) tagger is not able to use the relevant context, it may produce some extra errors by using the gender.\\n\\nAn interesting, albeit minor interest of not introducing gender distinction, is that there is then no problem with tagging phrases like mon allusion (my allusion) where the masculine form of the possessive determiner mon precedes a feminine singular noun that begins with a vowel, for euphonic reasons.\\n\\nAnother argument in favour of gender distinction is that some nouns are ambiguously masculine or feminine, with possible differences in meaning, e.g. poste, garde, manche, tour, page. A tagger that would carry on the distinction would then provide sense disambiguation for such words.\\n\\nActually, such gender-ambiguous words are not very frequent. On the same 12 000-word test corpus, we counted 46 occurrences of words which have different meanings for the masculine and the feminine noun readings. This number could be further reduced if extremely rare readings were removed from the lexicon, like masculine ombre (a kind of fish while the feminine reading means shadow or shade) or feminine litre (a religious ornament). We also counted 325 occurrences of nouns (proper nouns excluded) which do not have different meanings in the masculine and the feminine readings, e.g. lve, camarade, jeune.\\n\\nA reason not to distinguish the gender of such nouns, besides their sparsity, is that the immediate context does not always suffice to resolve the ambiguity. Basically, disambiguation is possible if there is an unambiguous masculine or feminine modifier attached to the noun as in le poste vs. la poste. This is often not the case, especially for preposition + noun sequences and for plural forms, as plural determiners themselves are often ambiguous with respect to gender. For instance, in our test corpus, we find expressions like en 225 pages,  leur tour, ces postes and pour les postes de responsabilit for which the contextual analysis does not help to disambiguate the gender of the head noun.\\n\\nFinally, carrying the gender information does not itself increase the disambiguation power of the tagger. A disambiguator that would explicitly mark gender distinctions in the tagset would not necessarily provide more information. A reasonable way to assess the disambiguating power of a tagger is to consider the ratio between the initial number of ambiguous tags vs. the final number of tags after disambiguation. For instance, it does not make any difference if the ambiguity class for a word like table is [feminine-noun, finite-verb] or [noun, finite-verb], in both cases the tagger reduces the ambiguity by a ratio of 2 to 1. The information that can be derived from this disambiguation is a matter of associating the tagged word with any relevant information like its base form, morphological features such as gender, or even its definition or its translation into some other language. This can be achieved by looking up the disambiguated word in the appropriate lexicon. Providing this derived information is not an intrinsic property of the tagger.\\n\\nOur point is that the objections do not hold very strongly. Gender information is certainly important in itself. We only argue that ignoring it at the level of part-of-speech tagging has no measurable effect on the overall quality of the tagger. On our test corpus of 12 000 words, only three errors violate gender agreement. This indicates how little the accuracy of the tagger could be improved by introducing gender distinction. On the other hand, we do not know how many errors would have been introduced if we had distinguished between the genders.\\n\\nRemaining categories\\n\\nWe avoid categories that are too small, i.e. rare words that do not fit into an existing category are collapsed together. Making a distinction between categories is not useful if there are not enough occurrences of them in the training sample. We made a category MISC for all those miscellaneous words that do not fit into any existing category. This accounts for words such as: interjection oh, salutation bonjour, onomatopoeia miaou, wordparts i.e. words that only exist as part of a multi-word expression, such as priori, as part of a priori.\\n\\nDividing a category\\n\\nIn a few instances, we introduced new categories for words that have a specific syntactic distribution. For instance, we introduced a word-specific tag PREP-DE for words de, des and du, and tag PREP-A for words , au and aux. Word-specific tags for other prepositions could be considered too. The other readings of the words were not removed, e.g. de is, ambiguously, still a determiner as well as PREP-DE.\\n\\nWhen we have only one tag for all the prepositions, for example, a sequence like determiner noun noun/verb preposition is frequently disambiguated in the wrong way by the statistical tagger, e.g. Le train part  cinq heures (The train leaves at 5 o\\'clock). The word part is ambiguous between a noun and a verb (singular, third person), and the tagger seems to prefer the noun reading between a singular noun and a preposition.\\n\\nWe succeeded in fixing this without modifying the tagset but the side-effect was that overall accuracy deteriorated. The main problem is that the preposition de, comparable to English of, is the most common preposition and also has a specific distribution. When we added new tags, say PREP-DE and PREP-A, for the specific prepositions while the other prepositions remained marked with PREP, we got the correct result, with no noticeable change in overall accuracy.\\n\\nBuilding the lexicon\\n\\nThe initial lexicon contains all the inflectional information. For instance, the word danses (the plural of the noun danse or a second person form of the verb danser (to dance) has the following analyses: danser +IndP +SG +P2 +Verb danser +SubjP +SG +P2 +Verb danse  +Fem +PL +Noun\\n\\nForms that include clitics are analysed as a sequence of items separated by the symbols [ or ] depending on whether the clitics precede or follow the head word. For instance vient-il (does he come, lit. comes-he) is analysed as: venir +IndP +SG +P3 +Verb ] il +Nom +Masc +SG +P3 +PC\\n\\nhandling cliticised forms appropriately for the tagger\\'s needs.\\n\\nswitching tagsets\\n\\nWith respect to switching tagsets, we use contextual two-level rules that turn the initial tags into new tags or to the void symbol if old tags must simply disappear. For instance, the symbol +Verb is transformed into +VERB-P3SG if the immediate left context consists of the symbols +SG +P3. The symbols +IndP, +SG and +P3 are then transduced to the void symbol, so that vient (or even the new token vient-) gets analysed merely as +VERB-P3SG instead of +IndP +SG +P3 +Verb.\\n\\nA final transformation consists in associating a given surface form with its ambiguity class, i.e. with the alphabetically ordered sequence of all its possible tags. For instance danses is associated with the ambiguity class [+NOUN-PL +VERB-P1P2], i.e. it is either a plural noun or a verb form that belongs to the collapsed first or second person paradigm.\\n\\nGuesser\\n\\nWords not found in the lexicon are analysed by a separate finite-state transducer, the guesser. We developed a simple, extremely compact and efficient guesser for French. It is based on the general assumption that neologisms and uncommon words tend to follow regular inflectional patterns.\\n\\nThe guesser is thus based on productive endings (like ment for adverbs, ible for adjectives, er for verbs). A given ending may point to various categories, e.g. er identifies not only infinitive verbs but also nouns, due to possible borrowings from English. For instance, the ambiguity class for killer is [NOUN-SG VERB-INF].\\n\\nThese endings belong to the most frequent ending patterns in the lexicon, where every rare word weights as much as any frequent word. Endings are not selected according to their frequency in running texts, because highly frequent words tend to have irregular endings, as shown by adverbs like jamais, toujours, peut-tre, hier, souvent (never, always, maybe...).\\n\\nSimilarly, verb neologisms belong to the regular conjugation paradigm characterised by the infinitive ending er, e.g. dballaduriser.\\n\\nWith respect to nouns, we first selected productive endings (iste, eau, eur, rice...), until we realised a better choice was to assign a noun tag to all endings, with the exception of those previously assigned to other classes. In the latter case, two situations may arise: either the prefix is shared between nouns and some other category (such as ment), or it must be barred from the list of noun endings (such as aient, an inflectional marking of third person plural verbs). We in fact introduced some hierarchy into the endings: e.g. ment is shared by adverbs and nouns, while iquement is assigned to adverbs only.\\n\\nGuessing based on endings offers some side advantages: unknown words often result from alternations, which occur at the beginning of the word, the rest remaining the same, e.g. derivational prefixes as in isralo-jordano-palestinienne but also oral transcriptions such as les z\\'oreilles (the ears), with z\\' marking the phonological liaison. Similarly, spelling errors which account for many of the unknown words actually affect the ending less than the internal structure of the word, e.g. the misspelt verb forms appellaient, geulait. Hyphens used to emphasise a word, e.g. har-mo-ni-ser, also leave endings unaltered. Those side advantages do not however operate when the alternation (prefix, spelling error) applies to a frequent word that does not follow regular ending patterns. For instance, the verb construit and the adverb trs are respectively misspelt as constuit and trs, and are not properly recognised.\\n\\nGenerally, the guesser does not recognise words belonging to closed classes (conjunctions, prepositions, etc.) under the assumption that closed classes are fully described in the basic lexicon. A possible improvement to the guesser would be to incorporate frequent spelling errors for words that are not otherwise recognised.\\n\\nTesting the guesser\\n\\nWe extracted, from a corpus of newspaper articles (Libration), a list of 13 500 words unknown to the basic lexicon. Of those unknown words, 9385 (i.e. about 70 %) are capitalised words, which are correctly and unambiguously analysed by the guesser as proper nouns with more than 95 % accuracy. Errors are mostly due to foreign capitalised words which are not proper nouns (such as Eight) and onomatopoeia (such as Ooooh).\\n\\nThe test on the remaining 4000 non-capitalised unknown words is more interesting. We randomly selected 800 of these words and ran the guesser on them. 1192 tags were assigned to those 800 words by the guesser, which gives an average of 1.5 tags per word.\\n\\nFor 113 words, at least one required tag was missing (118 tags were missing as a whole, 4 words were lacking more than one tag: they are misspelt irregular verbs that have not been recognised as such). This means that 86 % of the words got all the required tags from the guesser.\\n\\n273 of the 1192 tags were classified as irrelevant. This concerned 244 words, which means that 70 % of the words did not get any irrelevant tags. Finally, 63 % of the words got all the required tags and only those.\\n\\nIf we combine the evaluation on capitalised and non-capitalised words, 85 % of all unknown words are perfectly tagged by the guesser, and 92 % get all the necessary tags (with possibly some unwanted ones).\\n\\nThe test on the non-capitalised words was tough enough as we counted as irrelevant any tag that would be morphologically acceptable on general grounds, but which is not for a specific word. For instance, the misspelt word statisiticiens is tagged as [ADJ-PL NOUN-PL]; we count the ADJ-PL tag as irrelevant, on the ground that the underlying correct word statisticiens is a noun only (compare with the adjective platoniciens).\\n\\nThe same occurs with words ending in ement that are systematically tagged as [ADV NOUN-SG], unless a longer ending like iquement is recognised. This often, but not always, makes the NOUN-SG tag irrelevant.\\n\\nAs for missing tags, more than half are adjective tags for words that are otherwise correctly tagged as nouns or past participles (which somehow reduces the importance of the error, as the syntactic distribution of adjectives overlaps with those of nouns and past participles).\\n\\nThe remaining words that lack at least one tag include misspelt words belonging to closed classes (come, trs, vavec) or to irregular verbs (constuit), barbarisms resulting from the omission of blanks (proposde), or from the adjunction of superfluous blanks or hyphens (quand-mme, so cit). We also had a few examples of compound nouns improperly tagged as singular nouns, e.g. rencontres-tl, where the plural marking only appears on the first element of the compound.\\n\\nFinally, foreign words represent another class of problematic words, especially if they are not nouns. We found various English examples (at, born, of, enough, easy) but also Spanish, e.g. levantarse, and Italian ones, e.g. palazzi.\\n\\nConclusion\\n\\nAcknowledgments\\n\\nwant to thank Irene Maxwell and anonymous referees for useful comments.\\n\\nBibliography\\n\\nJean-Pierre Chanod. A Broad-Coverage French Grammar checker: Some Underlying Principles. in the Sixth International Conference on Symbolic and Logical Computing. pp 97-111, Dakota State University, 1993.\\n\\nJean-Pierre Chanod. Finite-State Composition of French Verb Morphology. Technical Report MLTT-005, 16 pages, Rank Xerox Research Centre, Grenoble, 1994. http://www.xerox.fr/\\n\\nJean-Pierre Chanod and Pasi Tapanainen. Statistical and Constraint-Based Taggers for French. Technical report MLTT-016, Rank Xerox Research Centre, Grenoble, 1994.\\n\\nJean-Pierre Chanod and Pasi Tapanainen. Tagging French - comparing a statistical and a constraint-based method. In the proceedings of the Seventh Conference of the European Chapter of the ACL. pages 149-156. Association for Computational Linguistics, Dublin, 1995. cmp-lg/9503003.\\n\\nAnne-Marie Derouault. Modlisation d\\'une langue naturelle pour dsambiguation des chanes phontiques. Doctorat d\\'tat s sciences, Universit VII, 1985.\\n\\nMarc El-Bze. Les modles de langage probabilistes: quelques domaines d\\'application. Thse d\\'habilitation  diriger des recherches. LIPN: Universit Paris-Nord, 1993.\\n\\nLauri Karttunen. Finite-State Lexicon Compiler. Technical report, Xerox Palo Alto Research Center, 1993.\\n\\nLauri Karttunen and Ken Beesley. Two-Level Rule Compiler. Technical report, Xerox Palo Alto Research Center, 1992.\\n\\nLauri Karttunen, Ron Kaplan and Annie Zaenen. Two-level morphology with composition. In proceedings of Coling-92. The fourteenth International Conference on Computational Linguistics. Vol I, pages 141-148. Nantes, 1992.\\n\\nAn example\\n\\nThis appendix contains an example of a tagged corpus.\\n\\nLes \\t\\t DET-PL travaux \\t\\t NOUN-PL devaient- \\t\\t VERB-P3PL -ils \\t\\t PRON se \\t\\t PC d\\'erouler \\t\\t VERB-INF en \\t\\t PREP s\\'eance \\t\\t NOUN-SG pl\\'eni`ere \\t\\t ADJ-SG ou \\t\\t CONN en \\t\\t PREP commissions \\t\\t NOUN-PL ? PUNCT Les \\t\\t DET-PL d\\'el\\'egu\\'es \\t\\t NOUN-PL pouvaient- \\t\\t VERB-P3PL -ils \\t\\t PRON , \\t\\t CM comme \\t\\t COMME d\\' \\t\\t PREP habitude \\t\\t NOUN-SG , \\t\\t CM ent\\'eriner \\t\\t VERB-INF des \\t\\t DET-PL r\\'esolutions \\t\\t NOUN-PL de \\t\\t PREP la \\t\\t DET-SG direction \\t\\t NOUN-SG du \\t\\t PREP parti \\t\\t NOUN-SG qui \\t\\t CONN n\\' \\t\\t NEG avaient \\t\\t VAUX-P3PL pas \\t\\t ADV \\'et\\'e \\t\\t VAUX-PAP discut\\'ees \\t\\t PAP-PL `a \\t\\t PREP la \\t\\t DET-SG base \\t\\t NOUN-SG ? PUNCT Ce \\t\\t DET-SG prologue \\t\\t NOUN-SG d\\'esordonn\\'e \\t\\t ADJ-SG enfin \\t\\t ADV termin\\'e \\t\\t ADJ-SG , \\t\\t CM le \\t\\t DET-SG pr\\'esident \\t\\t NOUN-SG en \\t\\t PREP exercice \\t\\t NOUN-SG de \\t\\t PREP la \\t\\t DET-SG Ligue \\t\\t NOUN-SG qui \\t\\t CONN , \\t\\t CM selon \\t\\t PREP la \\t\\t DET-SG r`egle \\t\\t NOUN-SG de \\t\\t PREP la \\t\\t DET-SG rotation \\t\\t NOUN-SG des \\t\\t PREP fonctions \\t\\t NOUN-PL , \\t\\t CM est \\t\\t VAUX-P3SG actuellement \\t\\t ADV un \\t\\t DET-SG Mac\\'edonien \\t\\t NOUN-SG , \\t\\t CM est \\t\\t VAUX-P3SG mont\\'e \\t\\t PAP-SG `a \\t\\t PREP la \\t\\t DET-SG tribune \\t\\t NOUN-SG pour \\t\\t PREP exposer \\t\\t VERB-INF la \\t\\t DET-SG nouvelle \\t\\t ADJ-SG strat\\'egie \\t\\t NOUN-SG des \\t\\t PREP communistes \\t\\t NOUN-PL yougoslaves \\t\\t ADJ-PL .\\n\\nPUNCT Dans \\t\\t PREP un \\t\\t DET-SG discours-fleuve \\t\\t NOUN-SG qui \\t\\t CONN constituait \\t\\t VERB-P3SG le \\t\\t DET-SG plus \\t\\t ADV petit \\t\\t ADJ-SG d\\'enominateur \\t\\t NOUN-SG commun \\t\\t ADJ-SG des \\t\\t PREP positions \\t\\t NOUN-PL respectives \\t\\t ADJ-PL des \\t\\t PREP partis \\t\\t NOUN-PL des \\t\\t PREP six \\t\\t NUM r\\'epubliques \\t\\t NOUN-PL et \\t\\t CONN des \\t\\t PREP deux \\t\\t NUM provinces \\t\\t NOUN-PL autonomes \\t\\t ADJ-PL , \\t\\t CM Mr \\t\\t NOUN-SG Milan \\t\\t NOUN-INV Pancevski \\t\\t NOUN-INV s\\' \\t\\t PC est \\t\\t VAUX-P3SG prononc\\'e \\t\\t PAP-SG pour \\t\\t PREP la \\t\\t DET-SG libert\\'e \\t\\t NOUN-SG d\\' \\t\\t PREP association \\t\\t NOUN-SG politique \\t\\t ADJ-SG ( \\t\\t PUNCT et \\t\\t CONN donc \\t\\t ADV l\\' \\t\\t DET-SG abandon \\t\\t NOUN-SG du \\t\\t PREP monopole \\t\\t NOUN-SG de \\t\\t PREP la \\t\\t DET-SG Ligue \\t\\t NOUN-SG ) \\t\\t PUNCT , \\t\\t CM pour \\t\\t PREP la \\t\\t DET-SG r\\'eforme \\t\\t NOUN-SG du \\t\\t PREP syst`eme \\t\\t NOUN-SG \\'economique \\t\\t ADJ-SG et \\t\\t CONN politique \\t\\t ADJ-SG , \\t\\t CM ainsi \\t\\t ADV que \\t\\t CONJQUE du \\t\\t PREP fonctionnement \\t\\t NOUN-SG de \\t\\t PREP la \\t\\t DET-SG LCY \\t\\t NOUN-INV . PUNCT C\\' \\t\\t PRON est \\t\\t VAUX-P3SG la \\t\\t DET-SG seule \\t\\t ADJ-SG faon \\t\\t NOUN-SG , \\t\\t CM a-t- \\t\\t VAUX-P3SG -il \\t\\t PRON dit \\t\\t PAP-SG , \\t\\t CM de \\t\\t PREP \" \\t\\t PUNCT pr\\'eserver \\t\\t VERB-INF les \\t\\t DET-PL valeurs \\t\\t NOUN-PL de \\t\\t PREP la \\t\\t DET-SG r\\'evolution \\t\\t NOUN-SG socialiste \\t\\t ADJ-SG yougoslave \\t\\t ADJ-SG \" \\t\\t PUNCT . PUNCT\\n\\nFootnotes', metadata={'source': '../data/raw/cmplg-xml/9503004.xml'})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 125.94it/s]\n"
     ]
    }
   ],
   "source": [
    "insert_embedded_documents(docs[:10], embeddings, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/isaac/FundamentlPartners/abinvenv-sol')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(f\"{BASE_DIR}/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1 \\n  \\nTestimony of  \\n \\nBlake Gerard  \\n \\nBefore the U.S.  House of Representatives   \\nCommittee on Agriculture  \\n \\nHearing to Review the Future of U.S. Farm Policy and the Formulation of the 2012 Farm Bill  \\n \\nGalesburg, Illinois  \\nMarch 23 , 2012  \\n \\nIntroduction  \\n \\nChairman Lucas, Ranking Member Peterson , and Members of the Committee, thank you for \\nholding this hearing concerning farm policy and the 2012 farm bill.   I appreciate the opportunity \\nto offer testimony on farm policy from the perspective of a diversified grain producer.    \\n \\nMy name is Blake Gerard.   I raise rice , soybean s, wheat, and corn in  Alexander and Union \\ncounties in southern Illinois  and I have been farming on my own now for 16 years.   I am the \\nfourth generation in my family to farm this land and this is my 13th year  to farm rice in Illinois.  I \\nam also co-owner in a seed conditioning facility that does contract seed production, \\nconditioning, packaging & warehousing.  All of our soybeans are raised for seed along with \\nabout 75% of our rice. In addition to my farm and seed business, I also serve as the \\ncommissioner for the East Cape Girardeau/Clear Creek Levee & Drainage District , the  Illinois \\nCrop Improvement Association and am a member of the USA Rice Producers’ Group  Board of \\nDirectors .  \\n \\nImportance of Agriculture an d Cost -Effective Farm Policy  \\n \\nU.S. agriculture shares a certain amount of pride for what we do for the nation’s economy.   \\nAgriculture still matters.  \\n \\nOver the course of the current economic downturn, here is an excerpt of what objective \\nsources ranging fro m the Federal Reserve to The Wall Street Journal  had to say about what \\nAmerica’s farmers and ranchers have been doing to help get our nation back on track and \\npeople back to work:  \\n “In 2010, rural America was at the forefront of the economic recovery… “[R ]ising exports of \\nfarm commodities and manufactured goods spurred job growth and income gains in rural \\ncommunities…If recent history holds true, rural America could lead U.S. economic gains in \\n2011.”   Federal Reserve of Kansas City, 2010 report.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 0}),\n",
       " Document(page_content='2 \\n \"Growers\\' improved lot is rippling out to other industries.\"   The Wall Street Journal, October 12, \\n2010.  \\nWe read the same kinds of reports during the last recession when the manufacturing sector \\nwas in crisis:  \\n“Farm Belt Is Becoming a Driver for Overall Economy…The present boom is proving that \\nagriculture still matters in the U.S. Rising farm incomes are helping to ease the blow of the loss \\nof manufacturing jobs in Midwest states…‘The farm sector is a significant source of strength for \\nthe U.S. economy,’ says Sung Wo n Sohn, chief economist of Wells Fargo Bank…Although \\nfarmers themselves are a tiny part of the population, they have an outsize impact on the \\neconomy because farming is such an expensive enterprise. A full -time Midwest grain farmer \\noften owns millions of d ollars of equipment and land, and spends hundreds of thousands of \\ndollars annually on supplies.”   The Wall Street Journal, December 17, 2003 . \\nAnd, for those old enough to remember the 1980s, publications such as The Economist  recalled \\nthe impact on the res t of the economy when agriculture was not doing well:   \\n“The 1990s were so good [for Chicago] partly because the 1980s had been so bad.   ‘Everything \\nthat could possibly have gone wrong did’ says William Testa, the senior economist at the \\nFederal Reserve Ba nk of Chicago.   The region was hit by a crushing combination of high energy \\nprices, a strong dollar, high interest rates, and a farm recession.”   The Economist, May 12, 2001  \\nLast year alone, U.S. farmers and ranchers spent nearly $320 billion in communitie s across the \\ncountry to produce agriculture products valued at some $410 billion.   Put in perspective, the \\nvalue of total U.S. agriculture production was greater than the 2010 GDP of all but 25 nations, \\nand total production cost was greater than all but 28 .  And, according to the Department of \\nAgriculture, U.S. agriculture is expected to positively contribute $26.5 billion to the U.S. balance \\nof trade in fiscal year 2012 after having contributed over $40 billion just the year before.               \\nAnd, one o f the reasons we are here today, I expect, is because while U.S. agriculture is critically \\nimportant to America, farm policy is also critically important to U.S. agriculture.  \\nWithout farm policy, U.S. producers would be unilaterally exposed to global marke ts distorted \\nby withering high foreign subsidies and tariffs, and have no comprehensive safety net.  In fact, \\nDTB & Associates issued a report last fall, similar to the study on tariffs and subsidies developed \\nand maintained by Texas Tech University ( http://www.depts.ttu.edu/ceri/index.aspx .), which \\nfound that:   \\n“U.S. subsidies…have dropped to very low levels in recent years.   In the meantime, there has \\nbeen a major increase in subsidization among adva nced developing countries… Since the \\ncountries involved are major producers and consumers of agricultural products, the trade -\\ndistorting effects of the subsidies are being felt globally. However, because the run -up in \\nsubsidies is a recent development, and  because countries have not reported the new programs', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 1}),\n",
       " Document(page_content='3 \\n to the WTO or have failed in their notifications to calculate properly the level of support, the \\nchanges have attracted little attention. We believe that when trade officials examine these \\ndevelopments,  they will discover clear violations of WTO commitments.”     \\nThis aggressive increase in foreign subsides and tariffs might also explain why foreign \\ncompetitors worked to derail WTO Doha Round negotiations, causing then Chairmen and \\nRanking Members of the Senate Finance Committee and House Ways & Means Committee to \\nregister their opposition to pursuing  a lopsided agreement against the U.S. interests:  \\n“Since the WTO Doha Round was launched in 2001, we have supported the administration’s \\nefforts to achieve a  balanced outcome that would provide meaningful new market access for \\nU.S. agricultural products…particularly from developed and key emerging markets.   \\nUnfortunately, the negotiating texts currently on the table would provide little if any new \\nmarket acces s for U.S. goods, and important developing countries are demanding even further \\nconcessions from the United States.”   Ways & Means Committee Chairman and Ranking \\nMember Rangel and McCrery and Finance Committee Chairman and Ranking Member Baucus \\nand Grassle y.   \\nMoreover, while many successfully negotiated trade agreements have promised market access \\ngains for agriculture, much of what was promised has yet to materialize or is continually \\nthreatened by artificial sanitary, phytosanitary (SPS) and other non -tariff barriers.    This is why \\nprograms such as the Market Access Program and Foreign Market Development Program are of \\nvital concern to the rice industry and must be reauthorized in the 2012 farm bill.  It has not \\ngone unnoticed that budget reductions curre ntly being considered (such as the elimination of \\nthe Direct Payment) will result in a dollar for dollar loss in farm income.  Producers must be \\nprovided the tools not only to attack these obstacles to trade but to increase exports through \\nmarket promotion  and thereby increase farm income through increased open and fair trade.    \\n \\nBut, beyond even these barriers that are imposed by foreign competitors are barriers to exports \\nimposed in whole or in part by the U.S. government.   For example, rice was complete ly \\nexcluded from the free trade agreement negotiated with South Korea, foreclosing for the \\nforeseeable future any new market access for U.S. rice producers in that country.   Iraq, once a \\ntop export market for U.S. rice, has instituted restrictive specifica tions on rice imports that have  \\nled to a 77 percent drop in sales of U.S. rice to that country.  In the pending Trans Pacific \\nPartnership (TPP) negotiations, Japan has indicated an interest in joining.  The U.S. rice industry \\nsupports Japan joining the neg otiations, but only if additional market access for U.S. rice into \\nJapan is part of the agreement.  Our industry cannot support an agreement where market \\naccess for our product is categorically off the negotiating table.  Another market that has the \\npotent ial to become a top five export market almost immediately is Cuba. Unfortunately, the \\nU.S. government maintains restrictions on our agricultural exports to this country. Cuba was \\nonce the number one export market for U.S. rice prior to the embargo and we b elieve it is \\npotentially a 400,000 to 600,000 ton market if normal commercial agricultural exports are \\nallowed to resume.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 2}),\n",
       " Document(page_content='4 \\n In total, U.S. rice exports to date for the current marketing year are down 24 percent compared \\nto last year.  \\nAnd, while the rice in dustry is still a long ways off from having a crop insurance product that is \\nrelevant to rice producers, the general need for federal involvement in insuring crops where \\nlosses are highly correlated is also obvious, as even the American Enterprise Institut e has \\nadmitted:    \\n “The empirical evidence on the viability of either area -yield or multiple -peril crop insurance \\nseems clear.   When normal commercial loading factors are applied, the premiums required by \\ninsurers to offer an actuarially viable private cr op insurance contract are sufficiently high to \\nreduce the demand for such contracts to zero…Thus, private markets for multiple -peril crop \\ninsurance are almost surely infeasible, and the weight of the empirical evidence indicates that \\narea -yield contracts a re also not commercially viable…”   American Enterprise Institute, “The \\nEconomics of Crop Insurance and Disaster Aid, 1995.  \\nFortunately, for the American taxpayer, in addition to all of these justifications on why we have \\na farm policy in this country, we c an add to the list at least one more reason:   farm policy is \\ncost-effective.  \\nIn fact, U.S. farm policy has operated under budget for over a decade  and accounts for only one \\nquarter of one percent of the total federal budget.   Not including additional cuts scheduled \\nunder sequestration, U.S. farm policy has, to date, been cut by about $18 billion over the past 9 \\nyears, including in the 2004 and 2010 Standard Reinsurance Agreements (SRAs), the FY2006 \\nreconciliation package, and the 2008 Farm Bill.  \\n \\nIn the mos t recent five years, average funding for U.S. farm policy, based on real funding levels, \\nincluding crop insurance, was $12.9 billion per year, which is 28% less than the previous five -\\nyear average of $17.9 billion and 31% less than the average of $18.8 bil lion that incurred in the \\npreceding five years.   In the current year, the Congressional Budget Office (CBO) estimates that \\ncrop insurance policy will cost slightly more than the current commodity policies.  And \\naccording to CBO projections for the next 10 years the estimated annual cost for commodity \\npolicy in the farm bill is $6.6 billion on average (before the expected reductions are made as \\npart of this farm bill process), while the estimated annual cost for crop insurance policy is $8.8 \\nbillion on avera ge.  With the current suite of crop insurance policies not working effectively for \\nrice producers, this puts our industry at a further disadvantage and highlights the need to \\nmaintain an effective commodity policy in the farm bill that will work for rice.   \\n \\nFunding of that portion of farm policy that assists rice producers has declined from $1.2 billion \\na decade ago to about $400 million annually, with this amount largely reflecting Direct \\nPayments.  \\n \\nMeanwhile, U.S. consumers are paying less than 10% of di sposable income on food, less than \\nconsumers in any other nation.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 3}),\n",
       " Document(page_content='5 \\n This is why I believe so firmly that future cuts must focus on areas of the budget outside of farm \\npolicy that have not yet contributed to deficit reduction yet comprise a significant share of the \\nfederal budget.   This is also why I would urge lawmakers to reject cuts to U.S. farm policy that \\nwould exceed the level specified by the House and Senate Agriculture Committee Chairs and \\nRanking Members in their letter to the Joint Committee on Defi cit Reduction last fall.   \\n \\n2008 Farm Bill Review  \\n \\nThe Food, Conservation, and Energy Act of 2008 (the Farm Bill) continued the traditional mix of \\npolicies consisting of the non -recourse marketing loan, loan deficiency payments, and the direct \\nand counter c yclical payments.   The farm bill also included the addition of Average Crop \\nRevenue Election (ACRE) as an alternative to counter cyclical payments for producers who \\nagree to a reduction in direct payments and marketing loan benefits.   The bill also added \\nSupplemental Revenue Assurance (SURE) as a standing disaster assistance supplement to \\nfederal crop insurance.  \\n \\nThe 2008 Farm Bill made very substantial changes to the payment eligibility provisions, \\nestablishing an aggressive adjusted gross income (AGI) me ans test and, albeit unintended by \\nCongress, resulting in the very significant tightening of “actively engaged” requirements for \\neligibility. USDA was still in the process of implementing many of the provisions of the 2008 \\nFarm Bill in 2010, and the final payment eligibility rules were only announced in January of that \\nsame year, a mere two years ago.   As a consequence, we are still adjusting to the many changes \\ncontained in the current farm bill, even as Congress considers the 2012 farm bill.  \\n \\nRegarding AC RE and SURE, frankly, neither policy has proved much value to rice farmers.   \\nSpecifically, in the first year of ACRE signup, only 8 rice farms representing less than 900 acres \\nwere enrolled nationwide.   With changes, this revenue program may provide more v alue for \\nsome rice growing regions like California.  And SURE has provided little, if any, assistance to rice \\nproducers, including those producers in the Mid South who suffered significant monetary losses \\nin 2009 due to heavy rains and flooding occurring pr ior to and during harvest, or the significant \\nlosses last year as a result of spring flooding in the Mid South.  SURE’s inability to provide \\ndisaster assistance for such catastrophic events further highlights the continuing gap in \\navailable programs design ed to help producers manage or alleviate their risk.    \\n \\nRegarding the traditional mix of farm policies, the nonrecourse marketing loan, loan deficiency \\npayment, and countercyclical payments have not yet provided payments to rice farmers under \\nthe 2008 Far m Bill.   The new price paradigm has, as a practical matter, greatly limited the \\nprotections afforded to producers under these farm p olicy  features.   In fact, if the protections \\nprovided were ever to trigger for rice farmers, the protections would help stem  some of the \\neconomic losses but, frankly, not enough to keep most rice farms in business through even a \\nsingle year of severely low market prices.  \\n \\nAs such, whatever its imperfections, the Direct Payment alone has assisted rice producers in \\nmeeting the o ngoing and serious price and production perils of farming today.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 4}),\n",
       " Document(page_content='6 \\n For rice producers, as for most other producers, the existing levels of price protection have \\nsimply not kept pace with the significant increases in production costs, costs such as energy an d \\nfertilizer that are exacerbated by escalating government regulations.   It is for this reason that \\nrice farmers believe strengthening farm policies in the 2012 Farm Bill would be helpful in \\nensuring that producers have the ability to adequately manage the ir risks and access needed \\ncredit.  \\n \\nCrop Insurance  \\n \\nRisk management products offered under Federal Crop Insurance have been of very limited \\nvalue to rice producers to date due to a number of factors, including artificially depressed \\nactual production histo ry (APH) guarantees, which I understand is also a problem for many \\nother producers; high premium costs for a relatively small insurance guarantee; and the fact \\nthat the risks associated with rice production are unique from the risks of producing many \\nother  major crops.  \\n \\nFor example, since rice is a flood -irrigated crop, drought conditions rarely result in significant \\nyield losses as growers simply pump additional irrigation water to maintain moisture levels to \\nachieve relatively stable yields.   However, dro ught conditions do result in very substantial \\nproduction cost increases as a result of pumping additional water.   As such, what rice farmers \\nneed from federal crop insurance are products that will help protect against increased \\nproduction and input costs, particularly for energy and energy -related inputs.   For example, \\nfuel, fertilizer, and other energy related inputs represent about 70 percent of total variable \\ncosts.  \\n \\nIn this vein, many in the rice industry have been working for over the past four years now to \\ndevelop a new generation of crop insurance products that might provide more meaningful risk \\nmanagement tools for rice producers in protecting against sharp, upward spikes in input costs.   \\nI serve on a rice industry task force that has been working t o develop and improve crop \\ninsurance products for rice,  and a lthough the objective was to gain approval from the Risk \\nManagement Agency (RMA) of at least two new products that could be available to growers in \\ntime for the 2012 crop year, this has not mater ialized.   But, it is important to stress that even if \\nthese products had become available this year, we do not believe that they would have put rice \\nproducers anywhere near on par with other crops in terms of the relevance that crop insurance \\nhas as a risk  management tool.               \\n \\nAs such, rice producers enter the 2012 farm bill debate at a very serious disadvantage, having \\nonly a single farm policy that effectively works and that farm policy being singled out for \\nelimination.  \\n \\n2012 Farm Bill  \\n \\nWith th e foregoing as a backdrop, the U.S. rice industry developed a set of farm policy priorities \\nin September of last year to guide us during consideration of the 2012 Farm Bill.   The U.S. rice', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 5}),\n",
       " Document(page_content='7 \\n industry is unified in its firm belief that farm policy designed to  support a strong and dynamic \\nU.S. agriculture sector is absolutely vital.   We also believe that the planting flexibility provided \\nunder the 1996 Farm Bill and the countercyclical policies that have been in place for more than \\na decade now have served this  nation and its farmers well.   In particular, as we noted earlier, \\nthe 1996 Farm Bill’s Direct Payments have provided critical help to rice farmers – offering \\ncapital farmers could tailor to their unique needs.   We are very proud to stand by this farm \\npolicy.   \\n \\nHowever, given budget pressures and other considerations facing Congress that have caused \\npolicymakers to consider altering this approach in favor of more directed and conditioned \\nassistance, we developed the following priorities:  \\n\\uf0b7 First, we believe the triggering mechanism for assistance should be updated to provide \\ntailored and reliable help should commodity prices decline below today’s production \\ncosts, and should include a floor or reference price to protect in multi -year low price \\nscenarios.  \\n\\uf0b7 Seco nd, as payments would only be made in loss situations, payment limits and means \\ntests for producers should be eliminated.  \\n\\uf0b7 Third, federal crop insurance should be improved to provide more effective risk \\nmanagement for rice in all production regions, beginni ng with the policy development \\nprocess.  \\nMore specifically relative to each of these points, we believe that:  \\nPrice Protection is a Must  \\nGiven price volatility for rice is the primary risk producers face that they do not have other good \\nmeans of protecting against, with price fluctuations largely driven by global supply and \\ndemand;   given rice is one of the most protected and sensitive global commodities in trade \\nnegotiations, thus limiting access to a number of key markets; given costs of production have \\nrisen to a point where the current $6.50 (loan rate)/$10.50 (target price) assistance triggers are \\nlargely irrelevant, we believe the first priority should be to concentrate on increasing the prices \\nor revenue levels at which farm policy would trigger so tha t it is actually meaningful to \\nproducers, and would reliably trigger should prices decline sharply.   \\nThe reference price for rice should be increased to $13.98/cwt ($6.30/bu).  This level would \\nmore closely reflect the significant increases in production costs for rice.  And we believe this \\nreference price should be a component of both the price -loss policy and the revenue -loss policy \\nto ensure downside price protection.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 6}),\n",
       " Document(page_content=\"8 \\n Options for Different Production Regions  \\nIn addition, there should be true options fo r producers that recognize that a one -size-fits-all \\napproach to farm policy does not work effectively for all crops or even the same crop such as \\nrice in different production regions.    \\nIn the Mid -south and Gulf Coast production regions, a price -based loss  policy is viewed as being \\nmost effective in meeting the risk management needs of producers.  Specifically, this policy \\nshould include a price protection level that is more relevant to current cost of production; paid \\non planted acres or p ercentage of plan ted acres; paid on more current yields ; and take into \\naccount the lack of effective crop insurance policies for rice . \\nIn the California production  region , although the existing revenue -based policy still does not \\nprovide effective risk management , efforts to analyze modification s which will increase its \\neffectiveness cont inue.   Since rice yields are highly correlated between the farm, county, crop \\nreporting district, and state levels, we believe the revenue plan should be administered for rice \\nat either the  county or crop reporting district level to reflect this situation rather than lowering \\nguarantee levels to use farm level yields .  By setting loss triggers that reflect local marke ting \\nconditions, delivering support sooner, and strengthening revenue guara ntees that account for \\nhigher production costs as well as the absence of effective crop insurance , California rice \\nproducers are hopeful that an effective revenue program can  be developed.  \\nWhile I have focused on the need for a choice for rice producers in  different regions, this also \\napplies for producers of most other grains.  I support having policy options available for corn, \\nsoybeans, and wheat, which I produce, and believe that both a price -based policy and a \\nrevenue -based policy  should be offered as options for these crops.  \\nWhatever is done should be plain and bankable.   The current SURE has too many factors and is \\nnot tailored to the multiple business risks producers  face — it is not plain.  The current ACRE, \\nwhile offering improved revenue -based pro tection, is complicated by requiring two loss \\ntriggers; providing payments nearly two years after a loss; and provides no minimum price \\nprotection — it is not bankable.  The marketing loan and target prices are plain and bankable — \\nunfortunately the trigge r prices are no longer relevant to current costs and prices.  \\nWhatever is done should be tailored and defendable.   We believe it makes sense to provide \\nassistance when factors beyond the producer's control create losses for producers. We \\ngenerally think mo re tailored farm policies are more defendable.   For this reason, we like the \\nthought of updating bases and yields or applying farm policies to planted acres/current \\nproduction and their triggering based on prices or revenue, depending on the option a \\nprod ucer chooses.  However, policy choices should not result in severe regional distortions in \\ncommodity policy budget baselines from which reauthorized commodity policies must be \\ndeveloped.\", metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 7}),\n",
       " Document(page_content=\"9 \\n Whatever is done should be built to withstand a multi -year low price scenario.  Whether in a \\nrevenue -based  plan, or a price -based plan, reference prices should protect producer income in \\na relevant way in the event of a series of low price years.  Ideally, this minimum could move \\nupward over time should production costs also  increase, this being of particular concern in the \\ncurrent regulatory environment.   \\nWhatever is done should not dictate or distort planting decisions.   Direct payments are \\nexcellent in this regard.  SURE or similar whole farm aggregations tend to discourag e \\ndiversification, which could be a problem for crops like rice.  Any commodity specific farm \\npolicy that is tied to planted acres must be designed with extreme care so as to not create \\npayment scenarios that incentivize farmers to plant for a farm policy.   Whatever is done should \\naccommodate history and economics and allow for proportional reductions to the baseline \\namong commodities.  Some commodities are currently more reliant on countercyclical farm \\npolicies (ACRE/CCP) while others are receiving only Di rect Payments in the baseline.  Generally, \\nthe least disruptive and fairest way to achieve savings across commodities would be to apply a \\npercentage reduction to each commodity baseline and restructure any new policy within the \\nreduced baseline amounts.  \\nThere have been concerns raised about higher reference prices distorting planting decisions \\nand resulting in significant acreage shifts  including  for rice.  W e are unaware of any  analysis \\nthat shows significant acreage shifts resulting from the reference pri ce levels included in the \\n2011 Farm Bill package .  In fact, for rice specifically, a reference price of $13.98/cwt that is paid \\non historic CCP payment yields and on 85% of planted acres results in a reference price level \\nwell below our average cost of pro duction, so I find it hard to imagine why someone would \\nplant simply due to this policy given these levels.    \\nPay Limits/Eligibility Tests Should Be Eliminated  \\nThe likely outcome of new farm policy is that it will provide less certainty for the producer ( a \\nlikely decrease or elimination of Direct Payments).  Since i t will likely be designed to provide \\nassistance only in loss situations, the second priority is that the policy should not be limited \\nbased on arbitrary dollar limits. Assistance should be tailo red to the size of loss.  A producer \\nshould not be precluded from participating in a farm policy because of past income experience.  \\nAny internal limits on assistance should be percentage -based (i.e. 25% of an expected crop \\nvalue) and not discriminate base d on the size of farm.  \\nCrop Insurance Should Be Maintained and Improved  \\nAlthough crop insurance does not currently work as well for rice as it does for other crops, the \\nthird priority would be to improve availability and effectiveness of crop insurance fo r rice as an \\navailable option.  I would also support improvement to the product development processes (we \\nhave struggled with two 508(h) submissions for over 4 years and are still not completed with \\nthe process), and to the APH system such that any farmer' s insurable yield (pre -deductible) \\nwould be reflective of what that farmer actually expects to produce.   In no case should the crop\", metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 8}),\n",
       " Document(page_content='10 \\n insurance tools, which are purchased by the producer, be encumbered with \\nenvironmental/conservation regulation or other cond itions that fall outside the scope of \\ninsurance.   \\n2011 Budget Control Act Efforts  \\n \\nAlthough the details of the 2011 Farm Bill package that was prepared by the House and Senate \\nAgriculture Committees in response to the Budget Control Act were not disclosed,  based on \\ndiscussions and reports we believe that that package at least represents a good framework on \\nwhich to build the 2012 Farm Bill. The 2011 package included a choice of risk management \\ntools that producers can tailor to the risks on their own farms,  providing under each of those \\noptions more meaningful price protection that is actually relevant to today’s production costs \\nand prices.  It also included provisions to improve crop insurance and expedite product \\ndevelopment for underserved crops such as rice.    \\n \\nWe are concerned that effective support for rice producers under the price -based option was \\nset well below cost of production that  late changes to the revenue -based option minimized its \\npotential as an effective risk management tool for rice produ cers, and that pay limits and AGI \\nrules would still serve as an arbitrary constraint upon U.S. competitiveness, globally.   Still, even \\nwith these areas for improvement, the U.S. rice industry very much appreciates the Members \\nand staff who put enormous tim e and effort into what we believe represents a good blue print \\nfor ongoing Farm Bill deliberations and we thank you.  \\n \\nAgain, thank you for this opportunity to offer my testimony.   We certainly look forward to \\nworking with you on an effective 2012 Farm Bill  we can all be proud of.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 9}),\n",
       " Document(page_content='BLAKE GERARD  \\n \\n \\n \\nEXPERIENCE  \\n \\n2002 to present  GERARD&CRAIN FARMS, INC. dba RIVERBEND RICE SEED CO. – McClure, IL  \\n   President/Manager  \\n• Founded this Agribusiness Compa ny specializing in rice seed production.  The primary \\ngoal is to provide a superior quality seed supply to the Mid -South rice producer.   \\n• Began processing soybeans in 2005 for regional seed companies. Grew from zero units to over 450,000 units in 6 years.  \\n• Oversee 6 employees and 2  seasonal employees  \\n• Grew market from 0 -80,000 bushels of seed rice in 4 years    \\n• Manage production, storage, processing and packaging for all seed rice.  \\n• Manage storage, processing and packaging for soybean seed. Assist in distribution.  \\n• Train and monitor contract producer s, purchase seed from producers, devise \\nproduction contracts, monitor seed production, maintain genetic purity for individual \\nseed varieties, maintain seed in storage, clean seed, package seed, and market seed.  \\n• Market  rice seed directly to farmers and through 8 wholesale locations in southeast \\nMissouri.  \\n \\n \\n1996 to present   BLAKE GERARD  FARMS – McClure, IL  \\n   Owner/Operator  \\n• Operate 20 00-acre Rice, Corn, S oybean , and Wheat   farm .   \\n• Make all management decisions involved w ith the farm from planning to producing, to \\nharvest and marketing.  \\n• Utilize cash, hedge to arrive, futures and options contracts to market products.  \\n• Oversee 2 full time employees and 6 seasonal employees  \\n• Maintain computerized accounting records using AgriSolutions and Ag Manager software producing detailed five year financial forecasts\\n \\n• Produce seed beans for Delta Grow  Seed Company  & Morsoy Seed  co., Stine Seed Co., \\nArmor Seed Co., and Steyer Seed Co.  \\n \\nMEMBERSHIPS AND SERVICE  \\n 1992 to present   Commissioner  - East Cape Girardeau/Clear Creek Levee & Drainage District  \\nAugust 2004 Guest Speaker – M iddle Mississippi River Management Conference at Southern Illinois \\nUniversity on “Potential for Rice as Wildlife Habitat in the Middle Mississippi River Valley ” \\n    Member - Illinois C rop Improvement Association  \\nDecember 2008  Member – USA Rice Producers Group  Crop Insurance Task Force  \\n    \\nBoard Member – Missouri Rice Producers Group   \\n       2011   Member  - US A Rice Producers ’ Group Board of Directors  \\nMember - USA Rice Pr oducers’ Group Farm Policy Task Force   \\n   Member – USA Rice Federation Marketability and Competitiveness T ask Force \\n    EDUCATION  \\n 1999   Southeast Missouri State University – Cape Girardeau, MO  \\nB.S. Agriculture with emphasis in Agronomy', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 10})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name = EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1 \n",
      "  \n",
      "Testimony of  \n",
      " \n",
      "Blake Gerard  \n",
      " \n",
      "Before the U.S.  House of Representatives   \n",
      "Committee on Agriculture  \n",
      " \n",
      "Hearing to Review the Future of U.S. Farm Policy and the Formulation of the 2012 Farm Bill  \n",
      " \n",
      "Galesburg, Illinois  \n",
      "March 23 , 2012  \n",
      " \n",
      "Introduction  \n",
      " \n",
      "Chairman Lucas, Ranking Member P\n",
      "10: BLAKE GERARD  \n",
      " \n",
      " \n",
      " \n",
      "EXPERIENCE  \n",
      " \n",
      "2002 to present  GERARD&CRAIN FARMS, INC. dba RIVERBEND RICE SEED CO. – McClure, IL  \n",
      "   President/Manager  \n",
      "• Founded this Agribusiness Compa ny specializing in rice seed production.  The primary \n",
      "goal is to provide a superior quality seed supply to the Mid -Sout\n"
     ]
    }
   ],
   "source": [
    "faiss_index = FAISS.from_documents(pages, embeddings)\n",
    "docs = faiss_index.similarity_search(\"What does Blake Gerard?\", k=2)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPDFs(path: str) -> List[Document]:\n",
    "    docs = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            print(os.path.join(path, file))\n",
    "            loader = PyPDFLoader(os.path.join(path, file))\n",
    "            pages = loader.load_and_split()\n",
    "            docs.extend(pages)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/JI27U7TQTGCH2WWIVTO3GRUAQ2AGBBDH.pdf\n",
      "/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf\n",
      "/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/guidance_observationalrainfalldata.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Promoter: Dannon Svab  \\n \\nLocation: Ridgewood High \\nSchool, West Lafayette, Ohio \\nDate:  April 5, 2008 \\n \\nCommissioner:   \\nInspectors:  \\nRandy Jarvis Ohio Athletic Commission \\nExecutive Director \\nBernie Profato \\nOffice: (330) 797-2556 \\nwww.aco.ohio.gov\\n          MMA \\n    Amateur \\nCaged Madness 3 Judges: \\nR Wince \\nW Messer \\nB Pethel \\nReferees: \\nC Snider \\nM Matheny \\nTimekeepers:   \\n K Matheny \\nPhysicians: \\n  Dr Chlovechuk \\n \\nSch \\nRnds  Contestants Weight Results Remarks \\nSteve Roberts 197 Lost  AM \\n  3 Jeremiah Street 194 Won UNAN \\nMatt Nelson 155 Won UNAN  \\nBradford Jordan 156 Lost  \\nLarry Shuck 203 Won UNAN  \\nJesus Santiago 197 Lost  \\nBrian Kerr 154 Lost   \\nRyan McLaughlin 154 Won TO Arm bar 1:22 1st Rd \\nRobert Hitchcock 181.5 Lost 60 Day suspension –Unsportmanship  \\nBill Jones 185 Won TO GUI 2:13 1st Rd \\nJason Lampshire 231 Lost 30 day suspension  \\nChris Alverson 228 Won TKO 22 secs 1st Rd \\nJohn McNeeley 145.5 Lost   \\nJosh Eddie 147 Won UNAN \\nJeff Hughes 247 Won TKO 1:12 2nd Rd  \\nTodd Berry 229 Lost 30 day suspension – Medical release \\nDustin Bradford 156.5 Lost   \\nMike Passio 162 Won TO RNC 2:44 2nd Rd \\nTerry Smith 263 Won UNAN - 120 days suspension – needs \\nmedical release  \\nJared Carle 261 Lost 120 days suspension – needs medical \\nrelease', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/JI27U7TQTGCH2WWIVTO3GRUAQ2AGBBDH.pdf', 'page': 0}),\n",
       " Document(page_content='1 \\n  \\nTestimony of  \\n \\nBlake Gerard  \\n \\nBefore the U.S.  House of Representatives   \\nCommittee on Agriculture  \\n \\nHearing to Review the Future of U.S. Farm Policy and the Formulation of the 2012 Farm Bill  \\n \\nGalesburg, Illinois  \\nMarch 23 , 2012  \\n \\nIntroduction  \\n \\nChairman Lucas, Ranking Member Peterson , and Members of the Committee, thank you for \\nholding this hearing concerning farm policy and the 2012 farm bill.   I appreciate the opportunity \\nto offer testimony on farm policy from the perspective of a diversified grain producer.    \\n \\nMy name is Blake Gerard.   I raise rice , soybean s, wheat, and corn in  Alexander and Union \\ncounties in southern Illinois  and I have been farming on my own now for 16 years.   I am the \\nfourth generation in my family to farm this land and this is my 13th year  to farm rice in Illinois.  I \\nam also co-owner in a seed conditioning facility that does contract seed production, \\nconditioning, packaging & warehousing.  All of our soybeans are raised for seed along with \\nabout 75% of our rice. In addition to my farm and seed business, I also serve as the \\ncommissioner for the East Cape Girardeau/Clear Creek Levee & Drainage District , the  Illinois \\nCrop Improvement Association and am a member of the USA Rice Producers’ Group  Board of \\nDirectors .  \\n \\nImportance of Agriculture an d Cost -Effective Farm Policy  \\n \\nU.S. agriculture shares a certain amount of pride for what we do for the nation’s economy.   \\nAgriculture still matters.  \\n \\nOver the course of the current economic downturn, here is an excerpt of what objective \\nsources ranging fro m the Federal Reserve to The Wall Street Journal  had to say about what \\nAmerica’s farmers and ranchers have been doing to help get our nation back on track and \\npeople back to work:  \\n “In 2010, rural America was at the forefront of the economic recovery… “[R ]ising exports of \\nfarm commodities and manufactured goods spurred job growth and income gains in rural \\ncommunities…If recent history holds true, rural America could lead U.S. economic gains in \\n2011.”   Federal Reserve of Kansas City, 2010 report.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 0}),\n",
       " Document(page_content='2 \\n \"Growers\\' improved lot is rippling out to other industries.\"   The Wall Street Journal, October 12, \\n2010.  \\nWe read the same kinds of reports during the last recession when the manufacturing sector \\nwas in crisis:  \\n“Farm Belt Is Becoming a Driver for Overall Economy…The present boom is proving that \\nagriculture still matters in the U.S. Rising farm incomes are helping to ease the blow of the loss \\nof manufacturing jobs in Midwest states…‘The farm sector is a significant source of strength for \\nthe U.S. economy,’ says Sung Wo n Sohn, chief economist of Wells Fargo Bank…Although \\nfarmers themselves are a tiny part of the population, they have an outsize impact on the \\neconomy because farming is such an expensive enterprise. A full -time Midwest grain farmer \\noften owns millions of d ollars of equipment and land, and spends hundreds of thousands of \\ndollars annually on supplies.”   The Wall Street Journal, December 17, 2003 . \\nAnd, for those old enough to remember the 1980s, publications such as The Economist  recalled \\nthe impact on the res t of the economy when agriculture was not doing well:   \\n“The 1990s were so good [for Chicago] partly because the 1980s had been so bad.   ‘Everything \\nthat could possibly have gone wrong did’ says William Testa, the senior economist at the \\nFederal Reserve Ba nk of Chicago.   The region was hit by a crushing combination of high energy \\nprices, a strong dollar, high interest rates, and a farm recession.”   The Economist, May 12, 2001  \\nLast year alone, U.S. farmers and ranchers spent nearly $320 billion in communitie s across the \\ncountry to produce agriculture products valued at some $410 billion.   Put in perspective, the \\nvalue of total U.S. agriculture production was greater than the 2010 GDP of all but 25 nations, \\nand total production cost was greater than all but 28 .  And, according to the Department of \\nAgriculture, U.S. agriculture is expected to positively contribute $26.5 billion to the U.S. balance \\nof trade in fiscal year 2012 after having contributed over $40 billion just the year before.               \\nAnd, one o f the reasons we are here today, I expect, is because while U.S. agriculture is critically \\nimportant to America, farm policy is also critically important to U.S. agriculture.  \\nWithout farm policy, U.S. producers would be unilaterally exposed to global marke ts distorted \\nby withering high foreign subsidies and tariffs, and have no comprehensive safety net.  In fact, \\nDTB & Associates issued a report last fall, similar to the study on tariffs and subsidies developed \\nand maintained by Texas Tech University ( http://www.depts.ttu.edu/ceri/index.aspx .), which \\nfound that:   \\n“U.S. subsidies…have dropped to very low levels in recent years.   In the meantime, there has \\nbeen a major increase in subsidization among adva nced developing countries… Since the \\ncountries involved are major producers and consumers of agricultural products, the trade -\\ndistorting effects of the subsidies are being felt globally. However, because the run -up in \\nsubsidies is a recent development, and  because countries have not reported the new programs', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 1}),\n",
       " Document(page_content='3 \\n to the WTO or have failed in their notifications to calculate properly the level of support, the \\nchanges have attracted little attention. We believe that when trade officials examine these \\ndevelopments,  they will discover clear violations of WTO commitments.”     \\nThis aggressive increase in foreign subsides and tariffs might also explain why foreign \\ncompetitors worked to derail WTO Doha Round negotiations, causing then Chairmen and \\nRanking Members of the Senate Finance Committee and House Ways & Means Committee to \\nregister their opposition to pursuing  a lopsided agreement against the U.S. interests:  \\n“Since the WTO Doha Round was launched in 2001, we have supported the administration’s \\nefforts to achieve a  balanced outcome that would provide meaningful new market access for \\nU.S. agricultural products…particularly from developed and key emerging markets.   \\nUnfortunately, the negotiating texts currently on the table would provide little if any new \\nmarket acces s for U.S. goods, and important developing countries are demanding even further \\nconcessions from the United States.”   Ways & Means Committee Chairman and Ranking \\nMember Rangel and McCrery and Finance Committee Chairman and Ranking Member Baucus \\nand Grassle y.   \\nMoreover, while many successfully negotiated trade agreements have promised market access \\ngains for agriculture, much of what was promised has yet to materialize or is continually \\nthreatened by artificial sanitary, phytosanitary (SPS) and other non -tariff barriers.    This is why \\nprograms such as the Market Access Program and Foreign Market Development Program are of \\nvital concern to the rice industry and must be reauthorized in the 2012 farm bill.  It has not \\ngone unnoticed that budget reductions curre ntly being considered (such as the elimination of \\nthe Direct Payment) will result in a dollar for dollar loss in farm income.  Producers must be \\nprovided the tools not only to attack these obstacles to trade but to increase exports through \\nmarket promotion  and thereby increase farm income through increased open and fair trade.    \\n \\nBut, beyond even these barriers that are imposed by foreign competitors are barriers to exports \\nimposed in whole or in part by the U.S. government.   For example, rice was complete ly \\nexcluded from the free trade agreement negotiated with South Korea, foreclosing for the \\nforeseeable future any new market access for U.S. rice producers in that country.   Iraq, once a \\ntop export market for U.S. rice, has instituted restrictive specifica tions on rice imports that have  \\nled to a 77 percent drop in sales of U.S. rice to that country.  In the pending Trans Pacific \\nPartnership (TPP) negotiations, Japan has indicated an interest in joining.  The U.S. rice industry \\nsupports Japan joining the neg otiations, but only if additional market access for U.S. rice into \\nJapan is part of the agreement.  Our industry cannot support an agreement where market \\naccess for our product is categorically off the negotiating table.  Another market that has the \\npotent ial to become a top five export market almost immediately is Cuba. Unfortunately, the \\nU.S. government maintains restrictions on our agricultural exports to this country. Cuba was \\nonce the number one export market for U.S. rice prior to the embargo and we b elieve it is \\npotentially a 400,000 to 600,000 ton market if normal commercial agricultural exports are \\nallowed to resume.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 2}),\n",
       " Document(page_content='4 \\n In total, U.S. rice exports to date for the current marketing year are down 24 percent compared \\nto last year.  \\nAnd, while the rice in dustry is still a long ways off from having a crop insurance product that is \\nrelevant to rice producers, the general need for federal involvement in insuring crops where \\nlosses are highly correlated is also obvious, as even the American Enterprise Institut e has \\nadmitted:    \\n “The empirical evidence on the viability of either area -yield or multiple -peril crop insurance \\nseems clear.   When normal commercial loading factors are applied, the premiums required by \\ninsurers to offer an actuarially viable private cr op insurance contract are sufficiently high to \\nreduce the demand for such contracts to zero…Thus, private markets for multiple -peril crop \\ninsurance are almost surely infeasible, and the weight of the empirical evidence indicates that \\narea -yield contracts a re also not commercially viable…”   American Enterprise Institute, “The \\nEconomics of Crop Insurance and Disaster Aid, 1995.  \\nFortunately, for the American taxpayer, in addition to all of these justifications on why we have \\na farm policy in this country, we c an add to the list at least one more reason:   farm policy is \\ncost-effective.  \\nIn fact, U.S. farm policy has operated under budget for over a decade  and accounts for only one \\nquarter of one percent of the total federal budget.   Not including additional cuts scheduled \\nunder sequestration, U.S. farm policy has, to date, been cut by about $18 billion over the past 9 \\nyears, including in the 2004 and 2010 Standard Reinsurance Agreements (SRAs), the FY2006 \\nreconciliation package, and the 2008 Farm Bill.  \\n \\nIn the mos t recent five years, average funding for U.S. farm policy, based on real funding levels, \\nincluding crop insurance, was $12.9 billion per year, which is 28% less than the previous five -\\nyear average of $17.9 billion and 31% less than the average of $18.8 bil lion that incurred in the \\npreceding five years.   In the current year, the Congressional Budget Office (CBO) estimates that \\ncrop insurance policy will cost slightly more than the current commodity policies.  And \\naccording to CBO projections for the next 10 years the estimated annual cost for commodity \\npolicy in the farm bill is $6.6 billion on average (before the expected reductions are made as \\npart of this farm bill process), while the estimated annual cost for crop insurance policy is $8.8 \\nbillion on avera ge.  With the current suite of crop insurance policies not working effectively for \\nrice producers, this puts our industry at a further disadvantage and highlights the need to \\nmaintain an effective commodity policy in the farm bill that will work for rice.   \\n \\nFunding of that portion of farm policy that assists rice producers has declined from $1.2 billion \\na decade ago to about $400 million annually, with this amount largely reflecting Direct \\nPayments.  \\n \\nMeanwhile, U.S. consumers are paying less than 10% of di sposable income on food, less than \\nconsumers in any other nation.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 3}),\n",
       " Document(page_content='5 \\n This is why I believe so firmly that future cuts must focus on areas of the budget outside of farm \\npolicy that have not yet contributed to deficit reduction yet comprise a significant share of the \\nfederal budget.   This is also why I would urge lawmakers to reject cuts to U.S. farm policy that \\nwould exceed the level specified by the House and Senate Agriculture Committee Chairs and \\nRanking Members in their letter to the Joint Committee on Defi cit Reduction last fall.   \\n \\n2008 Farm Bill Review  \\n \\nThe Food, Conservation, and Energy Act of 2008 (the Farm Bill) continued the traditional mix of \\npolicies consisting of the non -recourse marketing loan, loan deficiency payments, and the direct \\nand counter c yclical payments.   The farm bill also included the addition of Average Crop \\nRevenue Election (ACRE) as an alternative to counter cyclical payments for producers who \\nagree to a reduction in direct payments and marketing loan benefits.   The bill also added \\nSupplemental Revenue Assurance (SURE) as a standing disaster assistance supplement to \\nfederal crop insurance.  \\n \\nThe 2008 Farm Bill made very substantial changes to the payment eligibility provisions, \\nestablishing an aggressive adjusted gross income (AGI) me ans test and, albeit unintended by \\nCongress, resulting in the very significant tightening of “actively engaged” requirements for \\neligibility. USDA was still in the process of implementing many of the provisions of the 2008 \\nFarm Bill in 2010, and the final payment eligibility rules were only announced in January of that \\nsame year, a mere two years ago.   As a consequence, we are still adjusting to the many changes \\ncontained in the current farm bill, even as Congress considers the 2012 farm bill.  \\n \\nRegarding AC RE and SURE, frankly, neither policy has proved much value to rice farmers.   \\nSpecifically, in the first year of ACRE signup, only 8 rice farms representing less than 900 acres \\nwere enrolled nationwide.   With changes, this revenue program may provide more v alue for \\nsome rice growing regions like California.  And SURE has provided little, if any, assistance to rice \\nproducers, including those producers in the Mid South who suffered significant monetary losses \\nin 2009 due to heavy rains and flooding occurring pr ior to and during harvest, or the significant \\nlosses last year as a result of spring flooding in the Mid South.  SURE’s inability to provide \\ndisaster assistance for such catastrophic events further highlights the continuing gap in \\navailable programs design ed to help producers manage or alleviate their risk.    \\n \\nRegarding the traditional mix of farm policies, the nonrecourse marketing loan, loan deficiency \\npayment, and countercyclical payments have not yet provided payments to rice farmers under \\nthe 2008 Far m Bill.   The new price paradigm has, as a practical matter, greatly limited the \\nprotections afforded to producers under these farm p olicy  features.   In fact, if the protections \\nprovided were ever to trigger for rice farmers, the protections would help stem  some of the \\neconomic losses but, frankly, not enough to keep most rice farms in business through even a \\nsingle year of severely low market prices.  \\n \\nAs such, whatever its imperfections, the Direct Payment alone has assisted rice producers in \\nmeeting the o ngoing and serious price and production perils of farming today.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 4}),\n",
       " Document(page_content='6 \\n For rice producers, as for most other producers, the existing levels of price protection have \\nsimply not kept pace with the significant increases in production costs, costs such as energy an d \\nfertilizer that are exacerbated by escalating government regulations.   It is for this reason that \\nrice farmers believe strengthening farm policies in the 2012 Farm Bill would be helpful in \\nensuring that producers have the ability to adequately manage the ir risks and access needed \\ncredit.  \\n \\nCrop Insurance  \\n \\nRisk management products offered under Federal Crop Insurance have been of very limited \\nvalue to rice producers to date due to a number of factors, including artificially depressed \\nactual production histo ry (APH) guarantees, which I understand is also a problem for many \\nother producers; high premium costs for a relatively small insurance guarantee; and the fact \\nthat the risks associated with rice production are unique from the risks of producing many \\nother  major crops.  \\n \\nFor example, since rice is a flood -irrigated crop, drought conditions rarely result in significant \\nyield losses as growers simply pump additional irrigation water to maintain moisture levels to \\nachieve relatively stable yields.   However, dro ught conditions do result in very substantial \\nproduction cost increases as a result of pumping additional water.   As such, what rice farmers \\nneed from federal crop insurance are products that will help protect against increased \\nproduction and input costs, particularly for energy and energy -related inputs.   For example, \\nfuel, fertilizer, and other energy related inputs represent about 70 percent of total variable \\ncosts.  \\n \\nIn this vein, many in the rice industry have been working for over the past four years now to \\ndevelop a new generation of crop insurance products that might provide more meaningful risk \\nmanagement tools for rice producers in protecting against sharp, upward spikes in input costs.   \\nI serve on a rice industry task force that has been working t o develop and improve crop \\ninsurance products for rice,  and a lthough the objective was to gain approval from the Risk \\nManagement Agency (RMA) of at least two new products that could be available to growers in \\ntime for the 2012 crop year, this has not mater ialized.   But, it is important to stress that even if \\nthese products had become available this year, we do not believe that they would have put rice \\nproducers anywhere near on par with other crops in terms of the relevance that crop insurance \\nhas as a risk  management tool.               \\n \\nAs such, rice producers enter the 2012 farm bill debate at a very serious disadvantage, having \\nonly a single farm policy that effectively works and that farm policy being singled out for \\nelimination.  \\n \\n2012 Farm Bill  \\n \\nWith th e foregoing as a backdrop, the U.S. rice industry developed a set of farm policy priorities \\nin September of last year to guide us during consideration of the 2012 Farm Bill.   The U.S. rice', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 5}),\n",
       " Document(page_content='7 \\n industry is unified in its firm belief that farm policy designed to  support a strong and dynamic \\nU.S. agriculture sector is absolutely vital.   We also believe that the planting flexibility provided \\nunder the 1996 Farm Bill and the countercyclical policies that have been in place for more than \\na decade now have served this  nation and its farmers well.   In particular, as we noted earlier, \\nthe 1996 Farm Bill’s Direct Payments have provided critical help to rice farmers – offering \\ncapital farmers could tailor to their unique needs.   We are very proud to stand by this farm \\npolicy.   \\n \\nHowever, given budget pressures and other considerations facing Congress that have caused \\npolicymakers to consider altering this approach in favor of more directed and conditioned \\nassistance, we developed the following priorities:  \\n\\uf0b7 First, we believe the triggering mechanism for assistance should be updated to provide \\ntailored and reliable help should commodity prices decline below today’s production \\ncosts, and should include a floor or reference price to protect in multi -year low price \\nscenarios.  \\n\\uf0b7 Seco nd, as payments would only be made in loss situations, payment limits and means \\ntests for producers should be eliminated.  \\n\\uf0b7 Third, federal crop insurance should be improved to provide more effective risk \\nmanagement for rice in all production regions, beginni ng with the policy development \\nprocess.  \\nMore specifically relative to each of these points, we believe that:  \\nPrice Protection is a Must  \\nGiven price volatility for rice is the primary risk producers face that they do not have other good \\nmeans of protecting against, with price fluctuations largely driven by global supply and \\ndemand;   given rice is one of the most protected and sensitive global commodities in trade \\nnegotiations, thus limiting access to a number of key markets; given costs of production have \\nrisen to a point where the current $6.50 (loan rate)/$10.50 (target price) assistance triggers are \\nlargely irrelevant, we believe the first priority should be to concentrate on increasing the prices \\nor revenue levels at which farm policy would trigger so tha t it is actually meaningful to \\nproducers, and would reliably trigger should prices decline sharply.   \\nThe reference price for rice should be increased to $13.98/cwt ($6.30/bu).  This level would \\nmore closely reflect the significant increases in production costs for rice.  And we believe this \\nreference price should be a component of both the price -loss policy and the revenue -loss policy \\nto ensure downside price protection.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 6}),\n",
       " Document(page_content=\"8 \\n Options for Different Production Regions  \\nIn addition, there should be true options fo r producers that recognize that a one -size-fits-all \\napproach to farm policy does not work effectively for all crops or even the same crop such as \\nrice in different production regions.    \\nIn the Mid -south and Gulf Coast production regions, a price -based loss  policy is viewed as being \\nmost effective in meeting the risk management needs of producers.  Specifically, this policy \\nshould include a price protection level that is more relevant to current cost of production; paid \\non planted acres or p ercentage of plan ted acres; paid on more current yields ; and take into \\naccount the lack of effective crop insurance policies for rice . \\nIn the California production  region , although the existing revenue -based policy still does not \\nprovide effective risk management , efforts to analyze modification s which will increase its \\neffectiveness cont inue.   Since rice yields are highly correlated between the farm, county, crop \\nreporting district, and state levels, we believe the revenue plan should be administered for rice \\nat either the  county or crop reporting district level to reflect this situation rather than lowering \\nguarantee levels to use farm level yields .  By setting loss triggers that reflect local marke ting \\nconditions, delivering support sooner, and strengthening revenue guara ntees that account for \\nhigher production costs as well as the absence of effective crop insurance , California rice \\nproducers are hopeful that an effective revenue program can  be developed.  \\nWhile I have focused on the need for a choice for rice producers in  different regions, this also \\napplies for producers of most other grains.  I support having policy options available for corn, \\nsoybeans, and wheat, which I produce, and believe that both a price -based policy and a \\nrevenue -based policy  should be offered as options for these crops.  \\nWhatever is done should be plain and bankable.   The current SURE has too many factors and is \\nnot tailored to the multiple business risks producers  face — it is not plain.  The current ACRE, \\nwhile offering improved revenue -based pro tection, is complicated by requiring two loss \\ntriggers; providing payments nearly two years after a loss; and provides no minimum price \\nprotection — it is not bankable.  The marketing loan and target prices are plain and bankable — \\nunfortunately the trigge r prices are no longer relevant to current costs and prices.  \\nWhatever is done should be tailored and defendable.   We believe it makes sense to provide \\nassistance when factors beyond the producer's control create losses for producers. We \\ngenerally think mo re tailored farm policies are more defendable.   For this reason, we like the \\nthought of updating bases and yields or applying farm policies to planted acres/current \\nproduction and their triggering based on prices or revenue, depending on the option a \\nprod ucer chooses.  However, policy choices should not result in severe regional distortions in \\ncommodity policy budget baselines from which reauthorized commodity policies must be \\ndeveloped.\", metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 7}),\n",
       " Document(page_content=\"9 \\n Whatever is done should be built to withstand a multi -year low price scenario.  Whether in a \\nrevenue -based  plan, or a price -based plan, reference prices should protect producer income in \\na relevant way in the event of a series of low price years.  Ideally, this minimum could move \\nupward over time should production costs also  increase, this being of particular concern in the \\ncurrent regulatory environment.   \\nWhatever is done should not dictate or distort planting decisions.   Direct payments are \\nexcellent in this regard.  SURE or similar whole farm aggregations tend to discourag e \\ndiversification, which could be a problem for crops like rice.  Any commodity specific farm \\npolicy that is tied to planted acres must be designed with extreme care so as to not create \\npayment scenarios that incentivize farmers to plant for a farm policy.   Whatever is done should \\naccommodate history and economics and allow for proportional reductions to the baseline \\namong commodities.  Some commodities are currently more reliant on countercyclical farm \\npolicies (ACRE/CCP) while others are receiving only Di rect Payments in the baseline.  Generally, \\nthe least disruptive and fairest way to achieve savings across commodities would be to apply a \\npercentage reduction to each commodity baseline and restructure any new policy within the \\nreduced baseline amounts.  \\nThere have been concerns raised about higher reference prices distorting planting decisions \\nand resulting in significant acreage shifts  including  for rice.  W e are unaware of any  analysis \\nthat shows significant acreage shifts resulting from the reference pri ce levels included in the \\n2011 Farm Bill package .  In fact, for rice specifically, a reference price of $13.98/cwt that is paid \\non historic CCP payment yields and on 85% of planted acres results in a reference price level \\nwell below our average cost of pro duction, so I find it hard to imagine why someone would \\nplant simply due to this policy given these levels.    \\nPay Limits/Eligibility Tests Should Be Eliminated  \\nThe likely outcome of new farm policy is that it will provide less certainty for the producer ( a \\nlikely decrease or elimination of Direct Payments).  Since i t will likely be designed to provide \\nassistance only in loss situations, the second priority is that the policy should not be limited \\nbased on arbitrary dollar limits. Assistance should be tailo red to the size of loss.  A producer \\nshould not be precluded from participating in a farm policy because of past income experience.  \\nAny internal limits on assistance should be percentage -based (i.e. 25% of an expected crop \\nvalue) and not discriminate base d on the size of farm.  \\nCrop Insurance Should Be Maintained and Improved  \\nAlthough crop insurance does not currently work as well for rice as it does for other crops, the \\nthird priority would be to improve availability and effectiveness of crop insurance fo r rice as an \\navailable option.  I would also support improvement to the product development processes (we \\nhave struggled with two 508(h) submissions for over 4 years and are still not completed with \\nthe process), and to the APH system such that any farmer' s insurable yield (pre -deductible) \\nwould be reflective of what that farmer actually expects to produce.   In no case should the crop\", metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 8}),\n",
       " Document(page_content='10 \\n insurance tools, which are purchased by the producer, be encumbered with \\nenvironmental/conservation regulation or other cond itions that fall outside the scope of \\ninsurance.   \\n2011 Budget Control Act Efforts  \\n \\nAlthough the details of the 2011 Farm Bill package that was prepared by the House and Senate \\nAgriculture Committees in response to the Budget Control Act were not disclosed,  based on \\ndiscussions and reports we believe that that package at least represents a good framework on \\nwhich to build the 2012 Farm Bill. The 2011 package included a choice of risk management \\ntools that producers can tailor to the risks on their own farms,  providing under each of those \\noptions more meaningful price protection that is actually relevant to today’s production costs \\nand prices.  It also included provisions to improve crop insurance and expedite product \\ndevelopment for underserved crops such as rice.    \\n \\nWe are concerned that effective support for rice producers under the price -based option was \\nset well below cost of production that  late changes to the revenue -based option minimized its \\npotential as an effective risk management tool for rice produ cers, and that pay limits and AGI \\nrules would still serve as an arbitrary constraint upon U.S. competitiveness, globally.   Still, even \\nwith these areas for improvement, the U.S. rice industry very much appreciates the Members \\nand staff who put enormous tim e and effort into what we believe represents a good blue print \\nfor ongoing Farm Bill deliberations and we thank you.  \\n \\nAgain, thank you for this opportunity to offer my testimony.   We certainly look forward to \\nworking with you on an effective 2012 Farm Bill  we can all be proud of.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 9}),\n",
       " Document(page_content='BLAKE GERARD  \\n \\n \\n \\nEXPERIENCE  \\n \\n2002 to present  GERARD&CRAIN FARMS, INC. dba RIVERBEND RICE SEED CO. – McClure, IL  \\n   President/Manager  \\n• Founded this Agribusiness Compa ny specializing in rice seed production.  The primary \\ngoal is to provide a superior quality seed supply to the Mid -South rice producer.   \\n• Began processing soybeans in 2005 for regional seed companies. Grew from zero units to over 450,000 units in 6 years.  \\n• Oversee 6 employees and 2  seasonal employees  \\n• Grew market from 0 -80,000 bushels of seed rice in 4 years    \\n• Manage production, storage, processing and packaging for all seed rice.  \\n• Manage storage, processing and packaging for soybean seed. Assist in distribution.  \\n• Train and monitor contract producer s, purchase seed from producers, devise \\nproduction contracts, monitor seed production, maintain genetic purity for individual \\nseed varieties, maintain seed in storage, clean seed, package seed, and market seed.  \\n• Market  rice seed directly to farmers and through 8 wholesale locations in southeast \\nMissouri.  \\n \\n \\n1996 to present   BLAKE GERARD  FARMS – McClure, IL  \\n   Owner/Operator  \\n• Operate 20 00-acre Rice, Corn, S oybean , and Wheat   farm .   \\n• Make all management decisions involved w ith the farm from planning to producing, to \\nharvest and marketing.  \\n• Utilize cash, hedge to arrive, futures and options contracts to market products.  \\n• Oversee 2 full time employees and 6 seasonal employees  \\n• Maintain computerized accounting records using AgriSolutions and Ag Manager software producing detailed five year financial forecasts\\n \\n• Produce seed beans for Delta Grow  Seed Company  & Morsoy Seed  co., Stine Seed Co., \\nArmor Seed Co., and Steyer Seed Co.  \\n \\nMEMBERSHIPS AND SERVICE  \\n 1992 to present   Commissioner  - East Cape Girardeau/Clear Creek Levee & Drainage District  \\nAugust 2004 Guest Speaker – M iddle Mississippi River Management Conference at Southern Illinois \\nUniversity on “Potential for Rice as Wildlife Habitat in the Middle Mississippi River Valley ” \\n    Member - Illinois C rop Improvement Association  \\nDecember 2008  Member – USA Rice Producers Group  Crop Insurance Task Force  \\n    \\nBoard Member – Missouri Rice Producers Group   \\n       2011   Member  - US A Rice Producers ’ Group Board of Directors  \\nMember - USA Rice Pr oducers’ Group Farm Policy Task Force   \\n   Member – USA Rice Federation Marketability and Competitiveness T ask Force \\n    EDUCATION  \\n 1999   Southeast Missouri State University – Cape Girardeau, MO  \\nB.S. Agriculture with emphasis in Agronomy', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/ALQMFHOFCDKV7HIO2VGHCM3ZDDVVZPBW.pdf', 'page': 10}),\n",
       " Document(page_content='1 CENTRE FOR HUMANITARIAN DATA', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/guidance_observationalrainfalldata.pdf', 'page': 0}),\n",
       " Document(page_content='2 CENTRE FOR HUMANITARIAN DATAAbout this series: The climate crisis is intensifying humanitarian emergencies around the world and \\nhumanitarians  are increasingly  having to incorporate  climate data and forecasts into their analysis  \\nand planning.  This guidance  series has been developed to help humanitarians  to access,  analyze and \\ninterpret  common  types of climate and weather data, and to provide a common language for working  \\nwith meteorological  and hydrological services.\\n• Observational rainfall data such as CHIRPS or ARC2 captures how much rain has fallen over a \\ndefined period of time.\\n• The rainfall is measured at weather stations or estimated through satellite or radar imagery .\\n• The data can be presented as the total amount of rainfall observed over the period of time, \\nor as below, at or above normal conditions . \\n• Datasets that combine station data and satellite or radar imagery are the recommended \\nsource of data for humanitarian purposes .\\n• Observational rainfall data can be analyzed to answer a range of questions, such as:\\n• What areas are currently affected by drought, and how severely?\\n• What was the duration of the longest dry spell during the past rainy season?\\n• How many days of heavy rainfall were there in the week leading up to the floods?\\n• Which area received the most rain last month?\\n• How often does an area experience dry spells?\\n• How does this year’s rainfall compare to previous years?Key takeaways\\nWhat is observational rainfall data?\\nObservational rainfall data captures how much rain fell over a defined period of time, \\nranging from an hour to a day. This data can then be used to calculate the total rainfall for \\ndifferent time periods, such as 5 days ( pentad ), 10 days ( dekad ), a week, a month or a season. \\nObservational rainfall datasets can go back decades and have broad geographic coverage, \\nenabling historical trend analysis and comparisons.\\nObservational rainfall data is produced by measuring  the amount of rainfall at precise \\nlocations, which are called measuring stations or gauges (e.g., the stars in Figure 1 below). \\nStation data is typically obtained from national or regional meteorological services and should \\nonly be used in accordance with their guidance. Some of the advantages of station data are \\nthat it is accurate at the specific locations, it is collected frequently and it has been collected \\nover the long-term, allowing for trend analysis. On the other hand, station data is only \\navailable for specific locations and is subject to bias due to wind, evaporation, and changes in \\nmeasurement devices. Contact the local or regional meteorological services to learn whether \\nstation data is available, where the stations are located, how often the data is updated, and \\nwhether historical records are available.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/guidance_observationalrainfalldata.pdf', 'page': 1}),\n",
       " Document(page_content='3 CENTRE FOR HUMANITARIAN DATARainfall can also be estimated  using satellite or radar images that cover areas several square \\nkilometers wide (e.g., the squares in Figure 1 below). Satellite imagery is typically obtained \\nby global providers and academic or research centers (e.g., the Climate Hazards Center) who \\nshare their derived estimates publicly. Satellite or radar images are useful for understanding \\npatterns over larger geographic areas. Some of the advantages of satellite data are that it has \\nextensive geographic coverage, including in remote and inaccessible areas, and that it is shared \\nin a continuous and consistent way. On the other hand, satellite measurements are subject to \\nerrors due to cloud contamination, and the data has a shorter historical record.\\nFigure 1. Weather Stations in Zambia (red stars). The grid superimposed onto the map consists of individual  \\nboxes, each of which corresponds to a particular location where satellite weather measurements  are \\nconducted  by certain providers.  In this case, the grid is the one used by the ARC2 dataset (see below). The \\nsize of each box is 0.1 degree (about 11 km at the equator).\\nIn order to help correct for the inaccuracies and errors that are present in the two types of data \\nand take advantage of their individual strengths, research institutions have produced rainfall \\ndatasets that combine station and satellite/radar observations. Datasets that combine these \\nobservations are the recommended source of data for humanitarian purposes.  \\nObservational precipitation data can be presented in various formats, each providing unique \\ninsights into precipitation patterns. The most common representation is through absolute \\nvalues, which reflect the actual amount of precipitation observed at a specific location and time, \\ntypically measured in millimeters. Precipitation averages, which are calculated from historical \\nrecords over a period of typically at least 10 years, can be produced for a specific location, \\nseason, or year. A precipitation anomaly represents the deviation from the expected or average \\nprecipitation over a particular period, which can be expressed as a difference in millimeters \\nor as a percentage. The purpose of the analysis determines the most useful representation. \\nAbsolute values provide insight into the total precipitation received at a location, while \\nanomalies and averages help to understand long-term trends, including dry or wet periods, \\nwhich is valuable for assessing drought or excessive rainfall conditions severity.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/guidance_observationalrainfalldata.pdf', 'page': 2}),\n",
       " Document(page_content='4 CENTRE FOR HUMANITARIAN DATA1   https://data.humdata.org/organization/wfp?q=chirps\\n2   The dataset spans the latitude range 50°S-50°N, and all longitudes.\\n3   CHIRPS produces a preliminary version of their estimates with a latency of two days. The final version is published in the third week of the \\n    following month.Common sources of observational rainfall data\\nAmong the many sources of data available, we introduce here two common sources of \\nobservational rainfall data, both of which combine station and satellite data: CHIRPS and ARC2. \\nCHIRPS refers to the Climate Hazards Group InfraRed Precipitation with Station data and is \\nproduced by scientists from the US Geological Survey and the Climate Hazards Center at the \\nUniversity of Santa Barbara.  ARC2 refers to Africa Rainfall Climatology Version 2 and it is \\nproduced by The National Oceanic and Atmospheric Administration’s Climate Prediction Center \\nfor the Famine Early Warning System. You can access the data directly from these providers, or \\nuse the tabular CHIRPS datasets that provides pre-computed rainfall metrics at admin2 and is \\navailable on HDX.1\\nCOMPARISON OF CHIRPS AND ARC2 OBSERV ATIONAL RAINFALL DATA\\nSource CHIRPS ARC2\\nFormat Raster Raster\\nGeographical coverage Global (quasi2) Africa\\nSpatial granularity \\n(also referred to as spatial \\nresolution)0.05 degree (5.5 km at the equator) 0.1 degree (11 km at the equator)\\nTemporal granularity\\n(aso referred to as temporal \\nresolution)Daily rainfall3Daily rainfall\\nHistorical coverage 1981 - today 1983 - today\\nPublication Every month, around the third week of \\nthe following monthEvery day, with a 2-day lag\\nMost common uses • Monthly, seasonal, or yearly rainfall \\nmonitoring\\n• Included in other tools including  \\nthose used for drought monitoring and \\nFEWS NET assessments\\n• Trend analysis across years• Near real-time monitoring in Africa \\n• Year-to-year comparisons in Africa\\nQuestions to answer with observational rainfall data\\nObservational rainfall data can be used to answer a number of questions and inform decision \\nmaking in humanitarian operations. Examples include: \\n• How much rain fell in the past x days/weeks/months?  Observational rainfall datasets \\nprovide data on the total amount of rainfall for an area over a discrete period of time, which \\ncan be aggregated and used to calculate rainfall over different time intervals.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/guidance_observationalrainfalldata.pdf', 'page': 3}),\n",
       " Document(page_content='5 CENTRE FOR HUMANITARIAN DATA• Was the rainfall last month above or below normal?  And by how much?  Observational \\nrainfall data goes back decades and can therefore be used to determine whether rainfall for \\na month or a season is above, at or below the historical average. \\n• Were there more rainy days this month than last month? Observational rainfall data is \\navailable for each day and can be used to determine if it rained or not on a given day and to \\ncount the number of dry or wet days during a specific period of time. \\n• How many dry spells were there during the past rainy season? How long was the \\nlongest dry spell and the heaviest rain streak this past rainy season?  Observational \\nrainfall data can be used to detect the occurrence, location and duration of dry spells and \\nprolonged periods of heavy rain.  \\n• How often in the past decade were more than x millimeters of rainfall measured?  \\nObservational rainfall data provides daily rainfall totals in millimeters and can be used to \\nassess how often an area received rain above or below a given threshold.\\n• Which region received the most rain last month?  Observational rainfall data can be \\naggregated to provide rainfall totals for larger geographic areas and then used to compare \\nrain for that area. \\nObservational rainfall data cannot answer the following questions: \\n• How much will it rain during a period of time in the future?  Observational rainfall data is \\nreferred to as observational because it describes what happened in the past. It does not tell \\nyou anything about future rainfall. \\n• Is a severe flooding event about to occur? Are we going through a severe drought?  \\nObservational rainfall data does not provide sufficient information to inform you about the \\nrisk of flooding in the near future or how severe a drought is likely to be. This is because \\nfloods and droughts are complex phenomena caused by several interacting factors and they \\ncannot be explained exclusively by the amount of precipitation.\\n• How much rainfall from a given period of time in the past was due to specific tropical \\nstorm or typhoon?  It is hard to differentiate between the amount of rain caused by \\naverage atmospheric conditions and the amount of rain induced by a specific atmospheric \\nphenomenon. As a result, using observational rainfall data alone, it is not possible to say how \\nmuch rainfall is due strictly to a tropical storm or typhoon. More data is needed to infer the \\norigin of a heavy rainfall event.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/guidance_observationalrainfalldata.pdf', 'page': 4}),\n",
       " Document(page_content='6 CENTRE FOR HUMANITARIAN DATA4   Learn more: https://www.chc.ucsb.edu/data/chirps\\n5   Learn more here: https://wiki.chc.ucsb.edu/CHIRPS_FAQ#What_is_the_spatial_density_of_station_data_used_in_CHIRPS.3FCommon limitations with observations rainfall data\\nOnce you have identified how you want to use the data, it is important to bear in mind some \\ncommon limitations when conducting your analysis. \\n1. Low-amount or short-duration rainfall may be underreported in certain geographical \\nterrains or during extreme precipitation events.4 Gauges are constantly being improved \\nto reduce error and datasets that combine station and satellite/radar data are best at \\ncorrecting for these limitations. However, it is still important to keep these limitations in \\nmind, especially when doing analysis that is looking at meteorological events that involve \\nlow-amount or short-duration rainfall, such as dry spells. \\n2. There are fewer gauge stations in rural areas and certain developing countries, which \\nmeans that observational rainfall data is often less accurate in these locations. Fewer gauge \\nstations means that there are fewer measurements for that geographic area, resulting in \\na less complete picture of what is happening on the ground. Differences in the amount of \\nrainfall received in two locations might be missed if there are not stations to capture rainfall \\nin both locations.5 \\n3. Observational rainfall data is usually provided as a raster dataset, where the geographic \\narea is represented by a contiguous grid of cells, or pixels. Each cell represents a discrete \\narea of land, and has a value associated with it but the cells do not neatly fit within \\nadministrative boundaries. Calculating rainfall for an administrative area requires an \\naggregation. The choice of the aggregation method (average, maximum, total etc.) as well \\nas the choice of which cells to include within a given administrative division (all cells fully \\nincluded, or all cells touched etc.) will impact the results. This is especially true for lower \\nadministrative divisions. As a result, adding together the total rainfall for all the districts in a \\nregion may not equal the total rainfall calculated for the region.\\n4. If using station data, it is important to bear in mind that stations are easily subject to \\ndeterioration and require frequent maintenance, which may affect their accuracy. For \\nthis reason, calculating temporal averages, anomalies or trends over long periods of time \\nshould be performed with care. When possible, verify with the provider the adequacy of \\nthe measurements over long periods of time. This limitation does not apply to CHIRPS and \\nARC2 datasets, which have been calibrated and combined with satellite observations by \\nclimate institutions.\\n5. For station data, differences exist between measuring instruments and quality of their \\nmeasurements. This may heavily affect comparisons between different locations. Therefore, \\nwhen performing geographical averages, it is preferable to use radar data, satellite data or \\nobservational datasets already calibrated by institutions or universities such as CHIRPS or \\nARC2.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/guidance_observationalrainfalldata.pdf', 'page': 5}),\n",
       " Document(page_content='7 CENTRE FOR HUMANITARIAN DATAResources\\n• Learn more about the CHIRPS6 and ARC27 observational datasets and/or download data \\ndirectly from the providers.\\n• CHIRPS on HDX8: Download pre-computed tabular data (CSV format) on precipitation totals, \\naverages, anomalies per administrative region.\\n• ARC29: Download 1-month or 10-day cumulative precipitation totals (raster, tabular, image \\ndata).\\n• OCHA AnticiPy10: Download raster (NetCDF) data directly from CHIRPS and ARC2 using a \\nPython library.\\n6   https://www.chc.ucsb.edu/data/chirps\\n7   https://www .icpac.net/data-center/arc2/\\n8   https://data.humdata.org/dataset?q=CHIRPS\\n9    https://gmes.icpac.net/data-center/arc2-rfe\\n10 https://github.com/OCHA-DAP/ocha-anticipy\\n11   https://centre.humdata.org/\\n12  https://www.climate.columbia.edu/\\n13  https://iri.columbia.edu/\\n14  https://www.knmi.nl/over-het-knmi/about\\nThis guidance was developed by the OCHA Centre for Humanitarian Data11 with input from \\nColumbia  University Climate School12 International Research Institute for Climate and Society13, \\nthe Royal Netherlands  Meteorological Institute14, and the Earth Observation Unit of the Research, \\nAssessment and Monitoring division of the World Food Programme.', metadata={'source': '/Users/isaac/FundamentlPartners/abinvenv-sol/data/raw/guidance_observationalrainfalldata.pdf', 'page': 6})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadPDFs(f\"{BASE_DIR}/data/raw/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
